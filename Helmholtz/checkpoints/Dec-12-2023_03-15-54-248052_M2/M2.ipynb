{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "######################################################################################################\n",
    "def u(x, a_1, a_2):\n",
    "    return np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_xx(x, a_1, a_2):\n",
    "    return - (a_1 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_yy(x, a_1, a_2):\n",
    "    return - (a_2 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "# Forcing\n",
    "def f(x, a_1, a_2, lam):\n",
    "    return u_xx(x, a_1, a_2) + u_yy(x, a_1, a_2) + lam * u(x, a_1, a_2)\n",
    "\n",
    "def operator(u, x1, x2, lam, sigma_x1=1.0, sigma_x2=1.0):\n",
    "    u_x1 = tf.gradients(u, x1)[0] / sigma_x1\n",
    "    u_x2 = tf.gradients(u, x2)[0] / sigma_x2\n",
    "    u_xx1 = tf.gradients(u_x1, x1)[0] / sigma_x1\n",
    "    u_xx2 = tf.gradients(u_x2, x2)[0] / sigma_x2\n",
    "    residual = u_xx1 + u_xx2 + lam * u\n",
    "    return residual\n",
    "#######################################################################################################\n",
    "\n",
    "class Helmholtz2D:\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess):\n",
    "        # Normalization constants\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x1, self.sigma_x1 = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x2, self.sigma_x2 = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Helmoholtz constant\n",
    "        self.lam = tf.constant(lam, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = mode\n",
    "\n",
    "        # Record stiff ratio\n",
    "        # self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # if mode in ['M3', 'M4']:\n",
    "        #     # Initialize encoder weights and biases\n",
    "        #     self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        #     self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x1_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # Define placeholder for adaptive constant\n",
    "        self.adaptive_constant_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred = self.net_u(self.x1_bc1_tf, self.x2_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.x1_bc2_tf, self.x2_bc2_tf)\n",
    "        self.u_bc3_pred = self.net_u(self.x1_bc3_tf, self.x2_bc3_tf)\n",
    "        self.u_bc4_pred = self.net_u(self.x1_bc4_tf, self.x2_bc4_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x1_u_tf, self.x2_u_tf)\n",
    "        self.r_pred = self.net_r(self.x1_r_tf, self.x2_r_tf)\n",
    "\n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_tf - self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_tf - self.u_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.u_bc3_tf - self.u_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.u_bc4_tf - self.u_bc4_pred))\n",
    "        self.loss_bcs = self.adaptive_constant_tf * (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        # Compute and store the adaptive constant\n",
    "        self.adpative_constant_log = []\n",
    "        \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.adaptive_constant = self.max_grad_res / self.mean_grad_bcs\n",
    "\n",
    "        # # Stiff Ratio\n",
    "        # if self.stiff_ratio:\n",
    "        #     self.Hessian, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "        #     self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "        #     self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "        #     self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "        #     self.eigenvalue_log = []\n",
    "        #     self.eigenvalue_bcs_log = []\n",
    "        #     self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "     # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,  self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P), dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),  dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),  dtype='float32')\n",
    "\n",
    "        return H, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self,size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, x1, x2):\n",
    "        u = self.forward_pass(tf.concat([x1, x2], 1))\n",
    "        return u\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x1, x2):\n",
    "        u = self.net_u(x1, x2)\n",
    "        residual = self.operator(u, x1, x2,\n",
    "                                 self.lam,\n",
    "                                 self.sigma_x1,\n",
    "                                 self.sigma_x2)\n",
    "        return residual\n",
    "\n",
    "    # Feed minibatch\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter=10000, batch_size=128):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            if self.stiff_ratio:\n",
    "                if it % 1000 == 0:\n",
    "                    print(\"Eigenvalues information stored ...\")\n",
    "                    eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "                                                                                   self.eigenvalues_bcs,\n",
    "                                                                                   self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # Log eigenvalues\n",
    "                    self.eigenvalue_log.append(eigenvalues)\n",
    "                    self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                if self.model in ['M2', 'M4']:\n",
    "                    adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                    self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta) \\\n",
    "                                                 + self.beta * self.adaptive_constant_val\n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' %\n",
    "                      (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # Store gradients\n",
    "            if it % 10000 == 0:\n",
    "                self.save_gradients(tf_dict)\n",
    "                print(\"Gradients information stored ...\")\n",
    "\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter , bcbatch_size , fbatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Fetch boundary mini-batches\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], bcbatch_size)\n",
    "        X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], bcbatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, fbatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.u_bc3_tf: u_bc3_batch,\n",
    "                    self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.u_bc4_tf: u_bc4_batch,\n",
    "                    self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                    self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                    }\n",
    "\n",
    "\n",
    "        for it in range(nIter):\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                # self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                #     self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "\n",
    "                # self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "  # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value ,  loss_bcs_value, loss_res_value = self.sess.run([self.loss, self.loss_bcs, self.loss_res] , tf_dict)\n",
    "\n",
    " \n",
    "                self.print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, elapsed))\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "                self.print('adaptive_constant_val: %f' % (self.adaptive_constant_val))\n",
    "\n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_u_tf: X_star[:, 0:1], self.x2_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_r_tf: X_star[:, 0:1], self.x2_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n",
    "\n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $f(x)$' , 'Predicted $f(x)$' , 'Absolute error']\n",
    "        data = [U_star , U_pred ,  np.abs(U_star - U_pred) , F_star , F_pred ,  np.abs(F_star - F_pred) ]\n",
    "        \n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        kwargs_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60),\n",
    "                cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, kwargs, minmax, title in zip(grid, data, kwargs_list, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "                #  print( z[timeStp,:,:])\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, lam ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f ,  model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18417/2072779253.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18417/2072779253.py:83: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18417/2072779253.py:84: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18417/2072779253.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18417/3640563607.py:280: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18417/3640563607.py:119: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 03:10:37.616874: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 03:10:37.638704: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-12 03:10:37.639186: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561c793d9ee0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-12 03:10:37.639200: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-12 03:10:37.640264: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_18417/3640563607.py:171: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18417/3640563607.py:173: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18417/3640563607.py:223: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "It: 0, Loss: 6.135e+03, Loss_bcs: 2.758e-01, Loss_res: 6.134e+03,Time: 1.51\n",
      "adaptive_constant_val: 5.061229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 10, Loss: 6.640e+03, Loss_bcs: 1.263e-01, Loss_res: 6.640e+03,Time: 0.01\n",
      "adaptive_constant_val: 12.997747\n",
      "It: 20, Loss: 7.504e+03, Loss_bcs: 2.155e-01, Loss_res: 7.504e+03,Time: 0.01\n",
      "adaptive_constant_val: 14.422331\n",
      "It: 30, Loss: 6.982e+03, Loss_bcs: 2.244e-01, Loss_res: 6.982e+03,Time: 0.01\n",
      "adaptive_constant_val: 19.924026\n",
      "It: 40, Loss: 7.041e+03, Loss_bcs: 3.870e-01, Loss_res: 7.041e+03,Time: 0.01\n",
      "adaptive_constant_val: 18.378888\n",
      "It: 50, Loss: 7.357e+03, Loss_bcs: 2.504e-01, Loss_res: 7.357e+03,Time: 0.01\n",
      "adaptive_constant_val: 18.252252\n",
      "It: 60, Loss: 6.666e+03, Loss_bcs: 3.111e-01, Loss_res: 6.666e+03,Time: 0.01\n",
      "adaptive_constant_val: 18.576786\n",
      "It: 70, Loss: 6.383e+03, Loss_bcs: 2.438e-01, Loss_res: 6.383e+03,Time: 0.01\n",
      "adaptive_constant_val: 18.848311\n",
      "It: 80, Loss: 5.963e+03, Loss_bcs: 4.787e-01, Loss_res: 5.963e+03,Time: 0.01\n",
      "adaptive_constant_val: 18.756801\n",
      "It: 90, Loss: 6.523e+03, Loss_bcs: 3.146e-01, Loss_res: 6.522e+03,Time: 0.01\n",
      "adaptive_constant_val: 21.759211\n",
      "It: 100, Loss: 6.421e+03, Loss_bcs: 1.059e+00, Loss_res: 6.420e+03,Time: 0.01\n",
      "adaptive_constant_val: 21.932181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 110, Loss: 6.081e+03, Loss_bcs: 1.463e+00, Loss_res: 6.080e+03,Time: 0.01\n",
      "adaptive_constant_val: 22.023885\n",
      "It: 120, Loss: 6.719e+03, Loss_bcs: 2.099e+00, Loss_res: 6.717e+03,Time: 0.01\n",
      "adaptive_constant_val: 26.412887\n",
      "It: 130, Loss: 6.878e+03, Loss_bcs: 3.534e+00, Loss_res: 6.875e+03,Time: 0.01\n",
      "adaptive_constant_val: 26.018174\n",
      "It: 140, Loss: 8.036e+03, Loss_bcs: 5.254e+00, Loss_res: 8.031e+03,Time: 0.01\n",
      "adaptive_constant_val: 36.765090\n",
      "It: 150, Loss: 5.215e+03, Loss_bcs: 8.575e+00, Loss_res: 5.206e+03,Time: 0.01\n",
      "adaptive_constant_val: 37.109753\n",
      "It: 160, Loss: 6.224e+03, Loss_bcs: 8.007e+00, Loss_res: 6.216e+03,Time: 0.01\n",
      "adaptive_constant_val: 37.421227\n",
      "It: 170, Loss: 7.100e+03, Loss_bcs: 1.027e+01, Loss_res: 7.090e+03,Time: 0.01\n",
      "adaptive_constant_val: 39.906237\n",
      "It: 180, Loss: 7.327e+03, Loss_bcs: 1.865e+01, Loss_res: 7.308e+03,Time: 0.01\n",
      "adaptive_constant_val: 38.224676\n",
      "It: 190, Loss: 7.087e+03, Loss_bcs: 1.586e+01, Loss_res: 7.071e+03,Time: 0.01\n",
      "adaptive_constant_val: 36.967893\n",
      "It: 200, Loss: 7.714e+03, Loss_bcs: 1.455e+01, Loss_res: 7.700e+03,Time: 0.01\n",
      "adaptive_constant_val: 36.146812\n",
      "It: 210, Loss: 7.774e+03, Loss_bcs: 1.333e+01, Loss_res: 7.760e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.913484\n",
      "It: 220, Loss: 6.386e+03, Loss_bcs: 8.470e+00, Loss_res: 6.377e+03,Time: 0.01\n",
      "adaptive_constant_val: 35.511682\n",
      "It: 230, Loss: 6.664e+03, Loss_bcs: 1.158e+01, Loss_res: 6.653e+03,Time: 0.02\n",
      "adaptive_constant_val: 35.604684\n",
      "It: 240, Loss: 7.049e+03, Loss_bcs: 1.318e+01, Loss_res: 7.036e+03,Time: 0.01\n",
      "adaptive_constant_val: 36.267667\n",
      "It: 250, Loss: 7.154e+03, Loss_bcs: 1.527e+01, Loss_res: 7.139e+03,Time: 0.01\n",
      "adaptive_constant_val: 37.330411\n",
      "It: 260, Loss: 7.390e+03, Loss_bcs: 1.686e+01, Loss_res: 7.373e+03,Time: 0.01\n",
      "adaptive_constant_val: 35.432963\n",
      "It: 270, Loss: 7.551e+03, Loss_bcs: 1.081e+01, Loss_res: 7.540e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.108144\n",
      "It: 280, Loss: 7.735e+03, Loss_bcs: 9.589e+00, Loss_res: 7.726e+03,Time: 0.01\n",
      "adaptive_constant_val: 31.973677\n",
      "It: 290, Loss: 6.220e+03, Loss_bcs: 7.990e+00, Loss_res: 6.212e+03,Time: 0.01\n",
      "adaptive_constant_val: 32.594851\n",
      "It: 300, Loss: 5.520e+03, Loss_bcs: 1.128e+01, Loss_res: 5.508e+03,Time: 0.02\n",
      "adaptive_constant_val: 31.188593\n",
      "It: 310, Loss: 6.366e+03, Loss_bcs: 1.372e+01, Loss_res: 6.352e+03,Time: 0.01\n",
      "adaptive_constant_val: 29.928606\n",
      "It: 320, Loss: 7.127e+03, Loss_bcs: 9.406e+00, Loss_res: 7.118e+03,Time: 0.01\n",
      "adaptive_constant_val: 29.411590\n",
      "It: 330, Loss: 5.707e+03, Loss_bcs: 9.450e+00, Loss_res: 5.697e+03,Time: 0.01\n",
      "adaptive_constant_val: 31.723975\n",
      "It: 340, Loss: 6.649e+03, Loss_bcs: 1.539e+01, Loss_res: 6.633e+03,Time: 0.01\n",
      "adaptive_constant_val: 32.958417\n",
      "It: 350, Loss: 7.076e+03, Loss_bcs: 1.630e+01, Loss_res: 7.059e+03,Time: 0.01\n",
      "adaptive_constant_val: 32.124468\n",
      "It: 360, Loss: 6.933e+03, Loss_bcs: 1.844e+01, Loss_res: 6.915e+03,Time: 0.01\n",
      "adaptive_constant_val: 31.146826\n",
      "It: 370, Loss: 6.339e+03, Loss_bcs: 1.441e+01, Loss_res: 6.324e+03,Time: 0.01\n",
      "adaptive_constant_val: 29.461150\n",
      "It: 380, Loss: 6.900e+03, Loss_bcs: 1.050e+01, Loss_res: 6.890e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.021152\n",
      "It: 390, Loss: 6.657e+03, Loss_bcs: 1.485e+01, Loss_res: 6.642e+03,Time: 0.01\n",
      "adaptive_constant_val: 32.077229\n",
      "It: 400, Loss: 6.616e+03, Loss_bcs: 1.420e+01, Loss_res: 6.601e+03,Time: 0.01\n",
      "adaptive_constant_val: 33.207901\n",
      "It: 410, Loss: 6.626e+03, Loss_bcs: 1.766e+01, Loss_res: 6.609e+03,Time: 0.01\n",
      "adaptive_constant_val: 33.380131\n",
      "It: 420, Loss: 6.417e+03, Loss_bcs: 2.372e+01, Loss_res: 6.393e+03,Time: 0.01\n",
      "adaptive_constant_val: 35.655015\n",
      "It: 430, Loss: 6.285e+03, Loss_bcs: 2.928e+01, Loss_res: 6.255e+03,Time: 0.02\n",
      "adaptive_constant_val: 36.977196\n",
      "It: 440, Loss: 5.756e+03, Loss_bcs: 2.703e+01, Loss_res: 5.729e+03,Time: 0.01\n",
      "adaptive_constant_val: 35.598562\n",
      "It: 450, Loss: 6.902e+03, Loss_bcs: 2.379e+01, Loss_res: 6.878e+03,Time: 0.01\n",
      "adaptive_constant_val: 35.184032\n",
      "It: 460, Loss: 6.696e+03, Loss_bcs: 2.646e+01, Loss_res: 6.669e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.943880\n",
      "It: 470, Loss: 6.801e+03, Loss_bcs: 2.297e+01, Loss_res: 6.778e+03,Time: 0.01\n",
      "adaptive_constant_val: 38.240135\n",
      "It: 480, Loss: 6.712e+03, Loss_bcs: 4.107e+01, Loss_res: 6.670e+03,Time: 0.01\n",
      "adaptive_constant_val: 40.598219\n",
      "It: 490, Loss: 6.945e+03, Loss_bcs: 3.844e+01, Loss_res: 6.906e+03,Time: 0.01\n",
      "adaptive_constant_val: 56.135899\n",
      "It: 500, Loss: 7.095e+03, Loss_bcs: 4.575e+01, Loss_res: 7.049e+03,Time: 0.01\n",
      "adaptive_constant_val: 65.511184\n",
      "It: 510, Loss: 5.598e+03, Loss_bcs: 4.882e+01, Loss_res: 5.549e+03,Time: 0.01\n",
      "adaptive_constant_val: 75.845059\n",
      "It: 520, Loss: 6.035e+03, Loss_bcs: 5.778e+01, Loss_res: 5.977e+03,Time: 0.01\n",
      "adaptive_constant_val: 77.066087\n",
      "It: 530, Loss: 6.195e+03, Loss_bcs: 7.544e+01, Loss_res: 6.120e+03,Time: 0.01\n",
      "adaptive_constant_val: 76.449979\n",
      "It: 540, Loss: 3.928e+03, Loss_bcs: 5.535e+01, Loss_res: 3.873e+03,Time: 0.01\n",
      "adaptive_constant_val: 85.469129\n",
      "It: 550, Loss: 5.270e+03, Loss_bcs: 5.293e+01, Loss_res: 5.217e+03,Time: 0.01\n",
      "adaptive_constant_val: 83.447695\n",
      "It: 560, Loss: 3.958e+03, Loss_bcs: 4.799e+01, Loss_res: 3.910e+03,Time: 0.01\n",
      "adaptive_constant_val: 85.861763\n",
      "It: 570, Loss: 5.961e+03, Loss_bcs: 5.797e+01, Loss_res: 5.903e+03,Time: 0.01\n",
      "adaptive_constant_val: 80.349120\n",
      "It: 580, Loss: 4.512e+03, Loss_bcs: 4.188e+01, Loss_res: 4.470e+03,Time: 0.01\n",
      "adaptive_constant_val: 78.949105\n",
      "It: 590, Loss: 5.310e+03, Loss_bcs: 4.334e+01, Loss_res: 5.267e+03,Time: 0.01\n",
      "adaptive_constant_val: 88.768021\n",
      "It: 600, Loss: 6.335e+03, Loss_bcs: 4.358e+01, Loss_res: 6.292e+03,Time: 0.01\n",
      "adaptive_constant_val: 89.866857\n",
      "It: 610, Loss: 4.892e+03, Loss_bcs: 2.956e+01, Loss_res: 4.863e+03,Time: 0.01\n",
      "adaptive_constant_val: 90.242962\n",
      "It: 620, Loss: 3.987e+03, Loss_bcs: 2.484e+01, Loss_res: 3.962e+03,Time: 0.01\n",
      "adaptive_constant_val: 95.958102\n",
      "It: 630, Loss: 4.578e+03, Loss_bcs: 2.833e+01, Loss_res: 4.549e+03,Time: 0.01\n",
      "adaptive_constant_val: 91.174387\n",
      "It: 640, Loss: 4.402e+03, Loss_bcs: 2.839e+01, Loss_res: 4.373e+03,Time: 0.01\n",
      "adaptive_constant_val: 91.556312\n",
      "It: 650, Loss: 3.716e+03, Loss_bcs: 3.868e+01, Loss_res: 3.678e+03,Time: 0.01\n",
      "adaptive_constant_val: 88.534774\n",
      "It: 660, Loss: 4.630e+03, Loss_bcs: 2.986e+01, Loss_res: 4.600e+03,Time: 0.01\n",
      "adaptive_constant_val: 86.982535\n",
      "It: 670, Loss: 3.815e+03, Loss_bcs: 2.431e+01, Loss_res: 3.791e+03,Time: 0.01\n",
      "adaptive_constant_val: 105.409431\n",
      "It: 680, Loss: 3.339e+03, Loss_bcs: 3.352e+01, Loss_res: 3.306e+03,Time: 0.01\n",
      "adaptive_constant_val: 104.658214\n",
      "It: 690, Loss: 3.851e+03, Loss_bcs: 2.464e+01, Loss_res: 3.827e+03,Time: 0.01\n",
      "adaptive_constant_val: 103.093022\n",
      "It: 700, Loss: 3.226e+03, Loss_bcs: 2.198e+01, Loss_res: 3.204e+03,Time: 0.01\n",
      "adaptive_constant_val: 108.953424\n",
      "It: 710, Loss: 4.491e+03, Loss_bcs: 1.956e+01, Loss_res: 4.472e+03,Time: 0.01\n",
      "adaptive_constant_val: 107.964517\n",
      "It: 720, Loss: 3.549e+03, Loss_bcs: 2.247e+01, Loss_res: 3.527e+03,Time: 0.01\n",
      "adaptive_constant_val: 113.960666\n",
      "It: 730, Loss: 3.044e+03, Loss_bcs: 2.694e+01, Loss_res: 3.017e+03,Time: 0.01\n",
      "adaptive_constant_val: 111.169503\n",
      "It: 740, Loss: 3.038e+03, Loss_bcs: 2.798e+01, Loss_res: 3.010e+03,Time: 0.01\n",
      "adaptive_constant_val: 102.794354\n",
      "It: 750, Loss: 3.770e+03, Loss_bcs: 2.830e+01, Loss_res: 3.742e+03,Time: 0.01\n",
      "adaptive_constant_val: 98.769165\n",
      "It: 760, Loss: 2.199e+03, Loss_bcs: 2.686e+01, Loss_res: 2.172e+03,Time: 0.01\n",
      "adaptive_constant_val: 94.873101\n",
      "It: 770, Loss: 3.572e+03, Loss_bcs: 3.126e+01, Loss_res: 3.541e+03,Time: 0.01\n",
      "adaptive_constant_val: 89.764402\n",
      "It: 780, Loss: 1.869e+03, Loss_bcs: 4.532e+01, Loss_res: 1.824e+03,Time: 0.01\n",
      "adaptive_constant_val: 86.962357\n",
      "It: 790, Loss: 1.967e+03, Loss_bcs: 3.497e+01, Loss_res: 1.932e+03,Time: 0.01\n",
      "adaptive_constant_val: 82.321170\n",
      "It: 800, Loss: 2.568e+03, Loss_bcs: 3.428e+01, Loss_res: 2.534e+03,Time: 0.01\n",
      "adaptive_constant_val: 79.454287\n",
      "It: 810, Loss: 2.782e+03, Loss_bcs: 3.891e+01, Loss_res: 2.744e+03,Time: 0.01\n",
      "adaptive_constant_val: 80.062416\n",
      "It: 820, Loss: 1.880e+03, Loss_bcs: 4.048e+01, Loss_res: 1.839e+03,Time: 0.01\n",
      "adaptive_constant_val: 76.552513\n",
      "It: 830, Loss: 1.967e+03, Loss_bcs: 3.557e+01, Loss_res: 1.931e+03,Time: 0.00\n",
      "adaptive_constant_val: 77.552322\n",
      "It: 840, Loss: 2.250e+03, Loss_bcs: 3.730e+01, Loss_res: 2.212e+03,Time: 0.01\n",
      "adaptive_constant_val: 83.650122\n",
      "It: 850, Loss: 1.673e+03, Loss_bcs: 4.488e+01, Loss_res: 1.628e+03,Time: 0.01\n",
      "adaptive_constant_val: 80.274104\n",
      "It: 860, Loss: 2.665e+03, Loss_bcs: 3.483e+01, Loss_res: 2.630e+03,Time: 0.01\n",
      "adaptive_constant_val: 77.564566\n",
      "It: 870, Loss: 2.696e+03, Loss_bcs: 3.751e+01, Loss_res: 2.659e+03,Time: 0.01\n",
      "adaptive_constant_val: 73.793201\n",
      "It: 880, Loss: 2.090e+03, Loss_bcs: 2.688e+01, Loss_res: 2.063e+03,Time: 0.01\n",
      "adaptive_constant_val: 74.302141\n",
      "It: 890, Loss: 1.971e+03, Loss_bcs: 3.539e+01, Loss_res: 1.936e+03,Time: 0.00\n",
      "adaptive_constant_val: 75.378678\n",
      "It: 900, Loss: 1.621e+03, Loss_bcs: 2.994e+01, Loss_res: 1.591e+03,Time: 0.01\n",
      "adaptive_constant_val: 71.697571\n",
      "It: 910, Loss: 1.629e+03, Loss_bcs: 2.735e+01, Loss_res: 1.602e+03,Time: 0.01\n",
      "adaptive_constant_val: 70.828504\n",
      "It: 920, Loss: 2.269e+03, Loss_bcs: 2.459e+01, Loss_res: 2.244e+03,Time: 0.01\n",
      "adaptive_constant_val: 72.395538\n",
      "It: 930, Loss: 1.408e+03, Loss_bcs: 2.227e+01, Loss_res: 1.386e+03,Time: 0.01\n",
      "adaptive_constant_val: 77.208502\n",
      "It: 940, Loss: 1.313e+03, Loss_bcs: 2.480e+01, Loss_res: 1.288e+03,Time: 0.01\n",
      "adaptive_constant_val: 76.808415\n",
      "It: 950, Loss: 1.072e+03, Loss_bcs: 2.436e+01, Loss_res: 1.047e+03,Time: 0.01\n",
      "adaptive_constant_val: 76.704230\n",
      "It: 960, Loss: 1.335e+03, Loss_bcs: 2.364e+01, Loss_res: 1.312e+03,Time: 0.00\n",
      "adaptive_constant_val: 75.642854\n",
      "It: 970, Loss: 1.407e+03, Loss_bcs: 3.029e+01, Loss_res: 1.377e+03,Time: 0.01\n",
      "adaptive_constant_val: 73.124574\n",
      "It: 980, Loss: 2.068e+03, Loss_bcs: 2.294e+01, Loss_res: 2.045e+03,Time: 0.01\n",
      "adaptive_constant_val: 74.634020\n",
      "It: 990, Loss: 1.637e+03, Loss_bcs: 2.613e+01, Loss_res: 1.611e+03,Time: 0.01\n",
      "adaptive_constant_val: 77.496255\n",
      "It: 1000, Loss: 1.479e+03, Loss_bcs: 2.957e+01, Loss_res: 1.449e+03,Time: 0.01\n",
      "adaptive_constant_val: 77.224284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 1010, Loss: 6.724e+02, Loss_bcs: 2.785e+01, Loss_res: 6.446e+02,Time: 0.01\n",
      "adaptive_constant_val: 79.439139\n",
      "It: 1020, Loss: 9.441e+02, Loss_bcs: 3.484e+01, Loss_res: 9.093e+02,Time: 0.01\n",
      "adaptive_constant_val: 77.977588\n",
      "It: 1030, Loss: 1.045e+03, Loss_bcs: 2.682e+01, Loss_res: 1.018e+03,Time: 0.01\n",
      "adaptive_constant_val: 74.143838\n",
      "It: 1040, Loss: 1.131e+03, Loss_bcs: 2.918e+01, Loss_res: 1.102e+03,Time: 0.01\n",
      "adaptive_constant_val: 73.032564\n",
      "It: 1050, Loss: 1.128e+03, Loss_bcs: 3.558e+01, Loss_res: 1.092e+03,Time: 0.01\n",
      "adaptive_constant_val: 71.230304\n",
      "It: 1060, Loss: 9.493e+02, Loss_bcs: 3.924e+01, Loss_res: 9.101e+02,Time: 0.01\n",
      "adaptive_constant_val: 71.885775\n",
      "It: 1070, Loss: 1.235e+03, Loss_bcs: 4.070e+01, Loss_res: 1.194e+03,Time: 0.01\n",
      "adaptive_constant_val: 67.999576\n",
      "It: 1080, Loss: 6.836e+02, Loss_bcs: 4.018e+01, Loss_res: 6.434e+02,Time: 0.01\n",
      "adaptive_constant_val: 65.914090\n",
      "It: 1090, Loss: 1.125e+03, Loss_bcs: 4.277e+01, Loss_res: 1.083e+03,Time: 0.01\n",
      "adaptive_constant_val: 65.428380\n",
      "It: 1100, Loss: 7.399e+02, Loss_bcs: 5.414e+01, Loss_res: 6.857e+02,Time: 0.01\n",
      "adaptive_constant_val: 67.958244\n",
      "It: 1110, Loss: 1.055e+03, Loss_bcs: 4.136e+01, Loss_res: 1.014e+03,Time: 0.01\n",
      "adaptive_constant_val: 66.112325\n",
      "It: 1120, Loss: 7.624e+02, Loss_bcs: 5.114e+01, Loss_res: 7.112e+02,Time: 0.01\n",
      "adaptive_constant_val: 71.366360\n",
      "It: 1130, Loss: 9.101e+02, Loss_bcs: 5.005e+01, Loss_res: 8.601e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.837481\n",
      "It: 1140, Loss: 7.350e+02, Loss_bcs: 5.676e+01, Loss_res: 6.782e+02,Time: 0.01\n",
      "adaptive_constant_val: 64.338572\n",
      "It: 1150, Loss: 4.727e+02, Loss_bcs: 4.950e+01, Loss_res: 4.232e+02,Time: 0.01\n",
      "adaptive_constant_val: 62.355489\n",
      "It: 1160, Loss: 5.499e+02, Loss_bcs: 4.401e+01, Loss_res: 5.059e+02,Time: 0.01\n",
      "adaptive_constant_val: 61.574340\n",
      "It: 1170, Loss: 8.563e+02, Loss_bcs: 4.072e+01, Loss_res: 8.156e+02,Time: 0.01\n",
      "adaptive_constant_val: 72.602894\n",
      "It: 1180, Loss: 7.905e+02, Loss_bcs: 4.810e+01, Loss_res: 7.424e+02,Time: 0.01\n",
      "adaptive_constant_val: 80.249709\n",
      "It: 1190, Loss: 6.487e+02, Loss_bcs: 6.491e+01, Loss_res: 5.838e+02,Time: 0.01\n",
      "adaptive_constant_val: 83.851771\n",
      "It: 1200, Loss: 5.948e+02, Loss_bcs: 5.850e+01, Loss_res: 5.363e+02,Time: 0.01\n",
      "adaptive_constant_val: 87.853686\n",
      "It: 1210, Loss: 5.639e+02, Loss_bcs: 6.151e+01, Loss_res: 5.024e+02,Time: 0.01\n",
      "adaptive_constant_val: 85.371534\n",
      "It: 1220, Loss: 5.325e+02, Loss_bcs: 5.935e+01, Loss_res: 4.732e+02,Time: 0.01\n",
      "adaptive_constant_val: 85.282126\n",
      "It: 1230, Loss: 4.737e+02, Loss_bcs: 5.736e+01, Loss_res: 4.163e+02,Time: 0.01\n",
      "adaptive_constant_val: 85.508379\n",
      "It: 1240, Loss: 6.361e+02, Loss_bcs: 5.874e+01, Loss_res: 5.773e+02,Time: 0.01\n",
      "adaptive_constant_val: 80.546549\n",
      "It: 1250, Loss: 2.537e+02, Loss_bcs: 5.435e+01, Loss_res: 1.993e+02,Time: 0.01\n",
      "adaptive_constant_val: 81.065038\n",
      "It: 1260, Loss: 6.592e+02, Loss_bcs: 5.673e+01, Loss_res: 6.025e+02,Time: 0.01\n",
      "adaptive_constant_val: 84.121987\n",
      "It: 1270, Loss: 4.640e+02, Loss_bcs: 5.348e+01, Loss_res: 4.105e+02,Time: 0.01\n",
      "adaptive_constant_val: 82.703594\n",
      "It: 1280, Loss: 6.205e+02, Loss_bcs: 5.199e+01, Loss_res: 5.685e+02,Time: 0.01\n",
      "adaptive_constant_val: 83.466099\n",
      "It: 1290, Loss: 6.186e+02, Loss_bcs: 5.528e+01, Loss_res: 5.633e+02,Time: 0.01\n",
      "adaptive_constant_val: 85.046287\n",
      "It: 1300, Loss: 4.442e+02, Loss_bcs: 5.336e+01, Loss_res: 3.908e+02,Time: 0.01\n",
      "adaptive_constant_val: 82.550418\n",
      "It: 1310, Loss: 5.150e+02, Loss_bcs: 5.792e+01, Loss_res: 4.571e+02,Time: 0.01\n",
      "adaptive_constant_val: 78.767463\n",
      "It: 1320, Loss: 5.134e+02, Loss_bcs: 5.961e+01, Loss_res: 4.537e+02,Time: 0.01\n",
      "adaptive_constant_val: 75.663370\n",
      "It: 1330, Loss: 4.018e+02, Loss_bcs: 6.134e+01, Loss_res: 3.405e+02,Time: 0.01\n",
      "adaptive_constant_val: 72.599745\n",
      "It: 1340, Loss: 3.885e+02, Loss_bcs: 4.990e+01, Loss_res: 3.386e+02,Time: 0.01\n",
      "adaptive_constant_val: 72.370768\n",
      "It: 1350, Loss: 3.388e+02, Loss_bcs: 5.077e+01, Loss_res: 2.880e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.627920\n",
      "It: 1360, Loss: 3.881e+02, Loss_bcs: 4.619e+01, Loss_res: 3.419e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.074284\n",
      "It: 1370, Loss: 2.052e+02, Loss_bcs: 4.305e+01, Loss_res: 1.622e+02,Time: 0.01\n",
      "adaptive_constant_val: 67.002451\n",
      "It: 1380, Loss: 4.250e+02, Loss_bcs: 4.067e+01, Loss_res: 3.844e+02,Time: 0.01\n",
      "adaptive_constant_val: 66.600958\n",
      "It: 1390, Loss: 3.671e+02, Loss_bcs: 4.914e+01, Loss_res: 3.180e+02,Time: 0.01\n",
      "adaptive_constant_val: 69.133432\n",
      "It: 1400, Loss: 2.993e+02, Loss_bcs: 4.058e+01, Loss_res: 2.588e+02,Time: 0.01\n",
      "adaptive_constant_val: 66.057854\n",
      "It: 1410, Loss: 2.362e+02, Loss_bcs: 4.130e+01, Loss_res: 1.949e+02,Time: 0.01\n",
      "adaptive_constant_val: 65.283551\n",
      "It: 1420, Loss: 3.552e+02, Loss_bcs: 3.332e+01, Loss_res: 3.219e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.335669\n",
      "It: 1430, Loss: 3.540e+02, Loss_bcs: 4.197e+01, Loss_res: 3.120e+02,Time: 0.01\n",
      "adaptive_constant_val: 67.809387\n",
      "It: 1440, Loss: 2.576e+02, Loss_bcs: 4.133e+01, Loss_res: 2.163e+02,Time: 0.01\n",
      "adaptive_constant_val: 64.130587\n",
      "It: 1450, Loss: 1.731e+02, Loss_bcs: 4.353e+01, Loss_res: 1.295e+02,Time: 0.01\n",
      "adaptive_constant_val: 62.879810\n",
      "It: 1460, Loss: 1.591e+02, Loss_bcs: 3.516e+01, Loss_res: 1.239e+02,Time: 0.01\n",
      "adaptive_constant_val: 60.604119\n",
      "It: 1470, Loss: 1.574e+02, Loss_bcs: 3.839e+01, Loss_res: 1.190e+02,Time: 0.01\n",
      "adaptive_constant_val: 60.858899\n",
      "It: 1480, Loss: 2.150e+02, Loss_bcs: 2.991e+01, Loss_res: 1.851e+02,Time: 0.01\n",
      "adaptive_constant_val: 67.166184\n",
      "It: 1490, Loss: 2.052e+02, Loss_bcs: 3.639e+01, Loss_res: 1.688e+02,Time: 0.00\n",
      "adaptive_constant_val: 71.852145\n",
      "It: 1500, Loss: 1.474e+02, Loss_bcs: 3.299e+01, Loss_res: 1.144e+02,Time: 0.01\n",
      "adaptive_constant_val: 73.236634\n",
      "It: 1510, Loss: 1.548e+02, Loss_bcs: 3.000e+01, Loss_res: 1.248e+02,Time: 0.01\n",
      "adaptive_constant_val: 70.717935\n",
      "It: 1520, Loss: 1.597e+02, Loss_bcs: 2.588e+01, Loss_res: 1.339e+02,Time: 0.01\n",
      "adaptive_constant_val: 71.288911\n",
      "It: 1530, Loss: 1.735e+02, Loss_bcs: 3.648e+01, Loss_res: 1.370e+02,Time: 0.01\n",
      "adaptive_constant_val: 70.251773\n",
      "It: 1540, Loss: 1.862e+02, Loss_bcs: 2.916e+01, Loss_res: 1.570e+02,Time: 0.01\n",
      "adaptive_constant_val: 77.223238\n",
      "It: 1550, Loss: 1.702e+02, Loss_bcs: 2.709e+01, Loss_res: 1.431e+02,Time: 0.01\n",
      "adaptive_constant_val: 72.852336\n",
      "It: 1560, Loss: 1.330e+02, Loss_bcs: 2.735e+01, Loss_res: 1.056e+02,Time: 0.01\n",
      "adaptive_constant_val: 73.985724\n",
      "It: 1570, Loss: 1.791e+02, Loss_bcs: 2.302e+01, Loss_res: 1.561e+02,Time: 0.01\n",
      "adaptive_constant_val: 72.994404\n",
      "It: 1580, Loss: 1.297e+02, Loss_bcs: 2.323e+01, Loss_res: 1.064e+02,Time: 0.01\n",
      "adaptive_constant_val: 69.504535\n",
      "It: 1590, Loss: 1.274e+02, Loss_bcs: 2.045e+01, Loss_res: 1.070e+02,Time: 0.01\n",
      "adaptive_constant_val: 67.351141\n",
      "It: 1600, Loss: 1.149e+02, Loss_bcs: 2.059e+01, Loss_res: 9.428e+01,Time: 0.01\n",
      "adaptive_constant_val: 62.375448\n",
      "It: 1610, Loss: 1.759e+02, Loss_bcs: 1.764e+01, Loss_res: 1.583e+02,Time: 0.01\n",
      "adaptive_constant_val: 74.180964\n",
      "It: 1620, Loss: 1.206e+02, Loss_bcs: 1.597e+01, Loss_res: 1.046e+02,Time: 0.01\n",
      "adaptive_constant_val: 80.972308\n",
      "It: 1630, Loss: 1.660e+02, Loss_bcs: 1.530e+01, Loss_res: 1.507e+02,Time: 0.01\n",
      "adaptive_constant_val: 82.080429\n",
      "It: 1640, Loss: 1.384e+02, Loss_bcs: 2.131e+01, Loss_res: 1.171e+02,Time: 0.01\n",
      "adaptive_constant_val: 77.931679\n",
      "It: 1650, Loss: 9.873e+01, Loss_bcs: 1.623e+01, Loss_res: 8.250e+01,Time: 0.01\n",
      "adaptive_constant_val: 77.840680\n",
      "It: 1660, Loss: 9.671e+01, Loss_bcs: 1.842e+01, Loss_res: 7.829e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.166608\n",
      "It: 1670, Loss: 9.764e+01, Loss_bcs: 1.566e+01, Loss_res: 8.198e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.199155\n",
      "It: 1680, Loss: 1.340e+02, Loss_bcs: 1.326e+01, Loss_res: 1.207e+02,Time: 0.01\n",
      "adaptive_constant_val: 66.789866\n",
      "It: 1690, Loss: 8.707e+01, Loss_bcs: 9.822e+00, Loss_res: 7.724e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.236110\n",
      "It: 1700, Loss: 1.057e+02, Loss_bcs: 1.066e+01, Loss_res: 9.508e+01,Time: 0.01\n",
      "adaptive_constant_val: 72.857362\n",
      "It: 1710, Loss: 7.803e+01, Loss_bcs: 1.260e+01, Loss_res: 6.542e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.722068\n",
      "It: 1720, Loss: 8.878e+01, Loss_bcs: 1.047e+01, Loss_res: 7.831e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.869676\n",
      "It: 1730, Loss: 9.363e+01, Loss_bcs: 1.024e+01, Loss_res: 8.339e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.191260\n",
      "It: 1740, Loss: 9.275e+01, Loss_bcs: 1.132e+01, Loss_res: 8.143e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.570565\n",
      "It: 1750, Loss: 1.081e+02, Loss_bcs: 8.765e+00, Loss_res: 9.937e+01,Time: 0.01\n",
      "adaptive_constant_val: 78.272418\n",
      "It: 1760, Loss: 1.384e+02, Loss_bcs: 9.432e+00, Loss_res: 1.289e+02,Time: 0.01\n",
      "adaptive_constant_val: 90.754747\n",
      "It: 1770, Loss: 8.075e+01, Loss_bcs: 1.097e+01, Loss_res: 6.978e+01,Time: 0.01\n",
      "adaptive_constant_val: 86.285168\n",
      "It: 1780, Loss: 7.905e+01, Loss_bcs: 1.195e+01, Loss_res: 6.710e+01,Time: 0.02\n",
      "adaptive_constant_val: 84.246093\n",
      "It: 1790, Loss: 9.527e+01, Loss_bcs: 9.908e+00, Loss_res: 8.536e+01,Time: 0.01\n",
      "adaptive_constant_val: 86.866522\n",
      "It: 1800, Loss: 6.440e+01, Loss_bcs: 9.577e+00, Loss_res: 5.482e+01,Time: 0.01\n",
      "adaptive_constant_val: 82.277308\n",
      "It: 1810, Loss: 4.642e+01, Loss_bcs: 8.430e+00, Loss_res: 3.799e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.465829\n",
      "It: 1820, Loss: 1.036e+02, Loss_bcs: 8.753e+00, Loss_res: 9.481e+01,Time: 0.01\n",
      "adaptive_constant_val: 74.698163\n",
      "It: 1830, Loss: 6.694e+01, Loss_bcs: 7.006e+00, Loss_res: 5.993e+01,Time: 0.01\n",
      "adaptive_constant_val: 77.911601\n",
      "It: 1840, Loss: 6.837e+01, Loss_bcs: 5.507e+00, Loss_res: 6.286e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.168619\n",
      "It: 1850, Loss: 7.623e+01, Loss_bcs: 5.604e+00, Loss_res: 7.063e+01,Time: 0.01\n",
      "adaptive_constant_val: 91.325931\n",
      "It: 1860, Loss: 4.250e+01, Loss_bcs: 4.665e+00, Loss_res: 3.784e+01,Time: 0.01\n",
      "adaptive_constant_val: 99.353974\n",
      "It: 1870, Loss: 5.680e+01, Loss_bcs: 6.292e+00, Loss_res: 5.050e+01,Time: 0.01\n",
      "adaptive_constant_val: 95.529550\n",
      "It: 1880, Loss: 4.727e+01, Loss_bcs: 8.080e+00, Loss_res: 3.919e+01,Time: 0.01\n",
      "adaptive_constant_val: 88.506059\n",
      "It: 1890, Loss: 5.177e+01, Loss_bcs: 5.658e+00, Loss_res: 4.611e+01,Time: 0.01\n",
      "adaptive_constant_val: 85.023644\n",
      "It: 1900, Loss: 1.041e+02, Loss_bcs: 5.879e+00, Loss_res: 9.818e+01,Time: 0.01\n",
      "adaptive_constant_val: 89.788402\n",
      "It: 1910, Loss: 5.711e+01, Loss_bcs: 6.633e+00, Loss_res: 5.047e+01,Time: 0.01\n",
      "adaptive_constant_val: 86.301250\n",
      "It: 1920, Loss: 3.059e+01, Loss_bcs: 5.063e+00, Loss_res: 2.553e+01,Time: 0.01\n",
      "adaptive_constant_val: 80.081163\n",
      "It: 1930, Loss: 4.716e+01, Loss_bcs: 4.584e+00, Loss_res: 4.257e+01,Time: 0.01\n",
      "adaptive_constant_val: 88.509288\n",
      "It: 1940, Loss: 4.824e+01, Loss_bcs: 5.991e+00, Loss_res: 4.225e+01,Time: 0.01\n",
      "adaptive_constant_val: 81.745006\n",
      "It: 1950, Loss: 3.947e+01, Loss_bcs: 6.151e+00, Loss_res: 3.332e+01,Time: 0.00\n",
      "adaptive_constant_val: 81.413064\n",
      "It: 1960, Loss: 4.826e+01, Loss_bcs: 4.970e+00, Loss_res: 4.329e+01,Time: 0.01\n",
      "adaptive_constant_val: 82.885350\n",
      "It: 1970, Loss: 4.017e+01, Loss_bcs: 4.617e+00, Loss_res: 3.555e+01,Time: 0.01\n",
      "adaptive_constant_val: 78.238369\n",
      "It: 1980, Loss: 3.404e+01, Loss_bcs: 4.648e+00, Loss_res: 2.939e+01,Time: 0.01\n",
      "adaptive_constant_val: 74.295651\n",
      "It: 1990, Loss: 4.704e+01, Loss_bcs: 3.783e+00, Loss_res: 4.326e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.029854\n",
      "It: 2000, Loss: 2.358e+01, Loss_bcs: 3.856e+00, Loss_res: 1.973e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.064537\n",
      "It: 2010, Loss: 2.224e+01, Loss_bcs: 3.884e+00, Loss_res: 1.836e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.427712\n",
      "It: 2020, Loss: 5.063e+01, Loss_bcs: 4.159e+00, Loss_res: 4.647e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.223401\n",
      "It: 2030, Loss: 6.614e+01, Loss_bcs: 3.776e+00, Loss_res: 6.237e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.287277\n",
      "It: 2040, Loss: 2.821e+01, Loss_bcs: 4.623e+00, Loss_res: 2.359e+01,Time: 0.01\n",
      "adaptive_constant_val: 86.706912\n",
      "It: 2050, Loss: 5.667e+01, Loss_bcs: 4.478e+00, Loss_res: 5.219e+01,Time: 0.01\n",
      "adaptive_constant_val: 95.471419\n",
      "It: 2060, Loss: 2.820e+01, Loss_bcs: 4.730e+00, Loss_res: 2.347e+01,Time: 0.01\n",
      "adaptive_constant_val: 90.803494\n",
      "It: 2070, Loss: 4.754e+01, Loss_bcs: 4.224e+00, Loss_res: 4.332e+01,Time: 0.01\n",
      "adaptive_constant_val: 93.960221\n",
      "It: 2080, Loss: 3.182e+01, Loss_bcs: 4.589e+00, Loss_res: 2.723e+01,Time: 0.01\n",
      "adaptive_constant_val: 91.364104\n",
      "It: 2090, Loss: 5.938e+01, Loss_bcs: 4.781e+00, Loss_res: 5.460e+01,Time: 0.01\n",
      "adaptive_constant_val: 86.475489\n",
      "It: 2100, Loss: 3.178e+01, Loss_bcs: 4.005e+00, Loss_res: 2.778e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.027196\n",
      "It: 2110, Loss: 2.369e+01, Loss_bcs: 4.113e+00, Loss_res: 1.957e+01,Time: 0.01\n",
      "adaptive_constant_val: 80.874365\n",
      "It: 2120, Loss: 3.217e+01, Loss_bcs: 4.261e+00, Loss_res: 2.791e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.927500\n",
      "It: 2130, Loss: 2.797e+01, Loss_bcs: 5.267e+00, Loss_res: 2.270e+01,Time: 0.01\n",
      "adaptive_constant_val: 75.178915\n",
      "It: 2140, Loss: 4.393e+01, Loss_bcs: 4.133e+00, Loss_res: 3.979e+01,Time: 0.01\n",
      "adaptive_constant_val: 81.818840\n",
      "It: 2150, Loss: 4.677e+01, Loss_bcs: 5.009e+00, Loss_res: 4.177e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.474623\n",
      "It: 2160, Loss: 3.799e+01, Loss_bcs: 4.497e+00, Loss_res: 3.349e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.828518\n",
      "It: 2170, Loss: 5.000e+01, Loss_bcs: 4.109e+00, Loss_res: 4.589e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.574941\n",
      "It: 2180, Loss: 3.789e+01, Loss_bcs: 4.295e+00, Loss_res: 3.359e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.131275\n",
      "It: 2190, Loss: 3.901e+01, Loss_bcs: 4.253e+00, Loss_res: 3.476e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.118300\n",
      "It: 2200, Loss: 4.118e+01, Loss_bcs: 4.644e+00, Loss_res: 3.654e+01,Time: 0.01\n",
      "adaptive_constant_val: 78.496369\n",
      "It: 2210, Loss: 2.219e+01, Loss_bcs: 4.520e+00, Loss_res: 1.767e+01,Time: 0.01\n",
      "adaptive_constant_val: 77.845447\n",
      "It: 2220, Loss: 2.388e+01, Loss_bcs: 3.906e+00, Loss_res: 1.997e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.727468\n",
      "It: 2230, Loss: 1.900e+01, Loss_bcs: 3.546e+00, Loss_res: 1.545e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.642524\n",
      "It: 2240, Loss: 2.187e+01, Loss_bcs: 3.284e+00, Loss_res: 1.858e+01,Time: 0.01\n",
      "adaptive_constant_val: 65.335394\n",
      "It: 2250, Loss: 3.635e+01, Loss_bcs: 3.498e+00, Loss_res: 3.285e+01,Time: 0.01\n",
      "adaptive_constant_val: 65.946292\n",
      "It: 2260, Loss: 3.597e+01, Loss_bcs: 3.486e+00, Loss_res: 3.248e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.155230\n",
      "It: 2270, Loss: 2.818e+01, Loss_bcs: 3.898e+00, Loss_res: 2.428e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.799319\n",
      "It: 2280, Loss: 3.511e+01, Loss_bcs: 3.674e+00, Loss_res: 3.144e+01,Time: 0.01\n",
      "adaptive_constant_val: 80.052184\n",
      "It: 2290, Loss: 2.092e+01, Loss_bcs: 4.932e+00, Loss_res: 1.599e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.365650\n",
      "It: 2300, Loss: 2.049e+01, Loss_bcs: 3.676e+00, Loss_res: 1.681e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.983343\n",
      "It: 2310, Loss: 2.603e+01, Loss_bcs: 3.304e+00, Loss_res: 2.272e+01,Time: 0.01\n",
      "adaptive_constant_val: 99.503136\n",
      "It: 2320, Loss: 1.796e+01, Loss_bcs: 4.393e+00, Loss_res: 1.357e+01,Time: 0.01\n",
      "adaptive_constant_val: 93.216100\n",
      "It: 2330, Loss: 2.241e+01, Loss_bcs: 4.168e+00, Loss_res: 1.825e+01,Time: 0.01\n",
      "adaptive_constant_val: 88.571830\n",
      "It: 2340, Loss: 2.186e+01, Loss_bcs: 4.576e+00, Loss_res: 1.729e+01,Time: 0.00\n",
      "adaptive_constant_val: 86.314484\n",
      "It: 2350, Loss: 1.930e+01, Loss_bcs: 3.281e+00, Loss_res: 1.602e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.983043\n",
      "It: 2360, Loss: 1.797e+01, Loss_bcs: 3.828e+00, Loss_res: 1.414e+01,Time: 0.01\n",
      "adaptive_constant_val: 90.302850\n",
      "It: 2370, Loss: 1.851e+01, Loss_bcs: 3.912e+00, Loss_res: 1.460e+01,Time: 0.01\n",
      "adaptive_constant_val: 86.750756\n",
      "It: 2380, Loss: 2.123e+01, Loss_bcs: 3.921e+00, Loss_res: 1.730e+01,Time: 0.01\n",
      "adaptive_constant_val: 88.272590\n",
      "It: 2390, Loss: 2.219e+01, Loss_bcs: 3.844e+00, Loss_res: 1.834e+01,Time: 0.01\n",
      "adaptive_constant_val: 89.426973\n",
      "It: 2400, Loss: 1.639e+01, Loss_bcs: 3.550e+00, Loss_res: 1.284e+01,Time: 0.01\n",
      "adaptive_constant_val: 90.391481\n",
      "It: 2410, Loss: 2.542e+01, Loss_bcs: 4.307e+00, Loss_res: 2.111e+01,Time: 0.01\n",
      "adaptive_constant_val: 89.720551\n",
      "It: 2420, Loss: 1.932e+01, Loss_bcs: 4.268e+00, Loss_res: 1.505e+01,Time: 0.01\n",
      "adaptive_constant_val: 87.533730\n",
      "It: 2430, Loss: 1.252e+01, Loss_bcs: 3.773e+00, Loss_res: 8.751e+00,Time: 0.01\n",
      "adaptive_constant_val: 84.140931\n",
      "It: 2440, Loss: 2.437e+01, Loss_bcs: 3.496e+00, Loss_res: 2.088e+01,Time: 0.01\n",
      "adaptive_constant_val: 80.030799\n",
      "It: 2450, Loss: 2.466e+01, Loss_bcs: 3.542e+00, Loss_res: 2.112e+01,Time: 0.02\n",
      "adaptive_constant_val: 78.251703\n",
      "It: 2460, Loss: 1.829e+01, Loss_bcs: 3.247e+00, Loss_res: 1.504e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.291774\n",
      "It: 2470, Loss: 1.610e+01, Loss_bcs: 3.159e+00, Loss_res: 1.295e+01,Time: 0.01\n",
      "adaptive_constant_val: 72.749957\n",
      "It: 2480, Loss: 1.582e+01, Loss_bcs: 3.121e+00, Loss_res: 1.270e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.275143\n",
      "It: 2490, Loss: 1.931e+01, Loss_bcs: 3.397e+00, Loss_res: 1.591e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.014916\n",
      "It: 2500, Loss: 1.769e+01, Loss_bcs: 2.865e+00, Loss_res: 1.483e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.591070\n",
      "It: 2510, Loss: 1.753e+01, Loss_bcs: 3.093e+00, Loss_res: 1.444e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.365618\n",
      "It: 2520, Loss: 1.281e+01, Loss_bcs: 2.395e+00, Loss_res: 1.041e+01,Time: 0.02\n",
      "adaptive_constant_val: 65.711938\n",
      "It: 2530, Loss: 1.209e+01, Loss_bcs: 2.559e+00, Loss_res: 9.526e+00,Time: 0.02\n",
      "adaptive_constant_val: 67.177561\n",
      "It: 2540, Loss: 1.773e+01, Loss_bcs: 2.646e+00, Loss_res: 1.508e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.101285\n",
      "It: 2550, Loss: 1.496e+01, Loss_bcs: 2.927e+00, Loss_res: 1.204e+01,Time: 0.01\n",
      "adaptive_constant_val: 74.430413\n",
      "It: 2560, Loss: 1.576e+01, Loss_bcs: 2.923e+00, Loss_res: 1.284e+01,Time: 0.01\n",
      "adaptive_constant_val: 72.302949\n",
      "It: 2570, Loss: 1.384e+01, Loss_bcs: 2.706e+00, Loss_res: 1.113e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.822163\n",
      "It: 2580, Loss: 2.226e+01, Loss_bcs: 3.088e+00, Loss_res: 1.917e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.701963\n",
      "It: 2590, Loss: 1.296e+01, Loss_bcs: 2.988e+00, Loss_res: 9.977e+00,Time: 0.01\n",
      "adaptive_constant_val: 86.867046\n",
      "It: 2600, Loss: 1.314e+01, Loss_bcs: 2.832e+00, Loss_res: 1.031e+01,Time: 0.01\n",
      "adaptive_constant_val: 87.822652\n",
      "It: 2610, Loss: 1.919e+01, Loss_bcs: 3.797e+00, Loss_res: 1.539e+01,Time: 0.01\n",
      "adaptive_constant_val: 95.117700\n",
      "It: 2620, Loss: 1.367e+01, Loss_bcs: 3.527e+00, Loss_res: 1.014e+01,Time: 0.01\n",
      "adaptive_constant_val: 90.907552\n",
      "It: 2630, Loss: 1.158e+01, Loss_bcs: 2.776e+00, Loss_res: 8.805e+00,Time: 0.01\n",
      "adaptive_constant_val: 96.904456\n",
      "It: 2640, Loss: 1.840e+01, Loss_bcs: 3.003e+00, Loss_res: 1.539e+01,Time: 0.01\n",
      "adaptive_constant_val: 98.630380\n",
      "It: 2650, Loss: 1.216e+01, Loss_bcs: 3.175e+00, Loss_res: 8.982e+00,Time: 0.01\n",
      "adaptive_constant_val: 93.228946\n",
      "It: 2660, Loss: 1.213e+01, Loss_bcs: 3.481e+00, Loss_res: 8.649e+00,Time: 0.01\n",
      "adaptive_constant_val: 94.455176\n",
      "It: 2670, Loss: 1.170e+01, Loss_bcs: 3.462e+00, Loss_res: 8.239e+00,Time: 0.01\n",
      "adaptive_constant_val: 89.937663\n",
      "It: 2680, Loss: 1.693e+01, Loss_bcs: 3.180e+00, Loss_res: 1.375e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.686672\n",
      "It: 2690, Loss: 9.629e+00, Loss_bcs: 2.516e+00, Loss_res: 7.113e+00,Time: 0.01\n",
      "adaptive_constant_val: 79.494615\n",
      "It: 2700, Loss: 1.351e+01, Loss_bcs: 2.638e+00, Loss_res: 1.087e+01,Time: 0.01\n",
      "adaptive_constant_val: 80.273044\n",
      "It: 2710, Loss: 1.663e+01, Loss_bcs: 2.765e+00, Loss_res: 1.387e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.143952\n",
      "It: 2720, Loss: 1.603e+01, Loss_bcs: 2.569e+00, Loss_res: 1.347e+01,Time: 0.01\n",
      "adaptive_constant_val: 99.382719\n",
      "It: 2730, Loss: 1.233e+01, Loss_bcs: 4.022e+00, Loss_res: 8.307e+00,Time: 0.01\n",
      "adaptive_constant_val: 97.241217\n",
      "It: 2740, Loss: 1.521e+01, Loss_bcs: 3.253e+00, Loss_res: 1.195e+01,Time: 0.01\n",
      "adaptive_constant_val: 92.189001\n",
      "It: 2750, Loss: 1.287e+01, Loss_bcs: 2.910e+00, Loss_res: 9.958e+00,Time: 0.01\n",
      "adaptive_constant_val: 90.488738\n",
      "It: 2760, Loss: 1.806e+01, Loss_bcs: 2.489e+00, Loss_res: 1.557e+01,Time: 0.01\n",
      "adaptive_constant_val: 89.480808\n",
      "It: 2770, Loss: 1.207e+01, Loss_bcs: 2.842e+00, Loss_res: 9.225e+00,Time: 0.01\n",
      "adaptive_constant_val: 84.834081\n",
      "It: 2780, Loss: 1.361e+01, Loss_bcs: 3.072e+00, Loss_res: 1.054e+01,Time: 0.01\n",
      "adaptive_constant_val: 88.897413\n",
      "It: 2790, Loss: 1.041e+01, Loss_bcs: 2.606e+00, Loss_res: 7.801e+00,Time: 0.01\n",
      "adaptive_constant_val: 87.228526\n",
      "It: 2800, Loss: 1.643e+01, Loss_bcs: 2.766e+00, Loss_res: 1.367e+01,Time: 0.01\n",
      "adaptive_constant_val: 82.535092\n",
      "It: 2810, Loss: 1.837e+01, Loss_bcs: 2.229e+00, Loss_res: 1.614e+01,Time: 0.01\n",
      "adaptive_constant_val: 80.622581\n",
      "It: 2820, Loss: 1.463e+01, Loss_bcs: 2.461e+00, Loss_res: 1.217e+01,Time: 0.01\n",
      "adaptive_constant_val: 86.803068\n",
      "It: 2830, Loss: 1.299e+01, Loss_bcs: 2.472e+00, Loss_res: 1.052e+01,Time: 0.01\n",
      "adaptive_constant_val: 85.237796\n",
      "It: 2840, Loss: 8.792e+00, Loss_bcs: 2.936e+00, Loss_res: 5.855e+00,Time: 0.01\n",
      "adaptive_constant_val: 80.311369\n",
      "It: 2850, Loss: 1.228e+01, Loss_bcs: 2.528e+00, Loss_res: 9.754e+00,Time: 0.01\n",
      "adaptive_constant_val: 83.117718\n",
      "It: 2860, Loss: 9.274e+00, Loss_bcs: 2.402e+00, Loss_res: 6.872e+00,Time: 0.01\n",
      "adaptive_constant_val: 80.275264\n",
      "It: 2870, Loss: 1.109e+01, Loss_bcs: 2.226e+00, Loss_res: 8.863e+00,Time: 0.01\n",
      "adaptive_constant_val: 74.628480\n",
      "It: 2880, Loss: 9.936e+00, Loss_bcs: 2.040e+00, Loss_res: 7.895e+00,Time: 0.01\n",
      "adaptive_constant_val: 73.648141\n",
      "It: 2890, Loss: 1.058e+01, Loss_bcs: 2.017e+00, Loss_res: 8.562e+00,Time: 0.01\n",
      "adaptive_constant_val: 73.589841\n",
      "It: 2900, Loss: 8.342e+00, Loss_bcs: 2.045e+00, Loss_res: 6.298e+00,Time: 0.01\n",
      "adaptive_constant_val: 71.768164\n",
      "It: 2910, Loss: 1.210e+01, Loss_bcs: 2.529e+00, Loss_res: 9.566e+00,Time: 0.01\n",
      "adaptive_constant_val: 68.983860\n",
      "It: 2920, Loss: 1.048e+01, Loss_bcs: 2.354e+00, Loss_res: 8.128e+00,Time: 0.01\n",
      "adaptive_constant_val: 78.128091\n",
      "It: 2930, Loss: 1.188e+01, Loss_bcs: 2.023e+00, Loss_res: 9.857e+00,Time: 0.02\n",
      "adaptive_constant_val: 94.227097\n",
      "It: 2940, Loss: 1.106e+01, Loss_bcs: 2.783e+00, Loss_res: 8.281e+00,Time: 0.01\n",
      "adaptive_constant_val: 92.197304\n",
      "It: 2950, Loss: 9.081e+00, Loss_bcs: 2.714e+00, Loss_res: 6.367e+00,Time: 0.01\n",
      "adaptive_constant_val: 87.875979\n",
      "It: 2960, Loss: 8.907e+00, Loss_bcs: 2.079e+00, Loss_res: 6.828e+00,Time: 0.01\n",
      "adaptive_constant_val: 81.944994\n",
      "It: 2970, Loss: 1.280e+01, Loss_bcs: 2.198e+00, Loss_res: 1.060e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.296015\n",
      "It: 2980, Loss: 1.007e+01, Loss_bcs: 2.474e+00, Loss_res: 7.596e+00,Time: 0.01\n",
      "adaptive_constant_val: 74.959183\n",
      "It: 2990, Loss: 1.490e+01, Loss_bcs: 1.688e+00, Loss_res: 1.321e+01,Time: 0.01\n",
      "adaptive_constant_val: 96.896886\n",
      "It: 3000, Loss: 1.666e+01, Loss_bcs: 2.811e+00, Loss_res: 1.385e+01,Time: 0.01\n",
      "adaptive_constant_val: 111.686435\n",
      "It: 3010, Loss: 9.260e+00, Loss_bcs: 2.143e+00, Loss_res: 7.117e+00,Time: 0.01\n",
      "adaptive_constant_val: 105.028645\n",
      "It: 3020, Loss: 1.202e+01, Loss_bcs: 2.198e+00, Loss_res: 9.819e+00,Time: 0.01\n",
      "adaptive_constant_val: 100.705137\n",
      "It: 3030, Loss: 1.432e+01, Loss_bcs: 2.454e+00, Loss_res: 1.186e+01,Time: 0.01\n",
      "adaptive_constant_val: 99.763373\n",
      "It: 3040, Loss: 1.195e+01, Loss_bcs: 2.530e+00, Loss_res: 9.418e+00,Time: 0.01\n",
      "adaptive_constant_val: 95.898445\n",
      "It: 3050, Loss: 9.337e+00, Loss_bcs: 2.663e+00, Loss_res: 6.674e+00,Time: 0.01\n",
      "adaptive_constant_val: 91.636802\n",
      "It: 3060, Loss: 6.922e+00, Loss_bcs: 2.658e+00, Loss_res: 4.263e+00,Time: 0.01\n",
      "adaptive_constant_val: 87.066665\n",
      "It: 3070, Loss: 8.995e+00, Loss_bcs: 2.059e+00, Loss_res: 6.936e+00,Time: 0.01\n",
      "adaptive_constant_val: 83.548356\n",
      "It: 3080, Loss: 9.657e+00, Loss_bcs: 2.102e+00, Loss_res: 7.554e+00,Time: 0.01\n",
      "adaptive_constant_val: 85.869310\n",
      "It: 3090, Loss: 1.125e+01, Loss_bcs: 2.644e+00, Loss_res: 8.607e+00,Time: 0.01\n",
      "adaptive_constant_val: 84.750908\n",
      "It: 3100, Loss: 8.417e+00, Loss_bcs: 2.034e+00, Loss_res: 6.384e+00,Time: 0.01\n",
      "adaptive_constant_val: 83.703562\n",
      "It: 3110, Loss: 1.165e+01, Loss_bcs: 2.482e+00, Loss_res: 9.170e+00,Time: 0.01\n",
      "adaptive_constant_val: 82.308138\n",
      "It: 3120, Loss: 1.196e+01, Loss_bcs: 2.031e+00, Loss_res: 9.927e+00,Time: 0.01\n",
      "adaptive_constant_val: 78.601923\n",
      "It: 3130, Loss: 6.780e+00, Loss_bcs: 1.898e+00, Loss_res: 4.882e+00,Time: 0.01\n",
      "adaptive_constant_val: 77.224724\n",
      "It: 3140, Loss: 1.547e+01, Loss_bcs: 2.244e+00, Loss_res: 1.322e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.062734\n",
      "It: 3150, Loss: 8.315e+00, Loss_bcs: 2.484e+00, Loss_res: 5.831e+00,Time: 0.01\n",
      "adaptive_constant_val: 77.633211\n",
      "It: 3160, Loss: 6.935e+00, Loss_bcs: 2.015e+00, Loss_res: 4.919e+00,Time: 0.01\n",
      "adaptive_constant_val: 78.863711\n",
      "It: 3170, Loss: 1.184e+01, Loss_bcs: 1.931e+00, Loss_res: 9.907e+00,Time: 0.01\n",
      "adaptive_constant_val: 87.320739\n",
      "It: 3180, Loss: 7.534e+00, Loss_bcs: 2.355e+00, Loss_res: 5.179e+00,Time: 0.01\n",
      "adaptive_constant_val: 81.800280\n",
      "It: 3190, Loss: 1.045e+01, Loss_bcs: 2.283e+00, Loss_res: 8.166e+00,Time: 0.01\n",
      "adaptive_constant_val: 78.974080\n",
      "It: 3200, Loss: 9.310e+00, Loss_bcs: 2.192e+00, Loss_res: 7.118e+00,Time: 0.01\n",
      "adaptive_constant_val: 76.186862\n",
      "It: 3210, Loss: 1.289e+01, Loss_bcs: 1.891e+00, Loss_res: 1.100e+01,Time: 0.01\n",
      "adaptive_constant_val: 77.383176\n",
      "It: 3220, Loss: 9.813e+00, Loss_bcs: 2.243e+00, Loss_res: 7.570e+00,Time: 0.01\n",
      "adaptive_constant_val: 73.195482\n",
      "It: 3230, Loss: 7.538e+00, Loss_bcs: 1.942e+00, Loss_res: 5.596e+00,Time: 0.01\n",
      "adaptive_constant_val: 76.094885\n",
      "It: 3240, Loss: 8.355e+00, Loss_bcs: 1.754e+00, Loss_res: 6.600e+00,Time: 0.01\n",
      "adaptive_constant_val: 83.444971\n",
      "It: 3250, Loss: 1.016e+01, Loss_bcs: 2.247e+00, Loss_res: 7.917e+00,Time: 0.01\n",
      "adaptive_constant_val: 80.955863\n",
      "It: 3260, Loss: 8.938e+00, Loss_bcs: 2.253e+00, Loss_res: 6.685e+00,Time: 0.01\n",
      "adaptive_constant_val: 75.102214\n",
      "It: 3270, Loss: 6.923e+00, Loss_bcs: 2.270e+00, Loss_res: 4.653e+00,Time: 0.01\n",
      "adaptive_constant_val: 76.113058\n",
      "It: 3280, Loss: 7.628e+00, Loss_bcs: 1.675e+00, Loss_res: 5.953e+00,Time: 0.01\n",
      "adaptive_constant_val: 70.615088\n",
      "It: 3290, Loss: 7.245e+00, Loss_bcs: 1.625e+00, Loss_res: 5.620e+00,Time: 0.01\n",
      "adaptive_constant_val: 74.403541\n",
      "It: 3300, Loss: 9.051e+00, Loss_bcs: 1.746e+00, Loss_res: 7.304e+00,Time: 0.01\n",
      "adaptive_constant_val: 80.584255\n",
      "It: 3310, Loss: 8.319e+00, Loss_bcs: 1.863e+00, Loss_res: 6.456e+00,Time: 0.01\n",
      "adaptive_constant_val: 90.292428\n",
      "It: 3320, Loss: 8.531e+00, Loss_bcs: 2.151e+00, Loss_res: 6.380e+00,Time: 0.01\n",
      "adaptive_constant_val: 84.902108\n",
      "It: 3330, Loss: 9.996e+00, Loss_bcs: 2.122e+00, Loss_res: 7.874e+00,Time: 0.01\n",
      "adaptive_constant_val: 79.141728\n",
      "It: 3340, Loss: 6.259e+00, Loss_bcs: 1.906e+00, Loss_res: 4.353e+00,Time: 0.01\n",
      "adaptive_constant_val: 77.135612\n",
      "It: 3350, Loss: 1.011e+01, Loss_bcs: 2.011e+00, Loss_res: 8.095e+00,Time: 0.01\n",
      "adaptive_constant_val: 73.000546\n",
      "It: 3360, Loss: 8.703e+00, Loss_bcs: 1.898e+00, Loss_res: 6.805e+00,Time: 0.01\n",
      "adaptive_constant_val: 79.759714\n",
      "It: 3370, Loss: 1.110e+01, Loss_bcs: 2.159e+00, Loss_res: 8.937e+00,Time: 0.00\n",
      "adaptive_constant_val: 78.652049\n",
      "It: 3380, Loss: 8.828e+00, Loss_bcs: 2.184e+00, Loss_res: 6.644e+00,Time: 0.00\n",
      "adaptive_constant_val: 77.002244\n",
      "It: 3390, Loss: 7.686e+00, Loss_bcs: 1.511e+00, Loss_res: 6.175e+00,Time: 0.01\n",
      "adaptive_constant_val: 73.674296\n",
      "It: 3400, Loss: 1.088e+01, Loss_bcs: 1.721e+00, Loss_res: 9.159e+00,Time: 0.01\n",
      "adaptive_constant_val: 74.442114\n",
      "It: 3410, Loss: 6.623e+00, Loss_bcs: 1.575e+00, Loss_res: 5.047e+00,Time: 0.01\n",
      "adaptive_constant_val: 71.833175\n",
      "It: 3420, Loss: 6.825e+00, Loss_bcs: 1.627e+00, Loss_res: 5.198e+00,Time: 0.00\n",
      "adaptive_constant_val: 71.153717\n",
      "It: 3430, Loss: 8.080e+00, Loss_bcs: 2.000e+00, Loss_res: 6.079e+00,Time: 0.01\n",
      "adaptive_constant_val: 67.040595\n",
      "It: 3440, Loss: 8.239e+00, Loss_bcs: 1.701e+00, Loss_res: 6.537e+00,Time: 0.01\n",
      "adaptive_constant_val: 73.972232\n",
      "It: 3450, Loss: 1.226e+01, Loss_bcs: 1.804e+00, Loss_res: 1.046e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.821166\n",
      "It: 3460, Loss: 7.594e+00, Loss_bcs: 1.688e+00, Loss_res: 5.906e+00,Time: 0.01\n",
      "adaptive_constant_val: 89.269101\n",
      "It: 3470, Loss: 7.563e+00, Loss_bcs: 1.830e+00, Loss_res: 5.732e+00,Time: 0.01\n",
      "adaptive_constant_val: 90.850933\n",
      "It: 3480, Loss: 9.398e+00, Loss_bcs: 2.026e+00, Loss_res: 7.372e+00,Time: 0.01\n",
      "adaptive_constant_val: 97.317872\n",
      "It: 3490, Loss: 9.429e+00, Loss_bcs: 1.864e+00, Loss_res: 7.565e+00,Time: 0.01\n",
      "adaptive_constant_val: 91.512649\n",
      "It: 3500, Loss: 6.677e+00, Loss_bcs: 2.094e+00, Loss_res: 4.584e+00,Time: 0.01\n",
      "adaptive_constant_val: 85.377565\n",
      "It: 3510, Loss: 8.146e+00, Loss_bcs: 1.757e+00, Loss_res: 6.389e+00,Time: 0.01\n",
      "adaptive_constant_val: 87.139733\n",
      "It: 3520, Loss: 8.511e+00, Loss_bcs: 2.083e+00, Loss_res: 6.428e+00,Time: 0.01\n",
      "adaptive_constant_val: 84.929573\n",
      "It: 3530, Loss: 8.386e+00, Loss_bcs: 1.756e+00, Loss_res: 6.630e+00,Time: 0.01\n",
      "adaptive_constant_val: 81.920274\n",
      "It: 3540, Loss: 6.885e+00, Loss_bcs: 1.383e+00, Loss_res: 5.502e+00,Time: 0.01\n",
      "adaptive_constant_val: 84.339224\n",
      "It: 3550, Loss: 8.936e+00, Loss_bcs: 1.667e+00, Loss_res: 7.269e+00,Time: 0.01\n",
      "adaptive_constant_val: 80.848785\n",
      "It: 3560, Loss: 6.690e+00, Loss_bcs: 1.651e+00, Loss_res: 5.038e+00,Time: 0.01\n",
      "adaptive_constant_val: 76.576981\n",
      "It: 3570, Loss: 8.897e+00, Loss_bcs: 1.779e+00, Loss_res: 7.118e+00,Time: 0.01\n",
      "adaptive_constant_val: 79.883058\n",
      "It: 3580, Loss: 1.191e+01, Loss_bcs: 1.778e+00, Loss_res: 1.014e+01,Time: 0.01\n",
      "adaptive_constant_val: 78.037346\n",
      "It: 3590, Loss: 8.284e+00, Loss_bcs: 1.750e+00, Loss_res: 6.534e+00,Time: 0.01\n",
      "adaptive_constant_val: 78.452183\n",
      "It: 3600, Loss: 4.915e+00, Loss_bcs: 1.590e+00, Loss_res: 3.325e+00,Time: 0.01\n",
      "adaptive_constant_val: 73.123363\n",
      "It: 3610, Loss: 6.391e+00, Loss_bcs: 1.658e+00, Loss_res: 4.733e+00,Time: 0.00\n",
      "adaptive_constant_val: 75.580865\n",
      "It: 3620, Loss: 6.925e+00, Loss_bcs: 1.534e+00, Loss_res: 5.392e+00,Time: 0.01\n",
      "adaptive_constant_val: 74.192745\n",
      "It: 3630, Loss: 8.554e+00, Loss_bcs: 1.882e+00, Loss_res: 6.673e+00,Time: 0.01\n",
      "adaptive_constant_val: 86.179844\n",
      "It: 3640, Loss: 7.680e+00, Loss_bcs: 2.150e+00, Loss_res: 5.530e+00,Time: 0.01\n",
      "adaptive_constant_val: 93.966480\n",
      "It: 3650, Loss: 7.844e+00, Loss_bcs: 2.180e+00, Loss_res: 5.664e+00,Time: 0.01\n",
      "adaptive_constant_val: 92.787967\n",
      "It: 3660, Loss: 8.274e+00, Loss_bcs: 2.403e+00, Loss_res: 5.871e+00,Time: 0.01\n",
      "adaptive_constant_val: 85.955722\n",
      "It: 3670, Loss: 6.159e+00, Loss_bcs: 1.679e+00, Loss_res: 4.480e+00,Time: 0.01\n",
      "adaptive_constant_val: 81.537085\n",
      "It: 3680, Loss: 9.890e+00, Loss_bcs: 1.793e+00, Loss_res: 8.096e+00,Time: 0.01\n",
      "adaptive_constant_val: 80.852267\n",
      "It: 3690, Loss: 7.939e+00, Loss_bcs: 1.734e+00, Loss_res: 6.204e+00,Time: 0.01\n",
      "adaptive_constant_val: 84.421068\n",
      "It: 3700, Loss: 1.275e+01, Loss_bcs: 1.771e+00, Loss_res: 1.098e+01,Time: 0.01\n",
      "adaptive_constant_val: 87.597516\n",
      "It: 3710, Loss: 7.035e+00, Loss_bcs: 2.111e+00, Loss_res: 4.924e+00,Time: 0.01\n",
      "adaptive_constant_val: 80.871863\n",
      "It: 3720, Loss: 8.029e+00, Loss_bcs: 1.956e+00, Loss_res: 6.073e+00,Time: 0.01\n",
      "adaptive_constant_val: 76.156804\n",
      "It: 3730, Loss: 6.147e+00, Loss_bcs: 1.585e+00, Loss_res: 4.563e+00,Time: 0.00\n",
      "adaptive_constant_val: 74.560313\n",
      "It: 3740, Loss: 7.215e+00, Loss_bcs: 1.416e+00, Loss_res: 5.799e+00,Time: 0.01\n",
      "adaptive_constant_val: 77.924271\n",
      "It: 3750, Loss: 6.969e+00, Loss_bcs: 1.551e+00, Loss_res: 5.418e+00,Time: 0.01\n",
      "adaptive_constant_val: 78.829709\n",
      "It: 3760, Loss: 9.316e+00, Loss_bcs: 1.546e+00, Loss_res: 7.770e+00,Time: 0.01\n",
      "adaptive_constant_val: 86.911837\n",
      "It: 3770, Loss: 5.813e+00, Loss_bcs: 2.169e+00, Loss_res: 3.645e+00,Time: 0.01\n",
      "adaptive_constant_val: 82.780878\n",
      "It: 3780, Loss: 8.035e+00, Loss_bcs: 1.803e+00, Loss_res: 6.232e+00,Time: 0.01\n",
      "adaptive_constant_val: 84.242171\n",
      "It: 3790, Loss: 5.801e+00, Loss_bcs: 1.688e+00, Loss_res: 4.113e+00,Time: 0.01\n",
      "adaptive_constant_val: 77.959290\n",
      "It: 3800, Loss: 8.277e+00, Loss_bcs: 1.630e+00, Loss_res: 6.646e+00,Time: 0.01\n",
      "adaptive_constant_val: 75.591613\n",
      "It: 3810, Loss: 7.565e+00, Loss_bcs: 1.510e+00, Loss_res: 6.055e+00,Time: 0.01\n",
      "adaptive_constant_val: 81.707380\n",
      "It: 3820, Loss: 7.317e+00, Loss_bcs: 1.576e+00, Loss_res: 5.741e+00,Time: 0.01\n",
      "adaptive_constant_val: 81.943592\n",
      "It: 3830, Loss: 5.383e+00, Loss_bcs: 1.499e+00, Loss_res: 3.885e+00,Time: 0.01\n",
      "adaptive_constant_val: 79.393042\n",
      "It: 3840, Loss: 6.637e+00, Loss_bcs: 1.624e+00, Loss_res: 5.014e+00,Time: 0.01\n",
      "adaptive_constant_val: 77.385366\n",
      "It: 3850, Loss: 6.097e+00, Loss_bcs: 1.402e+00, Loss_res: 4.695e+00,Time: 0.01\n",
      "adaptive_constant_val: 78.403405\n",
      "It: 3860, Loss: 7.749e+00, Loss_bcs: 1.412e+00, Loss_res: 6.337e+00,Time: 0.01\n",
      "adaptive_constant_val: 75.863491\n",
      "It: 3870, Loss: 5.675e+00, Loss_bcs: 1.651e+00, Loss_res: 4.024e+00,Time: 0.00\n",
      "adaptive_constant_val: 72.723293\n",
      "It: 3880, Loss: 4.910e+00, Loss_bcs: 1.478e+00, Loss_res: 3.432e+00,Time: 0.01\n",
      "adaptive_constant_val: 67.743502\n",
      "It: 3890, Loss: 6.293e+00, Loss_bcs: 1.284e+00, Loss_res: 5.009e+00,Time: 0.01\n",
      "adaptive_constant_val: 71.528460\n",
      "It: 3900, Loss: 8.678e+00, Loss_bcs: 1.261e+00, Loss_res: 7.418e+00,Time: 0.00\n",
      "adaptive_constant_val: 73.250849\n",
      "It: 3910, Loss: 1.316e+01, Loss_bcs: 1.617e+00, Loss_res: 1.154e+01,Time: 0.00\n",
      "adaptive_constant_val: 87.758444\n",
      "It: 3920, Loss: 8.721e+00, Loss_bcs: 2.030e+00, Loss_res: 6.692e+00,Time: 0.01\n",
      "adaptive_constant_val: 88.463429\n",
      "It: 3930, Loss: 1.143e+01, Loss_bcs: 1.528e+00, Loss_res: 9.901e+00,Time: 0.01\n",
      "adaptive_constant_val: 98.420091\n",
      "It: 3940, Loss: 6.633e+00, Loss_bcs: 1.957e+00, Loss_res: 4.677e+00,Time: 0.01\n",
      "adaptive_constant_val: 93.571153\n",
      "It: 3950, Loss: 8.386e+00, Loss_bcs: 2.027e+00, Loss_res: 6.359e+00,Time: 0.01\n",
      "adaptive_constant_val: 87.988090\n",
      "It: 3960, Loss: 6.888e+00, Loss_bcs: 1.498e+00, Loss_res: 5.391e+00,Time: 0.01\n",
      "adaptive_constant_val: 89.185245\n",
      "It: 3970, Loss: 1.149e+01, Loss_bcs: 1.712e+00, Loss_res: 9.780e+00,Time: 0.01\n",
      "adaptive_constant_val: 87.467854\n",
      "It: 3980, Loss: 7.991e+00, Loss_bcs: 1.925e+00, Loss_res: 6.066e+00,Time: 0.01\n",
      "adaptive_constant_val: 81.826810\n",
      "It: 3990, Loss: 7.318e+00, Loss_bcs: 1.583e+00, Loss_res: 5.736e+00,Time: 0.01\n",
      "adaptive_constant_val: 91.303431\n",
      "It: 4000, Loss: 5.572e+00, Loss_bcs: 1.930e+00, Loss_res: 3.642e+00,Time: 0.01\n",
      "adaptive_constant_val: 85.257673\n",
      "Relative L2 error_u: 8.93e-02\n",
      "Relative L2 error_f: 3.35e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-12-2023_03-10-37-640559_M2\n",
      "Final loss total loss: 6.261885e+00\n",
      "Final loss loss_res: 4.402531e+00\n",
      "Final loss loss_bc1: 9.419847e-03\n",
      "Final loss loss_bc2: 1.756742e-03\n",
      "Final loss loss_bc3: 3.302682e-03\n",
      "Final loss loss_bc4: 5.885288e-03\n",
      "average lambda_bc7.4362e+01\n",
      "average lambda_res1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Method:  mini_batch\n",
      "\n",
      "average of time_list: 42.70894646644592\n",
      "average of error_u_list: 0.08928561071182864\n",
      "average of error_v_list: 0.03345679011108052\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =4001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "a_1 = 1\n",
    "a_2 = 4\n",
    "\n",
    "# Parameter\n",
    "lam = 1.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[-1.0, -1.0], [1.0, -1.0]])\n",
    "bc2_coords = np.array([[1.0, -1.0], [1.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "bc4_coords = np.array([[-1.0, 1.0], [-1.0, -1.0]])\n",
    "\n",
    "dom_coords = np.array([[-1.0, -1.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "x1 = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x2 = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star, a_1, a_2)\n",
    "f_star = f(X_star, a_1, a_2, lam)\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = None\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'            # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False    # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, a_1, a_2, lam), name='Forcing')\n",
    "\n",
    "        # [elapsed, error_u , error_f ,  mode] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , \n",
    "        #                                                                stiff_ratio , X_star ,u_star , f_star , nIter ,bcbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    " #def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                model.trainmb(nIter, batch_size=mbbatch_size )\n",
    "            else:\n",
    "                model.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            f_pred = model.predict_r(X_star)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "            model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( x1 , x2 , X_star , u_star , u_pred , f_star , f_pred)\n",
    "\n",
    "            model.print(\"average lambda_bc\" , np.average(model.adpative_constant_log))\n",
    "            model.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "            time_list.append(elapsed)\n",
    "            error_u_list.append(error_u)\n",
    "            error_f_list.append(error_f)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_v_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_f_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_Helmholtz_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### database is a vailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21548/2533309064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predicted solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mU_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mF_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u_pred' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot ###\n",
    "\n",
    "# Exact solution & Predicted solution\n",
    "# Exact soluton\n",
    "U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "# Predicted solution\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Exact $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Predicted $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual loss & Boundary loss\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "gradients_res_list = []\n",
    "gradients_bcs_list = []\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "for j in range(num_hidden_layers):\n",
    "    gradient_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradient_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    gradients_res_list.append(gradient_res)\n",
    "    gradients_bcs_list.append(gradient_bcs)\n",
    "\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim([-3.0, 3.0])\n",
    "    ax.set_ylim([0,100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = mode.eigenvalue_log\n",
    "    eigenvalues_bcs_list = mode.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = mode.eigenvalue_res_log\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
