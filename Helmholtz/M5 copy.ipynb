{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "######################################################################################################\n",
    "def u(x, a_1, a_2):\n",
    "    return np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_xx(x, a_1, a_2):\n",
    "    return - (a_1 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_yy(x, a_1, a_2):\n",
    "    return - (a_2 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "# Forcing\n",
    "def f(x, a_1, a_2, lam):\n",
    "    return u_xx(x, a_1, a_2) + u_yy(x, a_1, a_2) + lam * u(x, a_1, a_2)\n",
    "\n",
    "def operator(u, x1, x2, lam, sigma_x1=1.0, sigma_x2=1.0):\n",
    "    u_x1 = tf.gradients(u, x1)[0] / sigma_x1\n",
    "    u_x2 = tf.gradients(u, x2)[0] / sigma_x2\n",
    "    u_xx1 = tf.gradients(u_x1, x1)[0] / sigma_x1\n",
    "    u_xx2 = tf.gradients(u_x2, x2)[0] / sigma_x2\n",
    "    residual = u_xx1 + u_xx2 + lam * u\n",
    "    return residual\n",
    "#######################################################################################################\n",
    "\n",
    "class Helmholtz2D:\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess):\n",
    "        # Normalization constants\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x1, self.sigma_x1 = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x2, self.sigma_x2 = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Helmoholtz constant\n",
    "        self.lam = tf.constant(lam, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = mode\n",
    "\n",
    "        # Record stiff ratio\n",
    "        # self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive constant\n",
    "        self.rate = 0.9\n",
    "        self.adaptive_constant_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # if mode in ['M3', 'M4']:\n",
    "        #     # Initialize encoder weights and biases\n",
    "        #     self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        #     self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x1_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # Define placeholder for adaptive constant\n",
    "        self.adaptive_constant_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred = self.net_u(self.x1_bc1_tf, self.x2_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.x1_bc2_tf, self.x2_bc2_tf)\n",
    "        self.u_bc3_pred = self.net_u(self.x1_bc3_tf, self.x2_bc3_tf)\n",
    "        self.u_bc4_pred = self.net_u(self.x1_bc4_tf, self.x2_bc4_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x1_u_tf, self.x2_u_tf)\n",
    "        self.r_pred = self.net_r(self.x1_r_tf, self.x2_r_tf)\n",
    "\n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_tf - self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_tf - self.u_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.u_bc3_tf - self.u_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.u_bc4_tf - self.u_bc4_pred))\n",
    "        self.loss_bcs = self.adaptive_constant_tf * (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bcs,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        # Compute and store the adaptive constant\n",
    "        self.adpative_constant_log = []\n",
    "        \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        self.mean_adaptive_constant_res_log = []\n",
    "        self.mean_adaptive_constant_bcs_log = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_mean(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_mean(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.adaptive_constant = self.max_grad_res / self.mean_grad_bcs\n",
    "\n",
    "        # # Stiff Ratio\n",
    "        # if self.stiff_ratio:\n",
    "        #     self.Hessian, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "        #     self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "        #     self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "        #     self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "        #     self.eigenvalue_log = []\n",
    "        #     self.eigenvalue_bcs_log = []\n",
    "        #     self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "    def lambda_balance(self  , term  ):\n",
    "            histoy_mean =  np.mean(self.loss_history[term])\n",
    "            m = 2.0 #len(self.loss_list)\n",
    "            num = np.exp(m*  np.mean(self.loss_history[term][-99::]) )#/(self.T * histoy_mean)) np.exp( )\n",
    "            denum = 0 \n",
    "            loss_list = [ \"loss_res\"   , \"loss_bcs\" ]\n",
    "            for  key in loss_list:\n",
    "                denum +=  np.exp( m* np.mean(self.loss_history[key][-99::]) + 1e-12 )# /(self.T * histoy_mean))  np.exp(self.loss_history[key][-1] )\n",
    "\n",
    "            return m * (num / denum)\n",
    "    \n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                [loss ,  loss_res,  loss_bcs,  loss_bc1 ,loss_bc2 , loss_bc3, loss_bc4] = batch_losses\n",
    "\n",
    " \n",
    "                self.print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e,Time: %.2f' % (it, loss, loss_bcs, loss_res, elapsed))\n",
    "\n",
    "\n",
    "                update_loss_res = self.lambda_balance( \"loss_res\"  )\n",
    "                update_loss_bcs = self.lambda_balance( \"loss_bcs\"  )\n",
    "                self.print('update_loss_res: ' , ( update_loss_res ))\n",
    "                self.print('update_loss_bcs: ' , ( update_loss_bcs))\n",
    "                \n",
    "            if it % 1000 == 0:\n",
    "\n",
    "                adaptive_constant = self.sess.run( self.adaptive_constant , tf_dict)\n",
    "\n",
    "                # print(\"adaptive_constant : \" ,adaptive_constant )\n",
    "                \n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_val = adaptive_constant * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_val\n",
    "\n",
    "                \n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                max_grad_res , mean_grad_bcs,  = self.sess.run( [ self.max_grad_res , self.mean_grad_bcs ], tf_dict)\n",
    "\n",
    "                self.mean_adaptive_constant_res_log.append( max_grad_res)\n",
    "                self.mean_adaptive_constant_bcs_log.append( mean_grad_bcs)\n",
    "\n",
    "                self.print('max_grad_res/mean_grad_bcs: {:.3e}'.format( max_grad_res/mean_grad_bcs))\n",
    "\n",
    "                self.print('mean_grad_bcs/max_grad_res: {:.3e}'.format( mean_grad_bcs/max_grad_res))\n",
    "\n",
    "                self.print('adaptive_constant_res_val: {:.3e}'.format( self.adaptive_constant_val))\n",
    "                self.adpative_constant_log.append( self.adaptive_constant_val)\n",
    "\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "\n",
    "     # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,  self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P), dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),  dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),  dtype='float32')\n",
    "\n",
    "        return H, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self,size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, x1, x2):\n",
    "        u = self.forward_pass(tf.concat([x1, x2], 1))\n",
    "        return u\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x1, x2):\n",
    "        u = self.net_u(x1, x2)\n",
    "        residual = self.operator(u, x1, x2,\n",
    "                                 self.lam,\n",
    "                                 self.sigma_x1,\n",
    "                                 self.sigma_x2)\n",
    "        return residual\n",
    "\n",
    "    # Feed minibatch\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter , bcbatch_size , fbatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Fetch boundary mini-batches\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], bcbatch_size)\n",
    "        X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], bcbatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, fbatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.u_bc3_tf: u_bc3_batch,\n",
    "                    self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.u_bc4_tf: u_bc4_batch,\n",
    "                    self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                    self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                    }\n",
    "\n",
    "\n",
    "        for it in range(nIter):\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                # self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                #     self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "\n",
    "                # self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "  # Trains the model by minimizing the MSE loss\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_u_tf: X_star[:, 0:1], self.x2_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_r_tf: X_star[:, 0:1], self.x2_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n",
    "\n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $f(x)$' , 'Predicted $f(x)$' , 'Absolute error']\n",
    "        data = [U_star , U_pred ,  np.abs(U_star - U_pred) , F_star , F_pred ,  np.abs(F_star - F_pred) ]\n",
    "        \n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        kwargs_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60),\n",
    "                cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, kwargs, minmax, title in zip(grid, data, kwargs_list, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "                #  print( z[timeStp,:,:])\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fontsize = 17\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([16,8])\n",
    "        ax.semilogy(self.mean_adaptive_constant_bcs_log, label=r'$\\bar{\\nabla_\\theta {u_{bc}}}$' , color = 'tab:green')\n",
    "        ax.semilogy(self.mean_adaptive_constant_res_log, label=r'$Max{\\nabla_\\theta {u_{phy}}}$' , color = 'tab:red')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
    "        ax.tick_params(labelsize=fontsize)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
    "\n",
    "        ax2 = ax.twinx() \n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_size_inches([15,8])\n",
    "    \n",
    "        ax2.semilogy(self.adpative_constant_log, label=r'$\\bar{\\lambda_{bc}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
    "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
    "        ax2.tick_params(labelsize=fontsize)\n",
    "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, lam ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f ,  model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_15734/414925196.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_15734/414925196.py:83: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_15734/414925196.py:84: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_15734/414925196.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 05:12:11.869963: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 05:12:11.897484: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-28 05:12:11.898187: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563059a4c4e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-28 05:12:11.898210: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-28 05:12:11.899702: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_15734/778320846.py:372: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_15734/778320846.py:119: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_15734/778320846.py:171: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_15734/778320846.py:173: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_15734/778320846.py:224: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 6.587e+03, Loss_bcs: 1.644e-01, Loss_res: 6.587e+03,Time: 2.55\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:232: RuntimeWarning: overflow encountered in exp\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:236: RuntimeWarning: overflow encountered in exp\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:238: RuntimeWarning: invalid value encountered in double_scalars\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "max_grad_res/mean_grad_bcs: 2.938e+00\n",
      "mean_grad_bcs/max_grad_res: 3.404e-01\n",
      "adaptive_constant_res_val: 1.194e+00\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, Loss: 6.305e+03, Loss_bcs: 1.824e-01, Loss_res: 6.305e+03,Time: 18.88\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:232: RuntimeWarning: overflow encountered in exp\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:236: RuntimeWarning: overflow encountered in exp\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:238: RuntimeWarning: invalid value encountered in double_scalars\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, Loss: 6.355e+03, Loss_bcs: 3.483e+00, Loss_res: 6.352e+03,Time: 21.11\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:232: RuntimeWarning: overflow encountered in exp\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:236: RuntimeWarning: overflow encountered in exp\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:238: RuntimeWarning: invalid value encountered in double_scalars\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "It: 300, Loss: 7.159e+03, Loss_bcs: 5.085e+00, Loss_res: 7.154e+03,Time: 22.61\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "It: 400, Loss: 6.261e+03, Loss_bcs: 1.414e+01, Loss_res: 6.247e+03,Time: 24.11\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "It: 500, Loss: 7.209e+03, Loss_bcs: 2.089e+01, Loss_res: 7.188e+03,Time: 25.56\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "It: 600, Loss: 7.208e+03, Loss_bcs: 2.642e+01, Loss_res: 7.181e+03,Time: 27.06\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "It: 700, Loss: 4.616e+03, Loss_bcs: 1.987e+01, Loss_res: 4.596e+03,Time: 28.48\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "It: 800, Loss: 4.530e+03, Loss_bcs: 2.352e+01, Loss_res: 4.507e+03,Time: 29.96\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "It: 900, Loss: 1.985e+03, Loss_bcs: 3.148e+01, Loss_res: 1.954e+03,Time: 31.45\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "It: 1000, Loss: 6.599e+02, Loss_bcs: 3.234e+01, Loss_res: 6.275e+02,Time: 33.01\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "max_grad_res/mean_grad_bcs: 2.015e+01\n",
      "mean_grad_bcs/max_grad_res: 4.962e-02\n",
      "adaptive_constant_res_val: 3.090e+00\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, Loss: 4.304e+02, Loss_bcs: 4.575e+01, Loss_res: 3.846e+02,Time: 35.30\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:232: RuntimeWarning: overflow encountered in exp\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:236: RuntimeWarning: overflow encountered in exp\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:238: RuntimeWarning: invalid value encountered in double_scalars\n",
      "update_loss_res: nanupdate_loss_bcs: 0.0000e+00\n",
      "It: 1200, Loss: 2.033e+02, Loss_bcs: 3.636e+01, Loss_res: 1.669e+02,Time: 36.67\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 6.0924e-174\n",
      "It: 1300, Loss: 1.678e+02, Loss_bcs: 3.252e+01, Loss_res: 1.353e+02,Time: 38.11\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 1.0404e-89\n",
      "It: 1400, Loss: 7.116e+01, Loss_bcs: 2.537e+01, Loss_res: 4.579e+01,Time: 39.45\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 1.2205e-57\n",
      "It: 1500, Loss: 8.555e+01, Loss_bcs: 2.043e+01, Loss_res: 6.513e+01,Time: 40.83\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 3.1411e-40\n",
      "It: 1600, Loss: 7.146e+01, Loss_bcs: 1.682e+01, Loss_res: 5.464e+01,Time: 42.26\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 1.0393e-36\n",
      "It: 1700, Loss: 4.966e+01, Loss_bcs: 1.327e+01, Loss_res: 3.640e+01,Time: 43.70\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 5.2771e-28\n",
      "It: 1800, Loss: 3.931e+01, Loss_bcs: 1.176e+01, Loss_res: 2.755e+01,Time: 45.11\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 2.7940e-23\n",
      "It: 1900, Loss: 5.320e+01, Loss_bcs: 9.218e+00, Loss_res: 4.398e+01,Time: 46.58\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 7.1702e-21\n",
      "It: 2000, Loss: 4.373e+01, Loss_bcs: 8.353e+00, Loss_res: 3.537e+01,Time: 48.04\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 4.2684e-19\n",
      "max_grad_res/mean_grad_bcs: 2.271e+01\n",
      "mean_grad_bcs/max_grad_res: 4.404e-02\n",
      "adaptive_constant_res_val: 5.052e+00\n",
      "It: 2100, Loss: 3.236e+01, Loss_bcs: 1.013e+01, Loss_res: 2.223e+01,Time: 49.60\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 3.0395e-14\n",
      "It: 2200, Loss: 3.331e+01, Loss_bcs: 8.580e+00, Loss_res: 2.473e+01,Time: 51.05\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 3.4122e-13\n",
      "It: 2300, Loss: 3.024e+01, Loss_bcs: 7.538e+00, Loss_res: 2.270e+01,Time: 52.63\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 2.1270e-13\n",
      "It: 2400, Loss: 2.353e+01, Loss_bcs: 6.110e+00, Loss_res: 1.742e+01,Time: 54.15\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 1.5430e-10\n",
      "It: 2500, Loss: 1.950e+01, Loss_bcs: 5.621e+00, Loss_res: 1.388e+01,Time: 55.60\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 8.3360e-11\n",
      "It: 2600, Loss: 2.160e+01, Loss_bcs: 4.563e+00, Loss_res: 1.704e+01,Time: 57.07\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 8.3688e-10\n",
      "It: 2700, Loss: 2.382e+01, Loss_bcs: 4.288e+00, Loss_res: 1.953e+01,Time: 58.57\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 8.1317e-10\n",
      "It: 2800, Loss: 1.843e+01, Loss_bcs: 3.724e+00, Loss_res: 1.471e+01,Time: 59.98\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 6.0687e-10\n",
      "It: 2900, Loss: 1.656e+01, Loss_bcs: 3.432e+00, Loss_res: 1.312e+01,Time: 61.41\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 1.5606e-08\n",
      "It: 3000, Loss: 1.445e+01, Loss_bcs: 2.834e+00, Loss_res: 1.161e+01,Time: 62.80\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 1.0309e-08\n",
      "max_grad_res/mean_grad_bcs: 1.836e+01\n",
      "mean_grad_bcs/max_grad_res: 5.446e-02\n",
      "adaptive_constant_res_val: 6.383e+00\n",
      "It: 3100, Loss: 1.383e+01, Loss_bcs: 2.742e+00, Loss_res: 1.109e+01,Time: 64.29\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 5.8293e-08\n",
      "It: 3200, Loss: 1.063e+01, Loss_bcs: 2.644e+00, Loss_res: 7.988e+00,Time: 65.71\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 2.0974e-07\n",
      "It: 3300, Loss: 1.460e+01, Loss_bcs: 2.292e+00, Loss_res: 1.230e+01,Time: 67.19\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 7.0865e-07\n",
      "It: 3400, Loss: 1.463e+01, Loss_bcs: 1.762e+00, Loss_res: 1.287e+01,Time: 68.65\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 1.7148e-07\n",
      "It: 3500, Loss: 1.416e+01, Loss_bcs: 1.827e+00, Loss_res: 1.234e+01,Time: 70.07\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 2.5190e-07\n",
      "It: 3600, Loss: 1.011e+01, Loss_bcs: 1.734e+00, Loss_res: 8.374e+00,Time: 71.45\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 6.1339e-07\n",
      "It: 3700, Loss: 8.869e+00, Loss_bcs: 1.472e+00, Loss_res: 7.397e+00,Time: 72.86\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 1.3557e-06\n",
      "It: 3800, Loss: 1.163e+01, Loss_bcs: 1.340e+00, Loss_res: 1.029e+01,Time: 74.36\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 1.6256e-06\n",
      "It: 3900, Loss: 9.086e+00, Loss_bcs: 1.253e+00, Loss_res: 7.833e+00,Time: 76.00\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 2.4507e-06\n",
      "It: 4000, Loss: 1.036e+01, Loss_bcs: 1.281e+00, Loss_res: 9.078e+00,Time: 77.50\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 9.5161e-06\n",
      "max_grad_res/mean_grad_bcs: 2.412e+01\n",
      "mean_grad_bcs/max_grad_res: 4.146e-02\n",
      "adaptive_constant_res_val: 8.156e+00\n",
      "It: 4100, Loss: 8.356e+00, Loss_bcs: 1.501e+00, Loss_res: 6.855e+00,Time: 79.00\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 3.5213e-05\n",
      "It: 4200, Loss: 8.706e+00, Loss_bcs: 1.455e+00, Loss_res: 7.252e+00,Time: 80.43\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 3.3616e-05\n",
      "It: 4300, Loss: 9.652e+00, Loss_bcs: 1.293e+00, Loss_res: 8.359e+00,Time: 82.10\n",
      "update_loss_res: 2.0000e+00\n",
      "update_loss_bcs: 3.6673e-05\n",
      "It: 4400, Loss: 6.171e+00, Loss_bcs: 1.448e+00, Loss_res: 4.722e+00,Time: 83.53\n",
      "update_loss_res: 1.9999e+00\n",
      "update_loss_bcs: 6.9074e-05\n",
      "It: 4500, Loss: 7.897e+00, Loss_bcs: 1.271e+00, Loss_res: 6.625e+00,Time: 84.94\n",
      "update_loss_res: 1.9999e+00\n",
      "update_loss_bcs: 1.1463e-04\n",
      "It: 4600, Loss: 8.257e+00, Loss_bcs: 1.462e+00, Loss_res: 6.796e+00,Time: 86.37\n",
      "update_loss_res: 1.9998e+00\n",
      "update_loss_bcs: 1.5305e-04\n",
      "It: 4700, Loss: 7.413e+00, Loss_bcs: 1.116e+00, Loss_res: 6.297e+00,Time: 87.75\n",
      "update_loss_res: 1.9997e+00\n",
      "update_loss_bcs: 2.7400e-04\n",
      "It: 4800, Loss: 6.166e+00, Loss_bcs: 1.102e+00, Loss_res: 5.064e+00,Time: 89.13\n",
      "update_loss_res: 1.9997e+00\n",
      "update_loss_bcs: 3.1152e-04\n",
      "It: 4900, Loss: 5.399e+00, Loss_bcs: 1.159e+00, Loss_res: 4.240e+00,Time: 90.58\n",
      "update_loss_res: 1.9993e+00\n",
      "update_loss_bcs: 7.1152e-04\n",
      "It: 5000, Loss: 6.326e+00, Loss_bcs: 1.239e+00, Loss_res: 5.087e+00,Time: 92.05\n",
      "update_loss_res: 1.9988e+00\n",
      "update_loss_bcs: 1.1582e-03\n",
      "max_grad_res/mean_grad_bcs: 2.912e+01\n",
      "mean_grad_bcs/max_grad_res: 3.434e-02\n",
      "adaptive_constant_res_val: 1.025e+01\n",
      "It: 5100, Loss: 6.746e+00, Loss_bcs: 1.266e+00, Loss_res: 5.480e+00,Time: 93.50\n",
      "update_loss_res: 1.9982e+00\n",
      "update_loss_bcs: 1.7789e-03\n",
      "It: 5200, Loss: 6.647e+00, Loss_bcs: 1.264e+00, Loss_res: 5.382e+00,Time: 94.91\n",
      "update_loss_res: 1.9992e+00\n",
      "update_loss_bcs: 7.5669e-04\n",
      "It: 5300, Loss: 4.790e+00, Loss_bcs: 1.125e+00, Loss_res: 3.665e+00,Time: 96.30\n",
      "update_loss_res: 1.9982e+00\n",
      "update_loss_bcs: 1.8428e-03\n",
      "It: 5400, Loss: 6.692e+00, Loss_bcs: 1.169e+00, Loss_res: 5.523e+00,Time: 97.68\n",
      "update_loss_res: 1.9987e+00\n",
      "update_loss_bcs: 1.3083e-03\n",
      "It: 5500, Loss: 5.737e+00, Loss_bcs: 1.228e+00, Loss_res: 4.509e+00,Time: 99.13\n",
      "update_loss_res: 1.9996e+00\n",
      "update_loss_bcs: 3.9726e-04\n",
      "It: 5600, Loss: 4.955e+00, Loss_bcs: 1.150e+00, Loss_res: 3.805e+00,Time: 100.55\n",
      "update_loss_res: 1.9963e+00\n",
      "update_loss_bcs: 3.6540e-03\n",
      "It: 5700, Loss: 4.291e+00, Loss_bcs: 1.202e+00, Loss_res: 3.089e+00,Time: 101.97\n",
      "update_loss_res: 1.9952e+00\n",
      "update_loss_bcs: 4.8201e-03\n",
      "It: 5800, Loss: 4.557e+00, Loss_bcs: 9.594e-01, Loss_res: 3.597e+00,Time: 103.35\n",
      "update_loss_res: 1.9933e+00\n",
      "update_loss_bcs: 6.6934e-03\n",
      "It: 5900, Loss: 3.164e+00, Loss_bcs: 9.453e-01, Loss_res: 2.219e+00,Time: 104.74\n",
      "update_loss_res: 1.9982e+00\n",
      "update_loss_bcs: 1.7922e-03\n",
      "It: 6000, Loss: 4.779e+00, Loss_bcs: 1.042e+00, Loss_res: 3.737e+00,Time: 106.13\n",
      "update_loss_res: 1.9954e+00\n",
      "update_loss_bcs: 4.5686e-03\n",
      "max_grad_res/mean_grad_bcs: 1.749e+01\n",
      "mean_grad_bcs/max_grad_res: 5.716e-02\n",
      "adaptive_constant_res_val: 1.098e+01\n",
      "It: 6100, Loss: 6.246e+00, Loss_bcs: 9.794e-01, Loss_res: 5.267e+00,Time: 107.62\n",
      "update_loss_res: 1.9946e+00\n",
      "update_loss_bcs: 5.4215e-03\n",
      "It: 6200, Loss: 5.088e+00, Loss_bcs: 1.109e+00, Loss_res: 3.979e+00,Time: 109.06\n",
      "update_loss_res: 1.9874e+00\n",
      "update_loss_bcs: 1.2630e-02\n",
      "It: 6300, Loss: 4.325e+00, Loss_bcs: 9.106e-01, Loss_res: 3.414e+00,Time: 110.45\n",
      "update_loss_res: 1.9873e+00\n",
      "update_loss_bcs: 1.2711e-02\n",
      "It: 6400, Loss: 4.632e+00, Loss_bcs: 1.068e+00, Loss_res: 3.564e+00,Time: 111.86\n",
      "update_loss_res: 1.9845e+00\n",
      "update_loss_bcs: 1.5546e-02\n",
      "It: 6500, Loss: 3.473e+00, Loss_bcs: 8.518e-01, Loss_res: 2.621e+00,Time: 113.29\n",
      "update_loss_res: 1.9795e+00\n",
      "update_loss_bcs: 2.0492e-02\n",
      "It: 6600, Loss: 3.365e+00, Loss_bcs: 9.752e-01, Loss_res: 2.390e+00,Time: 114.74\n",
      "update_loss_res: 1.9925e+00\n",
      "update_loss_bcs: 7.4651e-03\n",
      "It: 6700, Loss: 4.693e+00, Loss_bcs: 9.553e-01, Loss_res: 3.737e+00,Time: 116.13\n",
      "update_loss_res: 1.9854e+00\n",
      "update_loss_bcs: 1.4635e-02\n",
      "It: 6800, Loss: 4.307e+00, Loss_bcs: 7.876e-01, Loss_res: 3.519e+00,Time: 117.54\n",
      "update_loss_res: 1.9734e+00\n",
      "update_loss_bcs: 2.6603e-02\n",
      "It: 6900, Loss: 4.160e+00, Loss_bcs: 8.643e-01, Loss_res: 3.296e+00,Time: 118.90\n",
      "update_loss_res: 1.9733e+00\n",
      "update_loss_bcs: 2.6742e-02\n",
      "It: 7000, Loss: 4.776e+00, Loss_bcs: 9.329e-01, Loss_res: 3.843e+00,Time: 120.27\n",
      "update_loss_res: 1.9946e+00\n",
      "update_loss_bcs: 5.4319e-03\n",
      "max_grad_res/mean_grad_bcs: 3.449e+01\n",
      "mean_grad_bcs/max_grad_res: 2.900e-02\n",
      "adaptive_constant_res_val: 1.333e+01\n",
      "It: 7100, Loss: 3.296e+00, Loss_bcs: 1.027e+00, Loss_res: 2.269e+00,Time: 121.75\n",
      "update_loss_res: 1.9607e+00\n",
      "update_loss_bcs: 3.9307e-02\n",
      "It: 7200, Loss: 3.299e+00, Loss_bcs: 9.310e-01, Loss_res: 2.368e+00,Time: 123.13\n",
      "update_loss_res: 1.9746e+00\n",
      "update_loss_bcs: 2.5434e-02\n",
      "It: 7300, Loss: 3.679e+00, Loss_bcs: 9.036e-01, Loss_res: 2.775e+00,Time: 124.51\n",
      "update_loss_res: 1.9729e+00\n",
      "update_loss_bcs: 2.7108e-02\n",
      "It: 7400, Loss: 3.402e+00, Loss_bcs: 8.551e-01, Loss_res: 2.547e+00,Time: 125.89\n",
      "update_loss_res: 1.9745e+00\n",
      "update_loss_bcs: 2.5472e-02\n",
      "It: 7500, Loss: 3.670e+00, Loss_bcs: 7.909e-01, Loss_res: 2.879e+00,Time: 127.27\n",
      "update_loss_res: 1.9438e+00\n",
      "update_loss_bcs: 5.6232e-02\n",
      "It: 7600, Loss: 2.559e+00, Loss_bcs: 7.974e-01, Loss_res: 1.762e+00,Time: 128.67\n",
      "update_loss_res: 1.9578e+00\n",
      "update_loss_bcs: 4.2221e-02\n",
      "It: 7700, Loss: 3.296e+00, Loss_bcs: 8.604e-01, Loss_res: 2.436e+00,Time: 130.05\n",
      "update_loss_res: 1.9652e+00\n",
      "update_loss_bcs: 3.4793e-02\n",
      "It: 7800, Loss: 3.690e+00, Loss_bcs: 9.238e-01, Loss_res: 2.766e+00,Time: 131.45\n",
      "update_loss_res: 1.9840e+00\n",
      "update_loss_bcs: 1.6032e-02\n",
      "It: 7900, Loss: 2.654e+00, Loss_bcs: 7.225e-01, Loss_res: 1.932e+00,Time: 132.84\n",
      "update_loss_res: 1.9487e+00\n",
      "update_loss_bcs: 5.1287e-02\n",
      "It: 8000, Loss: 4.644e+00, Loss_bcs: 6.832e-01, Loss_res: 3.961e+00,Time: 134.21\n",
      "update_loss_res: 1.9638e+00\n",
      "update_loss_bcs: 3.6212e-02\n",
      "max_grad_res/mean_grad_bcs: 2.598e+01\n",
      "mean_grad_bcs/max_grad_res: 3.848e-02\n",
      "adaptive_constant_res_val: 1.459e+01\n",
      "It: 8100, Loss: 2.935e+00, Loss_bcs: 8.010e-01, Loss_res: 2.134e+00,Time: 135.62\n",
      "update_loss_res: 1.9388e+00\n",
      "update_loss_bcs: 6.1232e-02\n",
      "It: 8200, Loss: 2.478e+00, Loss_bcs: 6.951e-01, Loss_res: 1.783e+00,Time: 137.05\n",
      "update_loss_res: 1.9238e+00\n",
      "update_loss_bcs: 7.6231e-02\n",
      "It: 8300, Loss: 2.521e+00, Loss_bcs: 6.215e-01, Loss_res: 1.900e+00,Time: 138.42\n",
      "update_loss_res: 1.9244e+00\n",
      "update_loss_bcs: 7.5605e-02\n",
      "It: 8400, Loss: 2.473e+00, Loss_bcs: 7.175e-01, Loss_res: 1.755e+00,Time: 139.78\n",
      "update_loss_res: 1.9171e+00\n",
      "update_loss_bcs: 8.2874e-02\n",
      "It: 8500, Loss: 2.622e+00, Loss_bcs: 6.282e-01, Loss_res: 1.994e+00,Time: 141.16\n",
      "update_loss_res: 1.9106e+00\n",
      "update_loss_bcs: 8.9370e-02\n",
      "It: 8600, Loss: 2.450e+00, Loss_bcs: 6.362e-01, Loss_res: 1.813e+00,Time: 142.61\n",
      "update_loss_res: 1.9485e+00\n",
      "update_loss_bcs: 5.1484e-02\n",
      "It: 8700, Loss: 2.807e+00, Loss_bcs: 5.454e-01, Loss_res: 2.262e+00,Time: 144.03\n",
      "update_loss_res: 1.9096e+00\n",
      "update_loss_bcs: 9.0408e-02\n",
      "It: 8800, Loss: 2.775e+00, Loss_bcs: 6.996e-01, Loss_res: 2.076e+00,Time: 145.44\n",
      "update_loss_res: 1.8816e+00\n",
      "update_loss_bcs: 1.1839e-01\n",
      "It: 8900, Loss: 3.035e+00, Loss_bcs: 6.198e-01, Loss_res: 2.415e+00,Time: 146.90\n",
      "update_loss_res: 1.9042e+00\n",
      "update_loss_bcs: 9.5837e-02\n",
      "It: 9000, Loss: 2.840e+00, Loss_bcs: 5.842e-01, Loss_res: 2.256e+00,Time: 148.32\n",
      "update_loss_res: 1.9159e+00\n",
      "update_loss_bcs: 8.4083e-02\n",
      "max_grad_res/mean_grad_bcs: 8.119e+00\n",
      "mean_grad_bcs/max_grad_res: 1.232e-01\n",
      "adaptive_constant_res_val: 1.395e+01\n",
      "It: 9100, Loss: 2.259e+00, Loss_bcs: 5.159e-01, Loss_res: 1.743e+00,Time: 149.73\n",
      "update_loss_res: 1.9050e+00\n",
      "update_loss_bcs: 9.4957e-02\n",
      "It: 9200, Loss: 2.011e+00, Loss_bcs: 5.397e-01, Loss_res: 1.472e+00,Time: 151.16\n",
      "update_loss_res: 1.9117e+00\n",
      "update_loss_bcs: 8.8286e-02\n",
      "It: 9300, Loss: 2.966e+00, Loss_bcs: 5.097e-01, Loss_res: 2.456e+00,Time: 152.54\n",
      "update_loss_res: 1.9142e+00\n",
      "update_loss_bcs: 8.5801e-02\n",
      "It: 9400, Loss: 1.859e+00, Loss_bcs: 4.630e-01, Loss_res: 1.396e+00,Time: 153.91\n",
      "update_loss_res: 1.8832e+00\n",
      "update_loss_bcs: 1.1682e-01\n",
      "It: 9500, Loss: 2.956e+00, Loss_bcs: 4.663e-01, Loss_res: 2.490e+00,Time: 155.27\n",
      "update_loss_res: 1.8958e+00\n",
      "update_loss_bcs: 1.0422e-01\n",
      "It: 9600, Loss: 2.963e+00, Loss_bcs: 5.001e-01, Loss_res: 2.463e+00,Time: 156.62\n",
      "update_loss_res: 1.8885e+00\n",
      "update_loss_bcs: 1.1152e-01\n",
      "It: 9700, Loss: 1.827e+00, Loss_bcs: 4.446e-01, Loss_res: 1.383e+00,Time: 158.01\n",
      "update_loss_res: 1.8634e+00\n",
      "update_loss_bcs: 1.3662e-01\n",
      "It: 9800, Loss: 2.166e+00, Loss_bcs: 3.958e-01, Loss_res: 1.770e+00,Time: 159.40\n",
      "update_loss_res: 1.8894e+00\n",
      "update_loss_bcs: 1.1065e-01\n",
      "It: 9900, Loss: 2.125e+00, Loss_bcs: 3.916e-01, Loss_res: 1.734e+00,Time: 160.75\n",
      "update_loss_res: 1.8566e+00\n",
      "update_loss_bcs: 1.4336e-01\n",
      "It: 10000, Loss: 5.971e+00, Loss_bcs: 4.585e-01, Loss_res: 5.513e+00,Time: 162.16\n",
      "update_loss_res: 1.8889e+00\n",
      "update_loss_bcs: 1.1110e-01\n",
      "max_grad_res/mean_grad_bcs: 8.376e+00\n",
      "mean_grad_bcs/max_grad_res: 1.194e-01\n",
      "adaptive_constant_res_val: 1.339e+01\n",
      "It: 10100, Loss: 1.830e+00, Loss_bcs: 3.503e-01, Loss_res: 1.480e+00,Time: 163.56\n",
      "update_loss_res: 1.8552e+00\n",
      "update_loss_bcs: 1.4476e-01\n",
      "It: 10200, Loss: 2.832e+00, Loss_bcs: 3.904e-01, Loss_res: 2.442e+00,Time: 164.92\n",
      "update_loss_res: 1.9218e+00\n",
      "update_loss_bcs: 7.8174e-02\n",
      "It: 10300, Loss: 1.878e+00, Loss_bcs: 3.930e-01, Loss_res: 1.485e+00,Time: 166.31\n",
      "update_loss_res: 1.8782e+00\n",
      "update_loss_bcs: 1.2178e-01\n",
      "It: 10400, Loss: 2.185e+00, Loss_bcs: 4.689e-01, Loss_res: 1.716e+00,Time: 167.69\n",
      "update_loss_res: 1.8479e+00\n",
      "update_loss_bcs: 1.5207e-01\n",
      "It: 10500, Loss: 1.705e+00, Loss_bcs: 3.606e-01, Loss_res: 1.344e+00,Time: 169.03\n",
      "update_loss_res: 1.8128e+00\n",
      "update_loss_bcs: 1.8720e-01\n",
      "It: 10600, Loss: 1.417e+00, Loss_bcs: 3.244e-01, Loss_res: 1.093e+00,Time: 170.39\n",
      "update_loss_res: 1.8512e+00\n",
      "update_loss_bcs: 1.4876e-01\n",
      "It: 10700, Loss: 1.576e+00, Loss_bcs: 3.287e-01, Loss_res: 1.248e+00,Time: 171.75\n",
      "update_loss_res: 1.8076e+00\n",
      "update_loss_bcs: 1.9239e-01\n",
      "It: 10800, Loss: 1.509e+00, Loss_bcs: 3.676e-01, Loss_res: 1.142e+00,Time: 173.23\n",
      "update_loss_res: 1.7911e+00\n",
      "update_loss_bcs: 2.0891e-01\n",
      "It: 10900, Loss: 1.918e+00, Loss_bcs: 3.447e-01, Loss_res: 1.573e+00,Time: 174.59\n",
      "update_loss_res: 1.8447e+00\n",
      "update_loss_bcs: 1.5528e-01\n",
      "It: 11000, Loss: 2.106e+00, Loss_bcs: 3.470e-01, Loss_res: 1.759e+00,Time: 175.97\n",
      "update_loss_res: 1.8903e+00\n",
      "update_loss_bcs: 1.0973e-01\n",
      "max_grad_res/mean_grad_bcs: 7.779e+00\n",
      "mean_grad_bcs/max_grad_res: 1.285e-01\n",
      "adaptive_constant_res_val: 1.283e+01\n",
      "It: 11100, Loss: 1.912e+00, Loss_bcs: 2.941e-01, Loss_res: 1.617e+00,Time: 177.38\n",
      "update_loss_res: 1.8179e+00\n",
      "update_loss_bcs: 1.8212e-01\n",
      "It: 11200, Loss: 1.988e+00, Loss_bcs: 3.379e-01, Loss_res: 1.650e+00,Time: 178.72\n",
      "update_loss_res: 1.7993e+00\n",
      "update_loss_bcs: 2.0069e-01\n",
      "It: 11300, Loss: 1.468e+00, Loss_bcs: 3.645e-01, Loss_res: 1.103e+00,Time: 180.12\n",
      "update_loss_res: 1.8242e+00\n",
      "update_loss_bcs: 1.7577e-01\n",
      "It: 11400, Loss: 1.552e+00, Loss_bcs: 3.277e-01, Loss_res: 1.224e+00,Time: 181.45\n",
      "update_loss_res: 1.7489e+00\n",
      "update_loss_bcs: 2.5110e-01\n",
      "It: 11500, Loss: 1.779e+00, Loss_bcs: 3.350e-01, Loss_res: 1.444e+00,Time: 182.80\n",
      "update_loss_res: 1.8200e+00\n",
      "update_loss_bcs: 1.8001e-01\n",
      "It: 11600, Loss: 1.581e+00, Loss_bcs: 3.413e-01, Loss_res: 1.240e+00,Time: 184.16\n",
      "update_loss_res: 1.7955e+00\n",
      "update_loss_bcs: 2.0451e-01\n",
      "It: 11700, Loss: 1.218e+00, Loss_bcs: 2.730e-01, Loss_res: 9.450e-01,Time: 185.52\n",
      "update_loss_res: 1.7135e+00\n",
      "update_loss_bcs: 2.8654e-01\n",
      "It: 11800, Loss: 1.488e+00, Loss_bcs: 2.810e-01, Loss_res: 1.207e+00,Time: 186.91\n",
      "update_loss_res: 1.7992e+00\n",
      "update_loss_bcs: 2.0079e-01\n",
      "It: 11900, Loss: 1.873e+00, Loss_bcs: 2.867e-01, Loss_res: 1.586e+00,Time: 188.33\n",
      "update_loss_res: 1.7689e+00\n",
      "update_loss_bcs: 2.3110e-01\n",
      "It: 12000, Loss: 2.365e+00, Loss_bcs: 2.606e-01, Loss_res: 2.104e+00,Time: 189.71\n",
      "update_loss_res: 1.7681e+00\n",
      "update_loss_bcs: 2.3188e-01\n",
      "max_grad_res/mean_grad_bcs: 3.378e+01\n",
      "mean_grad_bcs/max_grad_res: 2.960e-02\n",
      "adaptive_constant_res_val: 1.492e+01\n",
      "It: 12100, Loss: 1.582e+00, Loss_bcs: 2.806e-01, Loss_res: 1.301e+00,Time: 191.12\n",
      "update_loss_res: 1.7045e+00\n",
      "update_loss_bcs: 2.9553e-01\n",
      "It: 12200, Loss: 1.490e+00, Loss_bcs: 2.872e-01, Loss_res: 1.203e+00,Time: 192.50\n",
      "update_loss_res: 1.7919e+00\n",
      "update_loss_bcs: 2.0809e-01\n",
      "It: 12300, Loss: 1.172e+00, Loss_bcs: 2.794e-01, Loss_res: 8.921e-01,Time: 193.87\n",
      "update_loss_res: 1.6947e+00\n",
      "update_loss_bcs: 3.0525e-01\n",
      "It: 12400, Loss: 2.539e+00, Loss_bcs: 2.794e-01, Loss_res: 2.259e+00,Time: 195.29\n",
      "update_loss_res: 1.7281e+00\n",
      "update_loss_bcs: 2.7189e-01\n",
      "It: 12500, Loss: 1.476e+00, Loss_bcs: 2.628e-01, Loss_res: 1.213e+00,Time: 196.64\n",
      "update_loss_res: 1.7474e+00\n",
      "update_loss_bcs: 2.5264e-01\n",
      "It: 12600, Loss: 1.722e+00, Loss_bcs: 2.385e-01, Loss_res: 1.483e+00,Time: 198.03\n",
      "update_loss_res: 1.6735e+00\n",
      "update_loss_bcs: 3.2647e-01\n",
      "It: 12700, Loss: 1.182e+00, Loss_bcs: 2.672e-01, Loss_res: 9.147e-01,Time: 199.40\n",
      "update_loss_res: 1.7166e+00\n",
      "update_loss_bcs: 2.8340e-01\n",
      "It: 12800, Loss: 1.312e+00, Loss_bcs: 2.736e-01, Loss_res: 1.039e+00,Time: 200.76\n",
      "update_loss_res: 1.6859e+00\n",
      "update_loss_bcs: 3.1405e-01\n",
      "It: 12900, Loss: 1.289e+00, Loss_bcs: 2.940e-01, Loss_res: 9.951e-01,Time: 202.20\n",
      "update_loss_res: 1.7378e+00\n",
      "update_loss_bcs: 2.6220e-01\n",
      "It: 13000, Loss: 1.276e+00, Loss_bcs: 2.587e-01, Loss_res: 1.017e+00,Time: 203.57\n",
      "update_loss_res: 1.6887e+00\n",
      "update_loss_bcs: 3.1132e-01\n",
      "max_grad_res/mean_grad_bcs: 1.842e+01\n",
      "mean_grad_bcs/max_grad_res: 5.430e-02\n",
      "adaptive_constant_res_val: 1.527e+01\n",
      "It: 13100, Loss: 1.061e+00, Loss_bcs: 2.467e-01, Loss_res: 8.140e-01,Time: 204.95\n",
      "update_loss_res: 1.6797e+00\n",
      "update_loss_bcs: 3.2026e-01\n",
      "It: 13200, Loss: 1.939e+00, Loss_bcs: 2.475e-01, Loss_res: 1.692e+00,Time: 206.33\n",
      "update_loss_res: 1.7436e+00\n",
      "update_loss_bcs: 2.5643e-01\n",
      "It: 13300, Loss: 1.498e+00, Loss_bcs: 2.859e-01, Loss_res: 1.212e+00,Time: 207.71\n",
      "update_loss_res: 1.6797e+00\n",
      "update_loss_bcs: 3.2029e-01\n",
      "It: 13400, Loss: 1.354e+00, Loss_bcs: 2.775e-01, Loss_res: 1.077e+00,Time: 209.15\n",
      "update_loss_res: 1.7217e+00\n",
      "update_loss_bcs: 2.7825e-01\n",
      "It: 13500, Loss: 1.834e+00, Loss_bcs: 2.511e-01, Loss_res: 1.583e+00,Time: 210.54\n",
      "update_loss_res: 1.6398e+00\n",
      "update_loss_bcs: 3.6021e-01\n",
      "It: 13600, Loss: 1.077e+00, Loss_bcs: 2.497e-01, Loss_res: 8.273e-01,Time: 211.91\n",
      "update_loss_res: 1.6715e+00\n",
      "update_loss_bcs: 3.2853e-01\n",
      "It: 13700, Loss: 1.459e+00, Loss_bcs: 2.649e-01, Loss_res: 1.194e+00,Time: 213.33\n",
      "update_loss_res: 1.6631e+00\n",
      "update_loss_bcs: 3.3693e-01\n",
      "It: 13800, Loss: 1.371e+00, Loss_bcs: 2.476e-01, Loss_res: 1.123e+00,Time: 214.70\n",
      "update_loss_res: 1.6492e+00\n",
      "update_loss_bcs: 3.5078e-01\n",
      "It: 13900, Loss: 1.539e+00, Loss_bcs: 2.338e-01, Loss_res: 1.305e+00,Time: 216.09\n",
      "update_loss_res: 1.6512e+00\n",
      "update_loss_bcs: 3.4880e-01\n",
      "It: 14000, Loss: 1.146e+00, Loss_bcs: 1.943e-01, Loss_res: 9.516e-01,Time: 217.52\n",
      "update_loss_res: 1.6304e+00\n",
      "update_loss_bcs: 3.6960e-01\n",
      "max_grad_res/mean_grad_bcs: 2.215e+01\n",
      "mean_grad_bcs/max_grad_res: 4.514e-02\n",
      "adaptive_constant_res_val: 1.596e+01\n",
      "It: 14100, Loss: 2.248e+00, Loss_bcs: 2.793e-01, Loss_res: 1.968e+00,Time: 218.95\n",
      "update_loss_res: 1.7037e+00\n",
      "update_loss_bcs: 2.9629e-01\n",
      "It: 14200, Loss: 9.448e-01, Loss_bcs: 2.216e-01, Loss_res: 7.231e-01,Time: 220.34\n",
      "update_loss_res: 1.5978e+00\n",
      "update_loss_bcs: 4.0220e-01\n",
      "It: 14300, Loss: 1.179e+00, Loss_bcs: 2.095e-01, Loss_res: 9.693e-01,Time: 221.70\n",
      "update_loss_res: 1.5942e+00\n",
      "update_loss_bcs: 4.0580e-01\n",
      "It: 14400, Loss: 1.467e+00, Loss_bcs: 2.118e-01, Loss_res: 1.256e+00,Time: 223.13\n",
      "update_loss_res: 1.6106e+00\n",
      "update_loss_bcs: 3.8936e-01\n",
      "It: 14500, Loss: 9.814e-01, Loss_bcs: 2.026e-01, Loss_res: 7.788e-01,Time: 224.55\n",
      "update_loss_res: 1.6206e+00\n",
      "update_loss_bcs: 3.7939e-01\n",
      "It: 14600, Loss: 8.736e-01, Loss_bcs: 2.112e-01, Loss_res: 6.624e-01,Time: 225.94\n",
      "update_loss_res: 1.5856e+00\n",
      "update_loss_bcs: 4.1445e-01\n",
      "It: 14700, Loss: 1.175e+00, Loss_bcs: 2.321e-01, Loss_res: 9.426e-01,Time: 227.48\n",
      "update_loss_res: 1.5863e+00\n",
      "update_loss_bcs: 4.1366e-01\n",
      "It: 14800, Loss: 1.350e+00, Loss_bcs: 1.962e-01, Loss_res: 1.154e+00,Time: 228.87\n",
      "update_loss_res: 1.5821e+00\n",
      "update_loss_bcs: 4.1791e-01\n",
      "It: 14900, Loss: 9.318e-01, Loss_bcs: 2.149e-01, Loss_res: 7.169e-01,Time: 230.27\n",
      "update_loss_res: 1.6219e+00\n",
      "update_loss_bcs: 3.7814e-01\n",
      "It: 15000, Loss: 8.219e-01, Loss_bcs: 2.129e-01, Loss_res: 6.089e-01,Time: 231.74\n",
      "update_loss_res: 1.6250e+00\n",
      "update_loss_bcs: 3.7500e-01\n",
      "max_grad_res/mean_grad_bcs: 8.927e+00\n",
      "mean_grad_bcs/max_grad_res: 1.120e-01\n",
      "adaptive_constant_res_val: 1.526e+01\n",
      "It: 15100, Loss: 1.008e+00, Loss_bcs: 2.145e-01, Loss_res: 7.935e-01,Time: 233.23\n",
      "update_loss_res: 1.6325e+00\n",
      "update_loss_bcs: 3.6755e-01\n",
      "It: 15200, Loss: 1.255e+00, Loss_bcs: 2.029e-01, Loss_res: 1.052e+00,Time: 234.90\n",
      "update_loss_res: 1.5648e+00\n",
      "update_loss_bcs: 4.3520e-01\n",
      "It: 15300, Loss: 1.065e+00, Loss_bcs: 2.109e-01, Loss_res: 8.539e-01,Time: 236.49\n",
      "update_loss_res: 1.5650e+00\n",
      "update_loss_bcs: 4.3501e-01\n",
      "It: 15400, Loss: 7.176e-01, Loss_bcs: 1.818e-01, Loss_res: 5.358e-01,Time: 238.05\n",
      "update_loss_res: 1.5676e+00\n",
      "update_loss_bcs: 4.3242e-01\n",
      "It: 15500, Loss: 1.368e+00, Loss_bcs: 2.000e-01, Loss_res: 1.168e+00,Time: 239.52\n",
      "update_loss_res: 1.5430e+00\n",
      "update_loss_bcs: 4.5697e-01\n",
      "It: 15600, Loss: 8.784e-01, Loss_bcs: 2.339e-01, Loss_res: 6.445e-01,Time: 241.03\n",
      "update_loss_res: 1.5122e+00\n",
      "update_loss_bcs: 4.8784e-01\n",
      "It: 15700, Loss: 1.305e+00, Loss_bcs: 1.776e-01, Loss_res: 1.127e+00,Time: 242.53\n",
      "update_loss_res: 1.6062e+00\n",
      "update_loss_bcs: 3.9381e-01\n",
      "It: 15800, Loss: 1.415e+00, Loss_bcs: 1.829e-01, Loss_res: 1.232e+00,Time: 244.55\n",
      "update_loss_res: 1.5411e+00\n",
      "update_loss_bcs: 4.5891e-01\n",
      "It: 15900, Loss: 1.135e+00, Loss_bcs: 1.975e-01, Loss_res: 9.374e-01,Time: 246.20\n",
      "update_loss_res: 1.5735e+00\n",
      "update_loss_bcs: 4.2655e-01\n",
      "It: 16000, Loss: 8.488e-01, Loss_bcs: 1.829e-01, Loss_res: 6.659e-01,Time: 247.70\n",
      "update_loss_res: 1.6157e+00\n",
      "update_loss_bcs: 3.8432e-01\n",
      "max_grad_res/mean_grad_bcs: 3.498e+01\n",
      "mean_grad_bcs/max_grad_res: 2.859e-02\n",
      "adaptive_constant_res_val: 1.723e+01\n",
      "It: 16100, Loss: 1.325e+00, Loss_bcs: 2.103e-01, Loss_res: 1.115e+00,Time: 249.16\n",
      "update_loss_res: 1.5519e+00\n",
      "update_loss_bcs: 4.4812e-01\n",
      "It: 16200, Loss: 7.405e-01, Loss_bcs: 2.089e-01, Loss_res: 5.316e-01,Time: 250.66\n",
      "update_loss_res: 1.5186e+00\n",
      "update_loss_bcs: 4.8143e-01\n",
      "It: 16300, Loss: 9.585e-01, Loss_bcs: 2.071e-01, Loss_res: 7.513e-01,Time: 252.06\n",
      "update_loss_res: 1.5167e+00\n",
      "update_loss_bcs: 4.8332e-01\n",
      "It: 16400, Loss: 7.419e-01, Loss_bcs: 1.797e-01, Loss_res: 5.621e-01,Time: 253.51\n",
      "update_loss_res: 1.5034e+00\n",
      "update_loss_bcs: 4.9658e-01\n",
      "It: 16500, Loss: 8.593e-01, Loss_bcs: 1.993e-01, Loss_res: 6.600e-01,Time: 254.92\n",
      "update_loss_res: 1.5047e+00\n",
      "update_loss_bcs: 4.9526e-01\n",
      "It: 16600, Loss: 1.143e+00, Loss_bcs: 1.951e-01, Loss_res: 9.478e-01,Time: 256.36\n",
      "update_loss_res: 1.5314e+00\n",
      "update_loss_bcs: 4.6862e-01\n",
      "It: 16700, Loss: 9.012e-01, Loss_bcs: 1.892e-01, Loss_res: 7.120e-01,Time: 257.74\n",
      "update_loss_res: 1.5174e+00\n",
      "update_loss_bcs: 4.8258e-01\n",
      "It: 16800, Loss: 1.231e+00, Loss_bcs: 1.598e-01, Loss_res: 1.071e+00,Time: 259.11\n",
      "update_loss_res: 1.5451e+00\n",
      "update_loss_bcs: 4.5493e-01\n",
      "It: 16900, Loss: 9.552e-01, Loss_bcs: 2.042e-01, Loss_res: 7.510e-01,Time: 260.54\n",
      "update_loss_res: 1.5111e+00\n",
      "update_loss_bcs: 4.8895e-01\n",
      "It: 17000, Loss: 9.415e-01, Loss_bcs: 1.682e-01, Loss_res: 7.734e-01,Time: 262.00\n",
      "update_loss_res: 1.4816e+00\n",
      "update_loss_bcs: 5.1837e-01\n",
      "max_grad_res/mean_grad_bcs: 2.410e+01\n",
      "mean_grad_bcs/max_grad_res: 4.149e-02\n",
      "adaptive_constant_res_val: 1.792e+01\n",
      "It: 17100, Loss: 1.100e+00, Loss_bcs: 1.755e-01, Loss_res: 9.243e-01,Time: 263.58\n",
      "update_loss_res: 1.4894e+00\n",
      "update_loss_bcs: 5.1064e-01\n",
      "It: 17200, Loss: 1.098e+00, Loss_bcs: 2.166e-01, Loss_res: 8.817e-01,Time: 265.12\n",
      "update_loss_res: 1.4528e+00\n",
      "update_loss_bcs: 5.4724e-01\n",
      "It: 17300, Loss: 1.052e+00, Loss_bcs: 2.147e-01, Loss_res: 8.371e-01,Time: 266.56\n",
      "update_loss_res: 1.4872e+00\n",
      "update_loss_bcs: 5.1277e-01\n",
      "It: 17400, Loss: 6.904e-01, Loss_bcs: 1.779e-01, Loss_res: 5.125e-01,Time: 267.91\n",
      "update_loss_res: 1.4998e+00\n",
      "update_loss_bcs: 5.0018e-01\n",
      "It: 17500, Loss: 8.348e-01, Loss_bcs: 1.715e-01, Loss_res: 6.633e-01,Time: 269.29\n",
      "update_loss_res: 1.4711e+00\n",
      "update_loss_bcs: 5.2890e-01\n",
      "It: 17600, Loss: 7.727e-01, Loss_bcs: 1.644e-01, Loss_res: 6.083e-01,Time: 270.71\n",
      "update_loss_res: 1.5028e+00\n",
      "update_loss_bcs: 4.9717e-01\n",
      "It: 17700, Loss: 6.550e-01, Loss_bcs: 1.456e-01, Loss_res: 5.094e-01,Time: 272.18\n",
      "update_loss_res: 1.4658e+00\n",
      "update_loss_bcs: 5.3420e-01\n",
      "It: 17800, Loss: 7.420e-01, Loss_bcs: 1.667e-01, Loss_res: 5.753e-01,Time: 273.54\n",
      "update_loss_res: 1.4483e+00\n",
      "update_loss_bcs: 5.5166e-01\n",
      "It: 17900, Loss: 9.757e-01, Loss_bcs: 1.506e-01, Loss_res: 8.251e-01,Time: 274.99\n",
      "update_loss_res: 1.4870e+00\n",
      "update_loss_bcs: 5.1303e-01\n",
      "It: 18000, Loss: 8.425e-01, Loss_bcs: 2.076e-01, Loss_res: 6.349e-01,Time: 276.42\n",
      "update_loss_res: 1.4494e+00\n",
      "update_loss_bcs: 5.5061e-01\n",
      "max_grad_res/mean_grad_bcs: 1.596e+01\n",
      "mean_grad_bcs/max_grad_res: 6.265e-02\n",
      "adaptive_constant_res_val: 1.772e+01\n",
      "It: 18100, Loss: 6.710e-01, Loss_bcs: 1.869e-01, Loss_res: 4.842e-01,Time: 277.95\n",
      "update_loss_res: 1.4914e+00\n",
      "update_loss_bcs: 5.0856e-01\n",
      "It: 18200, Loss: 9.132e-01, Loss_bcs: 1.477e-01, Loss_res: 7.655e-01,Time: 279.34\n",
      "update_loss_res: 1.4247e+00\n",
      "update_loss_bcs: 5.7528e-01\n",
      "It: 18300, Loss: 7.517e-01, Loss_bcs: 1.601e-01, Loss_res: 5.916e-01,Time: 280.84\n",
      "update_loss_res: 1.4664e+00\n",
      "update_loss_bcs: 5.3357e-01\n",
      "It: 18400, Loss: 9.417e-01, Loss_bcs: 1.445e-01, Loss_res: 7.972e-01,Time: 282.31\n",
      "update_loss_res: 1.4536e+00\n",
      "update_loss_bcs: 5.4639e-01\n",
      "It: 18500, Loss: 8.213e-01, Loss_bcs: 1.808e-01, Loss_res: 6.405e-01,Time: 283.77\n",
      "update_loss_res: 1.4488e+00\n",
      "update_loss_bcs: 5.5115e-01\n",
      "It: 18600, Loss: 7.176e-01, Loss_bcs: 1.691e-01, Loss_res: 5.485e-01,Time: 285.17\n",
      "update_loss_res: 1.4635e+00\n",
      "update_loss_bcs: 5.3650e-01\n",
      "It: 18700, Loss: 8.739e-01, Loss_bcs: 1.446e-01, Loss_res: 7.293e-01,Time: 286.76\n",
      "update_loss_res: 1.4150e+00\n",
      "update_loss_bcs: 5.8503e-01\n",
      "It: 18800, Loss: 9.781e-01, Loss_bcs: 2.054e-01, Loss_res: 7.727e-01,Time: 288.69\n",
      "update_loss_res: 1.4538e+00\n",
      "update_loss_bcs: 5.4622e-01\n",
      "It: 18900, Loss: 7.650e-01, Loss_bcs: 1.618e-01, Loss_res: 6.032e-01,Time: 290.23\n",
      "update_loss_res: 1.4469e+00\n",
      "update_loss_bcs: 5.5308e-01\n",
      "It: 19000, Loss: 9.957e-01, Loss_bcs: 1.667e-01, Loss_res: 8.289e-01,Time: 291.65\n",
      "update_loss_res: 1.4516e+00\n",
      "update_loss_bcs: 5.4841e-01\n",
      "max_grad_res/mean_grad_bcs: 3.042e+01\n",
      "mean_grad_bcs/max_grad_res: 3.287e-02\n",
      "adaptive_constant_res_val: 1.899e+01\n",
      "It: 19100, Loss: 7.154e-01, Loss_bcs: 1.760e-01, Loss_res: 5.394e-01,Time: 293.19\n",
      "update_loss_res: 1.3968e+00\n",
      "update_loss_bcs: 6.0320e-01\n",
      "It: 19200, Loss: 7.005e-01, Loss_bcs: 1.456e-01, Loss_res: 5.548e-01,Time: 294.60\n",
      "update_loss_res: 1.4570e+00\n",
      "update_loss_bcs: 5.4301e-01\n",
      "It: 19300, Loss: 9.382e-01, Loss_bcs: 1.697e-01, Loss_res: 7.685e-01,Time: 295.92\n",
      "update_loss_res: 1.4024e+00\n",
      "update_loss_bcs: 5.9760e-01\n",
      "It: 19400, Loss: 6.248e-01, Loss_bcs: 1.675e-01, Loss_res: 4.573e-01,Time: 297.30\n",
      "update_loss_res: 1.3804e+00\n",
      "update_loss_bcs: 6.1958e-01\n",
      "It: 19500, Loss: 7.766e-01, Loss_bcs: 1.689e-01, Loss_res: 6.077e-01,Time: 298.80\n",
      "update_loss_res: 1.3920e+00\n",
      "update_loss_bcs: 6.0798e-01\n",
      "It: 19600, Loss: 5.477e-01, Loss_bcs: 1.477e-01, Loss_res: 4.001e-01,Time: 300.18\n",
      "update_loss_res: 1.3904e+00\n",
      "update_loss_bcs: 6.0962e-01\n",
      "It: 19700, Loss: 7.377e-01, Loss_bcs: 1.645e-01, Loss_res: 5.733e-01,Time: 301.59\n",
      "update_loss_res: 1.3990e+00\n",
      "update_loss_bcs: 6.0098e-01\n",
      "It: 19800, Loss: 6.614e-01, Loss_bcs: 1.309e-01, Loss_res: 5.305e-01,Time: 303.04\n",
      "update_loss_res: 1.4338e+00\n",
      "update_loss_bcs: 5.6619e-01\n",
      "It: 19900, Loss: 6.535e-01, Loss_bcs: 1.479e-01, Loss_res: 5.056e-01,Time: 304.39\n",
      "update_loss_res: 1.4223e+00\n",
      "update_loss_bcs: 5.7768e-01\n",
      "It: 20000, Loss: 7.712e-01, Loss_bcs: 1.744e-01, Loss_res: 5.967e-01,Time: 305.76\n",
      "update_loss_res: 1.4615e+00\n",
      "update_loss_bcs: 5.3846e-01\n",
      "max_grad_res/mean_grad_bcs: 1.949e+01\n",
      "mean_grad_bcs/max_grad_res: 5.132e-02\n",
      "adaptive_constant_res_val: 1.904e+01\n",
      "It: 20100, Loss: 7.121e-01, Loss_bcs: 1.587e-01, Loss_res: 5.534e-01,Time: 307.18\n",
      "update_loss_res: 1.3736e+00\n",
      "update_loss_bcs: 6.2639e-01\n",
      "It: 20200, Loss: 6.112e-01, Loss_bcs: 1.551e-01, Loss_res: 4.561e-01,Time: 308.63\n",
      "update_loss_res: 1.3409e+00\n",
      "update_loss_bcs: 6.5914e-01\n",
      "It: 20300, Loss: 8.043e-01, Loss_bcs: 1.423e-01, Loss_res: 6.619e-01,Time: 310.05\n",
      "update_loss_res: 1.4105e+00\n",
      "update_loss_bcs: 5.8946e-01\n",
      "It: 20400, Loss: 5.114e-01, Loss_bcs: 1.556e-01, Loss_res: 3.558e-01,Time: 311.43\n",
      "update_loss_res: 1.4074e+00\n",
      "update_loss_bcs: 5.9260e-01\n",
      "It: 20500, Loss: 6.335e-01, Loss_bcs: 1.316e-01, Loss_res: 5.019e-01,Time: 312.89\n",
      "update_loss_res: 1.3909e+00\n",
      "update_loss_bcs: 6.0907e-01\n",
      "It: 20600, Loss: 5.813e-01, Loss_bcs: 1.428e-01, Loss_res: 4.385e-01,Time: 314.24\n",
      "update_loss_res: 1.3808e+00\n",
      "update_loss_bcs: 6.1922e-01\n",
      "It: 20700, Loss: 7.348e-01, Loss_bcs: 1.545e-01, Loss_res: 5.802e-01,Time: 315.64\n",
      "update_loss_res: 1.4245e+00\n",
      "update_loss_bcs: 5.7547e-01\n",
      "It: 20800, Loss: 6.424e-01, Loss_bcs: 1.527e-01, Loss_res: 4.897e-01,Time: 317.01\n",
      "update_loss_res: 1.3585e+00\n",
      "update_loss_bcs: 6.4149e-01\n",
      "It: 20900, Loss: 8.930e-01, Loss_bcs: 1.472e-01, Loss_res: 7.458e-01,Time: 318.52\n",
      "update_loss_res: 1.4327e+00\n",
      "update_loss_bcs: 5.6732e-01\n",
      "It: 21000, Loss: 5.522e-01, Loss_bcs: 1.520e-01, Loss_res: 4.002e-01,Time: 319.97\n",
      "update_loss_res: 1.3873e+00\n",
      "update_loss_bcs: 6.1274e-01\n",
      "max_grad_res/mean_grad_bcs: 2.707e+01\n",
      "mean_grad_bcs/max_grad_res: 3.695e-02\n",
      "adaptive_constant_res_val: 1.984e+01\n",
      "It: 21100, Loss: 6.266e-01, Loss_bcs: 1.451e-01, Loss_res: 4.815e-01,Time: 321.49\n",
      "update_loss_res: 1.3377e+00\n",
      "update_loss_bcs: 6.6231e-01\n",
      "It: 21200, Loss: 6.376e-01, Loss_bcs: 1.413e-01, Loss_res: 4.963e-01,Time: 322.86\n",
      "update_loss_res: 1.3824e+00\n",
      "update_loss_bcs: 6.1759e-01\n",
      "It: 21300, Loss: 1.054e+00, Loss_bcs: 1.463e-01, Loss_res: 9.078e-01,Time: 324.29\n",
      "update_loss_res: 1.3650e+00\n",
      "update_loss_bcs: 6.3505e-01\n",
      "It: 21400, Loss: 6.036e-01, Loss_bcs: 1.703e-01, Loss_res: 4.333e-01,Time: 325.81\n",
      "update_loss_res: 1.3231e+00\n",
      "update_loss_bcs: 6.7693e-01\n",
      "It: 21500, Loss: 8.017e-01, Loss_bcs: 1.177e-01, Loss_res: 6.839e-01,Time: 327.24\n",
      "update_loss_res: 1.3397e+00\n",
      "update_loss_bcs: 6.6031e-01\n",
      "It: 21600, Loss: 5.956e-01, Loss_bcs: 1.349e-01, Loss_res: 4.607e-01,Time: 328.77\n",
      "update_loss_res: 1.3713e+00\n",
      "update_loss_bcs: 6.2871e-01\n",
      "It: 21700, Loss: 5.575e-01, Loss_bcs: 1.445e-01, Loss_res: 4.130e-01,Time: 330.13\n",
      "update_loss_res: 1.3499e+00\n",
      "update_loss_bcs: 6.5009e-01\n",
      "It: 21800, Loss: 6.052e-01, Loss_bcs: 1.370e-01, Loss_res: 4.682e-01,Time: 331.54\n",
      "update_loss_res: 1.3620e+00\n",
      "update_loss_bcs: 6.3804e-01\n",
      "It: 21900, Loss: 6.867e-01, Loss_bcs: 1.312e-01, Loss_res: 5.554e-01,Time: 332.92\n",
      "update_loss_res: 1.3394e+00\n",
      "update_loss_bcs: 6.6065e-01\n",
      "It: 22000, Loss: 5.563e-01, Loss_bcs: 1.454e-01, Loss_res: 4.109e-01,Time: 334.38\n",
      "update_loss_res: 1.3147e+00\n",
      "update_loss_bcs: 6.8526e-01\n",
      "max_grad_res/mean_grad_bcs: 1.212e+01\n",
      "mean_grad_bcs/max_grad_res: 8.251e-02\n",
      "adaptive_constant_res_val: 1.907e+01\n",
      "It: 22100, Loss: 7.324e-01, Loss_bcs: 1.433e-01, Loss_res: 5.891e-01,Time: 335.79\n",
      "update_loss_res: 1.3648e+00\n",
      "update_loss_bcs: 6.3523e-01\n",
      "It: 22200, Loss: 7.909e-01, Loss_bcs: 1.200e-01, Loss_res: 6.708e-01,Time: 337.19\n",
      "update_loss_res: 1.3432e+00\n",
      "update_loss_bcs: 6.5678e-01\n",
      "It: 22300, Loss: 4.964e-01, Loss_bcs: 1.278e-01, Loss_res: 3.686e-01,Time: 338.67\n",
      "update_loss_res: 1.3407e+00\n",
      "update_loss_bcs: 6.5928e-01\n",
      "It: 22400, Loss: 5.356e-01, Loss_bcs: 1.177e-01, Loss_res: 4.179e-01,Time: 340.08\n",
      "update_loss_res: 1.3467e+00\n",
      "update_loss_bcs: 6.5326e-01\n",
      "It: 22500, Loss: 6.206e-01, Loss_bcs: 1.083e-01, Loss_res: 5.123e-01,Time: 341.49\n",
      "update_loss_res: 1.3484e+00\n",
      "update_loss_bcs: 6.5160e-01\n",
      "It: 22600, Loss: 5.115e-01, Loss_bcs: 1.385e-01, Loss_res: 3.730e-01,Time: 342.87\n",
      "update_loss_res: 1.3305e+00\n",
      "update_loss_bcs: 6.6950e-01\n",
      "It: 22700, Loss: 6.746e-01, Loss_bcs: 1.151e-01, Loss_res: 5.595e-01,Time: 344.27\n",
      "update_loss_res: 1.3519e+00\n",
      "update_loss_bcs: 6.4807e-01\n",
      "It: 22800, Loss: 5.088e-01, Loss_bcs: 1.403e-01, Loss_res: 3.685e-01,Time: 345.68\n",
      "update_loss_res: 1.3307e+00\n",
      "update_loss_bcs: 6.6929e-01\n",
      "It: 22900, Loss: 5.895e-01, Loss_bcs: 1.010e-01, Loss_res: 4.886e-01,Time: 347.07\n",
      "update_loss_res: 1.3303e+00\n",
      "update_loss_bcs: 6.6970e-01\n",
      "It: 23000, Loss: 4.928e-01, Loss_bcs: 1.240e-01, Loss_res: 3.688e-01,Time: 348.47\n",
      "update_loss_res: 1.3177e+00\n",
      "update_loss_bcs: 6.8234e-01\n",
      "max_grad_res/mean_grad_bcs: 1.177e+01\n",
      "mean_grad_bcs/max_grad_res: 8.496e-02\n",
      "adaptive_constant_res_val: 1.834e+01\n",
      "It: 23100, Loss: 5.959e-01, Loss_bcs: 1.310e-01, Loss_res: 4.650e-01,Time: 349.97\n",
      "update_loss_res: 1.3262e+00\n",
      "update_loss_bcs: 6.7382e-01\n",
      "It: 23200, Loss: 8.147e-01, Loss_bcs: 1.267e-01, Loss_res: 6.880e-01,Time: 351.35\n",
      "update_loss_res: 1.3187e+00\n",
      "update_loss_bcs: 6.8128e-01\n",
      "It: 23300, Loss: 8.651e-01, Loss_bcs: 1.263e-01, Loss_res: 7.388e-01,Time: 352.80\n",
      "update_loss_res: 1.3458e+00\n",
      "update_loss_bcs: 6.5418e-01\n",
      "It: 23400, Loss: 8.179e-01, Loss_bcs: 1.271e-01, Loss_res: 6.908e-01,Time: 354.19\n",
      "update_loss_res: 1.3458e+00\n",
      "update_loss_bcs: 6.5417e-01\n",
      "It: 23500, Loss: 6.529e-01, Loss_bcs: 1.118e-01, Loss_res: 5.411e-01,Time: 355.61\n",
      "update_loss_res: 1.3409e+00\n",
      "update_loss_bcs: 6.5910e-01\n",
      "It: 23600, Loss: 5.231e-01, Loss_bcs: 1.219e-01, Loss_res: 4.013e-01,Time: 356.98\n",
      "update_loss_res: 1.3212e+00\n",
      "update_loss_bcs: 6.7876e-01\n",
      "It: 23700, Loss: 4.594e-01, Loss_bcs: 1.200e-01, Loss_res: 3.395e-01,Time: 358.38\n",
      "update_loss_res: 1.3062e+00\n",
      "update_loss_bcs: 6.9379e-01\n",
      "It: 23800, Loss: 6.316e-01, Loss_bcs: 1.217e-01, Loss_res: 5.099e-01,Time: 359.83\n",
      "update_loss_res: 1.3251e+00\n",
      "update_loss_bcs: 6.7487e-01\n",
      "It: 23900, Loss: 6.492e-01, Loss_bcs: 1.204e-01, Loss_res: 5.288e-01,Time: 361.32\n",
      "update_loss_res: 1.3081e+00\n",
      "update_loss_bcs: 6.9188e-01\n",
      "It: 24000, Loss: 3.525e-01, Loss_bcs: 1.105e-01, Loss_res: 2.420e-01,Time: 362.71\n",
      "update_loss_res: 1.3087e+00\n",
      "update_loss_bcs: 6.9129e-01\n",
      "max_grad_res/mean_grad_bcs: 1.071e+01\n",
      "mean_grad_bcs/max_grad_res: 9.338e-02\n",
      "adaptive_constant_res_val: 1.758e+01\n",
      "It: 24100, Loss: 8.484e-01, Loss_bcs: 1.215e-01, Loss_res: 7.269e-01,Time: 364.15\n",
      "update_loss_res: 1.3479e+00\n",
      "update_loss_bcs: 6.5207e-01\n",
      "It: 24200, Loss: 5.135e-01, Loss_bcs: 1.022e-01, Loss_res: 4.113e-01,Time: 365.58\n",
      "update_loss_res: 1.2975e+00\n",
      "update_loss_bcs: 7.0254e-01\n",
      "It: 24300, Loss: 1.402e+00, Loss_bcs: 1.056e-01, Loss_res: 1.297e+00,Time: 366.96\n",
      "update_loss_res: 1.3560e+00\n",
      "update_loss_bcs: 6.4401e-01\n",
      "It: 24400, Loss: 3.474e-01, Loss_bcs: 1.100e-01, Loss_res: 2.373e-01,Time: 368.39\n",
      "update_loss_res: 1.2973e+00\n",
      "update_loss_bcs: 7.0273e-01\n",
      "It: 24500, Loss: 5.241e-01, Loss_bcs: 1.180e-01, Loss_res: 4.061e-01,Time: 369.78\n",
      "update_loss_res: 1.3055e+00\n",
      "update_loss_bcs: 6.9449e-01\n",
      "It: 24600, Loss: 5.923e-01, Loss_bcs: 1.113e-01, Loss_res: 4.810e-01,Time: 371.19\n",
      "update_loss_res: 1.3189e+00\n",
      "update_loss_bcs: 6.8112e-01\n",
      "It: 24700, Loss: 6.414e-01, Loss_bcs: 1.036e-01, Loss_res: 5.378e-01,Time: 372.58\n",
      "update_loss_res: 1.3094e+00\n",
      "update_loss_bcs: 6.9060e-01\n",
      "It: 24800, Loss: 4.873e-01, Loss_bcs: 1.202e-01, Loss_res: 3.671e-01,Time: 373.95\n",
      "update_loss_res: 1.2934e+00\n",
      "update_loss_bcs: 7.0658e-01\n",
      "It: 24900, Loss: 6.341e-01, Loss_bcs: 1.058e-01, Loss_res: 5.283e-01,Time: 375.35\n",
      "update_loss_res: 1.3129e+00\n",
      "update_loss_bcs: 6.8713e-01\n",
      "It: 25000, Loss: 5.527e-01, Loss_bcs: 1.165e-01, Loss_res: 4.362e-01,Time: 376.85\n",
      "update_loss_res: 1.2908e+00\n",
      "update_loss_bcs: 7.0917e-01\n",
      "max_grad_res/mean_grad_bcs: 1.421e+01\n",
      "mean_grad_bcs/max_grad_res: 7.039e-02\n",
      "adaptive_constant_res_val: 1.724e+01\n",
      "It: 25100, Loss: 6.297e-01, Loss_bcs: 1.117e-01, Loss_res: 5.180e-01,Time: 378.26\n",
      "update_loss_res: 1.3206e+00\n",
      "update_loss_bcs: 6.7945e-01\n",
      "It: 25200, Loss: 5.926e-01, Loss_bcs: 9.805e-02, Loss_res: 4.945e-01,Time: 379.72\n",
      "update_loss_res: 1.3049e+00\n",
      "update_loss_bcs: 6.9507e-01\n",
      "It: 25300, Loss: 4.105e-01, Loss_bcs: 1.089e-01, Loss_res: 3.017e-01,Time: 381.23\n",
      "update_loss_res: 1.2955e+00\n",
      "update_loss_bcs: 7.0451e-01\n",
      "It: 25400, Loss: 5.544e-01, Loss_bcs: 1.075e-01, Loss_res: 4.469e-01,Time: 382.89\n",
      "update_loss_res: 1.3061e+00\n",
      "update_loss_bcs: 6.9391e-01\n",
      "It: 25500, Loss: 5.169e-01, Loss_bcs: 1.063e-01, Loss_res: 4.106e-01,Time: 384.32\n",
      "update_loss_res: 1.2989e+00\n",
      "update_loss_bcs: 7.0105e-01\n",
      "It: 25600, Loss: 5.648e-01, Loss_bcs: 1.107e-01, Loss_res: 4.541e-01,Time: 385.80\n",
      "update_loss_res: 1.2926e+00\n",
      "update_loss_bcs: 7.0737e-01\n",
      "It: 25700, Loss: 6.833e-01, Loss_bcs: 1.074e-01, Loss_res: 5.759e-01,Time: 387.26\n",
      "update_loss_res: 1.2977e+00\n",
      "update_loss_bcs: 7.0227e-01\n",
      "It: 25800, Loss: 4.040e-01, Loss_bcs: 1.007e-01, Loss_res: 3.033e-01,Time: 389.05\n",
      "update_loss_res: 1.3001e+00\n",
      "update_loss_bcs: 6.9994e-01\n",
      "It: 25900, Loss: 5.895e-01, Loss_bcs: 1.126e-01, Loss_res: 4.770e-01,Time: 390.45\n",
      "update_loss_res: 1.2873e+00\n",
      "update_loss_bcs: 7.1268e-01\n",
      "It: 26000, Loss: 4.737e-01, Loss_bcs: 9.727e-02, Loss_res: 3.765e-01,Time: 391.98\n",
      "update_loss_res: 1.2993e+00\n",
      "update_loss_bcs: 7.0068e-01\n",
      "max_grad_res/mean_grad_bcs: 1.926e+01\n",
      "mean_grad_bcs/max_grad_res: 5.193e-02\n",
      "adaptive_constant_res_val: 1.744e+01\n",
      "It: 26100, Loss: 4.476e-01, Loss_bcs: 1.009e-01, Loss_res: 3.467e-01,Time: 393.45\n",
      "update_loss_res: 1.2923e+00\n",
      "update_loss_bcs: 7.0767e-01\n",
      "It: 26200, Loss: 5.358e-01, Loss_bcs: 1.010e-01, Loss_res: 4.348e-01,Time: 394.89\n",
      "update_loss_res: 1.2942e+00\n",
      "update_loss_bcs: 7.0580e-01\n",
      "It: 26300, Loss: 3.934e-01, Loss_bcs: 1.139e-01, Loss_res: 2.795e-01,Time: 396.33\n",
      "update_loss_res: 1.3100e+00\n",
      "update_loss_bcs: 6.9003e-01\n",
      "It: 26400, Loss: 5.392e-01, Loss_bcs: 9.351e-02, Loss_res: 4.457e-01,Time: 397.86\n",
      "update_loss_res: 1.2906e+00\n",
      "update_loss_bcs: 7.0942e-01\n",
      "It: 26500, Loss: 4.293e-01, Loss_bcs: 1.200e-01, Loss_res: 3.093e-01,Time: 399.33\n",
      "update_loss_res: 1.2801e+00\n",
      "update_loss_bcs: 7.1993e-01\n",
      "It: 26600, Loss: 5.041e-01, Loss_bcs: 9.362e-02, Loss_res: 4.105e-01,Time: 400.71\n",
      "update_loss_res: 1.2745e+00\n",
      "update_loss_bcs: 7.2548e-01\n",
      "It: 26700, Loss: 4.801e-01, Loss_bcs: 8.325e-02, Loss_res: 3.969e-01,Time: 402.33\n",
      "update_loss_res: 1.2893e+00\n",
      "update_loss_bcs: 7.1067e-01\n",
      "It: 26800, Loss: 5.538e-01, Loss_bcs: 9.789e-02, Loss_res: 4.559e-01,Time: 403.93\n",
      "update_loss_res: 1.2886e+00\n",
      "update_loss_bcs: 7.1138e-01\n",
      "It: 26900, Loss: 5.363e-01, Loss_bcs: 1.084e-01, Loss_res: 4.279e-01,Time: 405.41\n",
      "update_loss_res: 1.2874e+00\n",
      "update_loss_bcs: 7.1257e-01\n",
      "It: 27000, Loss: 6.761e-01, Loss_bcs: 1.158e-01, Loss_res: 5.602e-01,Time: 407.06\n",
      "update_loss_res: 1.2830e+00\n",
      "update_loss_bcs: 7.1700e-01\n",
      "max_grad_res/mean_grad_bcs: 1.032e+01\n",
      "mean_grad_bcs/max_grad_res: 9.693e-02\n",
      "adaptive_constant_res_val: 1.673e+01\n",
      "It: 27100, Loss: 7.154e-01, Loss_bcs: 8.910e-02, Loss_res: 6.263e-01,Time: 408.54\n",
      "update_loss_res: 1.2747e+00\n",
      "update_loss_bcs: 7.2532e-01\n",
      "It: 27200, Loss: 6.568e-01, Loss_bcs: 9.758e-02, Loss_res: 5.592e-01,Time: 409.93\n",
      "update_loss_res: 1.2767e+00\n",
      "update_loss_bcs: 7.2327e-01\n",
      "It: 27300, Loss: 9.403e-01, Loss_bcs: 9.001e-02, Loss_res: 8.503e-01,Time: 411.32\n",
      "update_loss_res: 1.2626e+00\n",
      "update_loss_bcs: 7.3743e-01\n",
      "It: 27400, Loss: 4.581e-01, Loss_bcs: 9.925e-02, Loss_res: 3.589e-01,Time: 412.78\n",
      "update_loss_res: 1.2834e+00\n",
      "update_loss_bcs: 7.1656e-01\n",
      "It: 27500, Loss: 4.104e-01, Loss_bcs: 1.063e-01, Loss_res: 3.040e-01,Time: 414.18\n",
      "update_loss_res: 1.2773e+00\n",
      "update_loss_bcs: 7.2270e-01\n",
      "It: 27600, Loss: 4.668e-01, Loss_bcs: 8.029e-02, Loss_res: 3.865e-01,Time: 415.57\n",
      "update_loss_res: 1.2684e+00\n",
      "update_loss_bcs: 7.3162e-01\n",
      "It: 27700, Loss: 4.491e-01, Loss_bcs: 1.128e-01, Loss_res: 3.363e-01,Time: 417.10\n",
      "update_loss_res: 1.2436e+00\n",
      "update_loss_bcs: 7.5641e-01\n",
      "It: 27800, Loss: 4.436e-01, Loss_bcs: 8.949e-02, Loss_res: 3.541e-01,Time: 418.50\n",
      "update_loss_res: 1.2694e+00\n",
      "update_loss_bcs: 7.3061e-01\n",
      "It: 27900, Loss: 3.852e-01, Loss_bcs: 9.630e-02, Loss_res: 2.889e-01,Time: 419.90\n",
      "update_loss_res: 1.2602e+00\n",
      "update_loss_bcs: 7.3982e-01\n",
      "It: 28000, Loss: 3.400e-01, Loss_bcs: 1.054e-01, Loss_res: 2.345e-01,Time: 421.31\n",
      "update_loss_res: 1.2535e+00\n",
      "update_loss_bcs: 7.4654e-01\n",
      "max_grad_res/mean_grad_bcs: 1.031e+01\n",
      "mean_grad_bcs/max_grad_res: 9.704e-02\n",
      "adaptive_constant_res_val: 1.609e+01\n",
      "It: 28100, Loss: 3.038e-01, Loss_bcs: 8.605e-02, Loss_res: 2.178e-01,Time: 422.74\n",
      "update_loss_res: 1.2531e+00\n",
      "update_loss_bcs: 7.4687e-01\n",
      "It: 28200, Loss: 5.518e-01, Loss_bcs: 1.055e-01, Loss_res: 4.463e-01,Time: 424.18\n",
      "update_loss_res: 1.2518e+00\n",
      "update_loss_bcs: 7.4818e-01\n",
      "It: 28300, Loss: 3.392e-01, Loss_bcs: 9.461e-02, Loss_res: 2.446e-01,Time: 425.61\n",
      "update_loss_res: 1.2559e+00\n",
      "update_loss_bcs: 7.4411e-01\n",
      "It: 28400, Loss: 5.072e-01, Loss_bcs: 9.639e-02, Loss_res: 4.108e-01,Time: 427.00\n",
      "update_loss_res: 1.2876e+00\n",
      "update_loss_bcs: 7.1243e-01\n",
      "It: 28500, Loss: 3.724e-01, Loss_bcs: 9.354e-02, Loss_res: 2.789e-01,Time: 428.48\n",
      "update_loss_res: 1.2796e+00\n",
      "update_loss_bcs: 7.2035e-01\n",
      "It: 28600, Loss: 3.426e-01, Loss_bcs: 8.346e-02, Loss_res: 2.591e-01,Time: 429.88\n",
      "update_loss_res: 1.2776e+00\n",
      "update_loss_bcs: 7.2243e-01\n",
      "It: 28700, Loss: 4.974e-01, Loss_bcs: 8.358e-02, Loss_res: 4.139e-01,Time: 431.26\n",
      "update_loss_res: 1.2638e+00\n",
      "update_loss_bcs: 7.3617e-01\n",
      "It: 28800, Loss: 4.093e-01, Loss_bcs: 8.240e-02, Loss_res: 3.269e-01,Time: 432.65\n",
      "update_loss_res: 1.2586e+00\n",
      "update_loss_bcs: 7.4136e-01\n",
      "It: 28900, Loss: 4.283e-01, Loss_bcs: 9.737e-02, Loss_res: 3.310e-01,Time: 434.13\n",
      "update_loss_res: 1.2792e+00\n",
      "update_loss_bcs: 7.2078e-01\n",
      "It: 29000, Loss: 3.589e-01, Loss_bcs: 8.799e-02, Loss_res: 2.709e-01,Time: 435.51\n",
      "update_loss_res: 1.2745e+00\n",
      "update_loss_bcs: 7.2550e-01\n",
      "max_grad_res/mean_grad_bcs: 1.494e+01\n",
      "mean_grad_bcs/max_grad_res: 6.694e-02\n",
      "adaptive_constant_res_val: 1.597e+01\n",
      "It: 29100, Loss: 5.296e-01, Loss_bcs: 9.233e-02, Loss_res: 4.372e-01,Time: 436.95\n",
      "update_loss_res: 1.2534e+00\n",
      "update_loss_bcs: 7.4658e-01\n",
      "It: 29200, Loss: 5.123e-01, Loss_bcs: 8.535e-02, Loss_res: 4.270e-01,Time: 438.33\n",
      "update_loss_res: 1.2781e+00\n",
      "update_loss_bcs: 7.2185e-01\n",
      "It: 29300, Loss: 3.888e-01, Loss_bcs: 9.510e-02, Loss_res: 2.937e-01,Time: 439.71\n",
      "update_loss_res: 1.2582e+00\n",
      "update_loss_bcs: 7.4176e-01\n",
      "It: 29400, Loss: 3.828e-01, Loss_bcs: 9.197e-02, Loss_res: 2.908e-01,Time: 441.07\n",
      "update_loss_res: 1.2528e+00\n",
      "update_loss_bcs: 7.4718e-01\n",
      "It: 29500, Loss: 4.638e-01, Loss_bcs: 7.869e-02, Loss_res: 3.851e-01,Time: 442.44\n",
      "update_loss_res: 1.2546e+00\n",
      "update_loss_bcs: 7.4537e-01\n",
      "It: 29600, Loss: 5.147e-01, Loss_bcs: 9.398e-02, Loss_res: 4.207e-01,Time: 443.90\n",
      "update_loss_res: 1.2531e+00\n",
      "update_loss_bcs: 7.4690e-01\n",
      "It: 29700, Loss: 6.112e-01, Loss_bcs: 8.738e-02, Loss_res: 5.238e-01,Time: 445.34\n",
      "update_loss_res: 1.2391e+00\n",
      "update_loss_bcs: 7.6090e-01\n",
      "It: 29800, Loss: 7.028e-01, Loss_bcs: 9.942e-02, Loss_res: 6.034e-01,Time: 446.78\n",
      "update_loss_res: 1.2635e+00\n",
      "update_loss_bcs: 7.3648e-01\n",
      "It: 29900, Loss: 7.715e-01, Loss_bcs: 8.721e-02, Loss_res: 6.843e-01,Time: 448.13\n",
      "update_loss_res: 1.2722e+00\n",
      "update_loss_bcs: 7.2785e-01\n",
      "It: 30000, Loss: 4.515e-01, Loss_bcs: 9.136e-02, Loss_res: 3.601e-01,Time: 449.58\n",
      "update_loss_res: 1.2435e+00\n",
      "update_loss_bcs: 7.5651e-01\n",
      "max_grad_res/mean_grad_bcs: 1.475e+01\n",
      "mean_grad_bcs/max_grad_res: 6.779e-02\n",
      "adaptive_constant_res_val: 1.585e+01\n",
      "It: 30100, Loss: 3.808e-01, Loss_bcs: 8.909e-02, Loss_res: 2.917e-01,Time: 450.99\n",
      "update_loss_res: 1.2526e+00\n",
      "update_loss_bcs: 7.4736e-01\n",
      "It: 30200, Loss: 5.002e-01, Loss_bcs: 9.354e-02, Loss_res: 4.066e-01,Time: 452.36\n",
      "update_loss_res: 1.2576e+00\n",
      "update_loss_bcs: 7.4242e-01\n",
      "It: 30300, Loss: 3.573e-01, Loss_bcs: 8.217e-02, Loss_res: 2.751e-01,Time: 453.78\n",
      "update_loss_res: 1.2408e+00\n",
      "update_loss_bcs: 7.5924e-01\n",
      "It: 30400, Loss: 3.924e-01, Loss_bcs: 7.995e-02, Loss_res: 3.124e-01,Time: 455.21\n",
      "update_loss_res: 1.2484e+00\n",
      "update_loss_bcs: 7.5156e-01\n",
      "It: 30500, Loss: 6.016e-01, Loss_bcs: 9.174e-02, Loss_res: 5.098e-01,Time: 456.63\n",
      "update_loss_res: 1.2353e+00\n",
      "update_loss_bcs: 7.6471e-01\n",
      "It: 30600, Loss: 3.923e-01, Loss_bcs: 8.329e-02, Loss_res: 3.090e-01,Time: 458.06\n",
      "update_loss_res: 1.2788e+00\n",
      "update_loss_bcs: 7.2116e-01\n",
      "It: 30700, Loss: 3.990e-01, Loss_bcs: 7.892e-02, Loss_res: 3.201e-01,Time: 459.51\n",
      "update_loss_res: 1.2558e+00\n",
      "update_loss_bcs: 7.4425e-01\n",
      "It: 30800, Loss: 4.097e-01, Loss_bcs: 7.981e-02, Loss_res: 3.299e-01,Time: 460.86\n",
      "update_loss_res: 1.2536e+00\n",
      "update_loss_bcs: 7.4636e-01\n",
      "It: 30900, Loss: 3.396e-01, Loss_bcs: 8.524e-02, Loss_res: 2.544e-01,Time: 462.22\n",
      "update_loss_res: 1.2545e+00\n",
      "update_loss_bcs: 7.4552e-01\n",
      "It: 31000, Loss: 3.297e-01, Loss_bcs: 7.687e-02, Loss_res: 2.528e-01,Time: 463.65\n",
      "update_loss_res: 1.2435e+00\n",
      "update_loss_bcs: 7.5652e-01\n",
      "max_grad_res/mean_grad_bcs: 2.524e+01\n",
      "mean_grad_bcs/max_grad_res: 3.963e-02\n",
      "adaptive_constant_res_val: 1.679e+01\n",
      "It: 31100, Loss: 3.415e-01, Loss_bcs: 7.656e-02, Loss_res: 2.649e-01,Time: 465.15\n",
      "update_loss_res: 1.2203e+00\n",
      "update_loss_bcs: 7.7972e-01\n",
      "It: 31200, Loss: 3.555e-01, Loss_bcs: 9.982e-02, Loss_res: 2.557e-01,Time: 466.63\n",
      "update_loss_res: 1.2403e+00\n",
      "update_loss_bcs: 7.5967e-01\n",
      "It: 31300, Loss: 4.176e-01, Loss_bcs: 8.978e-02, Loss_res: 3.278e-01,Time: 468.14\n",
      "update_loss_res: 1.2393e+00\n",
      "update_loss_bcs: 7.6073e-01\n",
      "It: 31400, Loss: 5.033e-01, Loss_bcs: 8.055e-02, Loss_res: 4.228e-01,Time: 469.60\n",
      "update_loss_res: 1.2305e+00\n",
      "update_loss_bcs: 7.6947e-01\n",
      "It: 31500, Loss: 3.283e-01, Loss_bcs: 9.375e-02, Loss_res: 2.345e-01,Time: 471.03\n",
      "update_loss_res: 1.2400e+00\n",
      "update_loss_bcs: 7.5999e-01\n",
      "It: 31600, Loss: 3.159e-01, Loss_bcs: 7.987e-02, Loss_res: 2.361e-01,Time: 472.41\n",
      "update_loss_res: 1.2369e+00\n",
      "update_loss_bcs: 7.6307e-01\n",
      "It: 31700, Loss: 3.539e-01, Loss_bcs: 8.477e-02, Loss_res: 2.691e-01,Time: 473.85\n",
      "update_loss_res: 1.2429e+00\n",
      "update_loss_bcs: 7.5707e-01\n",
      "It: 31800, Loss: 4.042e-01, Loss_bcs: 9.426e-02, Loss_res: 3.100e-01,Time: 475.30\n",
      "update_loss_res: 1.2392e+00\n",
      "update_loss_bcs: 7.6080e-01\n",
      "It: 31900, Loss: 3.556e-01, Loss_bcs: 9.099e-02, Loss_res: 2.646e-01,Time: 476.78\n",
      "update_loss_res: 1.2440e+00\n",
      "update_loss_bcs: 7.5596e-01\n",
      "It: 32000, Loss: 4.107e-01, Loss_bcs: 8.711e-02, Loss_res: 3.236e-01,Time: 478.26\n",
      "update_loss_res: 1.2326e+00\n",
      "update_loss_bcs: 7.6736e-01\n",
      "max_grad_res/mean_grad_bcs: 1.813e+01\n",
      "mean_grad_bcs/max_grad_res: 5.515e-02\n",
      "adaptive_constant_res_val: 1.692e+01\n",
      "It: 32100, Loss: 3.750e-01, Loss_bcs: 9.287e-02, Loss_res: 2.821e-01,Time: 479.71\n",
      "update_loss_res: 1.2414e+00\n",
      "update_loss_bcs: 7.5860e-01\n",
      "It: 32200, Loss: 3.952e-01, Loss_bcs: 8.304e-02, Loss_res: 3.121e-01,Time: 481.64\n",
      "update_loss_res: 1.2343e+00\n",
      "update_loss_bcs: 7.6565e-01\n",
      "It: 32300, Loss: 4.400e-01, Loss_bcs: 8.607e-02, Loss_res: 3.539e-01,Time: 483.13\n",
      "update_loss_res: 1.2252e+00\n",
      "update_loss_bcs: 7.7481e-01\n",
      "It: 32400, Loss: 5.320e-01, Loss_bcs: 1.012e-01, Loss_res: 4.308e-01,Time: 484.82\n",
      "update_loss_res: 1.2351e+00\n",
      "update_loss_bcs: 7.6494e-01\n",
      "It: 32500, Loss: 4.918e-01, Loss_bcs: 8.757e-02, Loss_res: 4.043e-01,Time: 486.43\n",
      "update_loss_res: 1.2272e+00\n",
      "update_loss_bcs: 7.7282e-01\n",
      "It: 32600, Loss: 3.147e-01, Loss_bcs: 9.339e-02, Loss_res: 2.214e-01,Time: 488.19\n",
      "update_loss_res: 1.2250e+00\n",
      "update_loss_bcs: 7.7501e-01\n",
      "It: 32700, Loss: 4.473e-01, Loss_bcs: 8.242e-02, Loss_res: 3.649e-01,Time: 489.81\n",
      "update_loss_res: 1.2167e+00\n",
      "update_loss_bcs: 7.8325e-01\n",
      "It: 32800, Loss: 3.386e-01, Loss_bcs: 8.297e-02, Loss_res: 2.557e-01,Time: 491.95\n",
      "update_loss_res: 1.2262e+00\n",
      "update_loss_bcs: 7.7380e-01\n",
      "It: 32900, Loss: 5.122e-01, Loss_bcs: 9.646e-02, Loss_res: 4.158e-01,Time: 493.53\n",
      "update_loss_res: 1.2299e+00\n",
      "update_loss_bcs: 7.7009e-01\n",
      "It: 33000, Loss: 7.820e-01, Loss_bcs: 8.099e-02, Loss_res: 7.010e-01,Time: 495.00\n",
      "update_loss_res: 1.2334e+00\n",
      "update_loss_bcs: 7.6657e-01\n",
      "max_grad_res/mean_grad_bcs: 1.996e+01\n",
      "mean_grad_bcs/max_grad_res: 5.010e-02\n",
      "adaptive_constant_res_val: 1.723e+01\n",
      "It: 33100, Loss: 3.973e-01, Loss_bcs: 8.739e-02, Loss_res: 3.099e-01,Time: 496.72\n",
      "update_loss_res: 1.2172e+00\n",
      "update_loss_bcs: 7.8275e-01\n",
      "It: 33200, Loss: 4.402e-01, Loss_bcs: 9.277e-02, Loss_res: 3.474e-01,Time: 498.34\n",
      "update_loss_res: 1.2196e+00\n",
      "update_loss_bcs: 7.8043e-01\n",
      "It: 33300, Loss: 3.676e-01, Loss_bcs: 9.445e-02, Loss_res: 2.731e-01,Time: 499.86\n",
      "update_loss_res: 1.2239e+00\n",
      "update_loss_bcs: 7.7610e-01\n",
      "It: 33400, Loss: 3.624e-01, Loss_bcs: 7.964e-02, Loss_res: 2.827e-01,Time: 501.58\n",
      "update_loss_res: 1.2260e+00\n",
      "update_loss_bcs: 7.7398e-01\n",
      "It: 33500, Loss: 4.670e-01, Loss_bcs: 9.784e-02, Loss_res: 3.691e-01,Time: 503.16\n",
      "update_loss_res: 1.2292e+00\n",
      "update_loss_bcs: 7.7083e-01\n",
      "It: 33600, Loss: 5.399e-01, Loss_bcs: 9.137e-02, Loss_res: 4.485e-01,Time: 504.78\n",
      "update_loss_res: 1.2355e+00\n",
      "update_loss_bcs: 7.6453e-01\n",
      "It: 33700, Loss: 3.468e-01, Loss_bcs: 8.941e-02, Loss_res: 2.574e-01,Time: 506.40\n",
      "update_loss_res: 1.2153e+00\n",
      "update_loss_bcs: 7.8471e-01\n",
      "It: 33800, Loss: 4.091e-01, Loss_bcs: 8.076e-02, Loss_res: 3.283e-01,Time: 507.91\n",
      "update_loss_res: 1.2329e+00\n",
      "update_loss_bcs: 7.6711e-01\n",
      "It: 33900, Loss: 3.672e-01, Loss_bcs: 8.478e-02, Loss_res: 2.825e-01,Time: 509.32\n",
      "update_loss_res: 1.2236e+00\n",
      "update_loss_bcs: 7.7639e-01\n",
      "It: 34000, Loss: 5.343e-01, Loss_bcs: 8.996e-02, Loss_res: 4.443e-01,Time: 510.77\n",
      "update_loss_res: 1.2367e+00\n",
      "update_loss_bcs: 7.6332e-01\n",
      "max_grad_res/mean_grad_bcs: 1.992e+01\n",
      "mean_grad_bcs/max_grad_res: 5.020e-02\n",
      "adaptive_constant_res_val: 1.750e+01\n",
      "It: 34100, Loss: 3.911e-01, Loss_bcs: 9.156e-02, Loss_res: 2.995e-01,Time: 512.26\n",
      "update_loss_res: 1.2229e+00\n",
      "update_loss_bcs: 7.7706e-01\n",
      "It: 34200, Loss: 3.927e-01, Loss_bcs: 8.029e-02, Loss_res: 3.124e-01,Time: 513.72\n",
      "update_loss_res: 1.2120e+00\n",
      "update_loss_bcs: 7.8804e-01\n",
      "It: 34300, Loss: 4.402e-01, Loss_bcs: 9.162e-02, Loss_res: 3.486e-01,Time: 515.16\n",
      "update_loss_res: 1.2216e+00\n",
      "update_loss_bcs: 7.7838e-01\n",
      "It: 34400, Loss: 3.438e-01, Loss_bcs: 7.142e-02, Loss_res: 2.724e-01,Time: 516.84\n",
      "update_loss_res: 1.2235e+00\n",
      "update_loss_bcs: 7.7652e-01\n",
      "It: 34500, Loss: 4.002e-01, Loss_bcs: 8.171e-02, Loss_res: 3.185e-01,Time: 518.30\n",
      "update_loss_res: 1.2197e+00\n",
      "update_loss_bcs: 7.8031e-01\n",
      "It: 34600, Loss: 3.283e-01, Loss_bcs: 9.514e-02, Loss_res: 2.332e-01,Time: 519.72\n",
      "update_loss_res: 1.2112e+00\n",
      "update_loss_bcs: 7.8876e-01\n",
      "It: 34700, Loss: 6.130e-01, Loss_bcs: 9.102e-02, Loss_res: 5.219e-01,Time: 521.30\n",
      "update_loss_res: 1.2071e+00\n",
      "update_loss_bcs: 7.9295e-01\n",
      "It: 34800, Loss: 3.646e-01, Loss_bcs: 6.569e-02, Loss_res: 2.989e-01,Time: 522.87\n",
      "update_loss_res: 1.2001e+00\n",
      "update_loss_bcs: 7.9991e-01\n",
      "It: 34900, Loss: 5.062e-01, Loss_bcs: 9.825e-02, Loss_res: 4.080e-01,Time: 524.44\n",
      "update_loss_res: 1.2204e+00\n",
      "update_loss_bcs: 7.7956e-01\n",
      "It: 35000, Loss: 2.935e-01, Loss_bcs: 7.640e-02, Loss_res: 2.171e-01,Time: 525.96\n",
      "update_loss_res: 1.2000e+00\n",
      "update_loss_bcs: 7.9995e-01\n",
      "max_grad_res/mean_grad_bcs: 1.757e+01\n",
      "mean_grad_bcs/max_grad_res: 5.691e-02\n",
      "adaptive_constant_res_val: 1.750e+01\n",
      "It: 35100, Loss: 4.746e-01, Loss_bcs: 1.010e-01, Loss_res: 3.736e-01,Time: 527.44\n",
      "update_loss_res: 1.2103e+00\n",
      "update_loss_bcs: 7.8974e-01\n",
      "It: 35200, Loss: 2.679e-01, Loss_bcs: 8.915e-02, Loss_res: 1.787e-01,Time: 528.83\n",
      "update_loss_res: 1.2178e+00\n",
      "update_loss_bcs: 7.8224e-01\n",
      "It: 35300, Loss: 5.317e-01, Loss_bcs: 8.819e-02, Loss_res: 4.435e-01,Time: 530.31\n",
      "update_loss_res: 1.2155e+00\n",
      "update_loss_bcs: 7.8450e-01\n",
      "It: 35400, Loss: 3.891e-01, Loss_bcs: 1.028e-01, Loss_res: 2.863e-01,Time: 531.75\n",
      "update_loss_res: 1.2079e+00\n",
      "update_loss_bcs: 7.9210e-01\n",
      "It: 35500, Loss: 3.759e-01, Loss_bcs: 9.679e-02, Loss_res: 2.792e-01,Time: 533.36\n",
      "update_loss_res: 1.2194e+00\n",
      "update_loss_bcs: 7.8056e-01\n",
      "It: 35600, Loss: 3.595e-01, Loss_bcs: 9.508e-02, Loss_res: 2.644e-01,Time: 534.78\n",
      "update_loss_res: 1.2103e+00\n",
      "update_loss_bcs: 7.8973e-01\n",
      "It: 35700, Loss: 3.262e-01, Loss_bcs: 8.716e-02, Loss_res: 2.390e-01,Time: 536.39\n",
      "update_loss_res: 1.2147e+00\n",
      "update_loss_bcs: 7.8530e-01\n",
      "It: 35800, Loss: 3.988e-01, Loss_bcs: 9.558e-02, Loss_res: 3.032e-01,Time: 538.03\n",
      "update_loss_res: 1.2311e+00\n",
      "update_loss_bcs: 7.6891e-01\n",
      "It: 35900, Loss: 3.220e-01, Loss_bcs: 8.749e-02, Loss_res: 2.345e-01,Time: 539.52\n",
      "update_loss_res: 1.2108e+00\n",
      "update_loss_bcs: 7.8918e-01\n",
      "It: 36000, Loss: 3.386e-01, Loss_bcs: 7.721e-02, Loss_res: 2.614e-01,Time: 540.96\n",
      "update_loss_res: 1.2197e+00\n",
      "update_loss_bcs: 7.8032e-01\n",
      "max_grad_res/mean_grad_bcs: 2.616e+01\n",
      "mean_grad_bcs/max_grad_res: 3.822e-02\n",
      "adaptive_constant_res_val: 1.837e+01\n",
      "It: 36100, Loss: 3.940e-01, Loss_bcs: 1.019e-01, Loss_res: 2.921e-01,Time: 542.43\n",
      "update_loss_res: 1.2101e+00\n",
      "update_loss_bcs: 7.8992e-01\n",
      "It: 36200, Loss: 3.551e-01, Loss_bcs: 9.641e-02, Loss_res: 2.587e-01,Time: 543.95\n",
      "update_loss_res: 1.2198e+00\n",
      "update_loss_bcs: 7.8019e-01\n",
      "It: 36300, Loss: 3.654e-01, Loss_bcs: 8.158e-02, Loss_res: 2.838e-01,Time: 545.38\n",
      "update_loss_res: 1.1863e+00\n",
      "update_loss_bcs: 8.1365e-01\n",
      "It: 36400, Loss: 6.984e-01, Loss_bcs: 8.493e-02, Loss_res: 6.134e-01,Time: 546.92\n",
      "update_loss_res: 1.2123e+00\n",
      "update_loss_bcs: 7.8771e-01\n",
      "It: 36500, Loss: 2.947e-01, Loss_bcs: 9.454e-02, Loss_res: 2.002e-01,Time: 548.48\n",
      "update_loss_res: 1.2014e+00\n",
      "update_loss_bcs: 7.9858e-01\n",
      "It: 36600, Loss: 5.480e-01, Loss_bcs: 8.560e-02, Loss_res: 4.624e-01,Time: 550.24\n",
      "update_loss_res: 1.2034e+00\n",
      "update_loss_bcs: 7.9657e-01\n",
      "It: 36700, Loss: 4.405e-01, Loss_bcs: 1.026e-01, Loss_res: 3.378e-01,Time: 551.71\n",
      "update_loss_res: 1.2126e+00\n",
      "update_loss_bcs: 7.8743e-01\n",
      "It: 36800, Loss: 3.249e-01, Loss_bcs: 7.988e-02, Loss_res: 2.450e-01,Time: 553.32\n",
      "update_loss_res: 1.2026e+00\n",
      "update_loss_bcs: 7.9741e-01\n",
      "It: 36900, Loss: 4.453e-01, Loss_bcs: 8.476e-02, Loss_res: 3.606e-01,Time: 554.82\n",
      "update_loss_res: 1.1936e+00\n",
      "update_loss_bcs: 8.0637e-01\n",
      "It: 37000, Loss: 4.004e-01, Loss_bcs: 8.732e-02, Loss_res: 3.130e-01,Time: 556.34\n",
      "update_loss_res: 1.1877e+00\n",
      "update_loss_bcs: 8.1230e-01\n",
      "max_grad_res/mean_grad_bcs: 1.694e+01\n",
      "mean_grad_bcs/max_grad_res: 5.904e-02\n",
      "adaptive_constant_res_val: 1.823e+01\n",
      "It: 37100, Loss: 3.611e-01, Loss_bcs: 8.661e-02, Loss_res: 2.745e-01,Time: 557.90\n",
      "update_loss_res: 1.2020e+00\n",
      "update_loss_bcs: 7.9797e-01\n",
      "It: 37200, Loss: 3.833e-01, Loss_bcs: 8.735e-02, Loss_res: 2.959e-01,Time: 559.38\n",
      "update_loss_res: 1.1984e+00\n",
      "update_loss_bcs: 8.0161e-01\n",
      "It: 37300, Loss: 3.486e-01, Loss_bcs: 8.885e-02, Loss_res: 2.598e-01,Time: 560.99\n",
      "update_loss_res: 1.2056e+00\n",
      "update_loss_bcs: 7.9439e-01\n",
      "It: 37400, Loss: 4.287e-01, Loss_bcs: 8.928e-02, Loss_res: 3.394e-01,Time: 562.43\n",
      "update_loss_res: 1.2086e+00\n",
      "update_loss_bcs: 7.9140e-01\n",
      "It: 37500, Loss: 3.006e-01, Loss_bcs: 8.653e-02, Loss_res: 2.141e-01,Time: 563.93\n",
      "update_loss_res: 1.1994e+00\n",
      "update_loss_bcs: 8.0061e-01\n",
      "It: 37600, Loss: 2.927e-01, Loss_bcs: 9.603e-02, Loss_res: 1.967e-01,Time: 565.46\n",
      "update_loss_res: 1.2082e+00\n",
      "update_loss_bcs: 7.9182e-01\n",
      "It: 37700, Loss: 2.943e-01, Loss_bcs: 9.234e-02, Loss_res: 2.020e-01,Time: 566.92\n",
      "update_loss_res: 1.2008e+00\n",
      "update_loss_bcs: 7.9920e-01\n",
      "It: 37800, Loss: 4.102e-01, Loss_bcs: 9.749e-02, Loss_res: 3.127e-01,Time: 568.35\n",
      "update_loss_res: 1.2140e+00\n",
      "update_loss_bcs: 7.8598e-01\n",
      "It: 37900, Loss: 3.597e-01, Loss_bcs: 8.721e-02, Loss_res: 2.725e-01,Time: 569.84\n",
      "update_loss_res: 1.1947e+00\n",
      "update_loss_bcs: 8.0525e-01\n",
      "It: 38000, Loss: 2.766e-01, Loss_bcs: 8.739e-02, Loss_res: 1.892e-01,Time: 571.37\n",
      "update_loss_res: 1.2046e+00\n",
      "update_loss_bcs: 7.9536e-01\n",
      "max_grad_res/mean_grad_bcs: 1.034e+01\n",
      "mean_grad_bcs/max_grad_res: 9.673e-02\n",
      "adaptive_constant_res_val: 1.744e+01\n",
      "It: 38100, Loss: 2.828e-01, Loss_bcs: 7.793e-02, Loss_res: 2.048e-01,Time: 572.82\n",
      "update_loss_res: 1.2043e+00\n",
      "update_loss_bcs: 7.9566e-01\n",
      "It: 38200, Loss: 4.755e-01, Loss_bcs: 8.230e-02, Loss_res: 3.932e-01,Time: 574.48\n",
      "update_loss_res: 1.2112e+00\n",
      "update_loss_bcs: 7.8881e-01\n",
      "It: 38300, Loss: 3.469e-01, Loss_bcs: 8.900e-02, Loss_res: 2.579e-01,Time: 575.84\n",
      "update_loss_res: 1.1967e+00\n",
      "update_loss_bcs: 8.0326e-01\n",
      "It: 38400, Loss: 3.387e-01, Loss_bcs: 9.083e-02, Loss_res: 2.479e-01,Time: 577.29\n",
      "update_loss_res: 1.2078e+00\n",
      "update_loss_bcs: 7.9218e-01\n",
      "It: 38500, Loss: 6.060e-01, Loss_bcs: 7.444e-02, Loss_res: 5.316e-01,Time: 578.79\n",
      "update_loss_res: 1.2136e+00\n",
      "update_loss_bcs: 7.8639e-01\n",
      "It: 38600, Loss: 3.375e-01, Loss_bcs: 8.371e-02, Loss_res: 2.538e-01,Time: 580.22\n",
      "update_loss_res: 1.2246e+00\n",
      "update_loss_bcs: 7.7542e-01\n",
      "It: 38700, Loss: 3.968e-01, Loss_bcs: 8.126e-02, Loss_res: 3.156e-01,Time: 581.87\n",
      "update_loss_res: 1.2115e+00\n",
      "update_loss_bcs: 7.8851e-01\n",
      "It: 38800, Loss: 3.382e-01, Loss_bcs: 9.087e-02, Loss_res: 2.473e-01,Time: 583.28\n",
      "update_loss_res: 1.2103e+00\n",
      "update_loss_bcs: 7.8974e-01\n",
      "It: 38900, Loss: 4.906e-01, Loss_bcs: 9.227e-02, Loss_res: 3.984e-01,Time: 584.76\n",
      "update_loss_res: 1.2034e+00\n",
      "update_loss_bcs: 7.9662e-01\n",
      "It: 39000, Loss: 3.314e-01, Loss_bcs: 7.781e-02, Loss_res: 2.536e-01,Time: 586.26\n",
      "update_loss_res: 1.1977e+00\n",
      "update_loss_bcs: 8.0232e-01\n",
      "max_grad_res/mean_grad_bcs: 1.854e+01\n",
      "mean_grad_bcs/max_grad_res: 5.393e-02\n",
      "adaptive_constant_res_val: 1.755e+01\n",
      "It: 39100, Loss: 2.897e-01, Loss_bcs: 8.200e-02, Loss_res: 2.077e-01,Time: 587.84\n",
      "update_loss_res: 1.1948e+00\n",
      "update_loss_bcs: 8.0523e-01\n",
      "It: 39200, Loss: 3.116e-01, Loss_bcs: 8.274e-02, Loss_res: 2.288e-01,Time: 589.30\n",
      "update_loss_res: 1.2061e+00\n",
      "update_loss_bcs: 7.9395e-01\n",
      "It: 39300, Loss: 3.349e-01, Loss_bcs: 1.007e-01, Loss_res: 2.343e-01,Time: 590.72\n",
      "update_loss_res: 1.2122e+00\n",
      "update_loss_bcs: 7.8782e-01\n",
      "It: 39400, Loss: 3.817e-01, Loss_bcs: 9.102e-02, Loss_res: 2.907e-01,Time: 592.09\n",
      "update_loss_res: 1.1968e+00\n",
      "update_loss_bcs: 8.0320e-01\n",
      "It: 39500, Loss: 3.538e-01, Loss_bcs: 6.949e-02, Loss_res: 2.843e-01,Time: 593.51\n",
      "update_loss_res: 1.1941e+00\n",
      "update_loss_bcs: 8.0593e-01\n",
      "It: 39600, Loss: 3.025e-01, Loss_bcs: 7.669e-02, Loss_res: 2.258e-01,Time: 594.93\n",
      "update_loss_res: 1.2021e+00\n",
      "update_loss_bcs: 7.9788e-01\n",
      "It: 39700, Loss: 3.073e-01, Loss_bcs: 7.937e-02, Loss_res: 2.280e-01,Time: 596.37\n",
      "update_loss_res: 1.2069e+00\n",
      "update_loss_bcs: 7.9309e-01\n",
      "It: 39800, Loss: 4.007e-01, Loss_bcs: 8.096e-02, Loss_res: 3.197e-01,Time: 597.84\n",
      "update_loss_res: 1.1880e+00\n",
      "update_loss_bcs: 8.1200e-01\n",
      "It: 39900, Loss: 3.315e-01, Loss_bcs: 8.395e-02, Loss_res: 2.476e-01,Time: 599.28\n",
      "update_loss_res: 1.1934e+00\n",
      "update_loss_bcs: 8.0664e-01\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000, Loss: 3.870e-01, Loss_bcs: 7.158e-02, Loss_res: 3.155e-01,Time: 601.93\n",
      "update_loss_res: 1.2019e+00\n",
      "update_loss_bcs: 7.9810e-01\n",
      "max_grad_res/mean_grad_bcs: 1.887e+01\n",
      "mean_grad_bcs/max_grad_res: 5.299e-02\n",
      "adaptive_constant_res_val: 1.768e+01\n",
      "Relative L2 error_u: 2.47e-02\n",
      "Relative L2 error_f: 7.44e-03\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-28-2023_05-12-11-900127_M2\n",
      "Final loss total loss: 3.870352e-01\n",
      "Final loss loss_res: 3.154541e-01\n",
      "Final loss loss_bcs: 7.158107e-02\n",
      "Final loss loss_bc1: 8.050989e-04\n",
      "Final loss loss_bc2: 1.602485e-03\n",
      "Final loss loss_bc3: 9.793726e-04\n",
      "Final loss loss_bc4: 6.922412e-04\n",
      "average lambda_bc1.4996e+01\n",
      "average lambda_res1.0\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:6.0197e+02\n",
      "average of error_u_list:2.4728e-02\n",
      "average of error_v_list:7.4383e-03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "a_1 = 1\n",
    "a_2 = 4\n",
    "\n",
    "# Parameter\n",
    "lam = 1.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[-1.0, -1.0], [1.0, -1.0]])\n",
    "bc2_coords = np.array([[1.0, -1.0], [1.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "bc4_coords = np.array([[-1.0, 1.0], [-1.0, -1.0]])\n",
    "\n",
    "dom_coords = np.array([[-1.0, -1.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "x1 = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x2 = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star, a_1, a_2)\n",
    "f_star = f(X_star, a_1, a_2, lam)\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = None\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'            # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False    # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, a_1, a_2, lam), name='Forcing')\n",
    "\n",
    "        # [elapsed, error_u , error_f ,  mode] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , \n",
    "        #                                                                stiff_ratio , X_star ,u_star , f_star , nIter ,bcbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    " #def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                model.trainmb(nIter, batch_size=mbbatch_size )\n",
    "            else:\n",
    "                model.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            f_pred = model.predict_r(X_star)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "            model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( x1 , x2 , X_star , u_star , u_pred , f_star , f_pred)\n",
    "\n",
    "            model.print(\"average lambda_bc\" , np.average(model.adpative_constant_log))\n",
    "            model.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "            time_list.append(elapsed)\n",
    "            error_u_list.append(error_u)\n",
    "            error_f_list.append(error_f)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_v_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_f_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_Helmholtz_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### database is a vailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21548/2533309064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predicted solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mU_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mF_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u_pred' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot ###\n",
    "\n",
    "# Exact solution & Predicted solution\n",
    "# Exact soluton\n",
    "U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "# Predicted solution\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Exact $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Predicted $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual loss & Boundary loss\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "gradients_res_list = []\n",
    "gradients_bcs_list = []\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "for j in range(num_hidden_layers):\n",
    "    gradient_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradient_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    gradients_res_list.append(gradient_res)\n",
    "    gradients_bcs_list.append(gradient_bcs)\n",
    "\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim([-3.0, 3.0])\n",
    "    ax.set_ylim([0,100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = mode.eigenvalue_log\n",
    "    eigenvalues_bcs_list = mode.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = mode.eigenvalue_res_log\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
