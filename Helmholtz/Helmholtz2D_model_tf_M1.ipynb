{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "######################################################################################################\n",
    "def u(x, a_1, a_2):\n",
    "    return np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_xx(x, a_1, a_2):\n",
    "    return - (a_1 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_yy(x, a_1, a_2):\n",
    "    return - (a_2 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "# Forcing\n",
    "def f(x, a_1, a_2, lam):\n",
    "    return u_xx(x, a_1, a_2) + u_yy(x, a_1, a_2) + lam * u(x, a_1, a_2)\n",
    "\n",
    "def operator(u, x1, x2, lam, sigma_x1=1.0, sigma_x2=1.0):\n",
    "    u_x1 = tf.gradients(u, x1)[0] / sigma_x1\n",
    "    u_x2 = tf.gradients(u, x2)[0] / sigma_x2\n",
    "    u_xx1 = tf.gradients(u_x1, x1)[0] / sigma_x1\n",
    "    u_xx2 = tf.gradients(u_x2, x2)[0] / sigma_x2\n",
    "    residual = u_xx1 + u_xx2 + lam * u\n",
    "    return residual\n",
    "#######################################################################################################\n",
    "\n",
    "class Helmholtz2D:\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, model, stiff_ratio):\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x1, self.sigma_x1 = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x2, self.sigma_x2 = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Helmoholtz constant\n",
    "        self.lam = tf.constant(lam, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = model\n",
    "\n",
    "        # Record stiff ratio\n",
    "        self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        if model in ['M3', 'M4']:\n",
    "            # Initialize encoder weights and biases\n",
    "            self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "            self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x1_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # Define placeholder for adaptive constant\n",
    "        self.adaptive_constant_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred = self.net_u(self.x1_bc1_tf, self.x2_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.x1_bc2_tf, self.x2_bc2_tf)\n",
    "        self.u_bc3_pred = self.net_u(self.x1_bc3_tf, self.x2_bc3_tf)\n",
    "        self.u_bc4_pred = self.net_u(self.x1_bc4_tf, self.x2_bc4_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x1_u_tf, self.x2_u_tf)\n",
    "        self.r_pred = self.net_r(self.x1_r_tf, self.x2_r_tf)\n",
    "\n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_tf - self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_tf - self.u_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.u_bc3_tf - self.u_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.u_bc4_tf - self.u_bc4_pred))\n",
    "        self.loss_bcs = self.adaptive_constant_tf * (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                        1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        # Logger\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict(self.layers)\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict(self.layers)\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        # Compute and store the adaptive constant\n",
    "        self.adpative_constant_log = []\n",
    "        self.adaptive_constant_list = []\n",
    "        \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.adaptive_constant = self.max_grad_res / self.mean_grad_bcs\n",
    "\n",
    "        # Stiff Ratio\n",
    "        if self.stiff_ratio:\n",
    "            self.Hessian, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "            self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "            self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "            self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "            self.eigenvalue_log = []\n",
    "            self.eigenvalue_bcs_log = []\n",
    "            self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "     # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,\n",
    "                                                           self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,\n",
    "                                                  self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P),\n",
    "                      dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),\n",
    "                      dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "\n",
    "        return H, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self,size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, x1, x2):\n",
    "        u = self.forward_pass(tf.concat([x1, x2], 1))\n",
    "        return u\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x1, x2):\n",
    "        u = self.net_u(x1, x2)\n",
    "        residual = self.operator(u, x1, x2,\n",
    "                                 self.lam,\n",
    "                                 self.sigma_x1,\n",
    "                                 self.sigma_x2)\n",
    "        return residual\n",
    "\n",
    "    # Feed minibatch\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter=10000, batch_size=128):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            if self.stiff_ratio:\n",
    "                if it % 1000 == 0:\n",
    "                    print(\"Eigenvalues information stored ...\")\n",
    "                    eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "                                                                                   self.eigenvalues_bcs,\n",
    "                                                                                   self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # Log eigenvalues\n",
    "                    self.eigenvalue_log.append(eigenvalues)\n",
    "                    self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                if self.model in ['M2', 'M4']:\n",
    "                    adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                    self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta) \\\n",
    "                                                 + self.beta * self.adaptive_constant_val\n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' %\n",
    "                      (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # Store gradients\n",
    "            if it % 10000 == 0:\n",
    "                self.save_gradients(tf_dict)\n",
    "                print(\"Gradients information stored ...\")\n",
    "\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter , bcbatch_size , fbatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Fetch boundary mini-batches\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], bcbatch_size)\n",
    "        X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], bcbatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, fbatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.u_bc3_tf: u_bc3_batch,\n",
    "                    self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.u_bc4_tf: u_bc4_batch,\n",
    "                    self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                    self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                    }\n",
    "\n",
    "\n",
    "        for it in range(nIter):\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                # self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                #     self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "\n",
    "                # self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "  # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                # self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                #     self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "\n",
    "                # self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_u_tf: X_star[:, 0:1], self.x2_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_r_tf: X_star[:, 0:1], self.x2_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, lam ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f ,  model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_21548/2275519620.py:264: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_21548/2275519620.py:96: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_21548/2275519620.py:96: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_21548/2275519620.py:99: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 02:40:57.688884: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 02:40:57.710564: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-11-23 02:40:57.711068: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d4b18e6230 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-23 02:40:57.711086: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-11-23 02:40:57.712480: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_21548/2275519620.py:151: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_21548/2275519620.py:154: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_21548/2275519620.py:159: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_21548/2275519620.py:199: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "It: 0, Loss: 6.742e+03, Loss_bcs: 1.203e+00, Loss_res: 6.741e+03, Adaptive_Constant: 1.00 ,Time: 1.54\n",
      "It: 10, Loss: 7.243e+03, Loss_bcs: 9.495e-02, Loss_res: 7.243e+03, Adaptive_Constant: 1.00 ,Time: 0.11\n",
      "It: 20, Loss: 6.866e+03, Loss_bcs: 1.835e-02, Loss_res: 6.866e+03, Adaptive_Constant: 1.00 ,Time: 0.22\n",
      "It: 30, Loss: 6.179e+03, Loss_bcs: 7.998e-02, Loss_res: 6.179e+03, Adaptive_Constant: 1.00 ,Time: 0.26\n",
      "It: 40, Loss: 6.622e+03, Loss_bcs: 3.546e-01, Loss_res: 6.621e+03, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 50, Loss: 7.402e+03, Loss_bcs: 2.908e-01, Loss_res: 7.402e+03, Adaptive_Constant: 1.00 ,Time: 0.22\n",
      "It: 60, Loss: 7.283e+03, Loss_bcs: 2.415e-01, Loss_res: 7.283e+03, Adaptive_Constant: 1.00 ,Time: 0.22\n",
      "It: 70, Loss: 6.841e+03, Loss_bcs: 4.569e-01, Loss_res: 6.840e+03, Adaptive_Constant: 1.00 ,Time: 0.25\n",
      "It: 80, Loss: 6.839e+03, Loss_bcs: 6.538e-01, Loss_res: 6.838e+03, Adaptive_Constant: 1.00 ,Time: 0.22\n",
      "It: 90, Loss: 6.344e+03, Loss_bcs: 3.685e-01, Loss_res: 6.344e+03, Adaptive_Constant: 1.00 ,Time: 0.23\n",
      "It: 100, Loss: 6.713e+03, Loss_bcs: 9.785e-01, Loss_res: 6.712e+03, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 110, Loss: 6.489e+03, Loss_bcs: 6.200e-01, Loss_res: 6.489e+03, Adaptive_Constant: 1.00 ,Time: 0.11\n",
      "It: 120, Loss: 6.974e+03, Loss_bcs: 9.204e-01, Loss_res: 6.973e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 130, Loss: 6.987e+03, Loss_bcs: 1.448e+00, Loss_res: 6.985e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 140, Loss: 5.660e+03, Loss_bcs: 1.923e+00, Loss_res: 5.658e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 150, Loss: 7.627e+03, Loss_bcs: 1.194e+00, Loss_res: 7.626e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 160, Loss: 7.872e+03, Loss_bcs: 1.254e+00, Loss_res: 7.871e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 170, Loss: 8.001e+03, Loss_bcs: 2.596e+00, Loss_res: 7.998e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 180, Loss: 6.989e+03, Loss_bcs: 2.141e+00, Loss_res: 6.986e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 190, Loss: 6.587e+03, Loss_bcs: 3.146e+00, Loss_res: 6.584e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 200, Loss: 5.858e+03, Loss_bcs: 5.588e+00, Loss_res: 5.853e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 210, Loss: 6.905e+03, Loss_bcs: 6.435e+00, Loss_res: 6.899e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 220, Loss: 7.198e+03, Loss_bcs: 7.832e+00, Loss_res: 7.190e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 230, Loss: 6.029e+03, Loss_bcs: 7.263e+00, Loss_res: 6.022e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 240, Loss: 7.293e+03, Loss_bcs: 7.446e+00, Loss_res: 7.285e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 250, Loss: 6.359e+03, Loss_bcs: 8.124e+00, Loss_res: 6.351e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 260, Loss: 6.950e+03, Loss_bcs: 8.190e+00, Loss_res: 6.942e+03, Adaptive_Constant: 1.00 ,Time: 0.11\n",
      "It: 270, Loss: 6.408e+03, Loss_bcs: 1.064e+01, Loss_res: 6.398e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 280, Loss: 6.188e+03, Loss_bcs: 1.084e+01, Loss_res: 6.178e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 290, Loss: 7.171e+03, Loss_bcs: 1.234e+01, Loss_res: 7.159e+03, Adaptive_Constant: 1.00 ,Time: 0.11\n",
      "It: 300, Loss: 6.824e+03, Loss_bcs: 1.288e+01, Loss_res: 6.812e+03, Adaptive_Constant: 1.00 ,Time: 0.11\n",
      "It: 310, Loss: 6.920e+03, Loss_bcs: 1.253e+01, Loss_res: 6.908e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 320, Loss: 5.608e+03, Loss_bcs: 1.235e+01, Loss_res: 5.595e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 330, Loss: 7.392e+03, Loss_bcs: 1.450e+01, Loss_res: 7.378e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 340, Loss: 6.569e+03, Loss_bcs: 1.631e+01, Loss_res: 6.552e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 350, Loss: 6.866e+03, Loss_bcs: 1.953e+01, Loss_res: 6.847e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 360, Loss: 6.609e+03, Loss_bcs: 1.853e+01, Loss_res: 6.590e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 370, Loss: 5.817e+03, Loss_bcs: 2.040e+01, Loss_res: 5.796e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 380, Loss: 5.864e+03, Loss_bcs: 2.089e+01, Loss_res: 5.843e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 390, Loss: 5.561e+03, Loss_bcs: 1.568e+01, Loss_res: 5.546e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 400, Loss: 4.826e+03, Loss_bcs: 1.313e+01, Loss_res: 4.813e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 410, Loss: 4.815e+03, Loss_bcs: 1.163e+01, Loss_res: 4.803e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 420, Loss: 5.323e+03, Loss_bcs: 1.050e+01, Loss_res: 5.312e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 430, Loss: 5.112e+03, Loss_bcs: 9.337e+00, Loss_res: 5.103e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 440, Loss: 5.994e+03, Loss_bcs: 9.093e+00, Loss_res: 5.985e+03, Adaptive_Constant: 1.00 ,Time: 0.14\n",
      "It: 450, Loss: 3.991e+03, Loss_bcs: 9.002e+00, Loss_res: 3.982e+03, Adaptive_Constant: 1.00 ,Time: 0.12\n",
      "It: 460, Loss: 5.504e+03, Loss_bcs: 9.053e+00, Loss_res: 5.495e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 470, Loss: 5.449e+03, Loss_bcs: 7.607e+00, Loss_res: 5.442e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 480, Loss: 5.546e+03, Loss_bcs: 8.854e+00, Loss_res: 5.537e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 490, Loss: 5.171e+03, Loss_bcs: 8.163e+00, Loss_res: 5.162e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 500, Loss: 4.937e+03, Loss_bcs: 9.196e+00, Loss_res: 4.927e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 510, Loss: 4.605e+03, Loss_bcs: 8.512e+00, Loss_res: 4.596e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 520, Loss: 6.393e+03, Loss_bcs: 1.018e+01, Loss_res: 6.383e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 530, Loss: 4.632e+03, Loss_bcs: 8.862e+00, Loss_res: 4.623e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 540, Loss: 4.918e+03, Loss_bcs: 8.710e+00, Loss_res: 4.909e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 550, Loss: 4.578e+03, Loss_bcs: 9.109e+00, Loss_res: 4.568e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 560, Loss: 3.467e+03, Loss_bcs: 9.994e+00, Loss_res: 3.457e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 570, Loss: 4.503e+03, Loss_bcs: 1.195e+01, Loss_res: 4.491e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 580, Loss: 4.785e+03, Loss_bcs: 1.135e+01, Loss_res: 4.773e+03, Adaptive_Constant: 1.00 ,Time: 0.11\n",
      "It: 590, Loss: 4.429e+03, Loss_bcs: 1.297e+01, Loss_res: 4.416e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 600, Loss: 5.071e+03, Loss_bcs: 1.550e+01, Loss_res: 5.055e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 610, Loss: 3.527e+03, Loss_bcs: 1.528e+01, Loss_res: 3.512e+03, Adaptive_Constant: 1.00 ,Time: 0.11\n",
      "It: 620, Loss: 4.026e+03, Loss_bcs: 1.572e+01, Loss_res: 4.010e+03, Adaptive_Constant: 1.00 ,Time: 0.18\n",
      "It: 630, Loss: 3.780e+03, Loss_bcs: 1.665e+01, Loss_res: 3.764e+03, Adaptive_Constant: 1.00 ,Time: 0.14\n",
      "It: 640, Loss: 2.582e+03, Loss_bcs: 1.860e+01, Loss_res: 2.563e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 650, Loss: 2.083e+03, Loss_bcs: 2.071e+01, Loss_res: 2.062e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 660, Loss: 3.456e+03, Loss_bcs: 2.175e+01, Loss_res: 3.434e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 670, Loss: 3.022e+03, Loss_bcs: 2.260e+01, Loss_res: 3.000e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 680, Loss: 2.841e+03, Loss_bcs: 2.410e+01, Loss_res: 2.817e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 690, Loss: 2.073e+03, Loss_bcs: 2.340e+01, Loss_res: 2.050e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 700, Loss: 2.372e+03, Loss_bcs: 2.572e+01, Loss_res: 2.346e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 710, Loss: 2.674e+03, Loss_bcs: 2.505e+01, Loss_res: 2.649e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 720, Loss: 2.810e+03, Loss_bcs: 2.448e+01, Loss_res: 2.786e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 730, Loss: 2.407e+03, Loss_bcs: 2.550e+01, Loss_res: 2.382e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 740, Loss: 2.303e+03, Loss_bcs: 2.607e+01, Loss_res: 2.277e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 750, Loss: 2.527e+03, Loss_bcs: 2.638e+01, Loss_res: 2.501e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 760, Loss: 2.133e+03, Loss_bcs: 2.348e+01, Loss_res: 2.109e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 770, Loss: 1.932e+03, Loss_bcs: 2.515e+01, Loss_res: 1.907e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 780, Loss: 1.223e+03, Loss_bcs: 2.446e+01, Loss_res: 1.198e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 790, Loss: 1.995e+03, Loss_bcs: 2.281e+01, Loss_res: 1.973e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 800, Loss: 1.223e+03, Loss_bcs: 2.338e+01, Loss_res: 1.199e+03, Adaptive_Constant: 1.00 ,Time: 0.07\n",
      "It: 810, Loss: 1.055e+03, Loss_bcs: 2.353e+01, Loss_res: 1.031e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 820, Loss: 1.571e+03, Loss_bcs: 2.127e+01, Loss_res: 1.550e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 830, Loss: 1.668e+03, Loss_bcs: 2.340e+01, Loss_res: 1.644e+03, Adaptive_Constant: 1.00 ,Time: 0.11\n",
      "It: 840, Loss: 1.369e+03, Loss_bcs: 2.082e+01, Loss_res: 1.348e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 850, Loss: 1.246e+03, Loss_bcs: 2.160e+01, Loss_res: 1.224e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 860, Loss: 1.039e+03, Loss_bcs: 2.023e+01, Loss_res: 1.019e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 870, Loss: 1.225e+03, Loss_bcs: 2.077e+01, Loss_res: 1.204e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 880, Loss: 1.587e+03, Loss_bcs: 2.029e+01, Loss_res: 1.567e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 890, Loss: 9.888e+02, Loss_bcs: 1.953e+01, Loss_res: 9.693e+02, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 900, Loss: 1.011e+03, Loss_bcs: 1.871e+01, Loss_res: 9.924e+02, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 910, Loss: 1.292e+03, Loss_bcs: 2.080e+01, Loss_res: 1.271e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 920, Loss: 9.242e+02, Loss_bcs: 1.867e+01, Loss_res: 9.055e+02, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 930, Loss: 1.403e+03, Loss_bcs: 2.070e+01, Loss_res: 1.382e+03, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 940, Loss: 1.116e+03, Loss_bcs: 2.295e+01, Loss_res: 1.093e+03, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 950, Loss: 8.916e+02, Loss_bcs: 2.161e+01, Loss_res: 8.700e+02, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 960, Loss: 4.760e+02, Loss_bcs: 2.384e+01, Loss_res: 4.521e+02, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 970, Loss: 8.113e+02, Loss_bcs: 2.356e+01, Loss_res: 7.878e+02, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "It: 980, Loss: 9.573e+02, Loss_bcs: 2.379e+01, Loss_res: 9.335e+02, Adaptive_Constant: 1.00 ,Time: 0.08\n",
      "It: 990, Loss: 5.103e+02, Loss_bcs: 2.575e+01, Loss_res: 4.845e+02, Adaptive_Constant: 1.00 ,Time: 0.10\n",
      "It: 1000, Loss: 1.271e+03, Loss_bcs: 2.438e+01, Loss_res: 1.246e+03, Adaptive_Constant: 1.00 ,Time: 0.09\n",
      "Relative L2 error_u: 3.96e+00\n",
      "Relative L2 error_f: 3.43e-01\n",
      "\n",
      "\n",
      "Method:  mini_batch\n",
      "\n",
      "average of time_list: 14.135827779769897\n",
      "average of error_u_list: 3.9561259705449854\n",
      "average of error_v_list: 0.3431731284418151\n",
      "Method:  full_batch\n",
      "Epoch:  1\n",
      "It: 0, Loss: 6.888e+03, Loss_bcs: 5.334e-02, Loss_res: 6.887e+03, Adaptive_Constant: 1.00 ,Time: 1.57\n",
      "It: 10, Loss: 6.884e+03, Loss_bcs: 1.043e-01, Loss_res: 6.884e+03, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 20, Loss: 6.877e+03, Loss_bcs: 3.035e-01, Loss_res: 6.877e+03, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 30, Loss: 6.870e+03, Loss_bcs: 6.837e-01, Loss_res: 6.869e+03, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 40, Loss: 6.857e+03, Loss_bcs: 1.092e+00, Loss_res: 6.856e+03, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 50, Loss: 6.838e+03, Loss_bcs: 2.090e+00, Loss_res: 6.836e+03, Adaptive_Constant: 1.00 ,Time: 0.34\n",
      "It: 60, Loss: 6.818e+03, Loss_bcs: 3.359e+00, Loss_res: 6.815e+03, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 70, Loss: 6.802e+03, Loss_bcs: 5.453e+00, Loss_res: 6.797e+03, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 80, Loss: 6.782e+03, Loss_bcs: 7.616e+00, Loss_res: 6.774e+03, Adaptive_Constant: 1.00 ,Time: 0.33\n",
      "It: 90, Loss: 6.749e+03, Loss_bcs: 1.179e+01, Loss_res: 6.737e+03, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 100, Loss: 6.680e+03, Loss_bcs: 1.836e+01, Loss_res: 6.662e+03, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 110, Loss: 6.481e+03, Loss_bcs: 2.735e+01, Loss_res: 6.454e+03, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 120, Loss: 5.977e+03, Loss_bcs: 3.415e+01, Loss_res: 5.942e+03, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 130, Loss: 5.232e+03, Loss_bcs: 3.253e+01, Loss_res: 5.200e+03, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 140, Loss: 4.478e+03, Loss_bcs: 2.700e+01, Loss_res: 4.451e+03, Adaptive_Constant: 1.00 ,Time: 0.82\n",
      "It: 150, Loss: 3.943e+03, Loss_bcs: 2.475e+01, Loss_res: 3.918e+03, Adaptive_Constant: 1.00 ,Time: 0.66\n",
      "It: 160, Loss: 3.499e+03, Loss_bcs: 2.209e+01, Loss_res: 3.477e+03, Adaptive_Constant: 1.00 ,Time: 0.34\n",
      "It: 170, Loss: 3.089e+03, Loss_bcs: 2.360e+01, Loss_res: 3.065e+03, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 180, Loss: 2.742e+03, Loss_bcs: 2.705e+01, Loss_res: 2.715e+03, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 190, Loss: 2.422e+03, Loss_bcs: 3.011e+01, Loss_res: 2.392e+03, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 200, Loss: 2.045e+03, Loss_bcs: 2.732e+01, Loss_res: 2.017e+03, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 210, Loss: 1.714e+03, Loss_bcs: 2.298e+01, Loss_res: 1.691e+03, Adaptive_Constant: 1.00 ,Time: 0.28\n",
      "It: 220, Loss: 1.454e+03, Loss_bcs: 2.020e+01, Loss_res: 1.434e+03, Adaptive_Constant: 1.00 ,Time: 0.28\n",
      "It: 230, Loss: 1.271e+03, Loss_bcs: 1.899e+01, Loss_res: 1.252e+03, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 240, Loss: 1.127e+03, Loss_bcs: 2.052e+01, Loss_res: 1.106e+03, Adaptive_Constant: 1.00 ,Time: 0.35\n",
      "It: 250, Loss: 1.026e+03, Loss_bcs: 2.216e+01, Loss_res: 1.003e+03, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 260, Loss: 9.425e+02, Loss_bcs: 2.381e+01, Loss_res: 9.187e+02, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 270, Loss: 8.739e+02, Loss_bcs: 2.457e+01, Loss_res: 8.494e+02, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 280, Loss: 8.224e+02, Loss_bcs: 2.569e+01, Loss_res: 7.967e+02, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 290, Loss: 7.847e+02, Loss_bcs: 2.678e+01, Loss_res: 7.579e+02, Adaptive_Constant: 1.00 ,Time: 0.39\n",
      "It: 300, Loss: 7.558e+02, Loss_bcs: 2.835e+01, Loss_res: 7.274e+02, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 310, Loss: 7.343e+02, Loss_bcs: 2.925e+01, Loss_res: 7.050e+02, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 320, Loss: 7.168e+02, Loss_bcs: 2.978e+01, Loss_res: 6.870e+02, Adaptive_Constant: 1.00 ,Time: 0.42\n",
      "It: 330, Loss: 7.016e+02, Loss_bcs: 3.014e+01, Loss_res: 6.714e+02, Adaptive_Constant: 1.00 ,Time: 0.39\n",
      "It: 340, Loss: 6.877e+02, Loss_bcs: 3.049e+01, Loss_res: 6.572e+02, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 350, Loss: 6.756e+02, Loss_bcs: 3.074e+01, Loss_res: 6.448e+02, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 360, Loss: 6.625e+02, Loss_bcs: 3.109e+01, Loss_res: 6.314e+02, Adaptive_Constant: 1.00 ,Time: 0.27\n",
      "It: 370, Loss: 6.511e+02, Loss_bcs: 3.144e+01, Loss_res: 6.197e+02, Adaptive_Constant: 1.00 ,Time: 0.27\n",
      "It: 380, Loss: 6.408e+02, Loss_bcs: 3.180e+01, Loss_res: 6.090e+02, Adaptive_Constant: 1.00 ,Time: 0.27\n",
      "It: 390, Loss: 6.312e+02, Loss_bcs: 3.212e+01, Loss_res: 5.991e+02, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 400, Loss: 6.222e+02, Loss_bcs: 3.241e+01, Loss_res: 5.897e+02, Adaptive_Constant: 1.00 ,Time: 0.28\n",
      "It: 410, Loss: 6.138e+02, Loss_bcs: 3.267e+01, Loss_res: 5.811e+02, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 420, Loss: 6.058e+02, Loss_bcs: 3.293e+01, Loss_res: 5.729e+02, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 430, Loss: 5.981e+02, Loss_bcs: 3.319e+01, Loss_res: 5.649e+02, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 440, Loss: 5.900e+02, Loss_bcs: 3.347e+01, Loss_res: 5.565e+02, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 450, Loss: 5.805e+02, Loss_bcs: 3.389e+01, Loss_res: 5.466e+02, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 460, Loss: 5.610e+02, Loss_bcs: 3.420e+01, Loss_res: 5.267e+02, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 470, Loss: 5.313e+02, Loss_bcs: 3.412e+01, Loss_res: 4.971e+02, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 480, Loss: 4.756e+02, Loss_bcs: 3.406e+01, Loss_res: 4.416e+02, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 490, Loss: 3.881e+02, Loss_bcs: 3.526e+01, Loss_res: 3.528e+02, Adaptive_Constant: 1.00 ,Time: 0.28\n",
      "It: 500, Loss: 3.265e+02, Loss_bcs: 3.420e+01, Loss_res: 2.923e+02, Adaptive_Constant: 1.00 ,Time: 0.33\n",
      "It: 510, Loss: 2.742e+02, Loss_bcs: 3.245e+01, Loss_res: 2.417e+02, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 520, Loss: 2.315e+02, Loss_bcs: 3.028e+01, Loss_res: 2.012e+02, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 530, Loss: 2.019e+02, Loss_bcs: 2.799e+01, Loss_res: 1.739e+02, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 540, Loss: 1.776e+02, Loss_bcs: 2.545e+01, Loss_res: 1.522e+02, Adaptive_Constant: 1.00 ,Time: 0.28\n",
      "It: 550, Loss: 1.594e+02, Loss_bcs: 2.321e+01, Loss_res: 1.362e+02, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 560, Loss: 1.433e+02, Loss_bcs: 2.128e+01, Loss_res: 1.220e+02, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 570, Loss: 1.311e+02, Loss_bcs: 1.967e+01, Loss_res: 1.115e+02, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 580, Loss: 1.212e+02, Loss_bcs: 1.834e+01, Loss_res: 1.028e+02, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 590, Loss: 1.126e+02, Loss_bcs: 1.725e+01, Loss_res: 9.533e+01, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 600, Loss: 1.050e+02, Loss_bcs: 1.634e+01, Loss_res: 8.863e+01, Adaptive_Constant: 1.00 ,Time: 0.33\n",
      "It: 610, Loss: 9.807e+01, Loss_bcs: 1.556e+01, Loss_res: 8.251e+01, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 620, Loss: 9.171e+01, Loss_bcs: 1.488e+01, Loss_res: 7.683e+01, Adaptive_Constant: 1.00 ,Time: 0.39\n",
      "It: 630, Loss: 8.576e+01, Loss_bcs: 1.428e+01, Loss_res: 7.148e+01, Adaptive_Constant: 1.00 ,Time: 0.33\n",
      "It: 640, Loss: 8.035e+01, Loss_bcs: 1.372e+01, Loss_res: 6.663e+01, Adaptive_Constant: 1.00 ,Time: 0.36\n",
      "It: 650, Loss: 7.483e+01, Loss_bcs: 1.323e+01, Loss_res: 6.160e+01, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 660, Loss: 6.829e+01, Loss_bcs: 1.274e+01, Loss_res: 5.555e+01, Adaptive_Constant: 1.00 ,Time: 0.35\n",
      "It: 670, Loss: 6.088e+01, Loss_bcs: 1.228e+01, Loss_res: 4.860e+01, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 680, Loss: 5.328e+01, Loss_bcs: 1.189e+01, Loss_res: 4.139e+01, Adaptive_Constant: 1.00 ,Time: 0.35\n",
      "It: 690, Loss: 4.831e+01, Loss_bcs: 1.128e+01, Loss_res: 3.703e+01, Adaptive_Constant: 1.00 ,Time: 0.35\n",
      "It: 700, Loss: 4.440e+01, Loss_bcs: 1.072e+01, Loss_res: 3.368e+01, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 710, Loss: 4.088e+01, Loss_bcs: 1.029e+01, Loss_res: 3.058e+01, Adaptive_Constant: 1.00 ,Time: 0.34\n",
      "It: 720, Loss: 3.749e+01, Loss_bcs: 9.826e+00, Loss_res: 2.766e+01, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 730, Loss: 3.402e+01, Loss_bcs: 9.339e+00, Loss_res: 2.468e+01, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 740, Loss: 3.108e+01, Loss_bcs: 8.806e+00, Loss_res: 2.227e+01, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 750, Loss: 2.740e+01, Loss_bcs: 8.220e+00, Loss_res: 1.918e+01, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 760, Loss: 2.423e+01, Loss_bcs: 7.663e+00, Loss_res: 1.657e+01, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 770, Loss: 2.220e+01, Loss_bcs: 7.159e+00, Loss_res: 1.504e+01, Adaptive_Constant: 1.00 ,Time: 0.35\n",
      "It: 780, Loss: 2.053e+01, Loss_bcs: 6.739e+00, Loss_res: 1.379e+01, Adaptive_Constant: 1.00 ,Time: 0.37\n",
      "It: 790, Loss: 1.918e+01, Loss_bcs: 6.389e+00, Loss_res: 1.279e+01, Adaptive_Constant: 1.00 ,Time: 0.38\n",
      "It: 800, Loss: 1.802e+01, Loss_bcs: 6.089e+00, Loss_res: 1.194e+01, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 810, Loss: 1.703e+01, Loss_bcs: 5.826e+00, Loss_res: 1.120e+01, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 820, Loss: 1.616e+01, Loss_bcs: 5.595e+00, Loss_res: 1.057e+01, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 830, Loss: 1.540e+01, Loss_bcs: 5.386e+00, Loss_res: 1.001e+01, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 840, Loss: 1.519e+01, Loss_bcs: 5.194e+00, Loss_res: 9.997e+00, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 850, Loss: 1.429e+01, Loss_bcs: 5.018e+00, Loss_res: 9.274e+00, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 860, Loss: 1.364e+01, Loss_bcs: 4.841e+00, Loss_res: 8.798e+00, Adaptive_Constant: 1.00 ,Time: 0.35\n",
      "It: 870, Loss: 1.299e+01, Loss_bcs: 4.702e+00, Loss_res: 8.283e+00, Adaptive_Constant: 1.00 ,Time: 0.32\n",
      "It: 880, Loss: 1.250e+01, Loss_bcs: 4.563e+00, Loss_res: 7.941e+00, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 890, Loss: 1.205e+01, Loss_bcs: 4.425e+00, Loss_res: 7.628e+00, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 900, Loss: 1.164e+01, Loss_bcs: 4.295e+00, Loss_res: 7.342e+00, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 910, Loss: 1.125e+01, Loss_bcs: 4.175e+00, Loss_res: 7.075e+00, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "It: 920, Loss: 1.089e+01, Loss_bcs: 4.060e+00, Loss_res: 6.828e+00, Adaptive_Constant: 1.00 ,Time: 0.30\n",
      "It: 930, Loss: 1.055e+01, Loss_bcs: 3.950e+00, Loss_res: 6.601e+00, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 940, Loss: 1.027e+01, Loss_bcs: 3.846e+00, Loss_res: 6.427e+00, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 950, Loss: 1.100e+01, Loss_bcs: 3.741e+00, Loss_res: 7.257e+00, Adaptive_Constant: 1.00 ,Time: 0.34\n",
      "It: 960, Loss: 9.907e+00, Loss_bcs: 3.671e+00, Loss_res: 6.236e+00, Adaptive_Constant: 1.00 ,Time: 0.34\n",
      "It: 970, Loss: 9.411e+00, Loss_bcs: 3.570e+00, Loss_res: 5.841e+00, Adaptive_Constant: 1.00 ,Time: 0.37\n",
      "It: 980, Loss: 9.203e+00, Loss_bcs: 3.482e+00, Loss_res: 5.721e+00, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 990, Loss: 8.933e+00, Loss_bcs: 3.409e+00, Loss_res: 5.524e+00, Adaptive_Constant: 1.00 ,Time: 0.31\n",
      "It: 1000, Loss: 8.710e+00, Loss_bcs: 3.338e+00, Loss_res: 5.372e+00, Adaptive_Constant: 1.00 ,Time: 0.29\n",
      "Relative L2 error_u: 1.44e+00\n",
      "Relative L2 error_f: 3.27e-02\n",
      "\n",
      "\n",
      "Method:  full_batch\n",
      "\n",
      "average of time_list: 36.57236337661743\n",
      "average of error_u_list: 1.4438529382963543\n",
      "average of error_v_list: 0.032699448159087555\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "bcbatch_size = 128\n",
    "\n",
    "a_1 = 1\n",
    "a_2 = 4\n",
    "\n",
    "# Parameter\n",
    "lam = 1.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[-1.0, -1.0], [1.0, -1.0]])\n",
    "bc2_coords = np.array([[1.0, -1.0], [1.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "bc4_coords = np.array([[-1.0, 1.0], [-1.0, -1.0]])\n",
    "\n",
    "dom_coords = np.array([[-1.0, -1.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "x1 = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x2 = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star, a_1, a_2)\n",
    "f_star = f(X_star, a_1, a_2, lam)\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = None\n",
    "\n",
    "# Define model\n",
    "mode = 'M1'            # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False    # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, a_1, a_2, lam), name='Forcing')\n",
    "\n",
    "        [elapsed, error_u , error_f ,  model] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , \n",
    "                                                                       stiff_ratio , X_star ,u_star , f_star , nIter ,bcbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_f_list.append(error_f)\n",
    "\n",
    "    print(\"\\n\\nMethod: \", mtd)\n",
    "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    print(\"average of error_v_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_f_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(\"./M1_dataset/NS_model_\"+mode+\"_result_mb\"+str(bcbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot ###\n",
    "\n",
    "# Exact solution & Predicted solution\n",
    "# Exact soluton\n",
    "U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "# Predicted solution\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Exact $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Predicted $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual loss & Boundary loss\n",
    "loss_res = model.loss_res_log\n",
    "loss_bcs = model.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant = model.adpative_constant_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_res = model.dict_gradients_res_layers\n",
    "data_gradients_bcs = model.dict_gradients_bcs_layers\n",
    "\n",
    "gradients_res_list = []\n",
    "gradients_bcs_list = []\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "for j in range(num_hidden_layers):\n",
    "    gradient_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradient_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    gradients_res_list.append(gradient_res)\n",
    "    gradients_bcs_list.append(gradient_bcs)\n",
    "\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim([-3.0, 3.0])\n",
    "    ax.set_ylim([0,100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = model.eigenvalue_log\n",
    "    eigenvalues_bcs_list = model.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = model.eigenvalue_res_log\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
