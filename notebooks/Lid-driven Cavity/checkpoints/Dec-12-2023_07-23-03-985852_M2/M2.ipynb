{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "# from NS_model_tf import Sampler, Navier_Stokes2D\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "import timeit\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "def U_gamma_1(x):\n",
    "    num = x.shape[0]\n",
    "    return np.tile(np.array([1.0, 0.0]), (num, 1))\n",
    "\n",
    "\n",
    "def U_gamma_2(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "def operator(psi, p, x, y, Re, sigma_x=1.0, sigma_y=1.0):\n",
    "    u = tf.gradients(psi, y)[0] / sigma_y\n",
    "    v = - tf.gradients(psi, x)[0] / sigma_x\n",
    "\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_y = tf.gradients(u, y)[0] / sigma_y\n",
    "\n",
    "    v_x = tf.gradients(v, x)[0] / sigma_x\n",
    "    v_y = tf.gradients(v, y)[0] / sigma_y\n",
    "\n",
    "    p_x = tf.gradients(p, x)[0] / sigma_x\n",
    "    p_y = tf.gradients(p, y)[0] / sigma_y\n",
    "\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    u_yy = tf.gradients(u_y, y)[0] / sigma_y\n",
    "\n",
    "    v_xx = tf.gradients(v_x, x)[0] / sigma_x\n",
    "    v_yy = tf.gradients(v_y, y)[0] / sigma_y\n",
    "\n",
    "    Ru_momentum = u * u_x + v * u_y + p_x - (u_xx + u_yy) / Re\n",
    "    Rv_momentum = u * v_x + v * v_y + p_y - (v_xx + v_yy) / Re\n",
    "\n",
    "    return Ru_momentum, Rv_momentum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "class Navier_Stokes2D:\n",
    "    def __init__(self, layers, operator, bcs_sampler, res_sampler, Re, mode , sess):\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x, self.sigma_x = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_y, self.sigma_y = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "\n",
    "        # Navier Stokes constant\n",
    "        self.Re = tf.constant(Re, dtype=tf.float32)\n",
    "\n",
    "        # Adaptive re-weighting constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc1_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc2_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc3_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc4_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred, self.v_bc1_pred = self.net_uv(self.x_bc1_tf, self.y_bc1_tf)\n",
    "        self.u_bc2_pred, self.v_bc2_pred = self.net_uv(self.x_bc2_tf, self.y_bc2_tf)\n",
    "        self.u_bc3_pred, self.v_bc3_pred = self.net_uv(self.x_bc3_tf, self.y_bc3_tf)\n",
    "        self.u_bc4_pred, self.v_bc4_pred = self.net_uv(self.x_bc4_tf, self.y_bc4_tf)\n",
    "\n",
    "        self.U_bc1_pred = tf.concat([self.u_bc1_pred, self.v_bc1_pred], axis=1)\n",
    "        self.U_bc2_pred = tf.concat([self.u_bc2_pred, self.v_bc2_pred], axis=1)\n",
    "        self.U_bc3_pred = tf.concat([self.u_bc3_pred, self.v_bc3_pred], axis=1)\n",
    "        self.U_bc4_pred = tf.concat([self.u_bc4_pred, self.v_bc4_pred], axis=1)\n",
    "\n",
    "        self.psi_pred, self.p_pred = self.net_psi_p(self.x_u_tf, self.y_u_tf)\n",
    "        self.u_pred, self.v_pred = self.net_uv(self.x_u_tf, self.y_u_tf)\n",
    "        self.u_momentum_pred, self.v_momentum_pred = self.net_r(self.x_r_tf, self.y_r_tf)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_u_momentum = tf.reduce_mean(tf.square(self.u_momentum_pred))\n",
    "        self.loss_v_momentum = tf.reduce_mean(tf.square(self.v_momentum_pred))\n",
    "\n",
    "        self.loss_res = self.loss_u_momentum + self.loss_v_momentum\n",
    "        \n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.U_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.U_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.U_bc4_pred))\n",
    "        \n",
    "        self.loss_bcs = self.adaptive_constant_bcs_tf * tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf) +tf.square(self.U_bc2_pred) + tf.square(self.U_bc3_pred) + tf.square(self.U_bc4_pred))\n",
    "        \n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                        1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_res_log = []\n",
    "        self.loss_bcs_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        self.adpative_constant_bcs_list = []\n",
    "        self.adpative_constant_res_list = []\n",
    "        self.adpative_constant_bcs_log = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.adpative_constant_res_list.append( tf.reduce_max(tf.abs(self.grad_res[i])))\n",
    "            self.adpative_constant_bcs_list.append(  tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "\n",
    "        self.adaptive_constant_bcs = tf.reduce_max(tf.stack(self.adpative_constant_res_list))/  tf.reduce_mean(tf.stack(self.adpative_constant_bcs_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 ,  self.loss_bc2 ,  self.loss_bc3  ,  self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs1\" , \"loss_bcs2\", \"loss_bcs3\" ,  \"loss_bcs4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "\n",
    "    # Forward pass for stream-pressure formulation\n",
    "    def net_psi_p(self, x, y):\n",
    "        psi_p = self.forward_pass(tf.concat([x, y], 1))\n",
    "        psi = psi_p[:, 0:1]\n",
    "        p = psi_p[:, 1:2]\n",
    "        return psi, p\n",
    "\n",
    "    # Forward pass for velocities\n",
    "    def net_uv(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u = tf.gradients(psi, y)[0] / self.sigma_y\n",
    "        v = - tf.gradients(psi, x)[0] / self.sigma_x\n",
    "        return u, v\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u_momentum_pred, v_momentum_pred = self.operator(psi, p, x, y,\n",
    "                                                         self.Re,\n",
    "                                                         self.sigma_x, self.sigma_y)\n",
    "\n",
    "        return u_momentum_pred, v_momentum_pred\n",
    "\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter ,  bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        batch_size = bcbatch_size\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "        X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "        X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        batch_size = ubatch_size\n",
    "\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.U_bc1_tf: u_bc1_batch,\n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val\n",
    "                    }\n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _, batch_losses = self.sess.run([self.train_op, self.loss_tensor_list] ,tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # Compute the adaptive constant\n",
    "                adaptive_constant_bcs_val = self.sess.run(self.adaptive_constant_bcs, tf_dict)\n",
    "\n",
    "                self.adaptive_constant_bcs_val = adaptive_constant_bcs_val *  (1.0 - self.beta) + self.beta * self.adaptive_constant_bcs_val\n",
    "\n",
    "                # self.adpative_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "                # self.loss_bcs_log.append(loss_u_value)\n",
    "                # self.loss_res_log.append(loss_r_value)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' % (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "\n",
    "                print(\"constant_bcs_val: {:.3f}\".format(self.adaptive_constant_bcs_val))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter, batch_size):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.U_bc1_tf: u_bc1_batch,\n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _, batch_losses = self.sess.run([self.train_op, self.loss_tensor_list] ,tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "                self.print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' % (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "\n",
    "                if it % 10 == 0:\n",
    "\n",
    "                    # Compute the adaptive constant\n",
    "                    adaptive_constant_bcs_val = self.sess.run(self.adaptive_constant_bcs, tf_dict)\n",
    "\n",
    "                    self.adaptive_constant_bcs_val = adaptive_constant_bcs_val *  (1.0 - self.beta) + self.beta * self.adaptive_constant_bcs_val\n",
    "\n",
    "                    self.adpative_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "                    # self.loss_bcs_log.append(loss_u_value)\n",
    "                    # self.loss_res_log.append(loss_r_value)\n",
    "\n",
    "\n",
    "                    self.print(\"constant_bcs_val: {:.3f}\".format(self.adaptive_constant_bcs_val))\n",
    "\n",
    "  \n",
    "            sys.stdout.flush()\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_psi_p(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        psi_star = self.sess.run(self.psi_pred, tf_dict)\n",
    "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
    "        return psi_star, p_star\n",
    "\n",
    "    def predict_uv(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
    "        return u_star, v_star\n",
    "\n",
    "\n",
    "\n",
    " ############################################################\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "    def plot_loss_history(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    ############################################################\n",
    "  \n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=3)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $v(x)$' , 'Predicted $v(x)$' , 'Absolute error']\n",
    "        data = [U_star.T , U_pred ,  np.abs(U_star.T - U_pred) , F_star.T , F_pred ,  np.abs(F_star.T - F_pred) ]\n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            # kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60), cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, minmax, title in zip(grid, data, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_bcs_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        [elapsed, error_u , error_v ,  error_p] = test_method(mtd , layers, operator, bcs_sampler, res_sampler ,Re , stiff_ratio ,  X_star , X , Y)\n",
    "\n",
    "def test_method(method , layers, operator, bcs_sampler, res_sampler, Re ,  mode , X_star , X , Y , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "\n",
    "    model = Navier_Stokes2D(layers, operator, bcs_sampler, res_sampler, Re, mode)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    _, p_pred = model.predict_psi_p(X_star)\n",
    "    u_pred, v_pred = model.predict_uv(X_star)\n",
    "\n",
    "    # psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "    p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "    # u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "    # v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "    # velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # Reference\n",
    "    u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "    v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "    # velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "    u_pred = u_pred.reshape(100,100)\n",
    "    v_pred = v_pred.reshape(100,100)\n",
    "    p_pred = p_pred.reshape(100,100)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_u))\n",
    "    error_v = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_v))\n",
    "    error_p = np.linalg.norm(p_pred - p_star.T, 2) / np.linalg.norm(p_star, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_p))\n",
    "\n",
    "    return [elapsed, error_u , error_v ,error_p ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28350/76735864.py:59: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28350/76735864.py:60: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28350/76735864.py:61: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28350/76735864.py:61: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28350/1305667103.py:164: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28350/1305667103.py:51: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 06:19:50.420350: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 06:19:50.442648: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-12 06:19:50.443132: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558d163819e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-12 06:19:50.443145: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-12 06:19:50.443660: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_28350/1305667103.py:110: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28350/1305667103.py:113: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28350/1305667103.py:155: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 9.572e-01, Loss_u: 8.112e-01, Loss_r: 1.459e-01, Time: 15.32\n",
      "constant_bcs_val: 1.612\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 10, Loss: 6.440e-01, Loss_u: 5.913e-01, Loss_r: 5.265e-02, Time: 0.04\n",
      "constant_bcs_val: 2.049\n",
      "It: 20, Loss: 5.577e-01, Loss_u: 5.120e-01, Loss_r: 4.569e-02, Time: 0.04\n",
      "constant_bcs_val: 2.620\n",
      "It: 30, Loss: 5.393e-01, Loss_u: 4.860e-01, Loss_r: 5.333e-02, Time: 0.04\n",
      "constant_bcs_val: 3.386\n",
      "It: 40, Loss: 5.406e-01, Loss_u: 4.988e-01, Loss_r: 4.180e-02, Time: 0.04\n",
      "constant_bcs_val: 3.817\n",
      "It: 50, Loss: 5.437e-01, Loss_u: 4.969e-01, Loss_r: 4.682e-02, Time: 0.06\n",
      "constant_bcs_val: 4.641\n",
      "It: 60, Loss: 5.862e-01, Loss_u: 5.497e-01, Loss_r: 3.641e-02, Time: 0.06\n",
      "constant_bcs_val: 4.929\n",
      "It: 70, Loss: 5.254e-01, Loss_u: 4.991e-01, Loss_r: 2.638e-02, Time: 0.04\n",
      "constant_bcs_val: 5.158\n",
      "It: 80, Loss: 5.319e-01, Loss_u: 5.066e-01, Loss_r: 2.534e-02, Time: 0.04\n",
      "constant_bcs_val: 4.998\n",
      "It: 90, Loss: 4.853e-01, Loss_u: 4.559e-01, Loss_r: 2.949e-02, Time: 0.06\n",
      "constant_bcs_val: 5.736\n",
      "It: 100, Loss: 5.285e-01, Loss_u: 5.006e-01, Loss_r: 2.785e-02, Time: 0.04\n",
      "constant_bcs_val: 5.489\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 110, Loss: 5.278e-01, Loss_u: 5.011e-01, Loss_r: 2.671e-02, Time: 0.04\n",
      "constant_bcs_val: 5.320\n",
      "It: 120, Loss: 4.103e-01, Loss_u: 3.857e-01, Loss_r: 2.465e-02, Time: 0.04\n",
      "constant_bcs_val: 5.765\n",
      "It: 130, Loss: 4.350e-01, Loss_u: 4.078e-01, Loss_r: 2.720e-02, Time: 0.05\n",
      "constant_bcs_val: 5.560\n",
      "It: 140, Loss: 4.117e-01, Loss_u: 3.842e-01, Loss_r: 2.750e-02, Time: 0.04\n",
      "constant_bcs_val: 5.604\n",
      "It: 150, Loss: 4.046e-01, Loss_u: 3.830e-01, Loss_r: 2.165e-02, Time: 0.04\n",
      "constant_bcs_val: 5.546\n",
      "It: 160, Loss: 3.248e-01, Loss_u: 3.040e-01, Loss_r: 2.082e-02, Time: 0.04\n",
      "constant_bcs_val: 5.451\n",
      "It: 170, Loss: 3.732e-01, Loss_u: 3.508e-01, Loss_r: 2.249e-02, Time: 0.04\n",
      "constant_bcs_val: 5.493\n",
      "It: 180, Loss: 3.288e-01, Loss_u: 3.107e-01, Loss_r: 1.807e-02, Time: 0.05\n",
      "constant_bcs_val: 5.283\n",
      "It: 190, Loss: 3.278e-01, Loss_u: 3.131e-01, Loss_r: 1.471e-02, Time: 0.04\n",
      "constant_bcs_val: 5.032\n",
      "It: 200, Loss: 3.248e-01, Loss_u: 3.035e-01, Loss_r: 2.131e-02, Time: 0.05\n",
      "constant_bcs_val: 4.952\n",
      "It: 210, Loss: 2.996e-01, Loss_u: 2.812e-01, Loss_r: 1.843e-02, Time: 0.04\n",
      "constant_bcs_val: 5.000\n",
      "It: 220, Loss: 2.446e-01, Loss_u: 2.290e-01, Loss_r: 1.564e-02, Time: 0.04\n",
      "constant_bcs_val: 5.498\n",
      "It: 230, Loss: 2.928e-01, Loss_u: 2.784e-01, Loss_r: 1.443e-02, Time: 0.04\n",
      "constant_bcs_val: 5.734\n",
      "It: 240, Loss: 2.799e-01, Loss_u: 2.680e-01, Loss_r: 1.190e-02, Time: 0.04\n",
      "constant_bcs_val: 5.244\n",
      "It: 250, Loss: 2.078e-01, Loss_u: 1.968e-01, Loss_r: 1.101e-02, Time: 0.04\n",
      "constant_bcs_val: 5.231\n",
      "It: 260, Loss: 2.411e-01, Loss_u: 2.275e-01, Loss_r: 1.358e-02, Time: 0.06\n",
      "constant_bcs_val: 4.882\n",
      "It: 270, Loss: 2.327e-01, Loss_u: 2.205e-01, Loss_r: 1.224e-02, Time: 0.05\n",
      "constant_bcs_val: 4.543\n",
      "It: 280, Loss: 1.523e-01, Loss_u: 1.400e-01, Loss_r: 1.231e-02, Time: 0.07\n",
      "constant_bcs_val: 4.578\n",
      "It: 290, Loss: 1.690e-01, Loss_u: 1.586e-01, Loss_r: 1.049e-02, Time: 0.09\n",
      "constant_bcs_val: 4.530\n",
      "It: 300, Loss: 1.612e-01, Loss_u: 1.515e-01, Loss_r: 9.695e-03, Time: 0.04\n",
      "constant_bcs_val: 4.300\n",
      "It: 310, Loss: 1.648e-01, Loss_u: 1.553e-01, Loss_r: 9.525e-03, Time: 0.04\n",
      "constant_bcs_val: 4.019\n",
      "It: 320, Loss: 1.580e-01, Loss_u: 1.509e-01, Loss_r: 7.185e-03, Time: 0.08\n",
      "constant_bcs_val: 3.728\n",
      "It: 330, Loss: 1.336e-01, Loss_u: 1.260e-01, Loss_r: 7.543e-03, Time: 0.08\n",
      "constant_bcs_val: 3.473\n",
      "It: 340, Loss: 1.288e-01, Loss_u: 1.177e-01, Loss_r: 1.111e-02, Time: 0.05\n",
      "constant_bcs_val: 3.703\n",
      "It: 350, Loss: 1.091e-01, Loss_u: 9.183e-02, Loss_r: 1.725e-02, Time: 0.05\n",
      "constant_bcs_val: 4.460\n",
      "It: 360, Loss: 1.683e-01, Loss_u: 1.555e-01, Loss_r: 1.274e-02, Time: 0.06\n",
      "constant_bcs_val: 4.471\n",
      "It: 370, Loss: 1.321e-01, Loss_u: 1.232e-01, Loss_r: 8.926e-03, Time: 0.05\n",
      "constant_bcs_val: 4.519\n",
      "It: 380, Loss: 1.570e-01, Loss_u: 1.470e-01, Loss_r: 9.927e-03, Time: 0.07\n",
      "constant_bcs_val: 4.264\n",
      "It: 390, Loss: 1.519e-01, Loss_u: 1.414e-01, Loss_r: 1.051e-02, Time: 0.05\n",
      "constant_bcs_val: 4.105\n",
      "It: 400, Loss: 1.286e-01, Loss_u: 1.136e-01, Loss_r: 1.499e-02, Time: 0.05\n",
      "constant_bcs_val: 4.591\n",
      "l2 error: 4.82e-01\n",
      "l2 error: 9.51e-01\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "average lambda_bcnanaverage lambda_res1.0\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-12-2023_06-19-50-443919_M2\n",
      "Final loss total loss: 1.262866e-01\n",
      "Final loss loss_res: 9.580735e-03\n",
      "Final loss loss_bcs1: 1.000981e-02\n",
      "Final loss loss_bcs2: 1.125812e-02\n",
      "Final loss loss_bcs3: 7.007624e-03\n",
      "Final loss loss_bcs4: 1.561506e-04\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28350/76735864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m#             mode.plot_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt_prediction\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mu_ref\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mv_ref\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mv_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28350/1305667103.py\u001b[0m in \u001b[0;36mplt_prediction\u001b[0;34m(self, x1, x2, X_star, u_star, u_pred, f_star, f_pred)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;31m# if (timeStp == 0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m#  print( z[timeStp,:,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mpcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpcf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%.3e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mpcolor\u001b[0;34m(self, shading, alpha, norm, cmap, vmin, vmax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5786\u001b[0m         \u001b[0mshading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5787\u001b[0m         X, Y, C, shading = self._pcolorargs('pcolor', *args, shading=shading,\n\u001b[0;32m-> 5788\u001b[0;31m                                             kwargs=kwargs)\n\u001b[0m\u001b[1;32m   5789\u001b[0m         \u001b[0mNy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_pcolorargs\u001b[0;34m(self, funcname, shading, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5548\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5549\u001b[0m                     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5550\u001b[0;31m             \u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5552\u001b[0m             raise TypeError(f'{funcname}() takes 1 or 3 positional arguments '\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x360 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "nIter =401\n",
    "bcbatch_size = 350\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "# Parameters of equations\n",
    "Re = 100.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[0.0, 1.0], [1.0, 1.0]])\n",
    "bc2_coords = np.array([[0.0, 0.0], [0.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 0.0], [1.0, 1.0]])\n",
    "bc4_coords = np.array([[0.0, 0.0], [1.0, 0.0]])\n",
    "dom_coords = np.array([[0.0, 0.0], [1.0, 1.0]])\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'\n",
    "layers = [2, 50, 50, 50, 50 , 50 , 2]\n",
    "\n",
    "stiff_ratio = False  # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\" ]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_v_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: U_gamma_1(x), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: U_gamma_2(x), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: U_gamma_2(x), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: U_gamma_2(x), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x), name='Forcing')\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "\n",
    "            mode = Navier_Stokes2D(layers, operator, bcs_sampler, res_sampler, Re, mode , sess)\n",
    "\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                mode.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                mode.trainmb(nIter, mbbatch_size)\n",
    "            else:\n",
    "                mode.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Predictions\n",
    "            _, p_pred = mode.predict_psi_p(X_star)\n",
    "            u_pred, v_pred = mode.predict_uv(X_star)\n",
    "\n",
    "            # Reference\n",
    "            u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "            v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "            # velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "            u_pred = u_pred.reshape(100,100)\n",
    "            v_pred = v_pred.reshape(100,100)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "            error_v = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "\n",
    "            mode.print('l2 error: {:.2e}'.format(error_u))\n",
    "            mode.print('l2 error: {:.2e}'.format(error_v))\n",
    "\n",
    "       \n",
    "            mode.print(\"average lambda_bc\" , np.average(mode.adpative_constant_bcs_log))\n",
    "            mode.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close\n",
    "            #             mode.plot_grad()\n",
    "            mode.save_NN()\n",
    "            mode.plt_prediction( X , Y , X_star , u_ref , u_pred , v_ref , v_pred)\n",
    "\n",
    "\n",
    "        mode.print('elapsed: {:.2e}'.format(elapsed))\n",
    "        mode.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "        mode.print('Relative L2 error_v: {:.2e}'.format(error_v))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_v_list.append(error_v)\n",
    "\n",
    "    mode.print(\"\\n\\nMethod: \", mtd)\n",
    "    mode.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    mode.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    mode.print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(mode.dirname,\"\"+mtd+\"_Lid-driven-Cavity_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "    from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "    ### Plot ###\n",
    "\n",
    "    # Exact solution & Predicted solution\n",
    "    # Exact soluton\n",
    "    U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "    F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "    # Predicted solution\n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "    F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "    titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $v(x)$' , 'Predicted $v(x)$' , 'Absolute error']\n",
    "    data = [U_star.T , U_pred ,  np.abs(U_star.T - U_pred) , F_star.T , F_pred ,  np.abs(F_star.T - F_pred) ]\n",
    "\n",
    "    fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "    grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                    label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                    cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                    cbar_size=\"5%\", cbar_pad=0.0)\n",
    "# CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "    minmax_list = []\n",
    "    for d in data:\n",
    "        # if(local):\n",
    "        #     minmax_list.append([np.min(d), np.max(d)])\n",
    "        # else:\n",
    "        minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "        # kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60), cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "    for ax, z, minmax, title in zip(grid, data, minmax_list, titles):\n",
    "    #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "        #pcfsets.append(pcf)\n",
    "        # if (timeStp == 0):\n",
    "        pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "        cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "        ax.cax.tick_params(labelsize=14.5)\n",
    "        ax.set_title(title, fontsize=14.5, pad=7)\n",
    "        ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "        ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "        ax.tick_params(labelsize=14.5)\n",
    "        ax.set_xlim(x1.min(), x1.max())\n",
    "        ax.set_ylim(x2.min(), x2.max())\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "    fig_1.set_size_inches(15, 10, True)\n",
    "    fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "    plt.close(\"all\" , )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_ref.flatten()[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.io\n",
    "\n",
    "mode = 'M2'\n",
    "mbbatch_size = 128\n",
    "ubatch_size = 5000\n",
    "bcbatch_size = 500\n",
    "iterations = 40000\n",
    "\n",
    "time_list = []\n",
    "error_u_list = []\n",
    "error_v_list = []\n",
    "error_p_list = []\n",
    "    \n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "##Mini Batch\n",
    "time_list = [1831.07 , ]\n",
    "error_u_list = [0.2112 , 0.]\n",
    "error_v_list = [0.3271 , 0.]\n",
    "error_p_list = [0.094 , 0.]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "\n",
    "##Full Batch\n",
    "time_list = [8048.86 , ]\n",
    "error_u_list = [ 0.0367 ,0. ]\n",
    "error_v_list = [0.0706 , 0.]\n",
    "error_p_list = [0.465 , 0.]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "\n",
    "scipy.io.savemat(\"./dataset/NS_model_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Plot ###\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "# Predictions\n",
    "psi_pred, p_pred = mode.predict_psi_p(X_star)\n",
    "u_pred, v_pred = mode.predict_uv(X_star)\n",
    "\n",
    "psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "# Reference\n",
    "u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "# Relative error\n",
    "error = np.linalg.norm(u_star - u_pred.T, 2) / np.linalg.norm(u_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "error = np.linalg.norm(v_star - v_pred.T, 2) / np.linalg.norm(v_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "error = np.linalg.norm(p_pred - p_star.T, 2) / np.linalg.norm(p_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "\n",
    "### Plot ###\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference solution & Prediceted solution\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "fig_1.add_subplot(1, 3, 1)\n",
    "plt.pcolor(X.T, Y.T, velocity_ref, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Reference Velocity')\n",
    "\n",
    "fig_1.add_subplot(1, 3, 2)\n",
    "plt.pcolor(x, Y, velocity_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predicted Velocity')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_1.add_subplot(1, 3, 3)\n",
    "plt.pcolor(X, Y, np.abs(velocity_star - velocity_ref.T), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Absolute Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ## Loss ##\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_bcs_log\n",
    "    \n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradients #\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "num_hidden_layers = len(layers) -1\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    \n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_{u_b}$')\n",
    "\n",
    "    # ax.get_legend().remove()\n",
    "    ax.set_xlim([-1.0, 1.0])\n",
    "    ax.set_ylim([0, 100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
