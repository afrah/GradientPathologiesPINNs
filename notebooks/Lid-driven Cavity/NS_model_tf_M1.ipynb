{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "# from NS_model_tf import Sampler, Navier_Stokes2D\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "import timeit\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "def U_gamma_1(x):\n",
    "    num = x.shape[0]\n",
    "    return np.tile(np.array([1.0, 0.0]), (num, 1))\n",
    "\n",
    "\n",
    "def U_gamma_2(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "def operator(psi, p, x, y, Re, sigma_x=1.0, sigma_y=1.0):\n",
    "    u = tf.gradients(psi, y)[0] / sigma_y\n",
    "    v = - tf.gradients(psi, x)[0] / sigma_x\n",
    "\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_y = tf.gradients(u, y)[0] / sigma_y\n",
    "\n",
    "    v_x = tf.gradients(v, x)[0] / sigma_x\n",
    "    v_y = tf.gradients(v, y)[0] / sigma_y\n",
    "\n",
    "    p_x = tf.gradients(p, x)[0] / sigma_x\n",
    "    p_y = tf.gradients(p, y)[0] / sigma_y\n",
    "\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    u_yy = tf.gradients(u_y, y)[0] / sigma_y\n",
    "\n",
    "    v_xx = tf.gradients(v_x, x)[0] / sigma_x\n",
    "    v_yy = tf.gradients(v_y, y)[0] / sigma_y\n",
    "\n",
    "    Ru_momentum = u * u_x + v * u_y + p_x - (u_xx + u_yy) / Re\n",
    "    Rv_momentum = u * v_x + v * v_y + p_y - (v_xx + v_yy) / Re\n",
    "\n",
    "    return Ru_momentum, Rv_momentum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "class Navier_Stokes2D:\n",
    "    def __init__(self, layers, operator, bcs_sampler, res_sampler, Re, model):\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x, self.sigma_x = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_y, self.sigma_y = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Choose model\n",
    "        self.model = model\n",
    "\n",
    "        # Navier Stokes constant\n",
    "        self.Re = tf.constant(Re, dtype=tf.float32)\n",
    "\n",
    "        # Adaptive re-weighting constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        if model in ['M3', 'M4']:\n",
    "            # Initialize encoder weights and biases\n",
    "            self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "            self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc1_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc2_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc3_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc4_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred, self.v_bc1_pred = self.net_uv(self.x_bc1_tf, self.y_bc1_tf)\n",
    "        self.u_bc2_pred, self.v_bc2_pred = self.net_uv(self.x_bc2_tf, self.y_bc2_tf)\n",
    "        self.u_bc3_pred, self.v_bc3_pred = self.net_uv(self.x_bc3_tf, self.y_bc3_tf)\n",
    "        self.u_bc4_pred, self.v_bc4_pred = self.net_uv(self.x_bc4_tf, self.y_bc4_tf)\n",
    "\n",
    "        self.U_bc1_pred = tf.concat([self.u_bc1_pred, self.v_bc1_pred], axis=1)\n",
    "        self.U_bc2_pred = tf.concat([self.u_bc2_pred, self.v_bc2_pred], axis=1)\n",
    "        self.U_bc3_pred = tf.concat([self.u_bc3_pred, self.v_bc3_pred], axis=1)\n",
    "        self.U_bc4_pred = tf.concat([self.u_bc4_pred, self.v_bc4_pred], axis=1)\n",
    "\n",
    "        self.psi_pred, self.p_pred = self.net_psi_p(self.x_u_tf, self.y_u_tf)\n",
    "        self.u_pred, self.v_pred = self.net_uv(self.x_u_tf, self.y_u_tf)\n",
    "        self.u_momentum_pred, self.v_momentum_pred = self.net_r(self.x_r_tf, self.y_r_tf)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_u_momentum = tf.reduce_mean(tf.square(self.u_momentum_pred))\n",
    "        self.loss_v_momentum = tf.reduce_mean(tf.square(self.v_momentum_pred))\n",
    "\n",
    "        self.loss_res = self.loss_u_momentum + self.loss_v_momentum\n",
    "        \n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.U_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.U_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.U_bc4_pred))\n",
    "        \n",
    "        self.loss_bcs = self.adaptive_constant_bcs_tf * tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf) +tf.square(self.U_bc2_pred) + tf.square(self.U_bc3_pred) + tf.square(self.U_bc4_pred))\n",
    "        \n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                        1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        # Logger\n",
    "        self.loss_res_log = []\n",
    "        self.loss_bcs_log = []\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict(self.layers)\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict(self.layers)\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        self.adpative_constant_bcs_list = []\n",
    "        self.adpative_constant_bcs_log = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.adpative_constant_bcs_list.append(\n",
    "                tf.reduce_max(tf.abs(self.grad_res[i])) / tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        self.adaptive_constant_bcs = tf.reduce_max(tf.stack(self.adpative_constant_bcs_list))\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        \n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    # Save gradients during training\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for stream-pressure formulation\n",
    "    def net_psi_p(self, x, y):\n",
    "        psi_p = self.forward_pass(tf.concat([x, y], 1))\n",
    "        psi = psi_p[:, 0:1]\n",
    "        p = psi_p[:, 1:2]\n",
    "        return psi, p\n",
    "\n",
    "    # Forward pass for velocities\n",
    "    def net_uv(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u = tf.gradients(psi, y)[0] / self.sigma_y\n",
    "        v = - tf.gradients(psi, x)[0] / self.sigma_x\n",
    "        return u, v\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u_momentum_pred, v_momentum_pred = self.operator(psi, p, x, y,\n",
    "                                                         self.Re,\n",
    "                                                         self.sigma_x, self.sigma_y)\n",
    "\n",
    "        return u_momentum_pred, v_momentum_pred\n",
    "\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter ,bcbatch_size , ubatch_size   ):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], bcbatch_size)\n",
    "        X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], bcbatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.U_bc1_tf: u_bc1_batch,\n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val\n",
    "                    }\n",
    "        \n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' %(it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "\n",
    "                start_time = timeit.default_timer()\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.U_bc1_tf: u_bc1_batch,\n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' %(it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "\n",
    "                start_time = timeit.default_timer()\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_psi_p(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        psi_star = self.sess.run(self.psi_pred, tf_dict)\n",
    "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
    "        return psi_star, p_star\n",
    "\n",
    "    def predict_uv(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
    "        return u_star, v_star\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        [elapsed, error_u , error_v ,  error_p] = test_method(mtd , layers, operator, bcs_sampler, res_sampler ,Re , stiff_ratio ,  X_star , X , Y)\n",
    "\n",
    "def test_method(method , layers, operator, bcs_sampler, res_sampler, Re ,  mode , X_star , X , Y , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "\n",
    "    model = Navier_Stokes2D(layers, operator, bcs_sampler, res_sampler, Re, mode)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    _, p_pred = model.predict_psi_p(X_star)\n",
    "    u_pred, v_pred = model.predict_uv(X_star)\n",
    "\n",
    "    # psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "    p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "    # u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "    # v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "    # velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # Reference\n",
    "    u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "    v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "    # velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "    u_pred = u_pred.reshape(100,100)\n",
    "    v_pred = v_pred.reshape(100,100)\n",
    "    p_pred = p_pred.reshape(100,100)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_u))\n",
    "    error_v = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_v))\n",
    "    error_p = np.linalg.norm(p_pred - p_star.T, 2) / np.linalg.norm(p_star, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_p))\n",
    "\n",
    "    return [elapsed, error_u , error_v ,error_p ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_27350/4166078972.py:38: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_27350/4166078972.py:38: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_27350/4166078972.py:169: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_27350/4166078972.py:53: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 04:19:00.435752: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 04:19:00.462561: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-11-23 04:19:00.462992: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b65b187e40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-23 04:19:00.463007: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-11-23 04:19:00.464806: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_27350/4166078972.py:112: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_27350/4166078972.py:115: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_27350/4166078972.py:120: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_27350/4166078972.py:142: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "It: 0, Loss: 8.468e-01, Loss_u: 5.370e-01, Loss_r: 3.098e-01, Time: 16.35\n",
      "It: 10, Loss: 3.487e-01, Loss_u: 3.215e-01, Loss_r: 2.714e-02, Time: 0.72\n",
      "It: 20, Loss: 2.580e-01, Loss_u: 2.276e-01, Loss_r: 3.044e-02, Time: 0.77\n",
      "It: 30, Loss: 1.861e-01, Loss_u: 1.590e-01, Loss_r: 2.710e-02, Time: 0.79\n",
      "It: 40, Loss: 1.678e-01, Loss_u: 1.350e-01, Loss_r: 3.280e-02, Time: 0.81\n",
      "It: 50, Loss: 1.483e-01, Loss_u: 1.294e-01, Loss_r: 1.888e-02, Time: 0.74\n",
      "It: 60, Loss: 1.366e-01, Loss_u: 1.209e-01, Loss_r: 1.565e-02, Time: 0.77\n",
      "It: 70, Loss: 1.417e-01, Loss_u: 1.292e-01, Loss_r: 1.259e-02, Time: 0.86\n",
      "It: 80, Loss: 1.288e-01, Loss_u: 1.157e-01, Loss_r: 1.318e-02, Time: 0.77\n",
      "It: 90, Loss: 1.133e-01, Loss_u: 1.023e-01, Loss_r: 1.095e-02, Time: 0.83\n",
      "It: 100, Loss: 1.100e-01, Loss_u: 1.006e-01, Loss_r: 9.406e-03, Time: 0.84\n",
      "It: 110, Loss: 1.089e-01, Loss_u: 9.928e-02, Loss_r: 9.647e-03, Time: 0.73\n",
      "It: 120, Loss: 1.058e-01, Loss_u: 9.659e-02, Loss_r: 9.220e-03, Time: 0.61\n",
      "It: 130, Loss: 9.765e-02, Loss_u: 8.712e-02, Loss_r: 1.052e-02, Time: 0.97\n",
      "It: 140, Loss: 9.514e-02, Loss_u: 8.508e-02, Loss_r: 1.007e-02, Time: 1.51\n",
      "It: 150, Loss: 8.632e-02, Loss_u: 7.852e-02, Loss_r: 7.805e-03, Time: 1.94\n",
      "It: 160, Loss: 8.439e-02, Loss_u: 7.545e-02, Loss_r: 8.937e-03, Time: 0.94\n",
      "It: 170, Loss: 7.134e-02, Loss_u: 6.402e-02, Loss_r: 7.324e-03, Time: 0.81\n",
      "It: 180, Loss: 8.266e-02, Loss_u: 7.322e-02, Loss_r: 9.434e-03, Time: 0.85\n",
      "It: 190, Loss: 7.602e-02, Loss_u: 6.909e-02, Loss_r: 6.931e-03, Time: 0.88\n",
      "It: 200, Loss: 7.362e-02, Loss_u: 6.419e-02, Loss_r: 9.428e-03, Time: 0.78\n",
      "It: 210, Loss: 5.447e-02, Loss_u: 4.623e-02, Loss_r: 8.233e-03, Time: 0.84\n",
      "It: 220, Loss: 5.361e-02, Loss_u: 4.723e-02, Loss_r: 6.386e-03, Time: 0.92\n",
      "It: 230, Loss: 4.812e-02, Loss_u: 4.093e-02, Loss_r: 7.189e-03, Time: 0.80\n",
      "It: 240, Loss: 6.079e-02, Loss_u: 5.329e-02, Loss_r: 7.501e-03, Time: 0.76\n",
      "It: 250, Loss: 5.921e-02, Loss_u: 5.027e-02, Loss_r: 8.945e-03, Time: 0.88\n",
      "It: 260, Loss: 5.174e-02, Loss_u: 4.389e-02, Loss_r: 7.842e-03, Time: 0.81\n",
      "It: 270, Loss: 4.410e-02, Loss_u: 3.635e-02, Loss_r: 7.750e-03, Time: 0.80\n",
      "It: 280, Loss: 4.636e-02, Loss_u: 4.126e-02, Loss_r: 5.099e-03, Time: 0.81\n",
      "It: 290, Loss: 4.287e-02, Loss_u: 3.790e-02, Loss_r: 4.971e-03, Time: 0.78\n",
      "It: 300, Loss: 3.158e-02, Loss_u: 2.278e-02, Loss_r: 8.791e-03, Time: 0.71\n",
      "It: 310, Loss: 4.301e-02, Loss_u: 3.725e-02, Loss_r: 5.761e-03, Time: 0.75\n",
      "It: 320, Loss: 4.087e-02, Loss_u: 3.532e-02, Loss_r: 5.555e-03, Time: 0.75\n",
      "It: 330, Loss: 4.309e-02, Loss_u: 3.555e-02, Loss_r: 7.548e-03, Time: 0.81\n",
      "It: 340, Loss: 3.806e-02, Loss_u: 3.334e-02, Loss_r: 4.719e-03, Time: 0.70\n",
      "It: 350, Loss: 4.444e-02, Loss_u: 3.950e-02, Loss_r: 4.940e-03, Time: 0.76\n",
      "It: 360, Loss: 3.732e-02, Loss_u: 3.347e-02, Loss_r: 3.846e-03, Time: 0.73\n",
      "It: 370, Loss: 3.645e-02, Loss_u: 3.178e-02, Loss_r: 4.665e-03, Time: 0.78\n",
      "It: 380, Loss: 3.192e-02, Loss_u: 2.523e-02, Loss_r: 6.694e-03, Time: 0.70\n",
      "It: 390, Loss: 4.070e-02, Loss_u: 3.654e-02, Loss_r: 4.167e-03, Time: 0.72\n",
      "It: 400, Loss: 5.571e-02, Loss_u: 3.994e-02, Loss_r: 1.576e-02, Time: 0.73\n",
      "It: 410, Loss: 3.567e-02, Loss_u: 2.874e-02, Loss_r: 6.930e-03, Time: 0.77\n",
      "It: 420, Loss: 3.525e-02, Loss_u: 3.137e-02, Loss_r: 3.878e-03, Time: 0.73\n",
      "It: 430, Loss: 3.737e-02, Loss_u: 3.433e-02, Loss_r: 3.038e-03, Time: 0.68\n",
      "It: 440, Loss: 3.560e-02, Loss_u: 2.935e-02, Loss_r: 6.248e-03, Time: 0.71\n",
      "It: 450, Loss: 3.839e-02, Loss_u: 3.325e-02, Loss_r: 5.147e-03, Time: 0.77\n",
      "It: 460, Loss: 3.690e-02, Loss_u: 3.321e-02, Loss_r: 3.697e-03, Time: 0.72\n",
      "It: 470, Loss: 3.135e-02, Loss_u: 2.669e-02, Loss_r: 4.664e-03, Time: 0.70\n",
      "It: 480, Loss: 3.649e-02, Loss_u: 3.153e-02, Loss_r: 4.955e-03, Time: 0.74\n",
      "It: 490, Loss: 2.933e-02, Loss_u: 2.612e-02, Loss_r: 3.208e-03, Time: 0.82\n",
      "It: 500, Loss: 3.177e-02, Loss_u: 2.795e-02, Loss_r: 3.815e-03, Time: 0.72\n",
      "It: 510, Loss: 3.754e-02, Loss_u: 3.319e-02, Loss_r: 4.349e-03, Time: 0.69\n",
      "It: 520, Loss: 3.983e-02, Loss_u: 3.596e-02, Loss_r: 3.873e-03, Time: 0.81\n",
      "It: 530, Loss: 3.363e-02, Loss_u: 2.919e-02, Loss_r: 4.433e-03, Time: 0.69\n",
      "It: 540, Loss: 3.499e-02, Loss_u: 3.254e-02, Loss_r: 2.452e-03, Time: 0.70\n",
      "It: 550, Loss: 3.826e-02, Loss_u: 3.555e-02, Loss_r: 2.716e-03, Time: 0.65\n",
      "It: 560, Loss: 3.310e-02, Loss_u: 2.717e-02, Loss_r: 5.927e-03, Time: 0.73\n",
      "It: 570, Loss: 3.469e-02, Loss_u: 3.226e-02, Loss_r: 2.435e-03, Time: 0.69\n",
      "It: 580, Loss: 4.029e-02, Loss_u: 3.628e-02, Loss_r: 4.007e-03, Time: 0.86\n",
      "It: 590, Loss: 3.458e-02, Loss_u: 3.181e-02, Loss_r: 2.766e-03, Time: 0.76\n",
      "It: 600, Loss: 3.057e-02, Loss_u: 2.688e-02, Loss_r: 3.689e-03, Time: 0.97\n",
      "It: 610, Loss: 3.305e-02, Loss_u: 2.849e-02, Loss_r: 4.562e-03, Time: 0.93\n",
      "It: 620, Loss: 3.627e-02, Loss_u: 3.253e-02, Loss_r: 3.743e-03, Time: 0.74\n",
      "It: 630, Loss: 3.397e-02, Loss_u: 2.954e-02, Loss_r: 4.426e-03, Time: 0.81\n",
      "It: 640, Loss: 3.035e-02, Loss_u: 2.712e-02, Loss_r: 3.226e-03, Time: 0.78\n",
      "It: 650, Loss: 2.833e-02, Loss_u: 2.527e-02, Loss_r: 3.064e-03, Time: 0.66\n",
      "It: 660, Loss: 3.147e-02, Loss_u: 2.882e-02, Loss_r: 2.648e-03, Time: 0.76\n",
      "It: 670, Loss: 2.462e-02, Loss_u: 2.086e-02, Loss_r: 3.757e-03, Time: 0.72\n",
      "It: 680, Loss: 4.642e-02, Loss_u: 4.071e-02, Loss_r: 5.713e-03, Time: 0.73\n",
      "It: 690, Loss: 2.731e-02, Loss_u: 2.460e-02, Loss_r: 2.709e-03, Time: 0.65\n",
      "It: 700, Loss: 3.112e-02, Loss_u: 2.842e-02, Loss_r: 2.701e-03, Time: 0.74\n",
      "It: 710, Loss: 3.114e-02, Loss_u: 2.703e-02, Loss_r: 4.111e-03, Time: 0.70\n",
      "It: 720, Loss: 3.206e-02, Loss_u: 2.847e-02, Loss_r: 3.588e-03, Time: 0.82\n",
      "It: 730, Loss: 4.020e-02, Loss_u: 3.660e-02, Loss_r: 3.609e-03, Time: 0.72\n",
      "It: 740, Loss: 3.274e-02, Loss_u: 2.886e-02, Loss_r: 3.880e-03, Time: 0.70\n",
      "It: 750, Loss: 2.667e-02, Loss_u: 2.241e-02, Loss_r: 4.261e-03, Time: 1.11\n",
      "It: 760, Loss: 3.213e-02, Loss_u: 2.854e-02, Loss_r: 3.592e-03, Time: 0.78\n",
      "It: 770, Loss: 2.735e-02, Loss_u: 2.366e-02, Loss_r: 3.693e-03, Time: 0.77\n",
      "It: 780, Loss: 3.535e-02, Loss_u: 3.152e-02, Loss_r: 3.831e-03, Time: 0.75\n",
      "It: 790, Loss: 3.555e-02, Loss_u: 3.188e-02, Loss_r: 3.671e-03, Time: 0.87\n",
      "It: 800, Loss: 4.125e-02, Loss_u: 3.827e-02, Loss_r: 2.984e-03, Time: 0.74\n",
      "It: 810, Loss: 2.846e-02, Loss_u: 2.535e-02, Loss_r: 3.112e-03, Time: 0.76\n",
      "It: 820, Loss: 3.140e-02, Loss_u: 2.745e-02, Loss_r: 3.955e-03, Time: 0.74\n",
      "It: 830, Loss: 2.665e-02, Loss_u: 2.438e-02, Loss_r: 2.268e-03, Time: 0.78\n",
      "It: 840, Loss: 3.417e-02, Loss_u: 2.929e-02, Loss_r: 4.879e-03, Time: 0.76\n",
      "It: 850, Loss: 2.621e-02, Loss_u: 2.295e-02, Loss_r: 3.259e-03, Time: 0.75\n",
      "It: 860, Loss: 3.657e-02, Loss_u: 3.327e-02, Loss_r: 3.299e-03, Time: 0.79\n",
      "It: 870, Loss: 3.546e-02, Loss_u: 3.146e-02, Loss_r: 4.001e-03, Time: 0.73\n",
      "It: 880, Loss: 3.401e-02, Loss_u: 2.972e-02, Loss_r: 4.284e-03, Time: 0.72\n",
      "It: 890, Loss: 3.161e-02, Loss_u: 2.910e-02, Loss_r: 2.511e-03, Time: 1.08\n",
      "It: 900, Loss: 2.588e-02, Loss_u: 2.338e-02, Loss_r: 2.494e-03, Time: 0.78\n",
      "It: 910, Loss: 2.817e-02, Loss_u: 2.537e-02, Loss_r: 2.805e-03, Time: 0.72\n",
      "It: 920, Loss: 2.366e-02, Loss_u: 1.926e-02, Loss_r: 4.398e-03, Time: 0.90\n",
      "It: 930, Loss: 2.617e-02, Loss_u: 2.282e-02, Loss_r: 3.356e-03, Time: 0.70\n",
      "It: 940, Loss: 3.973e-02, Loss_u: 3.646e-02, Loss_r: 3.265e-03, Time: 0.77\n",
      "It: 950, Loss: 3.906e-02, Loss_u: 3.632e-02, Loss_r: 2.736e-03, Time: 0.64\n",
      "It: 960, Loss: 2.972e-02, Loss_u: 2.663e-02, Loss_r: 3.095e-03, Time: 0.71\n",
      "It: 970, Loss: 2.978e-02, Loss_u: 2.719e-02, Loss_r: 2.597e-03, Time: 0.72\n",
      "It: 980, Loss: 3.428e-02, Loss_u: 3.199e-02, Loss_r: 2.294e-03, Time: 0.79\n",
      "It: 990, Loss: 3.825e-02, Loss_u: 3.576e-02, Loss_r: 2.494e-03, Time: 0.75\n",
      "It: 1000, Loss: 2.911e-02, Loss_u: 2.494e-02, Loss_r: 4.172e-03, Time: 0.70\n",
      "l2 error: 3.62e-01\n",
      "l2 error: 9.06e-01\n",
      "l2 error: 8.55e-02\n",
      "elapsed: 1.07e+02\n",
      "Relative L2 error_u: 3.62e-01\n",
      "Relative L2 error_v: 9.06e-01\n",
      "Relative L2 error_p: 8.55e-02\n",
      "Epoch:  2\n",
      "It: 0, Loss: 1.537e+00, Loss_u: 7.670e-01, Loss_r: 7.699e-01, Time: 17.02\n",
      "It: 10, Loss: 6.114e-01, Loss_u: 4.968e-01, Loss_r: 1.146e-01, Time: 0.78\n",
      "It: 20, Loss: 4.529e-01, Loss_u: 3.617e-01, Loss_r: 9.121e-02, Time: 0.79\n",
      "It: 30, Loss: 2.542e-01, Loss_u: 2.195e-01, Loss_r: 3.469e-02, Time: 0.83\n",
      "It: 40, Loss: 1.880e-01, Loss_u: 1.767e-01, Loss_r: 1.129e-02, Time: 0.90\n",
      "It: 50, Loss: 1.557e-01, Loss_u: 1.446e-01, Loss_r: 1.114e-02, Time: 0.78\n",
      "It: 60, Loss: 1.314e-01, Loss_u: 1.157e-01, Loss_r: 1.573e-02, Time: 0.74\n",
      "It: 70, Loss: 1.273e-01, Loss_u: 1.137e-01, Loss_r: 1.364e-02, Time: 0.84\n",
      "It: 80, Loss: 1.166e-01, Loss_u: 1.059e-01, Loss_r: 1.070e-02, Time: 0.82\n",
      "It: 90, Loss: 1.113e-01, Loss_u: 1.010e-01, Loss_r: 1.023e-02, Time: 0.77\n",
      "It: 100, Loss: 1.219e-01, Loss_u: 1.133e-01, Loss_r: 8.623e-03, Time: 0.77\n",
      "It: 110, Loss: 1.138e-01, Loss_u: 1.059e-01, Loss_r: 7.833e-03, Time: 0.85\n",
      "It: 120, Loss: 1.050e-01, Loss_u: 9.677e-02, Loss_r: 8.208e-03, Time: 0.75\n",
      "It: 130, Loss: 1.158e-01, Loss_u: 1.094e-01, Loss_r: 6.398e-03, Time: 0.81\n",
      "It: 140, Loss: 9.171e-02, Loss_u: 8.571e-02, Loss_r: 6.004e-03, Time: 0.82\n",
      "It: 150, Loss: 9.694e-02, Loss_u: 9.137e-02, Loss_r: 5.565e-03, Time: 0.83\n",
      "It: 160, Loss: 9.977e-02, Loss_u: 9.405e-02, Loss_r: 5.720e-03, Time: 0.81\n",
      "It: 170, Loss: 8.798e-02, Loss_u: 8.359e-02, Loss_r: 4.397e-03, Time: 0.97\n",
      "It: 180, Loss: 9.603e-02, Loss_u: 8.942e-02, Loss_r: 6.618e-03, Time: 0.96\n",
      "It: 190, Loss: 8.075e-02, Loss_u: 7.460e-02, Loss_r: 6.145e-03, Time: 0.84\n",
      "It: 200, Loss: 8.610e-02, Loss_u: 7.996e-02, Loss_r: 6.132e-03, Time: 1.11\n",
      "It: 210, Loss: 8.043e-02, Loss_u: 7.431e-02, Loss_r: 6.113e-03, Time: 1.04\n",
      "It: 220, Loss: 7.725e-02, Loss_u: 7.271e-02, Loss_r: 4.535e-03, Time: 0.80\n",
      "It: 230, Loss: 8.481e-02, Loss_u: 7.824e-02, Loss_r: 6.567e-03, Time: 0.86\n",
      "It: 240, Loss: 7.211e-02, Loss_u: 6.644e-02, Loss_r: 5.679e-03, Time: 0.84\n",
      "It: 250, Loss: 7.519e-02, Loss_u: 6.972e-02, Loss_r: 5.469e-03, Time: 0.78\n",
      "It: 260, Loss: 7.615e-02, Loss_u: 6.980e-02, Loss_r: 6.353e-03, Time: 0.73\n",
      "It: 270, Loss: 7.494e-02, Loss_u: 6.910e-02, Loss_r: 5.845e-03, Time: 0.78\n",
      "It: 280, Loss: 7.214e-02, Loss_u: 6.747e-02, Loss_r: 4.670e-03, Time: 0.82\n",
      "It: 290, Loss: 7.367e-02, Loss_u: 6.790e-02, Loss_r: 5.769e-03, Time: 0.77\n",
      "It: 300, Loss: 6.939e-02, Loss_u: 6.421e-02, Loss_r: 5.180e-03, Time: 0.78\n",
      "It: 310, Loss: 6.900e-02, Loss_u: 6.262e-02, Loss_r: 6.378e-03, Time: 0.80\n",
      "It: 320, Loss: 6.428e-02, Loss_u: 5.833e-02, Loss_r: 5.949e-03, Time: 0.80\n",
      "It: 330, Loss: 6.770e-02, Loss_u: 5.895e-02, Loss_r: 8.756e-03, Time: 0.73\n",
      "It: 340, Loss: 5.710e-02, Loss_u: 5.095e-02, Loss_r: 6.150e-03, Time: 0.71\n",
      "It: 350, Loss: 5.957e-02, Loss_u: 5.459e-02, Loss_r: 4.974e-03, Time: 0.88\n",
      "It: 360, Loss: 5.354e-02, Loss_u: 4.849e-02, Loss_r: 5.046e-03, Time: 0.75\n",
      "It: 370, Loss: 6.034e-02, Loss_u: 5.333e-02, Loss_r: 7.012e-03, Time: 0.74\n",
      "It: 380, Loss: 5.181e-02, Loss_u: 4.588e-02, Loss_r: 5.930e-03, Time: 0.77\n",
      "It: 390, Loss: 4.743e-02, Loss_u: 4.196e-02, Loss_r: 5.471e-03, Time: 0.86\n",
      "It: 400, Loss: 5.205e-02, Loss_u: 4.660e-02, Loss_r: 5.449e-03, Time: 0.79\n",
      "It: 410, Loss: 5.663e-02, Loss_u: 4.993e-02, Loss_r: 6.698e-03, Time: 0.75\n",
      "It: 420, Loss: 4.374e-02, Loss_u: 3.768e-02, Loss_r: 6.063e-03, Time: 0.80\n",
      "It: 430, Loss: 3.929e-02, Loss_u: 3.489e-02, Loss_r: 4.403e-03, Time: 0.78\n",
      "It: 440, Loss: 5.312e-02, Loss_u: 4.664e-02, Loss_r: 6.481e-03, Time: 0.72\n",
      "It: 450, Loss: 4.198e-02, Loss_u: 3.823e-02, Loss_r: 3.745e-03, Time: 0.73\n",
      "It: 460, Loss: 4.859e-02, Loss_u: 4.338e-02, Loss_r: 5.211e-03, Time: 0.79\n",
      "It: 470, Loss: 4.699e-02, Loss_u: 4.275e-02, Loss_r: 4.246e-03, Time: 0.71\n",
      "It: 480, Loss: 4.553e-02, Loss_u: 4.087e-02, Loss_r: 4.661e-03, Time: 0.78\n",
      "It: 490, Loss: 4.763e-02, Loss_u: 4.226e-02, Loss_r: 5.373e-03, Time: 0.69\n",
      "It: 500, Loss: 4.226e-02, Loss_u: 3.707e-02, Loss_r: 5.183e-03, Time: 0.76\n",
      "It: 510, Loss: 3.689e-02, Loss_u: 3.299e-02, Loss_r: 3.906e-03, Time: 0.76\n",
      "It: 520, Loss: 4.705e-02, Loss_u: 4.259e-02, Loss_r: 4.466e-03, Time: 0.73\n",
      "It: 530, Loss: 4.096e-02, Loss_u: 3.587e-02, Loss_r: 5.082e-03, Time: 0.73\n",
      "It: 540, Loss: 4.324e-02, Loss_u: 3.815e-02, Loss_r: 5.088e-03, Time: 0.87\n",
      "It: 550, Loss: 3.870e-02, Loss_u: 3.351e-02, Loss_r: 5.193e-03, Time: 0.74\n",
      "It: 560, Loss: 3.562e-02, Loss_u: 3.173e-02, Loss_r: 3.889e-03, Time: 0.76\n",
      "It: 570, Loss: 3.866e-02, Loss_u: 3.503e-02, Loss_r: 3.628e-03, Time: 0.73\n",
      "It: 580, Loss: 4.408e-02, Loss_u: 3.953e-02, Loss_r: 4.553e-03, Time: 0.78\n",
      "It: 590, Loss: 4.505e-02, Loss_u: 4.154e-02, Loss_r: 3.506e-03, Time: 0.73\n",
      "It: 600, Loss: 3.923e-02, Loss_u: 3.476e-02, Loss_r: 4.476e-03, Time: 0.68\n",
      "It: 610, Loss: 3.870e-02, Loss_u: 3.436e-02, Loss_r: 4.336e-03, Time: 0.83\n",
      "It: 620, Loss: 4.109e-02, Loss_u: 3.808e-02, Loss_r: 3.012e-03, Time: 0.78\n",
      "It: 630, Loss: 3.993e-02, Loss_u: 3.543e-02, Loss_r: 4.500e-03, Time: 0.67\n",
      "It: 640, Loss: 3.354e-02, Loss_u: 3.055e-02, Loss_r: 2.987e-03, Time: 0.83\n",
      "It: 650, Loss: 3.945e-02, Loss_u: 3.519e-02, Loss_r: 4.256e-03, Time: 0.76\n",
      "It: 660, Loss: 3.980e-02, Loss_u: 3.340e-02, Loss_r: 6.395e-03, Time: 0.73\n",
      "It: 670, Loss: 3.801e-02, Loss_u: 3.397e-02, Loss_r: 4.032e-03, Time: 0.72\n",
      "It: 680, Loss: 3.736e-02, Loss_u: 3.335e-02, Loss_r: 4.013e-03, Time: 0.72\n",
      "It: 690, Loss: 4.044e-02, Loss_u: 3.543e-02, Loss_r: 5.008e-03, Time: 0.83\n",
      "It: 700, Loss: 3.820e-02, Loss_u: 3.555e-02, Loss_r: 2.649e-03, Time: 0.70\n",
      "It: 710, Loss: 3.782e-02, Loss_u: 3.291e-02, Loss_r: 4.908e-03, Time: 0.75\n",
      "It: 720, Loss: 3.555e-02, Loss_u: 3.070e-02, Loss_r: 4.848e-03, Time: 0.79\n",
      "It: 730, Loss: 3.766e-02, Loss_u: 3.376e-02, Loss_r: 3.906e-03, Time: 0.78\n",
      "It: 740, Loss: 4.403e-02, Loss_u: 4.078e-02, Loss_r: 3.251e-03, Time: 0.81\n",
      "It: 750, Loss: 3.738e-02, Loss_u: 3.315e-02, Loss_r: 4.234e-03, Time: 0.67\n",
      "It: 760, Loss: 3.899e-02, Loss_u: 3.459e-02, Loss_r: 4.398e-03, Time: 0.74\n",
      "It: 770, Loss: 4.349e-02, Loss_u: 3.983e-02, Loss_r: 3.659e-03, Time: 0.81\n",
      "It: 780, Loss: 3.732e-02, Loss_u: 3.360e-02, Loss_r: 3.727e-03, Time: 0.77\n",
      "It: 790, Loss: 3.620e-02, Loss_u: 3.260e-02, Loss_r: 3.604e-03, Time: 0.71\n",
      "It: 800, Loss: 2.923e-02, Loss_u: 2.469e-02, Loss_r: 4.540e-03, Time: 0.80\n",
      "It: 810, Loss: 3.560e-02, Loss_u: 3.279e-02, Loss_r: 2.817e-03, Time: 0.73\n",
      "It: 820, Loss: 3.507e-02, Loss_u: 3.097e-02, Loss_r: 4.107e-03, Time: 0.73\n",
      "It: 830, Loss: 4.244e-02, Loss_u: 3.828e-02, Loss_r: 4.155e-03, Time: 0.77\n",
      "It: 840, Loss: 3.551e-02, Loss_u: 3.199e-02, Loss_r: 3.512e-03, Time: 0.82\n",
      "It: 850, Loss: 3.417e-02, Loss_u: 3.170e-02, Loss_r: 2.469e-03, Time: 0.81\n",
      "It: 860, Loss: 3.174e-02, Loss_u: 2.721e-02, Loss_r: 4.529e-03, Time: 0.72\n",
      "It: 870, Loss: 3.202e-02, Loss_u: 2.910e-02, Loss_r: 2.920e-03, Time: 0.78\n",
      "It: 880, Loss: 3.941e-02, Loss_u: 3.617e-02, Loss_r: 3.238e-03, Time: 0.81\n",
      "It: 890, Loss: 3.296e-02, Loss_u: 2.928e-02, Loss_r: 3.679e-03, Time: 0.67\n",
      "It: 900, Loss: 3.310e-02, Loss_u: 3.008e-02, Loss_r: 3.017e-03, Time: 0.70\n",
      "It: 910, Loss: 4.563e-02, Loss_u: 4.268e-02, Loss_r: 2.950e-03, Time: 0.76\n",
      "It: 920, Loss: 2.765e-02, Loss_u: 2.346e-02, Loss_r: 4.189e-03, Time: 0.77\n",
      "It: 930, Loss: 3.292e-02, Loss_u: 2.954e-02, Loss_r: 3.382e-03, Time: 0.71\n",
      "It: 940, Loss: 3.257e-02, Loss_u: 2.844e-02, Loss_r: 4.125e-03, Time: 0.71\n",
      "It: 950, Loss: 3.538e-02, Loss_u: 3.226e-02, Loss_r: 3.123e-03, Time: 0.83\n",
      "It: 960, Loss: 3.377e-02, Loss_u: 3.049e-02, Loss_r: 3.280e-03, Time: 0.74\n",
      "It: 970, Loss: 3.543e-02, Loss_u: 3.299e-02, Loss_r: 2.439e-03, Time: 0.72\n",
      "It: 980, Loss: 2.829e-02, Loss_u: 2.384e-02, Loss_r: 4.448e-03, Time: 0.71\n",
      "It: 990, Loss: 3.590e-02, Loss_u: 3.166e-02, Loss_r: 4.242e-03, Time: 0.77\n",
      "It: 1000, Loss: 2.796e-02, Loss_u: 2.553e-02, Loss_r: 2.434e-03, Time: 0.76\n",
      "l2 error: 3.69e-01\n",
      "l2 error: 1.01e+00\n",
      "l2 error: 3.59e-01\n",
      "elapsed: 1.08e+02\n",
      "Relative L2 error_u: 3.69e-01\n",
      "Relative L2 error_v: 1.01e+00\n",
      "Relative L2 error_p: 3.59e-01\n",
      "\n",
      "\n",
      "Method:  mini_batch\n",
      "\n",
      "average of time_list: 107.41427326202393\n",
      "average of error_u_list: 0.3652707251897648\n",
      "average of error_v_list: 0.9576280807717232\n",
      "average of error_p_list: 0.2220778065576964\n",
      "Method:  full_batch\n",
      "Epoch:  1\n",
      "It: 0, Loss: 4.192e+00, Loss_u: 9.718e-01, Loss_r: 3.220e+00, Time: 17.58\n",
      "It: 10, Loss: 9.656e-01, Loss_u: 3.591e-01, Loss_r: 6.065e-01, Time: 3.03\n",
      "It: 20, Loss: 4.544e-01, Loss_u: 2.619e-01, Loss_r: 1.925e-01, Time: 3.15\n",
      "It: 30, Loss: 2.051e-01, Loss_u: 1.918e-01, Loss_r: 1.335e-02, Time: 3.11\n",
      "It: 40, Loss: 1.859e-01, Loss_u: 1.502e-01, Loss_r: 3.576e-02, Time: 3.08\n",
      "It: 50, Loss: 1.641e-01, Loss_u: 1.385e-01, Loss_r: 2.562e-02, Time: 2.99\n",
      "It: 60, Loss: 1.453e-01, Loss_u: 1.297e-01, Loss_r: 1.557e-02, Time: 3.15\n",
      "It: 70, Loss: 1.367e-01, Loss_u: 1.255e-01, Loss_r: 1.124e-02, Time: 3.18\n",
      "It: 80, Loss: 1.306e-01, Loss_u: 1.197e-01, Loss_r: 1.091e-02, Time: 3.30\n",
      "It: 90, Loss: 1.249e-01, Loss_u: 1.140e-01, Loss_r: 1.095e-02, Time: 3.16\n",
      "It: 100, Loss: 1.201e-01, Loss_u: 1.097e-01, Loss_r: 1.044e-02, Time: 3.21\n",
      "It: 110, Loss: 1.160e-01, Loss_u: 1.062e-01, Loss_r: 9.789e-03, Time: 3.25\n",
      "It: 120, Loss: 1.123e-01, Loss_u: 1.029e-01, Loss_r: 9.389e-03, Time: 3.27\n",
      "It: 130, Loss: 1.091e-01, Loss_u: 9.990e-02, Loss_r: 9.170e-03, Time: 3.38\n",
      "It: 140, Loss: 1.062e-01, Loss_u: 9.715e-02, Loss_r: 9.009e-03, Time: 3.18\n",
      "It: 150, Loss: 1.035e-01, Loss_u: 9.467e-02, Loss_r: 8.864e-03, Time: 3.58\n",
      "It: 160, Loss: 1.011e-01, Loss_u: 9.238e-02, Loss_r: 8.757e-03, Time: 3.20\n",
      "It: 170, Loss: 9.893e-02, Loss_u: 9.024e-02, Loss_r: 8.695e-03, Time: 3.24\n",
      "It: 180, Loss: 9.688e-02, Loss_u: 8.824e-02, Loss_r: 8.642e-03, Time: 3.24\n",
      "It: 190, Loss: 9.497e-02, Loss_u: 8.637e-02, Loss_r: 8.592e-03, Time: 3.11\n",
      "It: 200, Loss: 9.315e-02, Loss_u: 8.461e-02, Loss_r: 8.540e-03, Time: 3.15\n",
      "It: 210, Loss: 9.142e-02, Loss_u: 8.294e-02, Loss_r: 8.479e-03, Time: 3.10\n",
      "It: 220, Loss: 8.975e-02, Loss_u: 8.134e-02, Loss_r: 8.406e-03, Time: 3.13\n",
      "It: 230, Loss: 8.812e-02, Loss_u: 7.981e-02, Loss_r: 8.317e-03, Time: 3.28\n",
      "It: 240, Loss: 8.654e-02, Loss_u: 7.833e-02, Loss_r: 8.214e-03, Time: 3.19\n",
      "It: 250, Loss: 8.498e-02, Loss_u: 7.689e-02, Loss_r: 8.097e-03, Time: 3.09\n",
      "It: 260, Loss: 8.345e-02, Loss_u: 7.548e-02, Loss_r: 7.967e-03, Time: 3.11\n",
      "It: 270, Loss: 8.193e-02, Loss_u: 7.410e-02, Loss_r: 7.825e-03, Time: 3.57\n",
      "It: 280, Loss: 8.041e-02, Loss_u: 7.274e-02, Loss_r: 7.673e-03, Time: 3.12\n",
      "It: 290, Loss: 7.890e-02, Loss_u: 7.139e-02, Loss_r: 7.511e-03, Time: 3.43\n",
      "It: 300, Loss: 7.738e-02, Loss_u: 7.004e-02, Loss_r: 7.341e-03, Time: 3.24\n",
      "It: 310, Loss: 7.586e-02, Loss_u: 6.870e-02, Loss_r: 7.162e-03, Time: 3.34\n",
      "It: 320, Loss: 7.432e-02, Loss_u: 6.735e-02, Loss_r: 6.976e-03, Time: 3.31\n",
      "It: 330, Loss: 7.278e-02, Loss_u: 6.599e-02, Loss_r: 6.781e-03, Time: 3.36\n",
      "It: 340, Loss: 7.121e-02, Loss_u: 6.463e-02, Loss_r: 6.578e-03, Time: 3.28\n",
      "It: 350, Loss: 6.962e-02, Loss_u: 6.325e-02, Loss_r: 6.367e-03, Time: 3.25\n",
      "It: 360, Loss: 6.800e-02, Loss_u: 6.185e-02, Loss_r: 6.148e-03, Time: 3.51\n",
      "It: 370, Loss: 6.636e-02, Loss_u: 6.044e-02, Loss_r: 5.921e-03, Time: 3.48\n",
      "It: 380, Loss: 6.468e-02, Loss_u: 5.900e-02, Loss_r: 5.685e-03, Time: 3.49\n",
      "It: 390, Loss: 6.297e-02, Loss_u: 5.753e-02, Loss_r: 5.442e-03, Time: 3.39\n",
      "It: 400, Loss: 6.123e-02, Loss_u: 5.604e-02, Loss_r: 5.193e-03, Time: 3.41\n",
      "It: 410, Loss: 5.946e-02, Loss_u: 5.452e-02, Loss_r: 4.940e-03, Time: 3.43\n",
      "It: 420, Loss: 5.767e-02, Loss_u: 5.298e-02, Loss_r: 4.685e-03, Time: 3.35\n",
      "It: 430, Loss: 5.586e-02, Loss_u: 5.143e-02, Loss_r: 4.431e-03, Time: 3.38\n",
      "It: 440, Loss: 5.405e-02, Loss_u: 4.986e-02, Loss_r: 4.184e-03, Time: 3.42\n",
      "It: 450, Loss: 5.224e-02, Loss_u: 4.830e-02, Loss_r: 3.948e-03, Time: 3.28\n",
      "It: 460, Loss: 5.048e-02, Loss_u: 4.675e-02, Loss_r: 3.730e-03, Time: 3.41\n",
      "It: 470, Loss: 4.876e-02, Loss_u: 4.523e-02, Loss_r: 3.537e-03, Time: 3.29\n",
      "It: 480, Loss: 4.713e-02, Loss_u: 4.375e-02, Loss_r: 3.373e-03, Time: 3.24\n",
      "It: 490, Loss: 4.559e-02, Loss_u: 4.235e-02, Loss_r: 3.244e-03, Time: 3.16\n",
      "It: 500, Loss: 4.417e-02, Loss_u: 4.102e-02, Loss_r: 3.149e-03, Time: 3.18\n",
      "It: 510, Loss: 4.288e-02, Loss_u: 3.979e-02, Loss_r: 3.086e-03, Time: 3.23\n",
      "It: 520, Loss: 4.170e-02, Loss_u: 3.865e-02, Loss_r: 3.049e-03, Time: 3.17\n",
      "It: 530, Loss: 4.063e-02, Loss_u: 3.760e-02, Loss_r: 3.030e-03, Time: 3.28\n",
      "It: 540, Loss: 3.965e-02, Loss_u: 3.665e-02, Loss_r: 3.006e-03, Time: 3.10\n",
      "It: 550, Loss: 4.490e-02, Loss_u: 3.864e-02, Loss_r: 6.264e-03, Time: 3.16\n",
      "It: 560, Loss: 4.679e-02, Loss_u: 3.892e-02, Loss_r: 7.875e-03, Time: 3.08\n",
      "It: 570, Loss: 3.919e-02, Loss_u: 3.432e-02, Loss_r: 4.872e-03, Time: 3.07\n",
      "It: 580, Loss: 3.709e-02, Loss_u: 3.432e-02, Loss_r: 2.771e-03, Time: 3.22\n",
      "It: 590, Loss: 3.609e-02, Loss_u: 3.321e-02, Loss_r: 2.884e-03, Time: 3.33\n",
      "It: 600, Loss: 3.563e-02, Loss_u: 3.235e-02, Loss_r: 3.278e-03, Time: 3.30\n",
      "It: 610, Loss: 3.499e-02, Loss_u: 3.203e-02, Loss_r: 2.960e-03, Time: 3.43\n",
      "It: 620, Loss: 3.452e-02, Loss_u: 3.160e-02, Loss_r: 2.925e-03, Time: 3.24\n",
      "It: 630, Loss: 3.408e-02, Loss_u: 3.111e-02, Loss_r: 2.974e-03, Time: 3.07\n",
      "It: 640, Loss: 3.370e-02, Loss_u: 3.068e-02, Loss_r: 3.015e-03, Time: 3.28\n",
      "It: 650, Loss: 3.336e-02, Loss_u: 3.029e-02, Loss_r: 3.075e-03, Time: 3.30\n",
      "It: 660, Loss: 3.308e-02, Loss_u: 2.995e-02, Loss_r: 3.136e-03, Time: 3.32\n",
      "It: 670, Loss: 3.285e-02, Loss_u: 2.972e-02, Loss_r: 3.132e-03, Time: 3.29\n",
      "It: 680, Loss: 3.266e-02, Loss_u: 2.950e-02, Loss_r: 3.165e-03, Time: 3.28\n",
      "It: 690, Loss: 3.250e-02, Loss_u: 2.934e-02, Loss_r: 3.162e-03, Time: 3.19\n",
      "It: 700, Loss: 3.237e-02, Loss_u: 2.922e-02, Loss_r: 3.144e-03, Time: 3.36\n",
      "It: 710, Loss: 3.236e-02, Loss_u: 2.946e-02, Loss_r: 2.904e-03, Time: 3.34\n",
      "It: 720, Loss: 3.816e-02, Loss_u: 3.400e-02, Loss_r: 4.167e-03, Time: 3.59\n",
      "It: 730, Loss: 3.511e-02, Loss_u: 3.201e-02, Loss_r: 3.105e-03, Time: 3.29\n",
      "It: 740, Loss: 3.299e-02, Loss_u: 3.025e-02, Loss_r: 2.737e-03, Time: 3.32\n",
      "It: 750, Loss: 3.194e-02, Loss_u: 2.906e-02, Loss_r: 2.874e-03, Time: 3.30\n",
      "It: 760, Loss: 3.189e-02, Loss_u: 2.855e-02, Loss_r: 3.339e-03, Time: 3.13\n",
      "It: 770, Loss: 3.179e-02, Loss_u: 2.853e-02, Loss_r: 3.263e-03, Time: 3.26\n",
      "It: 780, Loss: 3.170e-02, Loss_u: 2.878e-02, Loss_r: 2.920e-03, Time: 3.34\n",
      "It: 790, Loss: 3.162e-02, Loss_u: 2.857e-02, Loss_r: 3.041e-03, Time: 3.56\n",
      "It: 800, Loss: 3.155e-02, Loss_u: 2.856e-02, Loss_r: 2.990e-03, Time: 3.40\n",
      "It: 810, Loss: 3.149e-02, Loss_u: 2.846e-02, Loss_r: 3.029e-03, Time: 3.56\n",
      "It: 820, Loss: 3.143e-02, Loss_u: 2.848e-02, Loss_r: 2.953e-03, Time: 3.23\n",
      "It: 830, Loss: 3.138e-02, Loss_u: 2.849e-02, Loss_r: 2.892e-03, Time: 3.20\n",
      "It: 840, Loss: 3.156e-02, Loss_u: 2.896e-02, Loss_r: 2.595e-03, Time: 3.49\n",
      "It: 850, Loss: 3.495e-02, Loss_u: 3.199e-02, Loss_r: 2.960e-03, Time: 3.48\n",
      "It: 860, Loss: 3.130e-02, Loss_u: 2.879e-02, Loss_r: 2.516e-03, Time: 3.83\n",
      "It: 870, Loss: 3.206e-02, Loss_u: 2.768e-02, Loss_r: 4.383e-03, Time: 3.59\n",
      "It: 880, Loss: 3.129e-02, Loss_u: 2.872e-02, Loss_r: 2.570e-03, Time: 3.86\n",
      "It: 890, Loss: 3.110e-02, Loss_u: 2.826e-02, Loss_r: 2.839e-03, Time: 3.32\n",
      "It: 900, Loss: 3.105e-02, Loss_u: 2.815e-02, Loss_r: 2.896e-03, Time: 3.33\n",
      "It: 910, Loss: 3.101e-02, Loss_u: 2.808e-02, Loss_r: 2.930e-03, Time: 3.25\n",
      "It: 920, Loss: 3.098e-02, Loss_u: 2.827e-02, Loss_r: 2.708e-03, Time: 3.27\n",
      "It: 930, Loss: 3.099e-02, Loss_u: 2.837e-02, Loss_r: 2.612e-03, Time: 3.27\n",
      "It: 940, Loss: 3.228e-02, Loss_u: 2.981e-02, Loss_r: 2.468e-03, Time: 3.30\n",
      "It: 950, Loss: 3.092e-02, Loss_u: 2.796e-02, Loss_r: 2.955e-03, Time: 3.34\n",
      "It: 960, Loss: 3.086e-02, Loss_u: 2.831e-02, Loss_r: 2.549e-03, Time: 3.45\n",
      "It: 970, Loss: 3.081e-02, Loss_u: 2.802e-02, Loss_r: 2.784e-03, Time: 3.35\n",
      "It: 980, Loss: 3.076e-02, Loss_u: 2.812e-02, Loss_r: 2.636e-03, Time: 3.23\n",
      "It: 990, Loss: 3.071e-02, Loss_u: 2.803e-02, Loss_r: 2.677e-03, Time: 3.58\n",
      "It: 1000, Loss: 3.074e-02, Loss_u: 2.765e-02, Loss_r: 3.087e-03, Time: 3.52\n",
      "l2 error: 3.60e-01\n",
      "l2 error: 9.51e-01\n",
      "l2 error: 1.12e-01\n",
      "elapsed: 3.73e+02\n",
      "Relative L2 error_u: 3.60e-01\n",
      "Relative L2 error_v: 9.51e-01\n",
      "Relative L2 error_p: 1.12e-01\n",
      "Epoch:  2\n",
      "It: 0, Loss: 1.374e+00, Loss_u: 7.187e-01, Loss_r: 6.552e-01, Time: 18.13\n",
      "It: 10, Loss: 3.047e-01, Loss_u: 2.848e-01, Loss_r: 1.988e-02, Time: 2.91\n",
      "It: 20, Loss: 2.447e-01, Loss_u: 1.807e-01, Loss_r: 6.392e-02, Time: 2.97\n",
      "It: 30, Loss: 2.003e-01, Loss_u: 1.689e-01, Loss_r: 3.139e-02, Time: 2.96\n",
      "It: 40, Loss: 1.736e-01, Loss_u: 1.620e-01, Loss_r: 1.163e-02, Time: 2.94\n",
      "It: 50, Loss: 1.508e-01, Loss_u: 1.377e-01, Loss_r: 1.313e-02, Time: 2.98\n",
      "It: 60, Loss: 1.383e-01, Loss_u: 1.258e-01, Loss_r: 1.256e-02, Time: 2.97\n",
      "It: 70, Loss: 1.283e-01, Loss_u: 1.194e-01, Loss_r: 8.874e-03, Time: 3.10\n",
      "It: 80, Loss: 1.201e-01, Loss_u: 1.106e-01, Loss_r: 9.455e-03, Time: 3.08\n",
      "It: 90, Loss: 1.134e-01, Loss_u: 1.043e-01, Loss_r: 9.168e-03, Time: 3.04\n",
      "It: 100, Loss: 1.078e-01, Loss_u: 9.877e-02, Loss_r: 9.055e-03, Time: 3.18\n",
      "It: 110, Loss: 1.030e-01, Loss_u: 9.394e-02, Loss_r: 9.053e-03, Time: 3.15\n",
      "It: 120, Loss: 9.874e-02, Loss_u: 8.970e-02, Loss_r: 9.041e-03, Time: 3.16\n",
      "It: 130, Loss: 9.496e-02, Loss_u: 8.586e-02, Loss_r: 9.092e-03, Time: 3.16\n",
      "It: 140, Loss: 9.154e-02, Loss_u: 8.243e-02, Loss_r: 9.110e-03, Time: 3.09\n",
      "It: 150, Loss: 8.842e-02, Loss_u: 7.933e-02, Loss_r: 9.088e-03, Time: 3.20\n",
      "It: 160, Loss: 8.553e-02, Loss_u: 7.648e-02, Loss_r: 9.044e-03, Time: 3.05\n",
      "It: 170, Loss: 8.281e-02, Loss_u: 7.387e-02, Loss_r: 8.941e-03, Time: 3.14\n",
      "It: 180, Loss: 8.024e-02, Loss_u: 7.145e-02, Loss_r: 8.789e-03, Time: 3.12\n",
      "It: 190, Loss: 7.777e-02, Loss_u: 6.918e-02, Loss_r: 8.592e-03, Time: 3.17\n",
      "It: 200, Loss: 7.540e-02, Loss_u: 6.704e-02, Loss_r: 8.355e-03, Time: 3.27\n",
      "It: 210, Loss: 7.311e-02, Loss_u: 6.502e-02, Loss_r: 8.093e-03, Time: 3.55\n",
      "It: 220, Loss: 7.090e-02, Loss_u: 6.309e-02, Loss_r: 7.813e-03, Time: 3.37\n",
      "It: 230, Loss: 6.878e-02, Loss_u: 6.125e-02, Loss_r: 7.523e-03, Time: 3.23\n",
      "It: 240, Loss: 6.673e-02, Loss_u: 5.950e-02, Loss_r: 7.230e-03, Time: 3.48\n",
      "It: 250, Loss: 6.478e-02, Loss_u: 5.783e-02, Loss_r: 6.941e-03, Time: 3.39\n",
      "It: 260, Loss: 8.394e-02, Loss_u: 6.143e-02, Loss_r: 2.251e-02, Time: 3.18\n",
      "It: 270, Loss: 6.716e-02, Loss_u: 5.717e-02, Loss_r: 9.984e-03, Time: 3.14\n",
      "It: 280, Loss: 6.082e-02, Loss_u: 5.407e-02, Loss_r: 6.744e-03, Time: 3.31\n",
      "It: 290, Loss: 5.843e-02, Loss_u: 5.279e-02, Loss_r: 5.636e-03, Time: 3.40\n",
      "It: 300, Loss: 5.686e-02, Loss_u: 5.147e-02, Loss_r: 5.396e-03, Time: 3.50\n",
      "It: 310, Loss: 5.543e-02, Loss_u: 5.022e-02, Loss_r: 5.212e-03, Time: 3.24\n",
      "It: 320, Loss: 5.406e-02, Loss_u: 4.897e-02, Loss_r: 5.094e-03, Time: 3.76\n",
      "It: 330, Loss: 5.267e-02, Loss_u: 4.788e-02, Loss_r: 4.793e-03, Time: 3.42\n",
      "It: 340, Loss: 5.134e-02, Loss_u: 4.675e-02, Loss_r: 4.585e-03, Time: 3.35\n",
      "It: 350, Loss: 5.003e-02, Loss_u: 4.562e-02, Loss_r: 4.409e-03, Time: 3.03\n",
      "It: 360, Loss: 4.875e-02, Loss_u: 4.449e-02, Loss_r: 4.264e-03, Time: 3.11\n",
      "It: 370, Loss: 4.751e-02, Loss_u: 4.335e-02, Loss_r: 4.153e-03, Time: 3.06\n",
      "It: 380, Loss: 4.630e-02, Loss_u: 4.226e-02, Loss_r: 4.047e-03, Time: 3.46\n",
      "It: 390, Loss: 4.516e-02, Loss_u: 4.115e-02, Loss_r: 4.004e-03, Time: 3.31\n",
      "It: 400, Loss: 4.409e-02, Loss_u: 4.005e-02, Loss_r: 4.043e-03, Time: 3.10\n",
      "It: 410, Loss: 5.194e-02, Loss_u: 3.886e-02, Loss_r: 1.307e-02, Time: 3.52\n",
      "It: 420, Loss: 4.810e-02, Loss_u: 3.817e-02, Loss_r: 9.929e-03, Time: 3.44\n",
      "It: 430, Loss: 4.165e-02, Loss_u: 3.763e-02, Loss_r: 4.016e-03, Time: 3.33\n",
      "It: 440, Loss: 4.160e-02, Loss_u: 3.777e-02, Loss_r: 3.835e-03, Time: 3.10\n",
      "It: 450, Loss: 4.012e-02, Loss_u: 3.615e-02, Loss_r: 3.966e-03, Time: 3.11\n",
      "It: 460, Loss: 3.955e-02, Loss_u: 3.529e-02, Loss_r: 4.263e-03, Time: 3.29\n",
      "It: 470, Loss: 3.891e-02, Loss_u: 3.475e-02, Loss_r: 4.164e-03, Time: 3.40\n",
      "It: 480, Loss: 3.831e-02, Loss_u: 3.435e-02, Loss_r: 3.959e-03, Time: 3.36\n",
      "It: 490, Loss: 3.778e-02, Loss_u: 3.401e-02, Loss_r: 3.762e-03, Time: 3.32\n",
      "It: 500, Loss: 3.725e-02, Loss_u: 3.349e-02, Loss_r: 3.765e-03, Time: 3.18\n",
      "It: 510, Loss: 3.677e-02, Loss_u: 3.318e-02, Loss_r: 3.593e-03, Time: 3.21\n",
      "It: 520, Loss: 3.632e-02, Loss_u: 3.281e-02, Loss_r: 3.511e-03, Time: 3.23\n",
      "It: 530, Loss: 3.599e-02, Loss_u: 3.278e-02, Loss_r: 3.209e-03, Time: 3.25\n",
      "It: 540, Loss: 4.051e-02, Loss_u: 3.630e-02, Loss_r: 4.214e-03, Time: 3.26\n",
      "It: 550, Loss: 3.725e-02, Loss_u: 3.416e-02, Loss_r: 3.085e-03, Time: 3.22\n",
      "It: 560, Loss: 3.527e-02, Loss_u: 3.193e-02, Loss_r: 3.341e-03, Time: 3.23\n",
      "It: 570, Loss: 3.516e-02, Loss_u: 3.186e-02, Loss_r: 3.295e-03, Time: 3.18\n",
      "It: 580, Loss: 3.480e-02, Loss_u: 3.190e-02, Loss_r: 2.905e-03, Time: 3.24\n",
      "It: 590, Loss: 3.448e-02, Loss_u: 3.161e-02, Loss_r: 2.869e-03, Time: 3.19\n",
      "It: 600, Loss: 3.427e-02, Loss_u: 3.117e-02, Loss_r: 3.102e-03, Time: 3.29\n",
      "It: 610, Loss: 3.408e-02, Loss_u: 3.125e-02, Loss_r: 2.821e-03, Time: 3.18\n",
      "It: 620, Loss: 3.397e-02, Loss_u: 3.135e-02, Loss_r: 2.620e-03, Time: 3.55\n",
      "It: 630, Loss: 3.714e-02, Loss_u: 3.382e-02, Loss_r: 3.313e-03, Time: 3.26\n",
      "It: 640, Loss: 3.818e-02, Loss_u: 2.912e-02, Loss_r: 9.058e-03, Time: 3.16\n",
      "It: 650, Loss: 3.405e-02, Loss_u: 3.025e-02, Loss_r: 3.799e-03, Time: 3.22\n",
      "It: 660, Loss: 3.416e-02, Loss_u: 3.194e-02, Loss_r: 2.218e-03, Time: 3.11\n",
      "It: 670, Loss: 3.337e-02, Loss_u: 3.065e-02, Loss_r: 2.716e-03, Time: 3.31\n",
      "It: 680, Loss: 3.332e-02, Loss_u: 3.035e-02, Loss_r: 2.969e-03, Time: 3.03\n",
      "It: 690, Loss: 3.449e-02, Loss_u: 3.157e-02, Loss_r: 2.922e-03, Time: 3.13\n",
      "It: 700, Loss: 3.403e-02, Loss_u: 3.126e-02, Loss_r: 2.772e-03, Time: 3.10\n",
      "It: 710, Loss: 3.325e-02, Loss_u: 3.093e-02, Loss_r: 2.318e-03, Time: 3.13\n",
      "It: 720, Loss: 3.398e-02, Loss_u: 3.178e-02, Loss_r: 2.202e-03, Time: 3.13\n",
      "It: 730, Loss: 3.371e-02, Loss_u: 2.933e-02, Loss_r: 4.386e-03, Time: 3.10\n",
      "It: 740, Loss: 3.504e-02, Loss_u: 3.253e-02, Loss_r: 2.505e-03, Time: 3.11\n",
      "It: 750, Loss: 3.268e-02, Loss_u: 3.024e-02, Loss_r: 2.444e-03, Time: 3.16\n",
      "It: 760, Loss: 3.289e-02, Loss_u: 2.931e-02, Loss_r: 3.579e-03, Time: 3.18\n",
      "It: 770, Loss: 3.264e-02, Loss_u: 3.037e-02, Loss_r: 2.267e-03, Time: 3.17\n",
      "It: 780, Loss: 3.245e-02, Loss_u: 2.997e-02, Loss_r: 2.479e-03, Time: 3.19\n",
      "It: 790, Loss: 3.261e-02, Loss_u: 2.917e-02, Loss_r: 3.444e-03, Time: 3.06\n",
      "It: 800, Loss: 3.528e-02, Loss_u: 2.832e-02, Loss_r: 6.966e-03, Time: 3.25\n",
      "It: 810, Loss: 3.464e-02, Loss_u: 3.215e-02, Loss_r: 2.492e-03, Time: 3.20\n",
      "It: 820, Loss: 3.426e-02, Loss_u: 2.833e-02, Loss_r: 5.930e-03, Time: 3.43\n",
      "It: 830, Loss: 3.428e-02, Loss_u: 3.182e-02, Loss_r: 2.454e-03, Time: 3.24\n",
      "It: 840, Loss: 3.424e-02, Loss_u: 2.820e-02, Loss_r: 6.048e-03, Time: 3.14\n",
      "It: 850, Loss: 3.329e-02, Loss_u: 3.114e-02, Loss_r: 2.155e-03, Time: 3.19\n",
      "It: 860, Loss: 3.261e-02, Loss_u: 2.868e-02, Loss_r: 3.928e-03, Time: 3.25\n",
      "It: 870, Loss: 3.251e-02, Loss_u: 3.041e-02, Loss_r: 2.104e-03, Time: 3.11\n",
      "It: 880, Loss: 3.410e-02, Loss_u: 3.155e-02, Loss_r: 2.544e-03, Time: 3.17\n",
      "It: 890, Loss: 3.259e-02, Loss_u: 3.053e-02, Loss_r: 2.056e-03, Time: 3.18\n",
      "It: 900, Loss: 3.236e-02, Loss_u: 2.814e-02, Loss_r: 4.213e-03, Time: 3.14\n",
      "It: 910, Loss: 3.272e-02, Loss_u: 3.058e-02, Loss_r: 2.141e-03, Time: 3.17\n",
      "It: 920, Loss: 3.164e-02, Loss_u: 2.860e-02, Loss_r: 3.042e-03, Time: 3.25\n",
      "It: 930, Loss: 3.295e-02, Loss_u: 2.740e-02, Loss_r: 5.552e-03, Time: 3.31\n",
      "It: 940, Loss: 3.142e-02, Loss_u: 2.882e-02, Loss_r: 2.606e-03, Time: 3.27\n",
      "It: 950, Loss: 3.320e-02, Loss_u: 3.073e-02, Loss_r: 2.478e-03, Time: 3.16\n",
      "It: 960, Loss: 3.167e-02, Loss_u: 2.784e-02, Loss_r: 3.835e-03, Time: 3.33\n",
      "It: 970, Loss: 3.168e-02, Loss_u: 2.779e-02, Loss_r: 3.888e-03, Time: 3.33\n",
      "It: 980, Loss: 3.277e-02, Loss_u: 3.032e-02, Loss_r: 2.447e-03, Time: 3.24\n",
      "It: 990, Loss: 3.140e-02, Loss_u: 2.780e-02, Loss_r: 3.603e-03, Time: 3.18\n",
      "It: 1000, Loss: 3.222e-02, Loss_u: 2.803e-02, Loss_r: 4.184e-03, Time: 3.16\n",
      "l2 error: 3.73e-01\n",
      "l2 error: 9.09e-01\n",
      "l2 error: 4.70e-01\n",
      "elapsed: 3.66e+02\n",
      "Relative L2 error_u: 3.73e-01\n",
      "Relative L2 error_v: 9.09e-01\n",
      "Relative L2 error_p: 4.70e-01\n",
      "\n",
      "\n",
      "Method:  full_batch\n",
      "\n",
      "average of time_list: 369.83696842193604\n",
      "average of error_u_list: 0.3660640620518079\n",
      "average of error_v_list: 0.9301972259069198\n",
      "average of error_p_list: 0.2909712686071857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "nIter =1001\n",
    "bcbatch_size = 350\n",
    "ubatch_size = 5000\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters of equations\n",
    "Re = 100.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[0.0, 1.0], [1.0, 1.0]])\n",
    "bc2_coords = np.array([[0.0, 0.0], [0.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 0.0], [1.0, 1.0]])\n",
    "bc4_coords = np.array([[0.0, 0.0], [1.0, 0.0]])\n",
    "dom_coords = np.array([[0.0, 0.0], [1.0, 1.0]])\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'\n",
    "layers = [2, 50, 50, 50, 50 , 50 , 2]\n",
    "\n",
    "stiff_ratio = False  # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "\n",
    "iterations = 2\n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_v_list = []\n",
    "    error_p_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: U_gamma_1(x), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: U_gamma_2(x), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: U_gamma_2(x), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: U_gamma_2(x), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x), name='Forcing')\n",
    "\n",
    "        [elapsed, error_u , error_v ,  error_p] = test_method(mtd , layers, operator, bcs_sampler, res_sampler ,Re , mode ,  X_star , X , Y  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        print('elapsed: {:.2e}'.format(elapsed))\n",
    "        print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "        print('Relative L2 error_v: {:.2e}'.format(error_v))\n",
    "        print('Relative L2 error_p: {:.2e}'.format(error_p))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_v_list.append(error_v)\n",
    "        error_p_list.append(error_p)\n",
    "\n",
    "    print(\"\\n\\nMethod: \", mtd)\n",
    "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "    print(\"average of error_p_list:\" , sum(error_p_list) / len(error_p_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(\"./NS_model_dataset/NS_model_\"+mode+\"_result_mb\"+str(batch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Method: mini_batch \n",
      "\n",
      "average of time_list: 1200.0109666666667\n",
      "average of error_u_list: 0.09684999999999999\n",
      "average of error_v_list: 0.16866666666666666\n",
      "average of error_p_list: 0.15598\n",
      "\n",
      "\n",
      "Method: full_batch\n",
      "\n",
      "average of time_list: 6953.333333333333\n",
      "average of error_u_list: 0.049883333333333335\n",
      "average of error_v_list: 0.0862\n",
      "average of error_p_list: 0.1196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import scipy.io\n",
    "\n",
    "mode = 'M1'\n",
    "mbbatch_size = 128\n",
    "ubatch_size = 5000\n",
    "bcbatch_size = 500\n",
    "iterations = 40000\n",
    "\n",
    "time_list = []\n",
    "error_u_list = []\n",
    "error_v_list = []\n",
    "error_p_list = []\n",
    "    \n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "##Mini Batch\n",
    "time_list = [ 1.44e+03 ,  1.44e+03, 1.44e+03  ,  6.58e-02,  1.44e+03,   1.44e+03]\n",
    "error_u_list = [  8.46e-02,  8.94e-02,  7.31e-02, 1.52e-01, 1.09e-01, 7.30e-02 ]\n",
    "error_v_list = [ 1.82e-01,  1.60e-01, 1.60e-01, 1.05e-01, 2.40e-01, 1.65e-01 ]\n",
    "error_p_list = [  7.27e-02 , 3.24e-01 ,  9.40e-02,  1.82e-02,  2.71e-01 ]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "    \n",
    "print(\"\\n\\nMethod: mini_batch \")\n",
    "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "print(\"average of error_p_list:\" , sum(error_p_list) / len(error_p_list) )\n",
    "\n",
    "\n",
    "##Full Batch\n",
    "time_list = [  6.65e+03 ,  6.70e+03,  6.75e+03,  6.76e+03,  6.86e+03,  8.00e+03 ]\n",
    "error_u_list = [ 3.51e-02  ,  5.72e-02, 6.18e-02 ,   5.01e-02,   5.57e-02,  3.94e-02  ]\n",
    "error_v_list = [ 7.65e-02,  8.93e-02,   1.16e-01,  8.45e-02,   9.09e-02,  6.00e-02  ]\n",
    "error_p_list = [ 7.27e-02 , 3.35e-02,3.70e-01 ,  2.42e-02, 1.93e-01, 2.42e-02  ]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "print(\"\\n\\nMethod: full_batch\")\n",
    "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "print(\"average of error_p_list:\" , sum(error_p_list) / len(error_p_list) )\n",
    "\n",
    "scipy.io.savemat(\"./dataset/NS_model_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
