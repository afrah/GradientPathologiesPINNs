{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "# from NS_model_tf import Sampler, Navier_Stokes2D\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "import timeit\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "def U_gamma_1(x):\n",
    "    num = x.shape[0]\n",
    "    return np.tile(np.array([1.0, 0.0]), (num, 1))\n",
    "\n",
    "\n",
    "def U_gamma_2(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "def operator(psi, p, x, y, Re, sigma_x=1.0, sigma_y=1.0):\n",
    "    u = tf.gradients(psi, y)[0] #/ sigma_y\n",
    "    v = - tf.gradients(psi, x)[0] #/ sigma_x\n",
    "\n",
    "    u_x = tf.gradients(u, x)[0] #/ sigma_x\n",
    "    u_y = tf.gradients(u, y)[0] #/ sigma_y\n",
    "\n",
    "    v_x = tf.gradients(v, x)[0] #/ sigma_x\n",
    "    v_y = tf.gradients(v, y)[0] #/ sigma_y\n",
    "\n",
    "    p_x = tf.gradients(p, x)[0] #/ sigma_x\n",
    "    p_y = tf.gradients(p, y)[0] #/ sigma_y\n",
    "\n",
    "    u_xx = tf.gradients(u_x, x)[0] #/ sigma_x\n",
    "    u_yy = tf.gradients(u_y, y)[0] #/ sigma_y\n",
    "\n",
    "    v_xx = tf.gradients(v_x, x)[0] #/ sigma_x\n",
    "    v_yy = tf.gradients(v_y, y)[0] #/ sigma_y\n",
    "\n",
    "    Ru_momentum = u * u_x + v * u_y + p_x - (u_xx + u_yy) / Re\n",
    "    Rv_momentum = u * v_x + v * v_y + p_y - (v_xx + v_yy) / Re\n",
    "\n",
    "    return Ru_momentum, Rv_momentum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################################################################################\n",
    "class neural_net(object):\n",
    "    def __init__(self, mean , std, layers):\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.num_layers = len(self.layers)\n",
    "        \n",
    "        # if len(inputs) == 0:\n",
    "        #     in_dim = self.layers[0]\n",
    "        #     self.X_mean = np.zeros([1, in_dim])\n",
    "        #     self.X_std = np.ones([1, in_dim])\n",
    "        # else:\n",
    "        # X = np.concatenate(inputs, 1)\n",
    "        self.X_mean = mean # X.mean(0, keepdims=True)\n",
    "        self.X_std = std #X.std(0, keepdims=True)\n",
    "        # print('self.X_mean : ' , self.X_mean)    \n",
    "        # print('self.X_std : ' , self.X_std)    \n",
    "\n",
    "        # if ExistModel== 1:\n",
    "        # self.weights , self.biases , self.gammas = self.load_NN(modelDir)\n",
    "        # else:\n",
    "        self.weights = [] \n",
    "        self.biases = []\n",
    "        # self.gammas = []\n",
    "        \n",
    "        self.weights , self.biases  = self.initialize_NN() #, self.gammas\n",
    "\n",
    "        # for l in range(0,self.num_layers-1):\n",
    "        #     in_dim = self.layers[l]\n",
    "        #     out_dim = self.layers[l+1]\n",
    "        #     W = np.random.normal(size=[in_dim, out_dim])\n",
    "        #     b = np.zeros([1, out_dim])\n",
    "        #     g = np.ones([1, out_dim])\n",
    "        #     # tensorflow variables\n",
    "        #     if ExistModel== 0:\n",
    "        #         self.weights.append(tf.Variable(W, dtype=tf.float32, trainable=True))\n",
    "        #         self.biases.append(tf.Variable(b, dtype=tf.float32, trainable=True))\n",
    "        #         self.gammas.append(tf.Variable(g, dtype=tf.float32, trainable=True))\n",
    "            \n",
    "    def __call__(self, *inputs):\n",
    "                \n",
    "        H =  (tf.concat(inputs, 1) - self.X_mean)/self.X_std\n",
    "    \n",
    "        for l in range( self.num_layers-2):\n",
    "            # print(l)\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            # g = self.gammas[l]\n",
    "            V = W/tf.norm(W, axis = 0, keepdims=True)\n",
    "            H = tf.matmul(H, V)\n",
    "            H = H + b\n",
    "            # H = H*tf.sigmoid(H)\n",
    "            H = tf.tanh(H)\n",
    "\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "                    # return Y\n",
    "        # H = tf.concat(inputs, 1)\n",
    "        # num_layers = len(self.layers)\n",
    "        # for l in range(0, num_layers - 2):\n",
    "        #     W = self.weights[l]\n",
    "        #     b = self.biases[l]\n",
    "        #     H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        # W = self.weights[-1]\n",
    "        # b = self.biases[-1]\n",
    "        # H = tf.add(tf.matmul(H, W), b)\n",
    "        # return H\n",
    "\n",
    "    \n",
    "    def load_NN(self, modelDir):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        gammas = []\n",
    "\n",
    "        num_layers = len(self.layers)\n",
    "        with open(modelDir, 'rb') as f:\n",
    "            uv_weights, uv_biases , uv_gammas = pickle.load(f)\n",
    "\n",
    "            # Stored model must has the same # of layers\n",
    "            assert num_layers == (len(uv_weights)+1)\n",
    "\n",
    "            for num in range(0, num_layers - 1):\n",
    "                W = tf.Variable(uv_weights[num], dtype=tf.float32)\n",
    "                b = tf.Variable(uv_biases[num],  dtype=tf.float32)\n",
    "                # g = tf.Variable(uv_biases[num],  dtype=tf.float32)\n",
    "\n",
    "                weights.append(W)\n",
    "                biases.append(b)\n",
    "                # gammas.append(g)\n",
    "            print(\" - Load NN parameters successfully...\")\n",
    "        return weights, biases #, gammas\n",
    "    \n",
    "    def initialize_NN(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        gammas = []\n",
    "\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[self.layers[l], self.layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, self.layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            # g = tf.Variable(tf.ones([1, self.layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "            # gammas.append(g)\n",
    "        return weights, biases #, gammas\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2.0 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim],  stddev=xavier_stddev,  dtype=tf.float32),dtype=tf.float32)\n",
    "#######################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "class Navier_Stokes2D:\n",
    "    def __init__(self, layers, operator, bcs_sampler, res_sampler, Re, mode , sess):\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x, self.sigma_x = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_y, self.sigma_y = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "\n",
    "        # Navier Stokes constant\n",
    "        self.Re = tf.constant(Re, dtype=tf.float32)\n",
    "\n",
    "        # Adaptive re-weighting constant\n",
    "        self.rate = 0.9\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "        self.adaptive_constant_res_val = np.array(1.0)\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc1_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc2_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc3_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc4_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "\n",
    "        self.adaptive_constant_res_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_val.shape)\n",
    "\n",
    "        self.net_cuvwp = neural_net( self.mu_X, self.sigma_X ,   layers =self.layers)\n",
    "        self.u_pred, self.v_pred = self.net_uv(self.x_u_tf, self.y_u_tf)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred, self.v_bc1_pred = self.net_uv(self.x_bc1_tf, self.y_bc1_tf)\n",
    "        self.u_bc2_pred, self.v_bc2_pred = self.net_uv(self.x_bc2_tf, self.y_bc2_tf)\n",
    "        self.u_bc3_pred, self.v_bc3_pred = self.net_uv(self.x_bc3_tf, self.y_bc3_tf)\n",
    "        self.u_bc4_pred, self.v_bc4_pred = self.net_uv(self.x_bc4_tf, self.y_bc4_tf)\n",
    "\n",
    "        self.U_bc1_pred = tf.concat([self.u_bc1_pred, self.v_bc1_pred], axis=1)\n",
    "        self.U_bc2_pred = tf.concat([self.u_bc2_pred, self.v_bc2_pred], axis=1)\n",
    "        self.U_bc3_pred = tf.concat([self.u_bc3_pred, self.v_bc3_pred], axis=1)\n",
    "        self.U_bc4_pred = tf.concat([self.u_bc4_pred, self.v_bc4_pred], axis=1)\n",
    "\n",
    "        self.psi_pred, self.p_pred = self.net_psi_p(self.x_u_tf, self.y_u_tf)\n",
    "        self.u_momentum_pred, self.v_momentum_pred = self.net_r(self.x_r_tf, self.y_r_tf)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_u_momentum = tf.reduce_mean(tf.square(self.u_momentum_pred))\n",
    "        self.loss_v_momentum = tf.reduce_mean(tf.square(self.v_momentum_pred))\n",
    "\n",
    "        self.loss_res = self.loss_u_momentum + self.loss_v_momentum\n",
    "        \n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.U_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.U_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.U_bc4_pred))\n",
    "        \n",
    "        self.loss_bcs =  tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf) +tf.square(self.U_bc2_pred) + tf.square(self.U_bc3_pred) + tf.square(self.U_bc4_pred))\n",
    "        \n",
    "        # Total loss\n",
    "        # self.loss =  self.adaptive_constant_res_tf * self.loss_res + self.adaptive_constant_bcs_tf * self.loss_bcs\n",
    "        self.loss =  self.adaptive_constant_res_tf * self.loss_res + 50 * self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bcs,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_res_log = []\n",
    "        self.loss_bcs_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.net_cuvwp.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.net_cuvwp.weights[i])[0])\n",
    "\n",
    "  \n",
    "        self.adpative_constant_bcs_list = []\n",
    "        self.adpative_constant_res_list = []\n",
    "\n",
    "        self.adpative_constant_bcs_log = []\n",
    "        self.adpative_constant_res_log = []\n",
    "\n",
    "        self.mean_adaptive_constant_res_log = []\n",
    "        self.mean_adaptive_constant_bcs_log = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.adpative_constant_res_list.append( tf.reduce_mean(tf.abs(self.grad_res[i])))\n",
    "            self.adpative_constant_bcs_list.append(  tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "\n",
    "        self.adaptive_constant_res = tf.reduce_mean(tf.stack(self.adpative_constant_res_list))\n",
    "        self.adaptive_constant_bcs =  tf.reduce_mean(tf.stack(self.adpative_constant_bcs_list))\n",
    "\n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "################################################################################################################\n",
    "                \n",
    "\n",
    "    def lambda_balance(self  , term  ):\n",
    "        histoy_mean =  np.mean(self.loss_history[term])\n",
    "        m = 3 #len(self.loss_list)\n",
    "        num = np.exp(  np.mean(self.loss_history[term][-99::]) )#/(self.T * histoy_mean)) np.exp( )\n",
    "        denum = 0 \n",
    "\n",
    "        loss_list = [\"loss_res\" , \"loss_bcs\"] \n",
    "\n",
    "        for  key in loss_list:\n",
    "            denum +=  np.exp(   np.mean(self.loss_history[key][-99::]) )# /(self.T * histoy_mean))  np.exp(self.loss_history[key][-1] )\n",
    "        return m * (num / denum)\n",
    "    \n",
    " # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter, batch_size):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.U_bc1_tf: u_bc1_batch,\n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
    "                       self.adaptive_constant_res_tf: self.adaptive_constant_res_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "           # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "           # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                [loss ,  loss_res,  loss_bcs,  loss_bc1 , loss_bc2 , loss_bc3, loss_bc4]  = batch_losses\n",
    "\n",
    "\n",
    "        # self.loss_tensor_list = [self.loss ,  self.loss_res,  (self.loss_bc1 + self.loss_bc2) , ( self.loss_ic_u  + self.loss_ic_u_t ) ] \n",
    "        # self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs\", \"loss_ics\"] \n",
    "        \n",
    "\n",
    "                self.print('It: %d, Loss: %.3e, loss_bcs: %.3e, Loss_r: %.3e, Time: %.2f' %(it, loss, loss_bcs , loss_res, elapsed))\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                mean_grad_res, mean_grad_bcs = self.sess.run( [self.adaptive_constant_res, self.adaptive_constant_bcs  ], tf_dict)\n",
    "\n",
    "                if it == 0 :\n",
    "\n",
    "                    self.adaptive_constant_bcs_val =  1000 / mean_grad_bcs #(adaptive_constant_res_val/adaptive_constant_bcs_val) * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "                    self.adaptive_constant_res_val =  1000 / mean_grad_res #(adaptive_constant_res_val/adaptive_constant_bcs_val) * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                            \n",
    "                self.print('adaptive_constant_bcs1_val: {:.3e}'.format( self.adaptive_constant_bcs_val))\n",
    "                self.print('adaptive_constant_res_val: {:.3e}'.format( self.adaptive_constant_res_val))\n",
    "                \n",
    "                self.print('mean_grad_res: {:.3e}'.format( mean_grad_res))\n",
    "                self.print('mean_grad_bcs: {:.3e}'.format( mean_grad_bcs))\n",
    "\n",
    "                self.adpative_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "                self.adpative_constant_res_log.append(self.adaptive_constant_res_val)\n",
    "\n",
    "            \n",
    "                self.mean_adaptive_constant_res_log.append( mean_grad_res)\n",
    "                self.mean_adaptive_constant_bcs_log.append( mean_grad_bcs)\n",
    "\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "\n",
    "    # Forward pass for stream-pressure formulation\n",
    "    def net_psi_p(self, x, y):\n",
    "        psi_p = self.net_cuvwp(tf.concat([x, y], 1))\n",
    "        psi = psi_p[:, 0:1]\n",
    "        p = psi_p[:, 1:2]\n",
    "        return psi, p\n",
    "\n",
    "    # Forward pass for velocities\n",
    "    def net_uv(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u = tf.gradients(psi, y)[0] #/ self.sigma_y\n",
    "        v = - tf.gradients(psi, x)[0] #/ self.sigma_x\n",
    "        return u, v\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u_momentum_pred, v_momentum_pred = self.operator(psi, p, x, y,  self.Re,  self.sigma_x, self.sigma_y)\n",
    "\n",
    "        return u_momentum_pred, v_momentum_pred\n",
    "\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter ,  bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        batch_size = bcbatch_size\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "        X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "        X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        batch_size = ubatch_size\n",
    "\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.U_bc1_tf: u_bc1_batch,\n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val\n",
    "                    }\n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _, batch_losses = self.sess.run([self.train_op, self.loss_tensor_list] ,tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # Compute the adaptive constant\n",
    "                adaptive_constant_bcs_val = self.sess.run(self.adaptive_constant_bcs, tf_dict)\n",
    "\n",
    "                self.adaptive_constant_bcs_val = adaptive_constant_bcs_val *  (1.0 - self.beta) + self.beta * self.adaptive_constant_bcs_val\n",
    "\n",
    "                # self.adpative_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "                # self.loss_bcs_log.append(loss_u_value)\n",
    "                # self.loss_res_log.append(loss_r_value)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' % (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "\n",
    "                print(\"constant_bcs_val: {:.3f}\".format(self.adaptive_constant_bcs_val))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "  \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_psi_p(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        psi_star = self.sess.run(self.psi_pred, tf_dict)\n",
    "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
    "        return psi_star, p_star\n",
    "\n",
    "    def predict_uv(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
    "        return u_star, v_star\n",
    "\n",
    "\n",
    "\n",
    " ############################################################\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "    def plot_loss_history(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    ############################################################\n",
    "  \n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=3)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $v(x)$' , 'Predicted $v(x)$' , 'Absolute error']\n",
    "        data = [U_star.T , U_pred ,  np.abs(U_star.T - U_pred) , F_star.T , F_pred ,  np.abs(F_star.T - F_pred) ]\n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            # kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60), cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, minmax, title in zip(grid, data, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_bcs_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fontsize = 17\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([16,8])\n",
    "        ax.semilogy(self.mean_adaptive_constant_bcs_log, label=r'$\\bar{\\nabla_\\theta {u_{bc}}}$' , color = 'tab:green')\n",
    "        ax.semilogy(self.mean_adaptive_constant_res_log, label=r'$Max{\\nabla_\\theta {u_{phy}}}$' , color = 'tab:red')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
    "        ax.tick_params(labelsize=fontsize)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
    "\n",
    "        ax2 = ax.twinx() \n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_size_inches([15,8])\n",
    "    \n",
    "        ax2.semilogy(self.adpative_constant_bcs_log, label=r'$\\bar{\\lambda_{bc}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
    "        ax2.semilogy(self.adpative_constant_res_log, label=r'$\\bar{\\lambda_{res}}$'  ,  linestyle='dashed' , color = 'tab:red') \n",
    "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
    "        ax2.tick_params(labelsize=fontsize)\n",
    "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = os.path.join(self.dirname,'lambda_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        [elapsed, error_u , error_v ,  error_p] = test_method(mtd , layers, operator, bcs_sampler, res_sampler ,Re , stiff_ratio ,  X_star , X , Y)\n",
    "\n",
    "def test_method(method , layers, operator, bcs_sampler, res_sampler, Re ,  mode , X_star , X , Y , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "\n",
    "    model = Navier_Stokes2D(layers, operator, bcs_sampler, res_sampler, Re, mode)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    _, p_pred = model.predict_psi_p(X_star)\n",
    "    u_pred, v_pred = model.predict_uv(X_star)\n",
    "\n",
    "    # psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "    p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "    # u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "    # v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "    # velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # Reference\n",
    "    u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "    v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "    # velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "    u_pred = u_pred.reshape(100,100)\n",
    "    v_pred = v_pred.reshape(100,100)\n",
    "    p_pred = p_pred.reshape(100,100)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_u))\n",
    "    error_v = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_v))\n",
    "    error_p = np.linalg.norm(p_pred - p_star.T, 2) / np.linalg.norm(p_star, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_p))\n",
    "\n",
    "    return [elapsed, error_u , error_v ,error_p ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_69776/939198120.py:59: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_69776/939198120.py:60: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_69776/939198120.py:61: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_69776/939198120.py:61: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_69776/2207500011.py:259: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_69776/2207500011.py:52: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 12:10:47.715182: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-16 12:10:47.736043: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2024-01-16 12:10:47.736443: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5561b1284940 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-16 12:10:47.736457: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-01-16 12:10:47.737206: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_69776/2207500011.py:113: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_69776/2207500011.py:115: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_69776/2207500011.py:159: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 2.840e+00, loss_bcs: 6.324e-01, Loss_r: 2.208e+00, Time: 16.16\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.043e-01\n",
      "mean_grad_bcs: 2.698e-02\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, Loss: 2.976e+03, loss_bcs: 7.709e-02, Loss_r: 2.438e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.284e-02\n",
      "mean_grad_bcs: 8.426e-03\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, Loss: 2.004e+03, loss_bcs: 5.089e-02, Loss_r: 2.405e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.334e-02\n",
      "mean_grad_bcs: 1.002e-02\n",
      "It: 300, Loss: 1.369e+03, loss_bcs: 3.511e-02, Loss_r: 1.384e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.503e-03\n",
      "mean_grad_bcs: 2.605e-02\n",
      "It: 400, Loss: 1.227e+03, loss_bcs: 3.090e-02, Loss_r: 1.682e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.154e-02\n",
      "mean_grad_bcs: 7.622e-03\n",
      "It: 500, Loss: 1.120e+03, loss_bcs: 2.844e-02, Loss_r: 1.352e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.759e-02\n",
      "mean_grad_bcs: 1.463e-02\n",
      "It: 600, Loss: 1.045e+03, loss_bcs: 2.411e-02, Loss_r: 3.100e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.037e-02\n",
      "mean_grad_bcs: 1.220e-02\n",
      "It: 700, Loss: 1.169e+03, loss_bcs: 2.760e-02, Loss_r: 2.981e-02, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.053e-02\n",
      "mean_grad_bcs: 1.821e-02\n",
      "It: 800, Loss: 8.294e+02, loss_bcs: 1.946e-02, Loss_r: 2.209e-02, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.087e-02\n",
      "mean_grad_bcs: 2.110e-02\n",
      "It: 900, Loss: 1.187e+03, loss_bcs: 2.868e-02, Loss_r: 2.531e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.992e-02\n",
      "mean_grad_bcs: 1.322e-02\n",
      "It: 1000, Loss: 6.534e+02, loss_bcs: 1.492e-02, Loss_r: 2.055e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.007e-02\n",
      "mean_grad_bcs: 1.039e-02\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, Loss: 8.586e+02, loss_bcs: 2.074e-02, Loss_r: 1.836e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.319e-02\n",
      "mean_grad_bcs: 2.374e-02\n",
      "It: 1200, Loss: 9.960e+02, loss_bcs: 2.389e-02, Loss_r: 2.260e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.891e-02\n",
      "mean_grad_bcs: 8.399e-03\n",
      "It: 1300, Loss: 9.812e+02, loss_bcs: 2.393e-02, Loss_r: 1.928e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.954e-02\n",
      "mean_grad_bcs: 1.260e-02\n",
      "It: 1400, Loss: 5.873e+02, loss_bcs: 1.335e-02, Loss_r: 1.892e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.187e-02\n",
      "mean_grad_bcs: 8.827e-03\n",
      "It: 1500, Loss: 1.025e+03, loss_bcs: 2.537e-02, Loss_r: 1.721e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.944e-02\n",
      "mean_grad_bcs: 1.586e-02\n",
      "It: 1600, Loss: 8.867e+02, loss_bcs: 1.791e-02, Loss_r: 4.553e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.840e-02\n",
      "mean_grad_bcs: 4.733e-03\n",
      "It: 1700, Loss: 8.019e+02, loss_bcs: 1.884e-02, Loss_r: 2.116e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.713e-02\n",
      "mean_grad_bcs: 6.606e-03\n",
      "It: 1800, Loss: 9.023e+02, loss_bcs: 2.251e-02, Loss_r: 1.391e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.647e-02\n",
      "mean_grad_bcs: 2.490e-02\n",
      "It: 1900, Loss: 5.686e+02, loss_bcs: 1.358e-02, Loss_r: 1.336e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.144e-02\n",
      "mean_grad_bcs: 1.657e-02\n",
      "It: 2000, Loss: 5.913e+02, loss_bcs: 1.298e-02, Loss_r: 2.257e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.218e-02\n",
      "mean_grad_bcs: 3.070e-02\n",
      "It: 2100, Loss: 8.746e+02, loss_bcs: 1.929e-02, Loss_r: 3.265e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.597e-02\n",
      "mean_grad_bcs: 1.291e-02\n",
      "It: 2200, Loss: 6.815e+02, loss_bcs: 1.567e-02, Loss_r: 2.063e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.880e-02\n",
      "mean_grad_bcs: 2.861e-02\n",
      "It: 2300, Loss: 8.902e+02, loss_bcs: 1.994e-02, Loss_r: 3.091e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.583e-02\n",
      "mean_grad_bcs: 1.660e-02\n",
      "It: 2400, Loss: 7.668e+02, loss_bcs: 1.783e-02, Loss_r: 2.168e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.692e-02\n",
      "mean_grad_bcs: 4.301e-03\n",
      "It: 2500, Loss: 7.152e+02, loss_bcs: 1.650e-02, Loss_r: 2.122e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.705e-02\n",
      "mean_grad_bcs: 1.152e-02\n",
      "It: 2600, Loss: 8.367e+02, loss_bcs: 1.715e-02, Loss_r: 4.106e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.477e-02\n",
      "mean_grad_bcs: 1.002e-02\n",
      "It: 2700, Loss: 6.873e+02, loss_bcs: 1.457e-02, Loss_r: 3.011e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.764e-02\n",
      "mean_grad_bcs: 1.475e-02\n",
      "It: 2800, Loss: 8.666e+02, loss_bcs: 2.132e-02, Loss_r: 1.564e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.052e-02\n",
      "mean_grad_bcs: 1.892e-02\n",
      "It: 2900, Loss: 7.772e+02, loss_bcs: 1.747e-02, Loss_r: 2.649e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.003e-02\n",
      "mean_grad_bcs: 2.471e-02\n",
      "It: 3000, Loss: 7.324e+02, loss_bcs: 1.530e-02, Loss_r: 3.380e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.963e-02\n",
      "mean_grad_bcs: 1.317e-02\n",
      "It: 3100, Loss: 6.673e+02, loss_bcs: 1.490e-02, Loss_r: 2.352e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.470e-02\n",
      "mean_grad_bcs: 4.869e-03\n",
      "It: 3200, Loss: 7.313e+02, loss_bcs: 1.835e-02, Loss_r: 1.051e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.499e-03\n",
      "mean_grad_bcs: 1.845e-02\n",
      "It: 3300, Loss: 5.725e+02, loss_bcs: 1.258e-02, Loss_r: 2.172e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.552e-02\n",
      "mean_grad_bcs: 4.158e-03\n",
      "It: 3400, Loss: 7.876e+02, loss_bcs: 1.745e-02, Loss_r: 2.882e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.976e-02\n",
      "mean_grad_bcs: 3.476e-03\n",
      "It: 3500, Loss: 6.003e+02, loss_bcs: 1.318e-02, Loss_r: 2.284e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.171e-02\n",
      "mean_grad_bcs: 8.849e-03\n",
      "It: 3600, Loss: 5.984e+02, loss_bcs: 1.471e-02, Loss_r: 1.087e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.866e-02\n",
      "mean_grad_bcs: 1.299e-02\n",
      "It: 3700, Loss: 6.406e+02, loss_bcs: 1.514e-02, Loss_r: 1.626e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.026e-02\n",
      "mean_grad_bcs: 1.645e-02\n",
      "It: 3800, Loss: 6.091e+02, loss_bcs: 1.416e-02, Loss_r: 1.723e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.522e-02\n",
      "mean_grad_bcs: 1.308e-02\n",
      "It: 3900, Loss: 5.731e+02, loss_bcs: 1.268e-02, Loss_r: 2.107e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.901e-02\n",
      "mean_grad_bcs: 1.114e-02\n",
      "It: 4000, Loss: 5.838e+02, loss_bcs: 1.281e-02, Loss_r: 2.232e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.929e-02\n",
      "mean_grad_bcs: 1.642e-02\n",
      "It: 4100, Loss: 1.006e+03, loss_bcs: 2.315e-02, Loss_r: 3.029e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.868e-02\n",
      "mean_grad_bcs: 1.572e-02\n",
      "It: 4200, Loss: 8.150e+02, loss_bcs: 1.700e-02, Loss_r: 3.779e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.527e-02\n",
      "mean_grad_bcs: 3.259e-02\n",
      "It: 4300, Loss: 5.610e+02, loss_bcs: 1.237e-02, Loss_r: 2.097e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.790e-02\n",
      "mean_grad_bcs: 7.546e-03\n",
      "It: 4400, Loss: 6.734e+02, loss_bcs: 1.416e-02, Loss_r: 3.035e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.501e-02\n",
      "mean_grad_bcs: 8.185e-03\n",
      "It: 4500, Loss: 6.639e+02, loss_bcs: 1.645e-02, Loss_r: 1.106e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.140e-02\n",
      "mean_grad_bcs: 2.687e-02\n",
      "It: 4600, Loss: 1.116e+03, loss_bcs: 2.733e-02, Loss_r: 2.102e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.287e-02\n",
      "mean_grad_bcs: 1.951e-02\n",
      "It: 4700, Loss: 5.048e+02, loss_bcs: 1.216e-02, Loss_r: 1.111e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.425e-02\n",
      "mean_grad_bcs: 2.293e-02\n",
      "It: 4800, Loss: 5.923e+02, loss_bcs: 1.201e-02, Loss_r: 3.005e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.237e-02\n",
      "mean_grad_bcs: 7.596e-03\n",
      "It: 4900, Loss: 7.775e+02, loss_bcs: 1.785e-02, Loss_r: 2.371e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.098e-02\n",
      "mean_grad_bcs: 2.388e-02\n",
      "It: 5000, Loss: 6.325e+02, loss_bcs: 1.378e-02, Loss_r: 2.490e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.208e-02\n",
      "mean_grad_bcs: 1.742e-02\n",
      "It: 5100, Loss: 8.695e+02, loss_bcs: 1.875e-02, Loss_r: 3.571e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.142e-02\n",
      "mean_grad_bcs: 1.445e-02\n",
      "It: 5200, Loss: 5.910e+02, loss_bcs: 1.455e-02, Loss_r: 1.058e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.827e-02\n",
      "mean_grad_bcs: 1.351e-02\n",
      "It: 5300, Loss: 5.972e+02, loss_bcs: 1.422e-02, Loss_r: 1.432e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.110e-02\n",
      "mean_grad_bcs: 2.818e-02\n",
      "It: 5400, Loss: 3.994e+02, loss_bcs: 9.033e-03, Loss_r: 1.319e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.874e-02\n",
      "mean_grad_bcs: 8.085e-03\n",
      "It: 5500, Loss: 6.472e+02, loss_bcs: 1.601e-02, Loss_r: 1.105e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.052e-02\n",
      "mean_grad_bcs: 1.110e-02\n",
      "It: 5600, Loss: 6.277e+02, loss_bcs: 1.342e-02, Loss_r: 2.660e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.877e-02\n",
      "mean_grad_bcs: 2.950e-02\n",
      "It: 5700, Loss: 4.235e+02, loss_bcs: 1.043e-02, Loss_r: 7.576e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.442e-02\n",
      "mean_grad_bcs: 7.922e-03\n",
      "It: 5800, Loss: 4.700e+02, loss_bcs: 9.802e-03, Loss_r: 2.182e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.471e-02\n",
      "mean_grad_bcs: 1.103e-02\n",
      "It: 5900, Loss: 1.003e+03, loss_bcs: 2.309e-02, Loss_r: 2.999e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.937e-02\n",
      "mean_grad_bcs: 3.330e-02\n",
      "It: 6000, Loss: 5.586e+02, loss_bcs: 1.204e-02, Loss_r: 2.300e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.466e-02\n",
      "mean_grad_bcs: 1.892e-02\n",
      "It: 6100, Loss: 6.960e+02, loss_bcs: 1.772e-02, Loss_r: 8.054e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.481e-02\n",
      "mean_grad_bcs: 2.206e-02\n",
      "It: 6200, Loss: 4.325e+02, loss_bcs: 9.734e-03, Loss_r: 1.467e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.097e-02\n",
      "mean_grad_bcs: 2.971e-02\n",
      "It: 6300, Loss: 5.461e+02, loss_bcs: 1.322e-02, Loss_r: 1.148e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.840e-02\n",
      "mean_grad_bcs: 1.051e-02\n",
      "It: 6400, Loss: 5.774e+02, loss_bcs: 1.197e-02, Loss_r: 2.735e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.808e-02\n",
      "mean_grad_bcs: 6.696e-03\n",
      "It: 6500, Loss: 5.790e+02, loss_bcs: 1.070e-02, Loss_r: 3.731e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.105e-02\n",
      "mean_grad_bcs: 9.919e-03\n",
      "It: 6600, Loss: 4.478e+02, loss_bcs: 9.161e-03, Loss_r: 2.213e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.774e-02\n",
      "mean_grad_bcs: 1.338e-02\n",
      "It: 6700, Loss: 4.925e+02, loss_bcs: 1.242e-02, Loss_r: 6.565e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.467e-03\n",
      "mean_grad_bcs: 1.989e-02\n",
      "It: 6800, Loss: 7.625e+02, loss_bcs: 1.905e-02, Loss_r: 1.155e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.467e-02\n",
      "mean_grad_bcs: 4.267e-03\n",
      "It: 6900, Loss: 4.237e+02, loss_bcs: 9.144e-03, Loss_r: 1.733e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.064e-02\n",
      "mean_grad_bcs: 1.127e-02\n",
      "It: 7000, Loss: 8.577e+02, loss_bcs: 1.754e-02, Loss_r: 4.248e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.325e-02\n",
      "mean_grad_bcs: 8.957e-03\n",
      "It: 7100, Loss: 4.602e+02, loss_bcs: 9.660e-03, Loss_r: 2.088e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.424e-02\n",
      "mean_grad_bcs: 1.063e-02\n",
      "It: 7200, Loss: 4.523e+02, loss_bcs: 1.084e-02, Loss_r: 1.037e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.038e-02\n",
      "mean_grad_bcs: 5.539e-03\n",
      "It: 7300, Loss: 4.811e+02, loss_bcs: 1.157e-02, Loss_r: 1.069e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.136e-02\n",
      "mean_grad_bcs: 9.714e-03\n",
      "It: 7400, Loss: 4.306e+02, loss_bcs: 1.032e-02, Loss_r: 9.838e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.287e-02\n",
      "mean_grad_bcs: 1.771e-02\n",
      "It: 7500, Loss: 4.871e+02, loss_bcs: 1.155e-02, Loss_r: 1.203e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.800e-02\n",
      "mean_grad_bcs: 2.231e-02\n",
      "It: 7600, Loss: 8.154e+02, loss_bcs: 1.568e-02, Loss_r: 4.792e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.085e-01\n",
      "mean_grad_bcs: 1.739e-02\n",
      "It: 7700, Loss: 3.718e+02, loss_bcs: 8.564e-03, Loss_r: 1.111e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.370e-02\n",
      "mean_grad_bcs: 1.190e-02\n",
      "It: 7800, Loss: 3.075e+02, loss_bcs: 6.593e-03, Loss_r: 1.290e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.454e-02\n",
      "mean_grad_bcs: 1.111e-02\n",
      "It: 7900, Loss: 4.063e+02, loss_bcs: 8.817e-03, Loss_r: 1.626e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.793e-02\n",
      "mean_grad_bcs: 1.047e-02\n",
      "It: 8000, Loss: 5.070e+02, loss_bcs: 1.239e-02, Loss_r: 9.803e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.224e-02\n",
      "mean_grad_bcs: 1.901e-02\n",
      "It: 8100, Loss: 6.651e+02, loss_bcs: 1.217e-02, Loss_r: 4.370e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.858e-02\n",
      "mean_grad_bcs: 1.857e-02\n",
      "It: 8200, Loss: 4.061e+02, loss_bcs: 9.339e-03, Loss_r: 1.226e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.792e-02\n",
      "mean_grad_bcs: 1.614e-02\n",
      "It: 8300, Loss: 3.764e+02, loss_bcs: 8.723e-03, Loss_r: 1.086e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.317e-02\n",
      "mean_grad_bcs: 2.690e-03\n",
      "It: 8400, Loss: 3.317e+02, loss_bcs: 7.571e-03, Loss_r: 1.045e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.342e-02\n",
      "mean_grad_bcs: 7.463e-03\n",
      "It: 8500, Loss: 2.259e+02, loss_bcs: 5.169e-03, Loss_r: 7.015e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.308e-02\n",
      "mean_grad_bcs: 1.052e-02\n",
      "It: 8600, Loss: 6.084e+02, loss_bcs: 1.444e-02, Loss_r: 1.494e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.001e-02\n",
      "mean_grad_bcs: 5.618e-03\n",
      "It: 8700, Loss: 4.257e+02, loss_bcs: 9.982e-03, Loss_r: 1.139e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.703e-02\n",
      "mean_grad_bcs: 5.721e-03\n",
      "It: 8800, Loss: 3.635e+02, loss_bcs: 7.776e-03, Loss_r: 1.539e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.628e-02\n",
      "mean_grad_bcs: 1.131e-02\n",
      "It: 8900, Loss: 5.005e+02, loss_bcs: 9.044e-03, Loss_r: 3.378e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.867e-02\n",
      "mean_grad_bcs: 1.989e-02\n",
      "It: 9000, Loss: 4.447e+02, loss_bcs: 1.080e-02, Loss_r: 9.106e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.085e-02\n",
      "mean_grad_bcs: 1.943e-02\n",
      "It: 9100, Loss: 4.997e+02, loss_bcs: 9.713e-03, Loss_r: 2.855e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.469e-02\n",
      "mean_grad_bcs: 7.800e-03\n",
      "It: 9200, Loss: 4.461e+02, loss_bcs: 1.030e-02, Loss_r: 1.314e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.625e-02\n",
      "mean_grad_bcs: 1.173e-02\n",
      "It: 9300, Loss: 4.547e+02, loss_bcs: 7.810e-03, Loss_r: 3.378e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.409e-02\n",
      "mean_grad_bcs: 9.556e-03\n",
      "It: 9400, Loss: 2.797e+02, loss_bcs: 5.783e-03, Loss_r: 1.337e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.546e-02\n",
      "mean_grad_bcs: 1.276e-02\n",
      "It: 9500, Loss: 6.653e+02, loss_bcs: 1.213e-02, Loss_r: 4.406e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.991e-02\n",
      "mean_grad_bcs: 1.784e-02\n",
      "It: 9600, Loss: 5.735e+02, loss_bcs: 1.303e-02, Loss_r: 1.851e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.554e-02\n",
      "mean_grad_bcs: 7.598e-03\n",
      "It: 9700, Loss: 5.227e+02, loss_bcs: 1.247e-02, Loss_r: 1.237e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.222e-02\n",
      "mean_grad_bcs: 1.725e-02\n",
      "It: 9800, Loss: 6.718e+02, loss_bcs: 1.438e-02, Loss_r: 2.834e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.863e-02\n",
      "mean_grad_bcs: 1.341e-02\n",
      "It: 9900, Loss: 3.737e+02, loss_bcs: 8.401e-03, Loss_r: 1.275e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.616e-02\n",
      "mean_grad_bcs: 8.223e-03\n",
      "It: 10000, Loss: 4.641e+02, loss_bcs: 1.008e-02, Loss_r: 1.852e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.104e-02\n",
      "mean_grad_bcs: 1.616e-02\n",
      "It: 10100, Loss: 3.917e+02, loss_bcs: 9.105e-03, Loss_r: 1.110e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.565e-02\n",
      "mean_grad_bcs: 8.706e-03\n",
      "It: 10200, Loss: 2.985e+02, loss_bcs: 3.558e-03, Loss_r: 3.405e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.636e-02\n",
      "mean_grad_bcs: 1.206e-02\n",
      "It: 10300, Loss: 3.852e+02, loss_bcs: 7.537e-03, Loss_r: 2.163e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.200e-02\n",
      "mean_grad_bcs: 1.056e-02\n",
      "It: 10400, Loss: 5.811e+02, loss_bcs: 1.493e-02, Loss_r: 5.698e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.197e-02\n",
      "mean_grad_bcs: 7.937e-03\n",
      "It: 10500, Loss: 6.096e+02, loss_bcs: 1.542e-02, Loss_r: 7.792e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.383e-02\n",
      "mean_grad_bcs: 8.202e-03\n",
      "It: 10600, Loss: 2.998e+02, loss_bcs: 7.297e-03, Loss_r: 6.009e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.499e-02\n",
      "mean_grad_bcs: 4.935e-03\n",
      "It: 10700, Loss: 3.271e+02, loss_bcs: 7.586e-03, Loss_r: 9.402e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.325e-02\n",
      "mean_grad_bcs: 2.075e-02\n",
      "It: 10800, Loss: 6.340e+02, loss_bcs: 1.629e-02, Loss_r: 6.218e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.074e-02\n",
      "mean_grad_bcs: 1.614e-02\n",
      "It: 10900, Loss: 3.316e+02, loss_bcs: 8.019e-03, Loss_r: 7.029e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.480e-02\n",
      "mean_grad_bcs: 7.979e-03\n",
      "It: 11000, Loss: 3.231e+02, loss_bcs: 6.383e-03, Loss_r: 1.769e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.752e-02\n",
      "mean_grad_bcs: 1.223e-02\n",
      "It: 11100, Loss: 2.702e+02, loss_bcs: 5.964e-03, Loss_r: 1.006e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.307e-02\n",
      "mean_grad_bcs: 5.843e-03\n",
      "It: 11200, Loss: 2.859e+02, loss_bcs: 6.612e-03, Loss_r: 8.351e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.547e-02\n",
      "mean_grad_bcs: 6.300e-03\n",
      "It: 11300, Loss: 3.218e+02, loss_bcs: 6.953e-03, Loss_r: 1.310e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.942e-02\n",
      "mean_grad_bcs: 9.924e-03\n",
      "It: 11400, Loss: 3.235e+02, loss_bcs: 6.130e-03, Loss_r: 1.968e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.998e-02\n",
      "mean_grad_bcs: 2.245e-03\n",
      "It: 11500, Loss: 4.155e+02, loss_bcs: 1.021e-02, Loss_r: 7.542e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.163e-02\n",
      "mean_grad_bcs: 1.674e-02\n",
      "It: 11600, Loss: 3.488e+02, loss_bcs: 8.774e-03, Loss_r: 4.820e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.862e-03\n",
      "mean_grad_bcs: 6.756e-03\n",
      "It: 11700, Loss: 3.142e+02, loss_bcs: 5.692e-03, Loss_r: 2.109e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.416e-02\n",
      "mean_grad_bcs: 6.257e-03\n",
      "It: 11800, Loss: 3.842e+02, loss_bcs: 9.685e-03, Loss_r: 5.165e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.706e-03\n",
      "mean_grad_bcs: 2.004e-02\n",
      "It: 11900, Loss: 5.583e+02, loss_bcs: 1.211e-02, Loss_r: 2.237e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.627e-02\n",
      "mean_grad_bcs: 8.587e-03\n",
      "It: 12000, Loss: 2.818e+02, loss_bcs: 6.880e-03, Loss_r: 5.491e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.878e-03\n",
      "mean_grad_bcs: 1.320e-02\n",
      "It: 12100, Loss: 6.940e+02, loss_bcs: 1.789e-02, Loss_r: 6.354e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.315e-03\n",
      "mean_grad_bcs: 2.741e-02\n",
      "It: 12200, Loss: 4.372e+02, loss_bcs: 8.141e-03, Loss_r: 2.769e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.614e-02\n",
      "mean_grad_bcs: 1.816e-02\n",
      "It: 12300, Loss: 3.451e+02, loss_bcs: 8.172e-03, Loss_r: 8.646e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.703e-02\n",
      "mean_grad_bcs: 1.130e-02\n",
      "It: 12400, Loss: 2.640e+02, loss_bcs: 5.596e-03, Loss_r: 1.156e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.856e-02\n",
      "mean_grad_bcs: 6.887e-03\n",
      "It: 12500, Loss: 3.078e+02, loss_bcs: 7.219e-03, Loss_r: 8.218e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.619e-02\n",
      "mean_grad_bcs: 2.558e-03\n",
      "It: 12600, Loss: 5.871e+02, loss_bcs: 1.461e-02, Loss_r: 9.328e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.338e-02\n",
      "mean_grad_bcs: 3.131e-02\n",
      "It: 12700, Loss: 4.953e+02, loss_bcs: 1.036e-02, Loss_r: 2.273e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.989e-02\n",
      "mean_grad_bcs: 1.482e-02\n",
      "It: 12800, Loss: 2.837e+02, loss_bcs: 6.071e-03, Loss_r: 1.200e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.114e-02\n",
      "mean_grad_bcs: 7.608e-03\n",
      "It: 12900, Loss: 3.576e+02, loss_bcs: 5.379e-03, Loss_r: 3.232e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.275e-02\n",
      "mean_grad_bcs: 1.661e-02\n",
      "It: 13000, Loss: 2.051e+02, loss_bcs: 3.487e-03, Loss_r: 1.549e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.661e-02\n",
      "mean_grad_bcs: 8.079e-03\n",
      "It: 13100, Loss: 3.161e+02, loss_bcs: 5.239e-03, Loss_r: 2.491e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.904e-02\n",
      "mean_grad_bcs: 1.339e-02\n",
      "It: 13200, Loss: 4.485e+02, loss_bcs: 1.049e-02, Loss_r: 1.224e-02, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.520e-02\n",
      "mean_grad_bcs: 2.271e-02\n",
      "It: 13300, Loss: 5.178e+02, loss_bcs: 1.332e-02, Loss_r: 4.935e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.331e-02\n",
      "mean_grad_bcs: 1.113e-02\n",
      "It: 13400, Loss: 2.991e+02, loss_bcs: 6.904e-03, Loss_r: 8.836e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.804e-02\n",
      "mean_grad_bcs: 4.497e-03\n",
      "It: 13500, Loss: 2.597e+02, loss_bcs: 5.687e-03, Loss_r: 1.001e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.040e-02\n",
      "mean_grad_bcs: 2.201e-02\n",
      "It: 13600, Loss: 3.300e+02, loss_bcs: 7.733e-03, Loss_r: 8.879e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.529e-02\n",
      "mean_grad_bcs: 3.092e-03\n",
      "It: 13700, Loss: 3.904e+02, loss_bcs: 6.944e-03, Loss_r: 2.719e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.711e-02\n",
      "mean_grad_bcs: 1.265e-02\n",
      "It: 13800, Loss: 3.972e+02, loss_bcs: 9.073e-03, Loss_r: 1.247e-02, Time: 0.11\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.516e-02\n",
      "mean_grad_bcs: 1.118e-02\n",
      "It: 13900, Loss: 2.487e+02, loss_bcs: 4.698e-03, Loss_r: 1.523e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.794e-02\n",
      "mean_grad_bcs: 4.004e-03\n",
      "It: 14000, Loss: 4.264e+02, loss_bcs: 1.043e-02, Loss_r: 8.170e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.571e-02\n",
      "mean_grad_bcs: 7.715e-03\n",
      "It: 14100, Loss: 2.322e+02, loss_bcs: 5.673e-03, Loss_r: 4.484e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.108e-02\n",
      "mean_grad_bcs: 3.526e-03\n",
      "It: 14200, Loss: 2.783e+02, loss_bcs: 6.696e-03, Loss_r: 6.154e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.682e-02\n",
      "mean_grad_bcs: 8.839e-03\n",
      "It: 14300, Loss: 2.204e+02, loss_bcs: 4.903e-03, Loss_r: 7.906e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.898e-02\n",
      "mean_grad_bcs: 7.526e-03\n",
      "It: 14400, Loss: 3.798e+02, loss_bcs: 8.227e-03, Loss_r: 1.530e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.435e-02\n",
      "mean_grad_bcs: 1.758e-02\n",
      "It: 14500, Loss: 6.850e+02, loss_bcs: 1.262e-02, Loss_r: 4.438e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.164e-01\n",
      "mean_grad_bcs: 1.363e-02\n",
      "It: 14600, Loss: 3.938e+02, loss_bcs: 8.639e-03, Loss_r: 1.504e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.584e-02\n",
      "mean_grad_bcs: 5.995e-03\n",
      "It: 14700, Loss: 3.453e+02, loss_bcs: 8.703e-03, Loss_r: 4.656e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.465e-03\n",
      "mean_grad_bcs: 4.131e-03\n",
      "It: 14800, Loss: 3.154e+02, loss_bcs: 6.839e-03, Loss_r: 1.266e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.782e-02\n",
      "mean_grad_bcs: 6.734e-03\n",
      "It: 14900, Loss: 2.633e+02, loss_bcs: 6.512e-03, Loss_r: 4.496e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.254e-03\n",
      "mean_grad_bcs: 3.068e-03\n",
      "It: 15000, Loss: 3.968e+02, loss_bcs: 9.451e-03, Loss_r: 9.521e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.020e-02\n",
      "mean_grad_bcs: 2.435e-02\n",
      "It: 15100, Loss: 3.492e+02, loss_bcs: 8.118e-03, Loss_r: 9.889e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.806e-02\n",
      "mean_grad_bcs: 8.317e-03\n",
      "It: 15200, Loss: 5.112e+02, loss_bcs: 1.284e-02, Loss_r: 7.187e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.243e-02\n",
      "mean_grad_bcs: 1.880e-02\n",
      "It: 15300, Loss: 2.414e+02, loss_bcs: 5.159e-03, Loss_r: 1.026e-02, Time: 0.11\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.815e-02\n",
      "mean_grad_bcs: 5.538e-03\n",
      "It: 15400, Loss: 3.786e+02, loss_bcs: 8.570e-03, Loss_r: 1.245e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.792e-02\n",
      "mean_grad_bcs: 1.436e-02\n",
      "It: 15500, Loss: 3.639e+02, loss_bcs: 9.295e-03, Loss_r: 3.972e-03, Time: 0.04\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.234e-03\n",
      "mean_grad_bcs: 6.242e-03\n",
      "It: 15600, Loss: 3.842e+02, loss_bcs: 9.159e-03, Loss_r: 9.141e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.449e-02\n",
      "mean_grad_bcs: 1.754e-02\n",
      "It: 15700, Loss: 3.563e+02, loss_bcs: 8.616e-03, Loss_r: 7.570e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.373e-02\n",
      "mean_grad_bcs: 7.826e-03\n",
      "It: 15800, Loss: 2.887e+02, loss_bcs: 7.177e-03, Loss_r: 4.640e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.124e-02\n",
      "mean_grad_bcs: 4.491e-03\n",
      "It: 15900, Loss: 4.008e+02, loss_bcs: 5.560e-03, Loss_r: 3.980e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.740e-02\n",
      "mean_grad_bcs: 5.137e-03\n",
      "It: 16000, Loss: 5.125e+02, loss_bcs: 1.317e-02, Loss_r: 5.005e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.017e-02\n",
      "mean_grad_bcs: 1.103e-02\n",
      "It: 16100, Loss: 5.599e+02, loss_bcs: 1.120e-02, Loss_r: 2.961e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.473e-02\n",
      "mean_grad_bcs: 1.268e-02\n",
      "It: 16200, Loss: 2.888e+02, loss_bcs: 6.916e-03, Loss_r: 6.640e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.842e-02\n",
      "mean_grad_bcs: 1.843e-02\n",
      "It: 16300, Loss: 3.523e+02, loss_bcs: 8.742e-03, Loss_r: 5.781e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.349e-02\n",
      "mean_grad_bcs: 1.646e-02\n",
      "It: 16400, Loss: 3.397e+02, loss_bcs: 7.584e-03, Loss_r: 1.199e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.983e-02\n",
      "mean_grad_bcs: 1.146e-02\n",
      "It: 16500, Loss: 2.323e+02, loss_bcs: 5.318e-03, Loss_r: 7.190e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.061e-03\n",
      "mean_grad_bcs: 4.915e-03\n",
      "It: 16600, Loss: 2.785e+02, loss_bcs: 6.514e-03, Loss_r: 7.568e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.721e-02\n",
      "mean_grad_bcs: 8.725e-03\n",
      "It: 16700, Loss: 3.997e+02, loss_bcs: 8.519e-03, Loss_r: 1.716e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.313e-02\n",
      "mean_grad_bcs: 9.873e-03\n",
      "It: 16800, Loss: 2.567e+02, loss_bcs: 6.006e-03, Loss_r: 6.968e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.251e-02\n",
      "mean_grad_bcs: 7.293e-03\n",
      "It: 16900, Loss: 3.557e+02, loss_bcs: 8.258e-03, Loss_r: 1.014e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.682e-02\n",
      "mean_grad_bcs: 7.745e-03\n",
      "It: 17000, Loss: 4.626e+02, loss_bcs: 8.034e-03, Loss_r: 3.369e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.229e-02\n",
      "mean_grad_bcs: 1.633e-02\n",
      "It: 17100, Loss: 2.180e+02, loss_bcs: 4.187e-03, Loss_r: 1.285e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.730e-02\n",
      "mean_grad_bcs: 4.252e-03\n",
      "It: 17200, Loss: 1.863e+02, loss_bcs: 4.178e-03, Loss_r: 6.416e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.062e-02\n",
      "mean_grad_bcs: 4.208e-03\n",
      "It: 17300, Loss: 7.034e+02, loss_bcs: 1.257e-02, Loss_r: 4.854e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.168e-01\n",
      "mean_grad_bcs: 1.244e-02\n",
      "It: 17400, Loss: 3.523e+02, loss_bcs: 8.946e-03, Loss_r: 4.251e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.253e-02\n",
      "mean_grad_bcs: 1.069e-02\n",
      "It: 17500, Loss: 4.672e+02, loss_bcs: 1.093e-02, Loss_r: 1.271e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.689e-02\n",
      "mean_grad_bcs: 4.367e-03\n",
      "It: 17600, Loss: 2.201e+02, loss_bcs: 5.369e-03, Loss_r: 4.328e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.481e-02\n",
      "mean_grad_bcs: 1.274e-02\n",
      "It: 17700, Loss: 2.053e+02, loss_bcs: 4.432e-03, Loss_r: 8.391e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.937e-02\n",
      "mean_grad_bcs: 4.708e-03\n",
      "It: 17800, Loss: 4.200e+02, loss_bcs: 9.826e-03, Loss_r: 1.142e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.717e-02\n",
      "mean_grad_bcs: 1.474e-02\n",
      "It: 17900, Loss: 2.924e+02, loss_bcs: 6.977e-03, Loss_r: 6.923e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.364e-02\n",
      "mean_grad_bcs: 9.852e-03\n",
      "It: 18000, Loss: 4.589e+02, loss_bcs: 1.175e-02, Loss_r: 4.760e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.871e-03\n",
      "mean_grad_bcs: 9.715e-03\n",
      "It: 18100, Loss: 2.913e+02, loss_bcs: 5.082e-03, Loss_r: 2.105e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.792e-02\n",
      "mean_grad_bcs: 4.862e-03\n",
      "It: 18200, Loss: 3.546e+02, loss_bcs: 9.071e-03, Loss_r: 3.759e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.960e-03\n",
      "mean_grad_bcs: 9.189e-03\n",
      "It: 18300, Loss: 3.221e+02, loss_bcs: 6.010e-03, Loss_r: 2.031e-02, Time: 0.22\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.880e-02\n",
      "mean_grad_bcs: 8.973e-03\n",
      "It: 18400, Loss: 2.560e+02, loss_bcs: 5.009e-03, Loss_r: 1.438e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.397e-02\n",
      "mean_grad_bcs: 3.333e-03\n",
      "It: 18500, Loss: 3.196e+02, loss_bcs: 7.026e-03, Loss_r: 1.211e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.646e-02\n",
      "mean_grad_bcs: 1.984e-02\n",
      "It: 18600, Loss: 4.234e+02, loss_bcs: 1.063e-02, Loss_r: 6.021e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.160e-02\n",
      "mean_grad_bcs: 1.045e-02\n",
      "It: 18700, Loss: 3.952e+02, loss_bcs: 6.201e-03, Loss_r: 3.379e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.895e-02\n",
      "mean_grad_bcs: 1.109e-02\n",
      "It: 18800, Loss: 2.510e+02, loss_bcs: 6.082e-03, Loss_r: 5.220e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.583e-02\n",
      "mean_grad_bcs: 5.932e-03\n",
      "It: 18900, Loss: 3.373e+02, loss_bcs: 8.145e-03, Loss_r: 7.240e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.208e-02\n",
      "mean_grad_bcs: 1.077e-02\n",
      "It: 19000, Loss: 1.953e+02, loss_bcs: 4.415e-03, Loss_r: 6.466e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.314e-02\n",
      "mean_grad_bcs: 8.328e-03\n",
      "It: 19100, Loss: 1.396e+02, loss_bcs: 2.483e-03, Loss_r: 9.717e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.046e-02\n",
      "mean_grad_bcs: 5.154e-03\n",
      "It: 19200, Loss: 2.714e+02, loss_bcs: 6.762e-03, Loss_r: 4.257e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.209e-03\n",
      "mean_grad_bcs: 5.187e-03\n",
      "It: 19300, Loss: 2.536e+02, loss_bcs: 6.199e-03, Loss_r: 4.878e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.487e-02\n",
      "mean_grad_bcs: 8.862e-03\n",
      "It: 19400, Loss: 3.588e+02, loss_bcs: 8.769e-03, Loss_r: 6.919e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.870e-02\n",
      "mean_grad_bcs: 1.947e-02\n",
      "It: 19500, Loss: 3.226e+02, loss_bcs: 7.524e-03, Loss_r: 8.934e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.519e-02\n",
      "mean_grad_bcs: 9.447e-03\n",
      "It: 19600, Loss: 3.414e+02, loss_bcs: 7.397e-03, Loss_r: 1.374e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.122e-02\n",
      "mean_grad_bcs: 9.392e-03\n",
      "It: 19700, Loss: 1.733e+02, loss_bcs: 4.043e-03, Loss_r: 4.806e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.938e-02\n",
      "mean_grad_bcs: 1.061e-02\n",
      "It: 19800, Loss: 3.396e+02, loss_bcs: 6.860e-03, Loss_r: 1.746e-02, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.239e-02\n",
      "mean_grad_bcs: 1.693e-02\n",
      "It: 19900, Loss: 3.329e+02, loss_bcs: 8.287e-03, Loss_r: 5.265e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.300e-02\n",
      "mean_grad_bcs: 1.016e-02\n",
      "It: 20000, Loss: 2.908e+02, loss_bcs: 7.312e-03, Loss_r: 4.048e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.200e-02\n",
      "mean_grad_bcs: 1.464e-02\n",
      "It: 20100, Loss: 1.780e+02, loss_bcs: 3.373e-03, Loss_r: 1.083e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.328e-02\n",
      "mean_grad_bcs: 7.235e-03\n",
      "It: 20200, Loss: 2.764e+02, loss_bcs: 6.711e-03, Loss_r: 5.661e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.573e-02\n",
      "mean_grad_bcs: 1.062e-02\n",
      "It: 20300, Loss: 4.065e+02, loss_bcs: 8.329e-03, Loss_r: 1.999e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.437e-02\n",
      "mean_grad_bcs: 1.503e-02\n",
      "It: 20400, Loss: 1.663e+02, loss_bcs: 3.675e-03, Loss_r: 6.140e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.136e-02\n",
      "mean_grad_bcs: 5.089e-03\n",
      "It: 20500, Loss: 2.988e+02, loss_bcs: 7.629e-03, Loss_r: 3.281e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.074e-02\n",
      "mean_grad_bcs: 1.164e-02\n",
      "It: 20600, Loss: 4.011e+02, loss_bcs: 9.235e-03, Loss_r: 1.203e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.959e-02\n",
      "mean_grad_bcs: 1.439e-02\n",
      "It: 20700, Loss: 1.188e+02, loss_bcs: 2.686e-03, Loss_r: 3.922e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.968e-02\n",
      "mean_grad_bcs: 3.911e-03\n",
      "It: 20800, Loss: 3.137e+02, loss_bcs: 7.214e-03, Loss_r: 9.478e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.758e-02\n",
      "mean_grad_bcs: 1.588e-02\n",
      "It: 20900, Loss: 4.483e+02, loss_bcs: 1.148e-02, Loss_r: 4.661e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.996e-02\n",
      "mean_grad_bcs: 1.648e-02\n",
      "It: 21000, Loss: 2.459e+02, loss_bcs: 5.194e-03, Loss_r: 1.092e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.975e-02\n",
      "mean_grad_bcs: 1.107e-02\n",
      "It: 21100, Loss: 3.145e+02, loss_bcs: 6.035e-03, Loss_r: 1.856e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.192e-01\n",
      "mean_grad_bcs: 1.848e-02\n",
      "It: 21200, Loss: 3.399e+02, loss_bcs: 8.560e-03, Loss_r: 4.631e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.527e-03\n",
      "mean_grad_bcs: 1.978e-02\n",
      "It: 21300, Loss: 2.705e+02, loss_bcs: 6.422e-03, Loss_r: 6.633e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.265e-02\n",
      "mean_grad_bcs: 2.235e-03\n",
      "It: 21400, Loss: 3.392e+02, loss_bcs: 6.071e-03, Loss_r: 2.334e-02, Time: 0.09\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.013e-02\n",
      "mean_grad_bcs: 1.000e-02\n",
      "It: 21500, Loss: 2.927e+02, loss_bcs: 7.416e-03, Loss_r: 3.643e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.544e-02\n",
      "mean_grad_bcs: 1.685e-02\n",
      "It: 21600, Loss: 1.906e+02, loss_bcs: 4.571e-03, Loss_r: 4.328e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.634e-02\n",
      "mean_grad_bcs: 1.690e-02\n",
      "It: 21700, Loss: 2.647e+02, loss_bcs: 4.620e-03, Loss_r: 1.909e-02, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.458e-02\n",
      "mean_grad_bcs: 8.670e-03\n",
      "It: 21800, Loss: 2.383e+02, loss_bcs: 5.884e-03, Loss_r: 4.128e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.104e-02\n",
      "mean_grad_bcs: 8.973e-03\n",
      "It: 21900, Loss: 3.849e+02, loss_bcs: 9.431e-03, Loss_r: 7.239e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.472e-02\n",
      "mean_grad_bcs: 1.333e-02\n",
      "It: 22000, Loss: 2.827e+02, loss_bcs: 5.739e-03, Loss_r: 1.431e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.528e-02\n",
      "mean_grad_bcs: 4.898e-03\n",
      "It: 22100, Loss: 3.491e+02, loss_bcs: 8.522e-03, Loss_r: 6.796e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.579e-02\n",
      "mean_grad_bcs: 1.761e-02\n",
      "It: 22200, Loss: 2.877e+02, loss_bcs: 6.920e-03, Loss_r: 6.389e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.394e-02\n",
      "mean_grad_bcs: 5.277e-03\n",
      "It: 22300, Loss: 5.039e+02, loss_bcs: 1.288e-02, Loss_r: 5.458e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.318e-02\n",
      "mean_grad_bcs: 1.859e-02\n",
      "It: 22400, Loss: 3.241e+02, loss_bcs: 7.562e-03, Loss_r: 8.964e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.178e-02\n",
      "mean_grad_bcs: 9.307e-03\n",
      "It: 22500, Loss: 1.879e+02, loss_bcs: 3.274e-03, Loss_r: 1.361e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.556e-02\n",
      "mean_grad_bcs: 3.267e-03\n",
      "It: 22600, Loss: 1.625e+02, loss_bcs: 3.622e-03, Loss_r: 5.774e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.330e-02\n",
      "mean_grad_bcs: 1.081e-02\n",
      "It: 22700, Loss: 4.485e+02, loss_bcs: 1.157e-02, Loss_r: 3.999e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.770e-02\n",
      "mean_grad_bcs: 1.233e-02\n",
      "It: 22800, Loss: 4.502e+02, loss_bcs: 1.128e-02, Loss_r: 6.560e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.283e-02\n",
      "mean_grad_bcs: 1.688e-02\n",
      "It: 22900, Loss: 4.721e+02, loss_bcs: 7.965e-03, Loss_r: 3.614e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.051e-02\n",
      "mean_grad_bcs: 6.553e-03\n",
      "It: 23000, Loss: 4.416e+02, loss_bcs: 9.197e-03, Loss_r: 2.059e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.423e-01\n",
      "mean_grad_bcs: 1.440e-02\n",
      "It: 23100, Loss: 2.251e+02, loss_bcs: 5.631e-03, Loss_r: 3.353e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.257e-02\n",
      "mean_grad_bcs: 8.883e-03\n",
      "It: 23200, Loss: 2.867e+02, loss_bcs: 6.494e-03, Loss_r: 9.414e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.378e-02\n",
      "mean_grad_bcs: 9.256e-03\n",
      "It: 23300, Loss: 4.788e+02, loss_bcs: 1.188e-02, Loss_r: 7.860e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.179e-02\n",
      "mean_grad_bcs: 6.623e-03\n",
      "It: 23400, Loss: 2.575e+02, loss_bcs: 6.462e-03, Loss_r: 3.683e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.795e-02\n",
      "mean_grad_bcs: 9.841e-03\n",
      "It: 23500, Loss: 1.965e+02, loss_bcs: 4.709e-03, Loss_r: 4.484e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.473e-02\n",
      "mean_grad_bcs: 3.452e-03\n",
      "It: 23600, Loss: 2.733e+02, loss_bcs: 4.562e-03, Loss_r: 2.130e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.097e-02\n",
      "mean_grad_bcs: 9.692e-03\n",
      "It: 23700, Loss: 2.358e+02, loss_bcs: 5.788e-03, Loss_r: 4.358e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.105e-02\n",
      "mean_grad_bcs: 1.211e-02\n",
      "It: 23800, Loss: 1.940e+02, loss_bcs: 4.834e-03, Loss_r: 3.037e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.233e-03\n",
      "mean_grad_bcs: 1.081e-02\n",
      "It: 23900, Loss: 1.927e+02, loss_bcs: 4.483e-03, Loss_r: 5.435e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.980e-02\n",
      "mean_grad_bcs: 3.994e-03\n",
      "It: 24000, Loss: 2.741e+02, loss_bcs: 6.985e-03, Loss_r: 3.115e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.992e-03\n",
      "mean_grad_bcs: 9.990e-03\n",
      "It: 24100, Loss: 3.776e+02, loss_bcs: 9.537e-03, Loss_r: 4.936e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.647e-02\n",
      "mean_grad_bcs: 1.223e-02\n",
      "It: 24200, Loss: 2.407e+02, loss_bcs: 5.114e-03, Loss_r: 1.045e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.731e-02\n",
      "mean_grad_bcs: 3.295e-03\n",
      "It: 24300, Loss: 2.613e+02, loss_bcs: 6.498e-03, Loss_r: 4.187e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.423e-02\n",
      "mean_grad_bcs: 9.496e-03\n",
      "It: 24400, Loss: 2.619e+02, loss_bcs: 6.012e-03, Loss_r: 7.986e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.642e-02\n",
      "mean_grad_bcs: 9.363e-03\n",
      "It: 24500, Loss: 2.114e+02, loss_bcs: 4.527e-03, Loss_r: 8.922e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.370e-02\n",
      "mean_grad_bcs: 6.919e-03\n",
      "It: 24600, Loss: 2.911e+02, loss_bcs: 6.868e-03, Loss_r: 7.467e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.939e-02\n",
      "mean_grad_bcs: 7.146e-03\n",
      "It: 24700, Loss: 1.301e+02, loss_bcs: 3.106e-03, Loss_r: 3.067e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.060e-02\n",
      "mean_grad_bcs: 8.156e-03\n",
      "It: 24800, Loss: 2.347e+02, loss_bcs: 5.518e-03, Loss_r: 6.172e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.633e-02\n",
      "mean_grad_bcs: 8.564e-03\n",
      "It: 24900, Loss: 3.377e+02, loss_bcs: 6.408e-03, Loss_r: 2.048e-02, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.081e-01\n",
      "mean_grad_bcs: 1.504e-02\n",
      "It: 25000, Loss: 2.382e+02, loss_bcs: 5.686e-03, Loss_r: 5.616e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.473e-02\n",
      "mean_grad_bcs: 1.209e-02\n",
      "It: 25100, Loss: 2.139e+02, loss_bcs: 5.037e-03, Loss_r: 5.572e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.484e-02\n",
      "mean_grad_bcs: 8.869e-03\n",
      "It: 25200, Loss: 2.239e+02, loss_bcs: 5.429e-03, Loss_r: 4.645e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.709e-02\n",
      "mean_grad_bcs: 5.708e-03\n",
      "It: 25300, Loss: 2.273e+02, loss_bcs: 5.606e-03, Loss_r: 4.002e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.853e-03\n",
      "mean_grad_bcs: 9.888e-03\n",
      "It: 25400, Loss: 2.300e+02, loss_bcs: 5.607e-03, Loss_r: 4.546e-03, Time: 0.05\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.767e-02\n",
      "mean_grad_bcs: 5.344e-03\n",
      "It: 25500, Loss: 4.099e+02, loss_bcs: 9.968e-03, Loss_r: 8.266e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.550e-02\n",
      "mean_grad_bcs: 7.033e-03\n",
      "It: 25600, Loss: 3.421e+02, loss_bcs: 8.758e-03, Loss_r: 3.570e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.055e-02\n",
      "mean_grad_bcs: 9.255e-03\n",
      "It: 25700, Loss: 2.195e+02, loss_bcs: 5.027e-03, Loss_r: 6.783e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.451e-02\n",
      "mean_grad_bcs: 5.017e-03\n",
      "It: 25800, Loss: 1.018e+02, loss_bcs: 1.851e-03, Loss_r: 6.785e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.662e-02\n",
      "mean_grad_bcs: 5.570e-03\n",
      "It: 25900, Loss: 2.271e+02, loss_bcs: 5.632e-03, Loss_r: 3.764e-03, Time: 0.12\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.468e-02\n",
      "mean_grad_bcs: 5.668e-03\n",
      "It: 26000, Loss: 2.213e+02, loss_bcs: 5.296e-03, Loss_r: 5.118e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.897e-02\n",
      "mean_grad_bcs: 1.210e-02\n",
      "It: 26100, Loss: 2.165e+02, loss_bcs: 5.067e-03, Loss_r: 5.862e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.112e-02\n",
      "mean_grad_bcs: 5.598e-03\n",
      "It: 26200, Loss: 2.114e+02, loss_bcs: 3.833e-03, Loss_r: 1.417e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.718e-02\n",
      "mean_grad_bcs: 8.716e-03\n",
      "It: 26300, Loss: 2.187e+02, loss_bcs: 4.703e-03, Loss_r: 9.073e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.356e-02\n",
      "mean_grad_bcs: 7.223e-03\n",
      "It: 26400, Loss: 1.984e+02, loss_bcs: 2.917e-03, Loss_r: 1.846e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.205e-02\n",
      "mean_grad_bcs: 1.100e-02\n",
      "It: 26500, Loss: 3.071e+02, loss_bcs: 6.647e-03, Loss_r: 1.243e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.540e-02\n",
      "mean_grad_bcs: 1.091e-02\n",
      "It: 26600, Loss: 3.358e+02, loss_bcs: 8.413e-03, Loss_r: 4.900e-03, Time: 0.09\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.526e-02\n",
      "mean_grad_bcs: 1.050e-02\n",
      "It: 26700, Loss: 2.033e+02, loss_bcs: 5.022e-03, Loss_r: 3.513e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.438e-02\n",
      "mean_grad_bcs: 1.139e-02\n",
      "It: 26800, Loss: 1.662e+02, loss_bcs: 4.047e-03, Loss_r: 3.317e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.387e-02\n",
      "mean_grad_bcs: 1.124e-02\n",
      "It: 26900, Loss: 1.215e+02, loss_bcs: 2.188e-03, Loss_r: 8.252e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.959e-02\n",
      "mean_grad_bcs: 7.208e-03\n",
      "It: 27000, Loss: 1.096e+02, loss_bcs: 2.606e-03, Loss_r: 2.660e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.623e-03\n",
      "mean_grad_bcs: 4.613e-03\n",
      "It: 27100, Loss: 3.082e+02, loss_bcs: 7.613e-03, Loss_r: 5.335e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.808e-02\n",
      "mean_grad_bcs: 1.857e-02\n",
      "It: 27200, Loss: 2.625e+02, loss_bcs: 6.276e-03, Loss_r: 6.117e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.552e-02\n",
      "mean_grad_bcs: 1.013e-02\n",
      "It: 27300, Loss: 2.321e+02, loss_bcs: 5.803e-03, Loss_r: 3.480e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.015e-02\n",
      "mean_grad_bcs: 3.126e-03\n",
      "It: 27400, Loss: 2.775e+02, loss_bcs: 6.975e-03, Loss_r: 3.874e-03, Time: 0.13\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.660e-02\n",
      "mean_grad_bcs: 1.150e-02\n",
      "It: 27500, Loss: 3.985e+02, loss_bcs: 1.037e-02, Loss_r: 2.899e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.721e-02\n",
      "mean_grad_bcs: 1.167e-02\n",
      "It: 27600, Loss: 2.574e+02, loss_bcs: 6.461e-03, Loss_r: 3.666e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.779e-03\n",
      "mean_grad_bcs: 6.324e-03\n",
      "It: 27700, Loss: 2.042e+02, loss_bcs: 5.133e-03, Loss_r: 2.861e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.102e-03\n",
      "mean_grad_bcs: 1.421e-02\n",
      "It: 27800, Loss: 2.129e+02, loss_bcs: 4.674e-03, Loss_r: 8.114e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.271e-02\n",
      "mean_grad_bcs: 8.168e-03\n",
      "It: 27900, Loss: 1.559e+02, loss_bcs: 3.645e-03, Loss_r: 4.256e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.726e-02\n",
      "mean_grad_bcs: 4.937e-03\n",
      "It: 28000, Loss: 4.113e+02, loss_bcs: 8.573e-03, Loss_r: 1.912e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.902e-02\n",
      "mean_grad_bcs: 1.381e-02\n",
      "It: 28100, Loss: 2.673e+02, loss_bcs: 6.607e-03, Loss_r: 4.581e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.081e-02\n",
      "mean_grad_bcs: 6.914e-03\n",
      "It: 28200, Loss: 3.779e+02, loss_bcs: 9.756e-03, Loss_r: 3.339e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.512e-03\n",
      "mean_grad_bcs: 6.206e-03\n",
      "It: 28300, Loss: 2.518e+02, loss_bcs: 6.362e-03, Loss_r: 3.287e-03, Time: 0.09\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.128e-02\n",
      "mean_grad_bcs: 4.185e-03\n",
      "It: 28400, Loss: 2.541e+02, loss_bcs: 6.456e-03, Loss_r: 3.023e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.779e-03\n",
      "mean_grad_bcs: 1.042e-02\n",
      "It: 28500, Loss: 1.979e+02, loss_bcs: 4.978e-03, Loss_r: 2.744e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.168e-03\n",
      "mean_grad_bcs: 3.856e-03\n",
      "It: 28600, Loss: 2.211e+02, loss_bcs: 5.399e-03, Loss_r: 4.283e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.454e-02\n",
      "mean_grad_bcs: 8.225e-03\n",
      "It: 28700, Loss: 3.773e+02, loss_bcs: 4.259e-03, Loss_r: 4.485e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.616e-01\n",
      "mean_grad_bcs: 8.108e-03\n",
      "It: 28800, Loss: 2.031e+02, loss_bcs: 4.655e-03, Loss_r: 6.239e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.985e-02\n",
      "mean_grad_bcs: 8.385e-03\n",
      "It: 28900, Loss: 4.025e+02, loss_bcs: 1.031e-02, Loss_r: 4.179e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.777e-02\n",
      "mean_grad_bcs: 3.386e-03\n",
      "It: 29000, Loss: 2.242e+02, loss_bcs: 5.073e-03, Loss_r: 7.391e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.216e-02\n",
      "mean_grad_bcs: 8.450e-03\n",
      "It: 29100, Loss: 2.131e+02, loss_bcs: 5.231e-03, Loss_r: 3.938e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.069e-02\n",
      "mean_grad_bcs: 1.885e-02\n",
      "It: 29200, Loss: 2.269e+02, loss_bcs: 5.660e-03, Loss_r: 3.491e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.938e-03\n",
      "mean_grad_bcs: 5.589e-03\n",
      "It: 29300, Loss: 1.314e+02, loss_bcs: 3.067e-03, Loss_r: 3.612e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.382e-02\n",
      "mean_grad_bcs: 6.922e-03\n",
      "It: 29400, Loss: 3.472e+02, loss_bcs: 8.528e-03, Loss_r: 6.357e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.354e-02\n",
      "mean_grad_bcs: 8.918e-03\n",
      "It: 29500, Loss: 1.768e+02, loss_bcs: 4.347e-03, Loss_r: 3.213e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.405e-03\n",
      "mean_grad_bcs: 1.447e-02\n",
      "It: 29600, Loss: 1.049e+02, loss_bcs: 2.455e-03, Loss_r: 2.850e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.962e-03\n",
      "mean_grad_bcs: 3.477e-03\n",
      "It: 29700, Loss: 2.335e+02, loss_bcs: 5.811e-03, Loss_r: 3.700e-03, Time: 0.12\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.437e-02\n",
      "mean_grad_bcs: 7.839e-03\n",
      "It: 29800, Loss: 3.612e+02, loss_bcs: 9.163e-03, Loss_r: 4.413e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.770e-03\n",
      "mean_grad_bcs: 1.186e-02\n",
      "It: 29900, Loss: 3.064e+02, loss_bcs: 7.659e-03, Loss_r: 4.618e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.361e-02\n",
      "mean_grad_bcs: 7.182e-03\n",
      "It: 30000, Loss: 3.782e+02, loss_bcs: 9.449e-03, Loss_r: 5.728e-03, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.520e-02\n",
      "mean_grad_bcs: 1.555e-02\n",
      "It: 30100, Loss: 2.721e+02, loss_bcs: 5.367e-03, Loss_r: 1.497e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.034e-02\n",
      "mean_grad_bcs: 4.225e-03\n",
      "It: 30200, Loss: 4.028e+02, loss_bcs: 1.058e-02, Loss_r: 2.220e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.476e-03\n",
      "mean_grad_bcs: 7.536e-03\n",
      "It: 30300, Loss: 8.898e+01, loss_bcs: 1.997e-03, Loss_r: 3.062e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.197e-02\n",
      "mean_grad_bcs: 9.687e-03\n",
      "It: 30400, Loss: 2.618e+02, loss_bcs: 5.258e-03, Loss_r: 1.368e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.590e-02\n",
      "mean_grad_bcs: 5.439e-03\n",
      "It: 30500, Loss: 2.773e+02, loss_bcs: 6.358e-03, Loss_r: 8.519e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.527e-02\n",
      "mean_grad_bcs: 5.000e-03\n",
      "It: 30600, Loss: 2.847e+02, loss_bcs: 7.294e-03, Loss_r: 2.937e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.567e-02\n",
      "mean_grad_bcs: 1.170e-02\n",
      "It: 30700, Loss: 1.287e+02, loss_bcs: 2.748e-03, Loss_r: 5.493e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.231e-02\n",
      "mean_grad_bcs: 6.402e-03\n",
      "It: 30800, Loss: 1.559e+02, loss_bcs: 3.669e-03, Loss_r: 4.079e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.770e-03\n",
      "mean_grad_bcs: 5.807e-03\n",
      "It: 30900, Loss: 4.469e+02, loss_bcs: 1.115e-02, Loss_r: 6.847e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.748e-02\n",
      "mean_grad_bcs: 1.168e-02\n",
      "It: 31000, Loss: 1.869e+02, loss_bcs: 4.677e-03, Loss_r: 2.774e-03, Time: 0.25\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.507e-03\n",
      "mean_grad_bcs: 6.803e-03\n",
      "It: 31100, Loss: 2.696e+02, loss_bcs: 6.408e-03, Loss_r: 6.561e-03, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.794e-02\n",
      "mean_grad_bcs: 8.514e-03\n",
      "It: 31200, Loss: 1.977e+02, loss_bcs: 4.889e-03, Loss_r: 3.368e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.675e-03\n",
      "mean_grad_bcs: 3.717e-03\n",
      "It: 31300, Loss: 1.569e+02, loss_bcs: 3.763e-03, Loss_r: 3.551e-03, Time: 0.11\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.069e-02\n",
      "mean_grad_bcs: 2.965e-03\n",
      "It: 31400, Loss: 1.590e+02, loss_bcs: 3.567e-03, Loss_r: 5.470e-03, Time: 0.09\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.373e-02\n",
      "mean_grad_bcs: 8.082e-03\n",
      "It: 31500, Loss: 3.726e+02, loss_bcs: 9.355e-03, Loss_r: 5.293e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.122e-02\n",
      "mean_grad_bcs: 5.667e-03\n",
      "It: 31600, Loss: 1.946e+02, loss_bcs: 4.869e-03, Loss_r: 2.896e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.983e-03\n",
      "mean_grad_bcs: 5.404e-03\n",
      "It: 31700, Loss: 4.638e+02, loss_bcs: 1.027e-02, Loss_r: 1.702e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.252e-01\n",
      "mean_grad_bcs: 5.194e-03\n",
      "It: 31800, Loss: 1.465e+02, loss_bcs: 3.363e-03, Loss_r: 4.461e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.158e-02\n",
      "mean_grad_bcs: 1.153e-02\n",
      "It: 31900, Loss: 2.711e+02, loss_bcs: 6.432e-03, Loss_r: 6.687e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.910e-02\n",
      "mean_grad_bcs: 5.263e-03\n",
      "It: 32000, Loss: 2.975e+02, loss_bcs: 7.663e-03, Loss_r: 2.752e-03, Time: 0.09\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.943e-03\n",
      "mean_grad_bcs: 3.071e-03\n",
      "It: 32100, Loss: 2.238e+02, loss_bcs: 5.102e-03, Loss_r: 7.082e-03, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.238e-02\n",
      "mean_grad_bcs: 1.085e-02\n",
      "It: 32200, Loss: 2.017e+02, loss_bcs: 5.095e-03, Loss_r: 2.638e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.611e-03\n",
      "mean_grad_bcs: 1.268e-02\n",
      "It: 32300, Loss: 2.617e+02, loss_bcs: 3.933e-03, Loss_r: 2.369e-02, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.060e-01\n",
      "mean_grad_bcs: 8.076e-03\n",
      "It: 32400, Loss: 1.731e+02, loss_bcs: 4.053e-03, Loss_r: 4.673e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.120e-02\n",
      "mean_grad_bcs: 7.043e-03\n",
      "It: 32500, Loss: 2.791e+02, loss_bcs: 6.886e-03, Loss_r: 4.875e-03, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.544e-02\n",
      "mean_grad_bcs: 1.013e-02\n",
      "It: 32600, Loss: 2.798e+02, loss_bcs: 6.489e-03, Loss_r: 8.042e-03, Time: 0.14\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.635e-02\n",
      "mean_grad_bcs: 1.248e-02\n",
      "It: 32700, Loss: 2.709e+02, loss_bcs: 5.706e-03, Loss_r: 1.214e-02, Time: 0.06\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.969e-02\n",
      "mean_grad_bcs: 6.380e-03\n",
      "It: 32800, Loss: 4.627e+02, loss_bcs: 8.231e-03, Loss_r: 3.222e-02, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.538e-01\n",
      "mean_grad_bcs: 7.133e-03\n",
      "It: 32900, Loss: 2.517e+02, loss_bcs: 6.380e-03, Loss_r: 3.106e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.483e-03\n",
      "mean_grad_bcs: 9.357e-03\n",
      "It: 33000, Loss: 2.483e+02, loss_bcs: 6.092e-03, Loss_r: 4.601e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.457e-02\n",
      "mean_grad_bcs: 8.376e-03\n",
      "It: 33100, Loss: 3.383e+02, loss_bcs: 8.396e-03, Loss_r: 5.540e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.219e-02\n",
      "mean_grad_bcs: 7.847e-03\n",
      "It: 33200, Loss: 2.758e+02, loss_bcs: 6.228e-03, Loss_r: 9.190e-03, Time: 0.09\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.417e-02\n",
      "mean_grad_bcs: 1.490e-02\n",
      "It: 33300, Loss: 2.065e+02, loss_bcs: 5.097e-03, Loss_r: 3.596e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.216e-02\n",
      "mean_grad_bcs: 4.270e-03\n",
      "It: 33400, Loss: 3.495e+02, loss_bcs: 8.792e-03, Loss_r: 4.846e-03, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.967e-02\n",
      "mean_grad_bcs: 5.986e-03\n",
      "It: 33500, Loss: 2.501e+02, loss_bcs: 5.536e-03, Loss_r: 9.182e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.704e-02\n",
      "mean_grad_bcs: 1.613e-02\n",
      "It: 33600, Loss: 1.615e+02, loss_bcs: 3.907e-03, Loss_r: 3.413e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.076e-02\n",
      "mean_grad_bcs: 5.049e-03\n",
      "It: 33700, Loss: 2.679e+02, loss_bcs: 4.859e-03, Loss_r: 1.794e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.529e-02\n",
      "mean_grad_bcs: 5.369e-03\n",
      "It: 33800, Loss: 1.479e+02, loss_bcs: 3.692e-03, Loss_r: 2.261e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.880e-03\n",
      "mean_grad_bcs: 6.003e-03\n",
      "It: 33900, Loss: 1.237e+02, loss_bcs: 2.696e-03, Loss_r: 4.861e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.229e-02\n",
      "mean_grad_bcs: 3.551e-03\n",
      "It: 34000, Loss: 2.863e+02, loss_bcs: 7.033e-03, Loss_r: 5.231e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.558e-02\n",
      "mean_grad_bcs: 1.093e-02\n",
      "It: 34100, Loss: 3.348e+02, loss_bcs: 8.612e-03, Loss_r: 3.196e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.494e-02\n",
      "mean_grad_bcs: 4.579e-03\n",
      "It: 34200, Loss: 1.518e+02, loss_bcs: 3.136e-03, Loss_r: 7.267e-03, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.887e-02\n",
      "mean_grad_bcs: 3.269e-03\n",
      "It: 34300, Loss: 8.886e+01, loss_bcs: 2.097e-03, Loss_r: 2.275e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.526e-03\n",
      "mean_grad_bcs: 3.047e-03\n",
      "It: 34400, Loss: 1.862e+02, loss_bcs: 4.669e-03, Loss_r: 2.688e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.844e-03\n",
      "mean_grad_bcs: 1.288e-02\n",
      "It: 34500, Loss: 1.939e+02, loss_bcs: 2.711e-03, Loss_r: 1.910e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.340e-01\n",
      "mean_grad_bcs: 8.178e-03\n",
      "It: 34600, Loss: 1.999e+02, loss_bcs: 4.634e-03, Loss_r: 5.762e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.334e-02\n",
      "mean_grad_bcs: 9.689e-03\n",
      "It: 34700, Loss: 1.745e+02, loss_bcs: 4.139e-03, Loss_r: 4.311e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.677e-02\n",
      "mean_grad_bcs: 1.144e-02\n",
      "It: 34800, Loss: 2.968e+02, loss_bcs: 7.639e-03, Loss_r: 2.787e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.937e-03\n",
      "mean_grad_bcs: 2.709e-03\n",
      "It: 34900, Loss: 2.749e+02, loss_bcs: 5.958e-03, Loss_r: 1.105e-02, Time: 0.14\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.458e-02\n",
      "mean_grad_bcs: 4.284e-03\n",
      "It: 35000, Loss: 1.072e+02, loss_bcs: 2.540e-03, Loss_r: 2.675e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.733e-03\n",
      "mean_grad_bcs: 6.403e-03\n",
      "It: 35100, Loss: 2.505e+02, loss_bcs: 5.705e-03, Loss_r: 7.986e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.386e-02\n",
      "mean_grad_bcs: 6.320e-03\n",
      "It: 35200, Loss: 1.858e+02, loss_bcs: 4.471e-03, Loss_r: 4.113e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.235e-02\n",
      "mean_grad_bcs: 7.026e-03\n",
      "It: 35300, Loss: 2.219e+02, loss_bcs: 5.374e-03, Loss_r: 4.636e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.092e-02\n",
      "mean_grad_bcs: 2.573e-03\n",
      "It: 35400, Loss: 1.362e+02, loss_bcs: 3.286e-03, Loss_r: 2.943e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.967e-03\n",
      "mean_grad_bcs: 8.176e-03\n",
      "It: 35500, Loss: 2.833e+02, loss_bcs: 6.610e-03, Loss_r: 7.836e-03, Time: 0.09\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.562e-02\n",
      "mean_grad_bcs: 8.382e-03\n",
      "It: 35600, Loss: 2.564e+02, loss_bcs: 6.274e-03, Loss_r: 4.880e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.623e-02\n",
      "mean_grad_bcs: 1.010e-02\n",
      "It: 35700, Loss: 2.381e+02, loss_bcs: 5.098e-03, Loss_r: 1.005e-02, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.079e-02\n",
      "mean_grad_bcs: 5.095e-03\n",
      "It: 35800, Loss: 2.222e+02, loss_bcs: 5.642e-03, Loss_r: 2.677e-03, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.073e-02\n",
      "mean_grad_bcs: 6.053e-03\n",
      "It: 35900, Loss: 2.677e+02, loss_bcs: 6.952e-03, Loss_r: 2.053e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.632e-03\n",
      "mean_grad_bcs: 6.830e-03\n",
      "It: 36000, Loss: 3.270e+02, loss_bcs: 8.379e-03, Loss_r: 3.363e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.444e-02\n",
      "mean_grad_bcs: 1.183e-02\n",
      "It: 36100, Loss: 2.648e+02, loss_bcs: 5.825e-03, Loss_r: 9.993e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.473e-02\n",
      "mean_grad_bcs: 6.367e-03\n",
      "It: 36200, Loss: 3.033e+02, loss_bcs: 7.095e-03, Loss_r: 8.248e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.626e-02\n",
      "mean_grad_bcs: 1.367e-02\n",
      "It: 36300, Loss: 1.870e+02, loss_bcs: 4.247e-03, Loss_r: 6.056e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.098e-02\n",
      "mean_grad_bcs: 5.704e-03\n",
      "It: 36400, Loss: 2.096e+02, loss_bcs: 4.840e-03, Loss_r: 6.178e-03, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.745e-02\n",
      "mean_grad_bcs: 5.655e-03\n",
      "It: 36500, Loss: 2.755e+02, loss_bcs: 6.077e-03, Loss_r: 1.027e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.452e-02\n",
      "mean_grad_bcs: 5.695e-03\n",
      "It: 36600, Loss: 2.297e+02, loss_bcs: 5.663e-03, Loss_r: 4.047e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 9.337e-03\n",
      "mean_grad_bcs: 7.674e-03\n",
      "It: 36700, Loss: 3.137e+02, loss_bcs: 8.026e-03, Loss_r: 3.317e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.325e-02\n",
      "mean_grad_bcs: 1.433e-02\n",
      "It: 36800, Loss: 2.390e+02, loss_bcs: 5.662e-03, Loss_r: 5.953e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.588e-02\n",
      "mean_grad_bcs: 3.975e-03\n",
      "It: 36900, Loss: 2.969e+02, loss_bcs: 7.698e-03, Loss_r: 2.377e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.815e-03\n",
      "mean_grad_bcs: 4.764e-03\n",
      "It: 37000, Loss: 2.612e+02, loss_bcs: 6.605e-03, Loss_r: 3.356e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.052e-02\n",
      "mean_grad_bcs: 2.868e-03\n",
      "It: 37100, Loss: 1.946e+02, loss_bcs: 4.462e-03, Loss_r: 5.975e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.684e-02\n",
      "mean_grad_bcs: 1.756e-02\n",
      "It: 37200, Loss: 2.424e+02, loss_bcs: 5.849e-03, Loss_r: 5.237e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.439e-03\n",
      "mean_grad_bcs: 9.033e-03\n",
      "It: 37300, Loss: 2.454e+02, loss_bcs: 6.043e-03, Loss_r: 4.378e-03, Time: 0.15\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.076e-02\n",
      "mean_grad_bcs: 6.997e-03\n",
      "It: 37400, Loss: 3.327e+02, loss_bcs: 6.856e-03, Loss_r: 1.607e-02, Time: 0.12\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.266e-01\n",
      "mean_grad_bcs: 1.142e-02\n",
      "It: 37500, Loss: 1.931e+02, loss_bcs: 4.586e-03, Loss_r: 4.731e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.970e-02\n",
      "mean_grad_bcs: 1.116e-02\n",
      "It: 37600, Loss: 1.119e+02, loss_bcs: 2.092e-03, Loss_r: 7.025e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.273e-02\n",
      "mean_grad_bcs: 7.123e-03\n",
      "It: 37700, Loss: 2.639e+02, loss_bcs: 6.603e-03, Loss_r: 3.928e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.507e-02\n",
      "mean_grad_bcs: 5.567e-03\n",
      "It: 37800, Loss: 1.354e+02, loss_bcs: 2.428e-03, Loss_r: 9.277e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.417e-02\n",
      "mean_grad_bcs: 1.571e-03\n",
      "It: 37900, Loss: 1.305e+02, loss_bcs: 3.071e-03, Loss_r: 3.405e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.077e-02\n",
      "mean_grad_bcs: 6.279e-03\n",
      "It: 38000, Loss: 1.566e+02, loss_bcs: 3.377e-03, Loss_r: 6.415e-03, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.061e-02\n",
      "mean_grad_bcs: 3.928e-03\n",
      "It: 38100, Loss: 3.916e+02, loss_bcs: 6.486e-03, Loss_r: 3.090e-02, Time: 0.10\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.107e-01\n",
      "mean_grad_bcs: 4.756e-03\n",
      "It: 38200, Loss: 1.061e+02, loss_bcs: 1.560e-03, Loss_r: 9.864e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.836e-02\n",
      "mean_grad_bcs: 5.352e-03\n",
      "It: 38300, Loss: 2.270e+02, loss_bcs: 4.781e-03, Loss_r: 1.018e-02, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.961e-02\n",
      "mean_grad_bcs: 9.622e-03\n",
      "It: 38400, Loss: 1.968e+02, loss_bcs: 4.667e-03, Loss_r: 4.877e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.958e-02\n",
      "mean_grad_bcs: 9.005e-03\n",
      "It: 38500, Loss: 1.912e+02, loss_bcs: 4.446e-03, Loss_r: 5.403e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 2.172e-02\n",
      "mean_grad_bcs: 3.846e-03\n",
      "It: 38600, Loss: 3.296e+02, loss_bcs: 8.290e-03, Loss_r: 4.581e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.171e-02\n",
      "mean_grad_bcs: 4.565e-03\n",
      "It: 38700, Loss: 1.393e+02, loss_bcs: 3.431e-03, Loss_r: 2.488e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 3.559e-03\n",
      "mean_grad_bcs: 4.722e-03\n",
      "It: 38800, Loss: 1.920e+02, loss_bcs: 4.704e-03, Loss_r: 3.600e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.693e-02\n",
      "mean_grad_bcs: 2.813e-03\n",
      "It: 38900, Loss: 9.918e+01, loss_bcs: 2.224e-03, Loss_r: 3.421e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.813e-03\n",
      "mean_grad_bcs: 3.369e-03\n",
      "It: 39000, Loss: 3.064e+02, loss_bcs: 7.811e-03, Loss_r: 3.463e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.214e-02\n",
      "mean_grad_bcs: 2.798e-03\n",
      "It: 39100, Loss: 3.137e+02, loss_bcs: 8.086e-03, Loss_r: 2.868e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.542e-03\n",
      "mean_grad_bcs: 1.027e-02\n",
      "It: 39200, Loss: 1.611e+02, loss_bcs: 4.035e-03, Loss_r: 2.367e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 4.753e-03\n",
      "mean_grad_bcs: 6.993e-03\n",
      "It: 39300, Loss: 3.049e+02, loss_bcs: 7.800e-03, Loss_r: 3.232e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.009e-02\n",
      "mean_grad_bcs: 5.628e-03\n",
      "It: 39400, Loss: 2.602e+02, loss_bcs: 6.415e-03, Loss_r: 4.583e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.022e-02\n",
      "mean_grad_bcs: 5.567e-03\n",
      "It: 39500, Loss: 1.518e+02, loss_bcs: 3.069e-03, Loss_r: 7.780e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 6.877e-02\n",
      "mean_grad_bcs: 8.913e-03\n",
      "It: 39600, Loss: 1.764e+02, loss_bcs: 4.408e-03, Loss_r: 2.666e-03, Time: 0.09\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 5.629e-03\n",
      "mean_grad_bcs: 1.227e-02\n",
      "It: 39700, Loss: 1.015e+02, loss_bcs: 2.322e-03, Loss_r: 3.156e-03, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 7.671e-03\n",
      "mean_grad_bcs: 1.037e-02\n",
      "It: 39800, Loss: 1.414e+02, loss_bcs: 3.286e-03, Loss_r: 4.004e-03, Time: 0.07\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.495e-02\n",
      "mean_grad_bcs: 4.494e-03\n",
      "It: 39900, Loss: 3.262e+02, loss_bcs: 6.715e-03, Loss_r: 1.581e-02, Time: 0.08\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 1.111e-01\n",
      "mean_grad_bcs: 5.350e-03\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000, Loss: 1.445e+02, loss_bcs: 3.406e-03, Loss_r: 3.742e-03, Time: 3.45\n",
      "adaptive_constant_bcs1_val: 3.706e+04\n",
      "adaptive_constant_res_val: 4.894e+03\n",
      "mean_grad_res: 8.033e-03\n",
      "mean_grad_bcs: 6.363e-03\n",
      "l2 error: 1.73e-01\n",
      "l2 error: 2.91e-01\n",
      "average lambda_bc3.7059e+04\n",
      "average lambda_res1.0\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Jan-16-2024_12-10-47-737457_M5\n",
      "Final loss total loss: 1.445435e+02\n",
      "Final loss loss_res: 3.741786e-03\n",
      "Final loss loss_bcs: 3.406158e-03\n",
      "Final loss loss_bc1: 1.382380e-03\n",
      "Final loss loss_bc2: 4.938394e-04\n",
      "Final loss loss_bc3: 1.527953e-03\n",
      "Final loss loss_bc4: 1.986121e-06\n",
      "elapsed: 2.75e+03\n",
      "Relative L2 error_u: 1.73e-01\n",
      "Relative L2 error_v: 2.91e-01\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:2.7495e+03\n",
      "average of error_u_list:1.7263e-01\n",
      "average of error_v_list:2.9055e-01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 350\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "# Parameters of equations\n",
    "Re = 100.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[0.0, 1.0], [1.0, 1.0]])\n",
    "bc2_coords = np.array([[0.0, 0.0], [0.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 0.0], [1.0, 1.0]])\n",
    "bc4_coords = np.array([[0.0, 0.0], [1.0, 0.0]])\n",
    "dom_coords = np.array([[0.0, 0.0], [1.0, 1.0]])\n",
    "\n",
    "# Define model\n",
    "model = 'M5'\n",
    "layers = [2, 50, 50, 50, 50 , 50 , 2]\n",
    "\n",
    "stiff_ratio = False  # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\" ]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_v_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: U_gamma_1(x), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: U_gamma_2(x), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: U_gamma_2(x), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: U_gamma_2(x), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x), name='Forcing')\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "\n",
    "            mode = Navier_Stokes2D(layers, operator, bcs_sampler, res_sampler, Re, model , sess)\n",
    "\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                mode.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                mode.trainmb(nIter, mbbatch_size)\n",
    "            else:\n",
    "                mode.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Predictions\n",
    "            _, p_pred = mode.predict_psi_p(X_star)\n",
    "            u_pred, v_pred = mode.predict_uv(X_star)\n",
    "\n",
    "            # Reference\n",
    "            u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "            v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "            # velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "            u_pred = u_pred.reshape(100,100)\n",
    "            v_pred = v_pred.reshape(100,100)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "            error_v = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "\n",
    "            mode.print('l2 error: {:.2e}'.format(error_u))\n",
    "            mode.print('l2 error: {:.2e}'.format(error_v))\n",
    "\n",
    "       \n",
    "            mode.print(\"average lambda_bc\" , np.average(mode.adpative_constant_bcs_log))\n",
    "            mode.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close\n",
    "            mode.plot_grad()\n",
    "            mode.save_NN()\n",
    "            mode.plt_prediction( X , Y , X_star , u_ref , u_pred , v_ref , v_pred)\n",
    "\n",
    "\n",
    "        mode.print('elapsed: {:.2e}'.format(elapsed))\n",
    "        mode.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "        mode.print('Relative L2 error_v: {:.2e}'.format(error_v))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_v_list.append(error_v)\n",
    "\n",
    "    mode.print(\"\\n\\nMethod: \", mtd)\n",
    "    mode.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    mode.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    mode.print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(mode.dirname,\"\"+mtd+\"_Lid-driven-Cavity_\"+model+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Method: mini_batch\n",
    "\n",
    "average of time_list:5.3197e+03\n",
    "average of error_u_list:9.8587e-02\n",
    "average of error_v_list:2.3701e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l2 error: 9.86e-02\n",
      "l2 error: 2.37e-01\n",
      "average lambda_bc1.9133e+03\n",
      "average lambda_res1.0\n",
      "elapsed: 5.32e+03\n",
      "Relative L2 error_u: 9.86e-02\n",
      "Relative L2 error_v: 2.37e-01\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:5.3197e+03\n",
      "average of error_u_list:9.8587e-02\n",
      "average of error_v_list:2.3701e-01\n"
     ]
    }
   ],
   "source": [
    "# Relative error\n",
    "error_u = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "error_v = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "\n",
    "mode.print('l2 error: {:.2e}'.format(error_u))\n",
    "mode.print('l2 error: {:.2e}'.format(error_v))\n",
    "\n",
    "\n",
    "mode.print(\"average lambda_bc\" , np.average(mode.adpative_constant_bcs_log))\n",
    "mode.print(\"average lambda_res\" , str(1.0))\n",
    "# sess.close\n",
    "mode.plot_grad()\n",
    "# mode.save_NN()\n",
    "mode.plot_lambda()\n",
    "mode.plt_prediction( X , Y , X_star , u_ref , u_pred , v_ref , v_pred)\n",
    "\n",
    "\n",
    "mode.print('elapsed: {:.2e}'.format(elapsed))\n",
    "mode.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "mode.print('Relative L2 error_v: {:.2e}'.format(error_v))\n",
    "\n",
    "time_list.append(elapsed)\n",
    "error_u_list.append(error_u)\n",
    "error_v_list.append(error_v)\n",
    "\n",
    "mode.print(\"\\n\\nMethod: \", mtd)\n",
    "mode.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "mode.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "mode.print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "\n",
    "result_dict[mtd] = [time_list ,error_u_list ,error_v_list ]\n",
    "# scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(os.path.join(mode.dirname,\"\"+mtd+\"_Lid-driven-Cavity_\"+model+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "    from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "    ### Plot ###\n",
    "\n",
    "    # Exact solution & Predicted solution\n",
    "    # Exact soluton\n",
    "    U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "    F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "    # Predicted solution\n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "    F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "    titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $v(x)$' , 'Predicted $v(x)$' , 'Absolute error']\n",
    "    data = [U_star.T , U_pred ,  np.abs(U_star.T - U_pred) , F_star.T , F_pred ,  np.abs(F_star.T - F_pred) ]\n",
    "\n",
    "    fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "    grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                    label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                    cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                    cbar_size=\"5%\", cbar_pad=0.0)\n",
    "# CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "    minmax_list = []\n",
    "    for d in data:\n",
    "        # if(local):\n",
    "        #     minmax_list.append([np.min(d), np.max(d)])\n",
    "        # else:\n",
    "        minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "        # kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60), cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "    for ax, z, minmax, title in zip(grid, data, minmax_list, titles):\n",
    "    #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "        #pcfsets.append(pcf)\n",
    "        # if (timeStp == 0):\n",
    "        pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "        cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "        ax.cax.tick_params(labelsize=14.5)\n",
    "        ax.set_title(title, fontsize=14.5, pad=7)\n",
    "        ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "        ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "        ax.tick_params(labelsize=14.5)\n",
    "        ax.set_xlim(x1.min(), x1.max())\n",
    "        ax.set_ylim(x2.min(), x2.max())\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "    fig_1.set_size_inches(15, 10, True)\n",
    "    fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "    plt.close(\"all\" , )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_ref.flatten()[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.io\n",
    "\n",
    "mode = 'M2'\n",
    "mbbatch_size = 128\n",
    "ubatch_size = 5000\n",
    "bcbatch_size = 500\n",
    "iterations = 40000\n",
    "\n",
    "time_list = []\n",
    "error_u_list = []\n",
    "error_v_list = []\n",
    "error_p_list = []\n",
    "    \n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "##Mini Batch\n",
    "time_list = [1831.07 , ]\n",
    "error_u_list = [0.2112 , 0.]\n",
    "error_v_list = [0.3271 , 0.]\n",
    "error_p_list = [0.094 , 0.]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "\n",
    "##Full Batch\n",
    "time_list = [8048.86 , ]\n",
    "error_u_list = [ 0.0367 ,0. ]\n",
    "error_v_list = [0.0706 , 0.]\n",
    "error_p_list = [0.465 , 0.]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "\n",
    "scipy.io.savemat(\"./dataset/NS_model_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Plot ###\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "# Predictions\n",
    "psi_pred, p_pred = mode.predict_psi_p(X_star)\n",
    "u_pred, v_pred = mode.predict_uv(X_star)\n",
    "\n",
    "psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "# Reference\n",
    "u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "# Relative error\n",
    "error = np.linalg.norm(u_star - u_pred.T, 2) / np.linalg.norm(u_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "error = np.linalg.norm(v_star - v_pred.T, 2) / np.linalg.norm(v_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "error = np.linalg.norm(p_pred - p_star.T, 2) / np.linalg.norm(p_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "\n",
    "### Plot ###\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference solution & Prediceted solution\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "fig_1.add_subplot(1, 3, 1)\n",
    "plt.pcolor(X.T, Y.T, velocity_ref, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Reference Velocity')\n",
    "\n",
    "fig_1.add_subplot(1, 3, 2)\n",
    "plt.pcolor(x, Y, velocity_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predicted Velocity')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_1.add_subplot(1, 3, 3)\n",
    "plt.pcolor(X, Y, np.abs(velocity_star - velocity_ref.T), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Absolute Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ## Loss ##\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_bcs_log\n",
    "    \n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradients #\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "num_hidden_layers = len(layers) -1\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    \n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_{u_b}$')\n",
    "\n",
    "    # ax.get_legend().remove()\n",
    "    ax.set_xlim([-1.0, 1.0])\n",
    "    ax.set_ylim([0, 100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
