{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import griddata\n",
    "# from Klein_Gordon_model_tf import Sampler, Klein_Gordon\n",
    "import timeit\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "def u(x):\n",
    "    \"\"\"\n",
    "    :param x: x = (t, x)\n",
    "    \"\"\"\n",
    "    return x[:, 1:2] * np.cos(5 * np.pi * x[:, 0:1]) + (x[:, 0:1] * x[:, 1:2])**3\n",
    "\n",
    "def u_tt(x):\n",
    "    return - 25 * np.pi**2 * x[:, 1:2] * np.cos(5 * np.pi * x[:, 0:1]) + 6 * x[:,0:1] * x[:,1:2]**3\n",
    "\n",
    "def u_xx(x):\n",
    "    return np.zeros((x.shape[0], 1)) +  6 * x[:,1:2] * x[:,0:1]**3\n",
    "\n",
    "def f(x, alpha, beta, gamma, k):\n",
    "    return u_tt(x) + alpha * u_xx(x) + beta * u(x) + gamma * u(x)**k\n",
    "\n",
    "def operator(u, t, x, alpha, beta, gamma, k,  sigma_t=1.0, sigma_x=1.0):\n",
    "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    residual = u_tt + alpha * u_xx + beta * u + gamma * u**k\n",
    "    return residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "class Klein_Gordon:\n",
    "    # Initialize the class\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, mode, sess):\n",
    "            # mode = Klein_Gordon(layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, mode, sess)\n",
    "\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Klein_Gordon constant\n",
    "        self.alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "        self.beta = tf.constant(beta, dtype=tf.float32)\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "        self.k = tf.constant(k, dtype=tf.float32)\n",
    "\n",
    "\n",
    "        # Record stiff ratio\n",
    "        # self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive re-weighting constant\n",
    "        self.rate = 0.9\n",
    "        self.adaptive_constant_ics_val = np.array(1.0)\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # if model in ['M3', 'M4']:\n",
    "        #     # Initialize encoder weights and biases\n",
    "        #     self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        #     self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_f_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_f_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_f_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
    "\n",
    "        self.f_pred = self.net_u(self.t_f_tf, self.x_f_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
    "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
    "\n",
    "        # Boundary loss and Initial loss\n",
    "        self.loss_ic_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
    "        self.loss_ic_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred - self.u_bc1_tf))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred - self.u_bc2_tf))\n",
    "\n",
    "        self.loss_bcs =(self.loss_bc1 + self.loss_ic_u + self.loss_bc2)\n",
    "        self.loss_ics = ( self.loss_ic_u_t)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred - self.r_tf))\n",
    "\n",
    "        self.loss_f = tf.reduce_mean(tf.square(self.f_pred - self.u_f_tf))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.loss_f + self.loss_bcs+ self.loss_ics\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res ,  self.loss_f,  self.loss_ics , self.loss_bcs ] \n",
    "        self.loss_list = [\"loss\" , \"loss_res\", \"loss_f\" , \"loss_ics\", \"loss_bcs\"] \n",
    "        \n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_ics_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_ics = []\n",
    "        self.grad_bcs = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_f, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "            self.grad_ics.append(tf.gradients(self.loss_ics, self.weights[i])[0])\n",
    "\n",
    "        # Store the adaptive constant\n",
    "        self.mean_grad_ics_log = []\n",
    "        self.mean_grad_bcs_log = []\n",
    "        self.mean_grad_res_log = []\n",
    "\n",
    "        # Compute the adaptive constant\n",
    "        self.adaptive_constant_ics_list = []\n",
    "        self.adaptive_constant_bcs_list = []\n",
    "        \n",
    "        self.mean_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        self.mean_grad_ics_list = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.mean_grad_res_list.append(tf.reduce_mean(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "            self.mean_grad_ics_list.append(tf.reduce_mean(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "        self.mean_grad_res = tf.reduce_mean(tf.stack(self.mean_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.mean_grad_ics = tf.reduce_mean(tf.stack(self.mean_grad_ics_list))\n",
    "        \n",
    "        # self.mean_grad_bcs = self.mean_grad_res / self.mean_grad_bcs\n",
    "        # self.mean_grad_ics = self.mean_grad_res / self.mean_grad_ics\n",
    "\n",
    "        # # Stiff Ratio\n",
    "        # if self.stiff_ratio:\n",
    "        #     self.Hessian, self.Hessian_ics, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "        #     self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "        #     self.eigenvalues_ics, _ = tf.linalg.eigh(self.Hessian_ics)\n",
    "        #     self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "        #     self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "        #     self.eigenvalue_log = []\n",
    "        #     self.eigenvalue_ics_log = []\n",
    "        #     self.eigenvalue_bcs_log = []\n",
    "        #     self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_ics_value , grad_bcs_value, grad_res_value= self.sess.run([self.grad_ics[i],\n",
    "                                                                            self.grad_bcs[i],\n",
    "                                                                            self.grad_res[i]],\n",
    "                                                                            feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_ics(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_ics, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,\n",
    "                                                   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,\n",
    "                                          self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P),\n",
    "                      dtype='float32')\n",
    "        H_ics = tf.map_fn(self.get_Hv_ics, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "\n",
    "        return H, H_ics, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # if self.model in ['M3', 'M4']:\n",
    "    #     num_layers = len(self.layers)\n",
    "    #     encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "    #     encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "    #     for l in range(0, num_layers - 2):\n",
    "    #         W = self.weights[l]\n",
    "    #         b = self.biases[l]\n",
    "    #         H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "    #             tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "    #     W = self.weights[-1]\n",
    "    #     b = self.biases[-1]\n",
    "    #     H = tf.add(tf.matmul(H, W), b)\n",
    "    #     return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1))\n",
    "        return u\n",
    "\n",
    "    def net_u_t(self, t, x):\n",
    "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
    "        return u_t\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, t, x):\n",
    "        u = self.net_u(t, x)\n",
    "        residual = self.operator(u, t, x,  self.alpha, self.beta, self.gamma, self.k, self.sigma_t, self.sigma_x)\n",
    "        return residual\n",
    "\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "################################################################################################################\n",
    "\n",
    "    def train(self, nIter=10000):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        batch_size = 500\n",
    "        # Fetch boundary mini-batches\n",
    "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size)\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "\n",
    "        batch_size = 5000\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                    self.u_ics_tf: u_ics_batch,\n",
    "                    self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.r_tf: f_res_batch,\n",
    "                   self.t_f_tf: X_res_batch[:, 0:1], self.x_f_tf: X_res_batch[:, 1:2],\n",
    "                    self.u_f_tf: u(X_res_batch),\n",
    "                    self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val}\n",
    "\n",
    "        # Run the Tensorflow session to minimize the loss\n",
    "\n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_u, self.loss_res], tf_dict)\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                # Compute the adaptive constant\n",
    "                adaptive_constant_ics_val, adaptive_constant_bcs_val = self.sess.run( [self.mean_grad_ics, self.mean_grad_bcs], tf_dict)\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                # # Store loss and adaptive weights\n",
    "                # self.loss_u_log.append(loss_u_value)\n",
    "                # self.loss_r_log.append(loss_r_value)\n",
    "\n",
    "                # self.adaptive_constant_ics_log.append(self.adaptive_constant_ics_val)\n",
    "                # self.adaptive_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "                if it % 1000 == 0:\n",
    "\n",
    "                    print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' %  (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "                    print(\"constant_ics_val: {:.3f}, constant_bcs_val: {:.3f}\".format(  self.adaptive_constant_ics_val,  self.adaptive_constant_bcs_val))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_ics, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                                         self.eigenvalues_ics,\n",
    "            #                                                                                         self.eigenvalues_bcs,\n",
    "            #                                                                                         self.eigenvalues_res], tf_dict)\n",
    "            #         self.eigenvalue_log.append(eigenvalues)\n",
    "            #         self.eigenvalue_ics_log.append(eigenvalues_bcs)\n",
    "            #         self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "            #         self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "################################################################################################################\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size)\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                       self.u_ics_tf: u_ics_batch,\n",
    "                       self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.r_tf: f_res_batch,\n",
    "                       self.t_f_tf: X_res_batch[:, 0:1], self.x_f_tf: X_res_batch[:, 1:2],\n",
    "                       self.u_f_tf: u(X_res_batch),\n",
    "                       self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val}\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                [loss ,  loss_res ,  loss_f,  loss_ics , loss_bcs ] = batch_losses\n",
    "                self.print('It: %d| loss: %.3e| loss_res: %.3e| loss_f: %.3e| loss_ics: %.3e| loss_bcs: %.3e| Time: %.2f' %(it, loss , loss_res , loss_f, loss_ics, loss_bcs , elapsed))\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                \n",
    "            if it % 100 == 0:\n",
    "\n",
    "                # Compute the adaptive constant\n",
    "                mean_grad_res , mean_grad_ics, mean_grad_bcs = self.sess.run( [self.mean_grad_res , self.mean_grad_ics, self.mean_grad_bcs], tf_dict)\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_ics_val = mean_grad_ics * (1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs_val = mean_grad_bcs * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                # # Store loss and adaptive weights\n",
    "                # self.loss_u_log.append(loss_u_value)\n",
    "                # self.loss_r_log.append(loss_r_value)\n",
    "\n",
    "                self.mean_grad_res_log.append(mean_grad_res)\n",
    "                self.mean_grad_ics_log.append(mean_grad_ics)\n",
    "                self.mean_grad_bcs_log.append(mean_grad_bcs)\n",
    "                \n",
    "\n",
    "                self.print(\"mean_grad_res: {:.3f}\".format( mean_grad_res))\n",
    "                self.print(\"mean_grad_ics: {:.3f}\".format( mean_grad_ics))\n",
    "                self.print(\"mean_grad_bcs: {:.3f}\".format( mean_grad_bcs))\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1  , grad_ics  = self.sess.run([ self.grad_res[i],self.grad_bcs[i],self.grad_ics[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_ics = self.dict_gradients_ics_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "            sns.distplot(gradients_ics, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{ics}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=3)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $f(x)$' , 'Predicted $f(x)$' , 'Absolute error']\n",
    "        data = [U_star , U_pred ,  np.abs(U_star - U_pred) , F_star , F_pred ,  np.abs(F_star - F_pred) ]\n",
    "        \n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        kwargs_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60),\n",
    "                cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, kwargs, minmax, title in zip(grid, data, kwargs_list, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "                #  print( z[timeStp,:,:])\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"time\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.mean_grad_res_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{res}}}$')\n",
    "        ax.semilogy(self.mean_grad_bcs_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.semilogy(self.mean_grad_ics_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{ic}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma ,k ,mode , stiff_ratio ,  X_star , u_star , f_star):\n",
    "\n",
    "\n",
    "    model = Klein_Gordon(layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter=40001 )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter=40001, batch_size=128)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_53421/2874740645.py:66: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_53421/2874740645.py:67: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_53421/2874740645.py:68: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_53421/2874740645.py:68: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_53421/2726919218.py:271: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_53421/2726919218.py:68: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 18:13:04.000881: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 18:13:04.024476: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2024-01-11 18:13:04.024928: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564de77c0df0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-11 18:13:04.024943: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-01-11 18:13:04.025673: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_53421/2726919218.py:125: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_53421/2726919218.py:127: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_53421/2726919218.py:193: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It: 0| loss: 1.527e+01| loss_res: 9.573e+03| loss_f: 1.348e+01| loss_ics: 5.676e-02| loss_bcs: 1.725e+00| Time: 1.37\n",
      "mean_grad_res: 0.050\n",
      "mean_grad_ics: 0.009\n",
      "mean_grad_bcs: 0.080\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100| loss: 1.690e+01| loss_res: 9.182e+03| loss_f: 1.546e+01| loss_ics: 5.985e-02| loss_bcs: 1.379e+00| Time: 0.01\n",
      "mean_grad_res: 0.157\n",
      "mean_grad_ics: 0.053\n",
      "mean_grad_bcs: 0.076\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200| loss: 1.284e+01| loss_res: 1.098e+04| loss_f: 1.110e+01| loss_ics: 1.957e-02| loss_bcs: 1.723e+00| Time: 0.01\n",
      "mean_grad_res: 0.113\n",
      "mean_grad_ics: 0.015\n",
      "mean_grad_bcs: 0.138\n",
      "It: 300| loss: 1.798e+01| loss_res: 1.137e+04| loss_f: 1.667e+01| loss_ics: 3.390e-02| loss_bcs: 1.277e+00| Time: 0.01\n",
      "mean_grad_res: 0.246\n",
      "mean_grad_ics: 0.013\n",
      "mean_grad_bcs: 0.109\n",
      "It: 400| loss: 1.128e+01| loss_res: 1.367e+04| loss_f: 1.006e+01| loss_ics: 2.959e-02| loss_bcs: 1.193e+00| Time: 0.01\n",
      "mean_grad_res: 0.198\n",
      "mean_grad_ics: 0.018\n",
      "mean_grad_bcs: 0.164\n",
      "It: 500| loss: 1.749e+01| loss_res: 3.919e+04| loss_f: 1.590e+01| loss_ics: 1.609e-02| loss_bcs: 1.568e+00| Time: 0.01\n",
      "mean_grad_res: 0.456\n",
      "mean_grad_ics: 0.011\n",
      "mean_grad_bcs: 0.341\n",
      "It: 600| loss: 1.070e+01| loss_res: 1.344e+05| loss_f: 9.283e+00| loss_ics: 1.501e-02| loss_bcs: 1.400e+00| Time: 0.01\n",
      "mean_grad_res: 0.255\n",
      "mean_grad_ics: 0.012\n",
      "mean_grad_bcs: 0.185\n",
      "It: 700| loss: 8.030e+00| loss_res: 1.065e+05| loss_f: 5.929e+00| loss_ics: 1.829e-02| loss_bcs: 2.083e+00| Time: 0.01\n",
      "mean_grad_res: 0.096\n",
      "mean_grad_ics: 0.026\n",
      "mean_grad_bcs: 0.836\n",
      "It: 800| loss: 4.859e+00| loss_res: 1.866e+05| loss_f: 3.707e+00| loss_ics: 1.441e-02| loss_bcs: 1.138e+00| Time: 0.01\n",
      "mean_grad_res: 0.262\n",
      "mean_grad_ics: 0.019\n",
      "mean_grad_bcs: 0.192\n",
      "It: 900| loss: 1.300e+01| loss_res: 3.129e+05| loss_f: 1.170e+01| loss_ics: 3.071e-02| loss_bcs: 1.278e+00| Time: 0.01\n",
      "mean_grad_res: 0.390\n",
      "mean_grad_ics: 0.035\n",
      "mean_grad_bcs: 0.370\n",
      "It: 1000| loss: 1.222e+01| loss_res: 7.848e+05| loss_f: 1.096e+01| loss_ics: 2.317e-02| loss_bcs: 1.240e+00| Time: 0.01\n",
      "mean_grad_res: 0.467\n",
      "mean_grad_ics: 0.021\n",
      "mean_grad_bcs: 0.377\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100| loss: 9.666e+00| loss_res: 3.911e+05| loss_f: 8.263e+00| loss_ics: 8.431e-03| loss_bcs: 1.394e+00| Time: 0.01\n",
      "mean_grad_res: 0.231\n",
      "mean_grad_ics: 0.007\n",
      "mean_grad_bcs: 0.324\n",
      "It: 1200| loss: 9.901e+00| loss_res: 1.224e+05| loss_f: 7.958e+00| loss_ics: 2.377e-02| loss_bcs: 1.919e+00| Time: 0.01\n",
      "mean_grad_res: 0.546\n",
      "mean_grad_ics: 0.038\n",
      "mean_grad_bcs: 0.612\n",
      "It: 1300| loss: 4.837e+00| loss_res: 6.658e+05| loss_f: 3.106e+00| loss_ics: 1.143e-02| loss_bcs: 1.720e+00| Time: 0.01\n",
      "mean_grad_res: 0.100\n",
      "mean_grad_ics: 0.009\n",
      "mean_grad_bcs: 1.116\n",
      "It: 1400| loss: 1.047e+01| loss_res: 5.190e+05| loss_f: 9.314e+00| loss_ics: 1.770e-02| loss_bcs: 1.134e+00| Time: 0.01\n",
      "mean_grad_res: 0.733\n",
      "mean_grad_ics: 0.018\n",
      "mean_grad_bcs: 0.726\n",
      "It: 1500| loss: 1.011e+01| loss_res: 7.499e+05| loss_f: 8.895e+00| loss_ics: 2.357e-02| loss_bcs: 1.190e+00| Time: 0.01\n",
      "mean_grad_res: 0.511\n",
      "mean_grad_ics: 0.051\n",
      "mean_grad_bcs: 0.468\n",
      "It: 1600| loss: 4.769e+00| loss_res: 8.708e+05| loss_f: 3.292e+00| loss_ics: 1.964e-02| loss_bcs: 1.457e+00| Time: 0.01\n",
      "mean_grad_res: 0.170\n",
      "mean_grad_ics: 0.025\n",
      "mean_grad_bcs: 2.960\n",
      "It: 1700| loss: 8.973e+00| loss_res: 6.622e+05| loss_f: 7.730e+00| loss_ics: 2.545e-02| loss_bcs: 1.217e+00| Time: 0.01\n",
      "mean_grad_res: 0.460\n",
      "mean_grad_ics: 0.017\n",
      "mean_grad_bcs: 0.397\n",
      "It: 1800| loss: 1.396e+01| loss_res: 4.002e+06| loss_f: 1.232e+01| loss_ics: 2.504e-02| loss_bcs: 1.607e+00| Time: 0.01\n",
      "mean_grad_res: 0.861\n",
      "mean_grad_ics: 0.057\n",
      "mean_grad_bcs: 1.565\n",
      "It: 1900| loss: 5.775e+00| loss_res: 6.257e+06| loss_f: 4.759e+00| loss_ics: 1.739e-02| loss_bcs: 9.988e-01| Time: 0.01\n",
      "mean_grad_res: 0.369\n",
      "mean_grad_ics: 0.019\n",
      "mean_grad_bcs: 0.476\n",
      "It: 2000| loss: 1.620e+01| loss_res: 2.834e+06| loss_f: 1.514e+01| loss_ics: 2.999e-02| loss_bcs: 1.032e+00| Time: 0.01\n",
      "mean_grad_res: 0.887\n",
      "mean_grad_ics: 0.021\n",
      "mean_grad_bcs: 0.255\n",
      "It: 2100| loss: 8.018e+00| loss_res: 4.410e+06| loss_f: 6.844e+00| loss_ics: 5.859e-02| loss_bcs: 1.115e+00| Time: 0.01\n",
      "mean_grad_res: 0.327\n",
      "mean_grad_ics: 0.104\n",
      "mean_grad_bcs: 0.258\n",
      "It: 2200| loss: 1.124e+01| loss_res: 1.095e+07| loss_f: 1.047e+01| loss_ics: 2.515e-02| loss_bcs: 7.390e-01| Time: 0.01\n",
      "mean_grad_res: 1.278\n",
      "mean_grad_ics: 0.022\n",
      "mean_grad_bcs: 0.171\n",
      "It: 2300| loss: 7.484e+00| loss_res: 2.201e+06| loss_f: 6.475e+00| loss_ics: 1.572e-02| loss_bcs: 9.927e-01| Time: 0.01\n",
      "mean_grad_res: 0.243\n",
      "mean_grad_ics: 0.027\n",
      "mean_grad_bcs: 0.446\n",
      "It: 2400| loss: 8.837e+00| loss_res: 9.007e+06| loss_f: 7.925e+00| loss_ics: 2.104e-02| loss_bcs: 8.911e-01| Time: 0.01\n",
      "mean_grad_res: 0.219\n",
      "mean_grad_ics: 0.027\n",
      "mean_grad_bcs: 0.178\n",
      "It: 2500| loss: 1.338e+01| loss_res: 8.738e+06| loss_f: 1.183e+01| loss_ics: 4.109e-02| loss_bcs: 1.507e+00| Time: 0.01\n",
      "mean_grad_res: 0.431\n",
      "mean_grad_ics: 0.044\n",
      "mean_grad_bcs: 0.587\n",
      "It: 2600| loss: 9.457e+00| loss_res: 3.341e+06| loss_f: 8.441e+00| loss_ics: 6.735e-03| loss_bcs: 1.010e+00| Time: 0.01\n",
      "mean_grad_res: 0.560\n",
      "mean_grad_ics: 0.020\n",
      "mean_grad_bcs: 0.417\n",
      "It: 2700| loss: 9.008e+00| loss_res: 1.209e+07| loss_f: 7.240e+00| loss_ics: 2.265e-02| loss_bcs: 1.745e+00| Time: 0.01\n",
      "mean_grad_res: 0.699\n",
      "mean_grad_ics: 0.060\n",
      "mean_grad_bcs: 1.334\n",
      "It: 2800| loss: 1.234e+01| loss_res: 1.918e+08| loss_f: 1.150e+01| loss_ics: 9.893e-03| loss_bcs: 8.285e-01| Time: 0.01\n",
      "mean_grad_res: 6.281\n",
      "mean_grad_ics: 0.053\n",
      "mean_grad_bcs: 1.087\n",
      "It: 2900| loss: 8.847e+00| loss_res: 9.028e+05| loss_f: 7.919e+00| loss_ics: 1.641e-02| loss_bcs: 9.118e-01| Time: 0.01\n",
      "mean_grad_res: 0.303\n",
      "mean_grad_ics: 0.023\n",
      "mean_grad_bcs: 0.674\n",
      "It: 3000| loss: 8.387e+00| loss_res: 4.776e+07| loss_f: 7.615e+00| loss_ics: 1.438e-02| loss_bcs: 7.581e-01| Time: 0.01\n",
      "mean_grad_res: 0.383\n",
      "mean_grad_ics: 0.039\n",
      "mean_grad_bcs: 0.988\n",
      "It: 3100| loss: 6.403e+00| loss_res: 9.549e+06| loss_f: 5.622e+00| loss_ics: 7.066e-03| loss_bcs: 7.738e-01| Time: 0.01\n",
      "mean_grad_res: 0.399\n",
      "mean_grad_ics: 0.023\n",
      "mean_grad_bcs: 0.327\n",
      "It: 3200| loss: 1.001e+01| loss_res: 2.535e+08| loss_f: 8.827e+00| loss_ics: 4.656e-02| loss_bcs: 1.136e+00| Time: 0.01\n",
      "mean_grad_res: 0.738\n",
      "mean_grad_ics: 0.082\n",
      "mean_grad_bcs: 1.086\n",
      "It: 3300| loss: 5.774e+00| loss_res: 2.011e+05| loss_f: 4.510e+00| loss_ics: 6.258e-02| loss_bcs: 1.201e+00| Time: 0.01\n",
      "mean_grad_res: 0.203\n",
      "mean_grad_ics: 0.098\n",
      "mean_grad_bcs: 0.945\n",
      "It: 3400| loss: 1.239e+01| loss_res: 7.927e+07| loss_f: 1.168e+01| loss_ics: 1.910e-02| loss_bcs: 6.947e-01| Time: 0.01\n",
      "mean_grad_res: 2.269\n",
      "mean_grad_ics: 0.091\n",
      "mean_grad_bcs: 0.269\n",
      "It: 3500| loss: 5.135e+00| loss_res: 1.044e+07| loss_f: 4.423e+00| loss_ics: 3.918e-02| loss_bcs: 6.735e-01| Time: 0.01\n",
      "mean_grad_res: 0.334\n",
      "mean_grad_ics: 0.124\n",
      "mean_grad_bcs: 0.591\n",
      "It: 3600| loss: 3.624e+00| loss_res: 1.001e+07| loss_f: 2.711e+00| loss_ics: 9.094e-03| loss_bcs: 9.037e-01| Time: 0.01\n",
      "mean_grad_res: 0.140\n",
      "mean_grad_ics: 0.065\n",
      "mean_grad_bcs: 0.747\n",
      "It: 3700| loss: 3.555e+00| loss_res: 2.339e+06| loss_f: 2.694e+00| loss_ics: 1.320e-02| loss_bcs: 8.479e-01| Time: 0.01\n",
      "mean_grad_res: 0.205\n",
      "mean_grad_ics: 0.070\n",
      "mean_grad_bcs: 0.637\n",
      "It: 3800| loss: 4.764e+00| loss_res: 1.549e+07| loss_f: 3.961e+00| loss_ics: 1.447e-02| loss_bcs: 7.888e-01| Time: 0.01\n",
      "mean_grad_res: 0.114\n",
      "mean_grad_ics: 0.047\n",
      "mean_grad_bcs: 0.328\n",
      "It: 3900| loss: 7.554e+00| loss_res: 3.832e+06| loss_f: 6.686e+00| loss_ics: 3.522e-02| loss_bcs: 8.329e-01| Time: 0.01\n",
      "mean_grad_res: 0.372\n",
      "mean_grad_ics: 0.100\n",
      "mean_grad_bcs: 0.395\n",
      "It: 4000| loss: 8.293e+00| loss_res: 2.146e+07| loss_f: 7.365e+00| loss_ics: 2.392e-01| loss_bcs: 6.881e-01| Time: 0.01\n",
      "mean_grad_res: 0.391\n",
      "mean_grad_ics: 0.304\n",
      "mean_grad_bcs: 0.280\n",
      "It: 4100| loss: 7.254e+00| loss_res: 3.454e+08| loss_f: 6.718e+00| loss_ics: 1.281e-02| loss_bcs: 5.237e-01| Time: 0.01\n",
      "mean_grad_res: 2.860\n",
      "mean_grad_ics: 0.049\n",
      "mean_grad_bcs: 0.728\n",
      "It: 4200| loss: 4.979e+00| loss_res: 1.294e+07| loss_f: 4.120e+00| loss_ics: 5.961e-02| loss_bcs: 7.995e-01| Time: 0.04\n",
      "mean_grad_res: 0.238\n",
      "mean_grad_ics: 0.223\n",
      "mean_grad_bcs: 1.264\n",
      "It: 4300| loss: 1.341e+01| loss_res: 3.044e+07| loss_f: 1.250e+01| loss_ics: 1.601e-01| loss_bcs: 7.516e-01| Time: 0.01\n",
      "mean_grad_res: 1.746\n",
      "mean_grad_ics: 0.578\n",
      "mean_grad_bcs: 0.314\n",
      "It: 4400| loss: 6.117e+00| loss_res: 7.365e+07| loss_f: 5.220e+00| loss_ics: 7.874e-02| loss_bcs: 8.186e-01| Time: 0.01\n",
      "mean_grad_res: 0.688\n",
      "mean_grad_ics: 0.272\n",
      "mean_grad_bcs: 0.662\n",
      "It: 4500| loss: 1.634e+01| loss_res: 5.397e+07| loss_f: 1.519e+01| loss_ics: 1.171e-01| loss_bcs: 1.026e+00| Time: 0.01\n",
      "mean_grad_res: 1.090\n",
      "mean_grad_ics: 0.484\n",
      "mean_grad_bcs: 0.933\n",
      "It: 4600| loss: 1.347e+01| loss_res: 9.022e+07| loss_f: 1.293e+01| loss_ics: 1.208e-02| loss_bcs: 5.302e-01| Time: 0.01\n",
      "mean_grad_res: 0.758\n",
      "mean_grad_ics: 0.051\n",
      "mean_grad_bcs: 0.271\n",
      "It: 4700| loss: 6.165e+00| loss_res: 9.123e+06| loss_f: 5.042e+00| loss_ics: 1.596e-02| loss_bcs: 1.107e+00| Time: 0.01\n",
      "mean_grad_res: 0.343\n",
      "mean_grad_ics: 0.109\n",
      "mean_grad_bcs: 1.616\n",
      "It: 4800| loss: 7.075e+00| loss_res: 1.684e+07| loss_f: 4.720e+00| loss_ics: 1.488e-01| loss_bcs: 2.207e+00| Time: 0.01\n",
      "mean_grad_res: 0.349\n",
      "mean_grad_ics: 0.236\n",
      "mean_grad_bcs: 1.309\n",
      "It: 4900| loss: 4.311e+00| loss_res: 3.989e+08| loss_f: 3.562e+00| loss_ics: 2.337e-02| loss_bcs: 7.263e-01| Time: 0.05\n",
      "mean_grad_res: 1.732\n",
      "mean_grad_ics: 0.205\n",
      "mean_grad_bcs: 1.752\n",
      "It: 5000| loss: 4.150e+00| loss_res: 1.072e+08| loss_f: 3.398e+00| loss_ics: 4.523e-02| loss_bcs: 7.064e-01| Time: 0.01\n",
      "mean_grad_res: 0.344\n",
      "mean_grad_ics: 0.601\n",
      "mean_grad_bcs: 0.169\n",
      "It: 5100| loss: 7.343e+00| loss_res: 6.485e+07| loss_f: 6.480e+00| loss_ics: 1.614e-01| loss_bcs: 7.012e-01| Time: 0.01\n",
      "mean_grad_res: 0.593\n",
      "mean_grad_ics: 0.671\n",
      "mean_grad_bcs: 0.929\n",
      "It: 5200| loss: 6.572e+00| loss_res: 8.898e+06| loss_f: 5.771e+00| loss_ics: 3.774e-02| loss_bcs: 7.625e-01| Time: 0.01\n",
      "mean_grad_res: 0.687\n",
      "mean_grad_ics: 0.455\n",
      "mean_grad_bcs: 1.024\n",
      "It: 5300| loss: 5.863e+00| loss_res: 6.698e+07| loss_f: 5.162e+00| loss_ics: 6.020e-02| loss_bcs: 6.402e-01| Time: 0.01\n",
      "mean_grad_res: 0.501\n",
      "mean_grad_ics: 0.113\n",
      "mean_grad_bcs: 1.193\n",
      "It: 5400| loss: 8.404e+00| loss_res: 1.193e+08| loss_f: 7.694e+00| loss_ics: 4.947e-02| loss_bcs: 6.606e-01| Time: 0.01\n",
      "mean_grad_res: 0.753\n",
      "mean_grad_ics: 0.223\n",
      "mean_grad_bcs: 0.907\n",
      "It: 5500| loss: 8.971e+00| loss_res: 1.180e+08| loss_f: 8.313e+00| loss_ics: 5.814e-02| loss_bcs: 6.006e-01| Time: 0.01\n",
      "mean_grad_res: 0.250\n",
      "mean_grad_ics: 0.093\n",
      "mean_grad_bcs: 0.255\n",
      "It: 5600| loss: 2.136e+00| loss_res: 2.445e+07| loss_f: 1.346e+00| loss_ics: 2.560e-02| loss_bcs: 7.644e-01| Time: 0.01\n",
      "mean_grad_res: 0.260\n",
      "mean_grad_ics: 0.289\n",
      "mean_grad_bcs: 0.664\n",
      "It: 5700| loss: 9.744e+00| loss_res: 3.139e+06| loss_f: 8.300e+00| loss_ics: 6.207e-01| loss_bcs: 8.238e-01| Time: 0.01\n",
      "mean_grad_res: 0.788\n",
      "mean_grad_ics: 0.424\n",
      "mean_grad_bcs: 1.006\n",
      "It: 5800| loss: 6.916e+00| loss_res: 1.456e+07| loss_f: 6.117e+00| loss_ics: 8.169e-02| loss_bcs: 7.175e-01| Time: 0.01\n",
      "mean_grad_res: 0.333\n",
      "mean_grad_ics: 0.370\n",
      "mean_grad_bcs: 0.457\n",
      "It: 5900| loss: 6.368e+00| loss_res: 8.558e+07| loss_f: 5.448e+00| loss_ics: 1.534e-02| loss_bcs: 9.053e-01| Time: 0.01\n",
      "mean_grad_res: 0.576\n",
      "mean_grad_ics: 0.140\n",
      "mean_grad_bcs: 3.306\n",
      "It: 6000| loss: 1.540e+01| loss_res: 2.129e+06| loss_f: 1.472e+01| loss_ics: 1.069e-01| loss_bcs: 5.749e-01| Time: 0.01\n",
      "mean_grad_res: 0.784\n",
      "mean_grad_ics: 0.630\n",
      "mean_grad_bcs: 0.630\n",
      "It: 6100| loss: 6.632e+00| loss_res: 5.785e+07| loss_f: 5.720e+00| loss_ics: 1.512e-01| loss_bcs: 7.609e-01| Time: 0.01\n",
      "mean_grad_res: 0.687\n",
      "mean_grad_ics: 0.683\n",
      "mean_grad_bcs: 0.600\n",
      "It: 6200| loss: 3.443e+00| loss_res: 8.874e+07| loss_f: 2.701e+00| loss_ics: 1.626e-01| loss_bcs: 5.798e-01| Time: 0.01\n",
      "mean_grad_res: 0.425\n",
      "mean_grad_ics: 1.253\n",
      "mean_grad_bcs: 0.423\n",
      "It: 6300| loss: 7.432e+00| loss_res: 4.237e+06| loss_f: 6.996e+00| loss_ics: 3.079e-02| loss_bcs: 4.059e-01| Time: 0.01\n",
      "mean_grad_res: 0.746\n",
      "mean_grad_ics: 0.644\n",
      "mean_grad_bcs: 0.488\n",
      "It: 6400| loss: 1.196e+00| loss_res: 4.140e+07| loss_f: 6.570e-01| loss_ics: 5.989e-02| loss_bcs: 4.792e-01| Time: 0.01\n",
      "mean_grad_res: 0.120\n",
      "mean_grad_ics: 1.020\n",
      "mean_grad_bcs: 0.691\n",
      "It: 6500| loss: 6.812e+00| loss_res: 2.116e+07| loss_f: 5.949e+00| loss_ics: 4.239e-01| loss_bcs: 4.394e-01| Time: 0.01\n",
      "mean_grad_res: 0.234\n",
      "mean_grad_ics: 0.879\n",
      "mean_grad_bcs: 0.964\n",
      "It: 6600| loss: 6.039e+00| loss_res: 1.040e+07| loss_f: 5.513e+00| loss_ics: 2.638e-02| loss_bcs: 4.993e-01| Time: 0.01\n",
      "mean_grad_res: 0.829\n",
      "mean_grad_ics: 1.516\n",
      "mean_grad_bcs: 1.380\n",
      "It: 6700| loss: 6.799e+00| loss_res: 2.340e+08| loss_f: 6.218e+00| loss_ics: 6.488e-02| loss_bcs: 5.159e-01| Time: 0.01\n",
      "mean_grad_res: 4.549\n",
      "mean_grad_ics: 2.009\n",
      "mean_grad_bcs: 1.042\n",
      "It: 6800| loss: 4.599e+00| loss_res: 3.342e+07| loss_f: 4.054e+00| loss_ics: 4.088e-02| loss_bcs: 5.037e-01| Time: 0.01\n",
      "mean_grad_res: 0.505\n",
      "mean_grad_ics: 1.047\n",
      "mean_grad_bcs: 0.386\n",
      "It: 6900| loss: 6.591e+00| loss_res: 1.196e+08| loss_f: 6.175e+00| loss_ics: 3.032e-02| loss_bcs: 3.866e-01| Time: 0.01\n",
      "mean_grad_res: 1.194\n",
      "mean_grad_ics: 1.188\n",
      "mean_grad_bcs: 0.449\n",
      "It: 7000| loss: 6.252e+00| loss_res: 6.311e+07| loss_f: 5.662e+00| loss_ics: 4.636e-02| loss_bcs: 5.443e-01| Time: 0.01\n",
      "mean_grad_res: 0.854\n",
      "mean_grad_ics: 1.800\n",
      "mean_grad_bcs: 7.066\n",
      "It: 7100| loss: 1.025e+01| loss_res: 1.233e+07| loss_f: 9.492e+00| loss_ics: 5.298e-02| loss_bcs: 7.077e-01| Time: 0.01\n",
      "mean_grad_res: 0.723\n",
      "mean_grad_ics: 0.280\n",
      "mean_grad_bcs: 0.966\n",
      "It: 7200| loss: 9.985e+00| loss_res: 4.773e+07| loss_f: 9.407e+00| loss_ics: 5.826e-02| loss_bcs: 5.198e-01| Time: 0.01\n",
      "mean_grad_res: 1.264\n",
      "mean_grad_ics: 3.558\n",
      "mean_grad_bcs: 0.977\n",
      "It: 7300| loss: 2.897e+00| loss_res: 2.998e+06| loss_f: 2.273e+00| loss_ics: 2.070e-01| loss_bcs: 4.162e-01| Time: 0.01\n",
      "mean_grad_res: 0.532\n",
      "mean_grad_ics: 1.318\n",
      "mean_grad_bcs: 1.351\n",
      "It: 7400| loss: 7.688e+00| loss_res: 5.322e+07| loss_f: 7.045e+00| loss_ics: 1.792e-01| loss_bcs: 4.640e-01| Time: 0.01\n",
      "mean_grad_res: 0.699\n",
      "mean_grad_ics: 1.515\n",
      "mean_grad_bcs: 0.491\n",
      "It: 7500| loss: 3.154e+00| loss_res: 3.652e+08| loss_f: 2.593e+00| loss_ics: 2.003e-01| loss_bcs: 3.613e-01| Time: 0.01\n",
      "mean_grad_res: 0.806\n",
      "mean_grad_ics: 1.701\n",
      "mean_grad_bcs: 0.394\n",
      "It: 7600| loss: 6.964e+00| loss_res: 2.733e+06| loss_f: 5.952e+00| loss_ics: 3.312e-02| loss_bcs: 9.790e-01| Time: 0.01\n",
      "mean_grad_res: 0.319\n",
      "mean_grad_ics: 2.251\n",
      "mean_grad_bcs: 2.664\n",
      "It: 7700| loss: 7.828e+00| loss_res: 2.917e+07| loss_f: 7.251e+00| loss_ics: 1.772e-01| loss_bcs: 3.997e-01| Time: 0.01\n",
      "mean_grad_res: 0.843\n",
      "mean_grad_ics: 6.684\n",
      "mean_grad_bcs: 0.476\n",
      "It: 7800| loss: 2.695e+00| loss_res: 2.129e+07| loss_f: 2.095e+00| loss_ics: 1.139e-01| loss_bcs: 4.863e-01| Time: 0.01\n",
      "mean_grad_res: 0.334\n",
      "mean_grad_ics: 0.778\n",
      "mean_grad_bcs: 3.168\n",
      "It: 7900| loss: 1.037e+01| loss_res: 7.506e+08| loss_f: 9.921e+00| loss_ics: 1.228e-01| loss_bcs: 3.222e-01| Time: 0.01\n",
      "mean_grad_res: 5.213\n",
      "mean_grad_ics: 1.684\n",
      "mean_grad_bcs: 0.650\n",
      "It: 8000| loss: 2.707e+00| loss_res: 2.762e+07| loss_f: 2.259e+00| loss_ics: 7.924e-02| loss_bcs: 3.684e-01| Time: 0.01\n",
      "mean_grad_res: 0.216\n",
      "mean_grad_ics: 0.214\n",
      "mean_grad_bcs: 0.656\n",
      "It: 8100| loss: 6.983e+00| loss_res: 4.077e+08| loss_f: 6.295e+00| loss_ics: 7.991e-02| loss_bcs: 6.078e-01| Time: 0.01\n",
      "mean_grad_res: 0.778\n",
      "mean_grad_ics: 0.939\n",
      "mean_grad_bcs: 3.111\n",
      "It: 8200| loss: 2.832e+00| loss_res: 5.327e+08| loss_f: 2.443e+00| loss_ics: 4.288e-02| loss_bcs: 3.459e-01| Time: 0.01\n",
      "mean_grad_res: 1.323\n",
      "mean_grad_ics: 3.866\n",
      "mean_grad_bcs: 1.056\n",
      "It: 8300| loss: 8.316e+00| loss_res: 7.166e+07| loss_f: 7.981e+00| loss_ics: 2.395e-02| loss_bcs: 3.112e-01| Time: 0.01\n",
      "mean_grad_res: 1.211\n",
      "mean_grad_ics: 1.200\n",
      "mean_grad_bcs: 0.152\n",
      "It: 8400| loss: 3.271e+00| loss_res: 1.258e+07| loss_f: 2.783e+00| loss_ics: 1.547e-02| loss_bcs: 4.729e-01| Time: 0.01\n",
      "mean_grad_res: 0.928\n",
      "mean_grad_ics: 2.260\n",
      "mean_grad_bcs: 1.052\n",
      "It: 8500| loss: 6.739e+00| loss_res: 5.914e+08| loss_f: 5.412e+00| loss_ics: 8.821e-01| loss_bcs: 4.451e-01| Time: 0.01\n",
      "mean_grad_res: 3.209\n",
      "mean_grad_ics: 2.325\n",
      "mean_grad_bcs: 0.334\n",
      "It: 8600| loss: 9.189e+00| loss_res: 2.719e+08| loss_f: 8.851e+00| loss_ics: 7.409e-02| loss_bcs: 2.635e-01| Time: 0.01\n",
      "mean_grad_res: 1.333\n",
      "mean_grad_ics: 0.987\n",
      "mean_grad_bcs: 0.392\n",
      "It: 8700| loss: 7.453e+00| loss_res: 2.866e+09| loss_f: 6.906e+00| loss_ics: 4.602e-02| loss_bcs: 5.009e-01| Time: 0.01\n",
      "mean_grad_res: 1.776\n",
      "mean_grad_ics: 1.013\n",
      "mean_grad_bcs: 3.686\n",
      "It: 8800| loss: 1.381e+01| loss_res: 2.191e+07| loss_f: 1.336e+01| loss_ics: 9.992e-02| loss_bcs: 3.556e-01| Time: 0.01\n",
      "mean_grad_res: 1.006\n",
      "mean_grad_ics: 3.093\n",
      "mean_grad_bcs: 0.320\n",
      "It: 8900| loss: 3.520e+00| loss_res: 4.053e+07| loss_f: 3.011e+00| loss_ics: 1.365e-01| loss_bcs: 3.728e-01| Time: 0.01\n",
      "mean_grad_res: 0.759\n",
      "mean_grad_ics: 5.378\n",
      "mean_grad_bcs: 0.566\n",
      "It: 9000| loss: 2.096e+00| loss_res: 1.035e+07| loss_f: 1.284e+00| loss_ics: 1.264e-01| loss_bcs: 6.858e-01| Time: 0.01\n",
      "mean_grad_res: 0.297\n",
      "mean_grad_ics: 0.300\n",
      "mean_grad_bcs: 1.484\n",
      "It: 9100| loss: 4.196e+00| loss_res: 1.165e+07| loss_f: 3.805e+00| loss_ics: 6.193e-02| loss_bcs: 3.290e-01| Time: 0.01\n",
      "mean_grad_res: 0.832\n",
      "mean_grad_ics: 2.175\n",
      "mean_grad_bcs: 0.609\n",
      "It: 9200| loss: 6.630e+00| loss_res: 9.551e+07| loss_f: 6.298e+00| loss_ics: 5.790e-02| loss_bcs: 2.746e-01| Time: 0.01\n",
      "mean_grad_res: 1.475\n",
      "mean_grad_ics: 1.285\n",
      "mean_grad_bcs: 0.880\n",
      "It: 9300| loss: 7.062e+00| loss_res: 1.338e+07| loss_f: 6.378e+00| loss_ics: 2.596e-01| loss_bcs: 4.244e-01| Time: 0.01\n",
      "mean_grad_res: 0.393\n",
      "mean_grad_ics: 2.715\n",
      "mean_grad_bcs: 0.323\n",
      "It: 9400| loss: 5.119e+00| loss_res: 4.603e+08| loss_f: 4.525e+00| loss_ics: 2.737e-01| loss_bcs: 3.205e-01| Time: 0.01\n",
      "mean_grad_res: 1.792\n",
      "mean_grad_ics: 1.123\n",
      "mean_grad_bcs: 0.282\n",
      "It: 9500| loss: 5.466e+00| loss_res: 2.963e+07| loss_f: 4.911e+00| loss_ics: 1.931e-01| loss_bcs: 3.613e-01| Time: 0.01\n",
      "mean_grad_res: 0.541\n",
      "mean_grad_ics: 0.494\n",
      "mean_grad_bcs: 0.600\n",
      "It: 9600| loss: 2.743e+00| loss_res: 2.098e+08| loss_f: 2.353e+00| loss_ics: 9.309e-02| loss_bcs: 2.970e-01| Time: 0.01\n",
      "mean_grad_res: 0.637\n",
      "mean_grad_ics: 0.935\n",
      "mean_grad_bcs: 0.368\n",
      "It: 9700| loss: 2.591e+00| loss_res: 9.559e+08| loss_f: 1.989e+00| loss_ics: 2.596e-01| loss_bcs: 3.418e-01| Time: 0.01\n",
      "mean_grad_res: 2.519\n",
      "mean_grad_ics: 1.999\n",
      "mean_grad_bcs: 0.627\n",
      "It: 9800| loss: 2.838e+00| loss_res: 3.041e+07| loss_f: 2.481e+00| loss_ics: 5.364e-02| loss_bcs: 3.033e-01| Time: 0.01\n",
      "mean_grad_res: 0.846\n",
      "mean_grad_ics: 0.973\n",
      "mean_grad_bcs: 0.553\n",
      "It: 9900| loss: 7.926e+00| loss_res: 4.511e+09| loss_f: 7.566e+00| loss_ics: 3.327e-02| loss_bcs: 3.267e-01| Time: 0.01\n",
      "mean_grad_res: 9.174\n",
      "mean_grad_ics: 2.540\n",
      "mean_grad_bcs: 0.859\n",
      "It: 10000| loss: 4.227e+00| loss_res: 4.878e+07| loss_f: 3.606e+00| loss_ics: 1.018e-01| loss_bcs: 5.184e-01| Time: 0.01\n",
      "mean_grad_res: 0.706\n",
      "mean_grad_ics: 3.912\n",
      "mean_grad_bcs: 1.180\n",
      "It: 10100| loss: 5.684e+00| loss_res: 1.321e+08| loss_f: 5.331e+00| loss_ics: 9.152e-02| loss_bcs: 2.607e-01| Time: 0.01\n",
      "mean_grad_res: 1.236\n",
      "mean_grad_ics: 3.766\n",
      "mean_grad_bcs: 0.459\n",
      "It: 10200| loss: 6.693e+00| loss_res: 6.554e+08| loss_f: 5.077e+00| loss_ics: 9.224e-01| loss_bcs: 6.936e-01| Time: 0.01\n",
      "mean_grad_res: 0.452\n",
      "mean_grad_ics: 7.200\n",
      "mean_grad_bcs: 2.889\n",
      "It: 10300| loss: 8.022e+00| loss_res: 9.125e+06| loss_f: 7.410e+00| loss_ics: 1.127e-01| loss_bcs: 4.998e-01| Time: 0.01\n",
      "mean_grad_res: 0.594\n",
      "mean_grad_ics: 2.233\n",
      "mean_grad_bcs: 0.800\n",
      "It: 10400| loss: 5.483e+00| loss_res: 2.273e+08| loss_f: 5.085e+00| loss_ics: 4.514e-02| loss_bcs: 3.527e-01| Time: 0.01\n",
      "mean_grad_res: 4.452\n",
      "mean_grad_ics: 2.124\n",
      "mean_grad_bcs: 2.052\n",
      "It: 10500| loss: 1.591e+00| loss_res: 9.971e+07| loss_f: 1.311e+00| loss_ics: 4.650e-02| loss_bcs: 2.331e-01| Time: 0.01\n",
      "mean_grad_res: 0.414\n",
      "mean_grad_ics: 2.516\n",
      "mean_grad_bcs: 0.343\n",
      "It: 10600| loss: 1.191e+01| loss_res: 8.124e+08| loss_f: 1.148e+01| loss_ics: 6.422e-02| loss_bcs: 3.679e-01| Time: 0.01\n",
      "mean_grad_res: 1.562\n",
      "mean_grad_ics: 2.437\n",
      "mean_grad_bcs: 0.512\n",
      "It: 10700| loss: 6.583e+00| loss_res: 2.500e+09| loss_f: 6.210e+00| loss_ics: 8.934e-02| loss_bcs: 2.833e-01| Time: 0.01\n",
      "mean_grad_res: 2.666\n",
      "mean_grad_ics: 1.435\n",
      "mean_grad_bcs: 0.548\n",
      "It: 10800| loss: 1.611e+01| loss_res: 5.820e+08| loss_f: 1.542e+01| loss_ics: 2.690e-01| loss_bcs: 4.209e-01| Time: 0.01\n",
      "mean_grad_res: 2.589\n",
      "mean_grad_ics: 1.982\n",
      "mean_grad_bcs: 0.680\n",
      "It: 10900| loss: 4.370e+00| loss_res: 4.118e+07| loss_f: 4.014e+00| loss_ics: 2.738e-02| loss_bcs: 3.281e-01| Time: 0.01\n",
      "mean_grad_res: 0.668\n",
      "mean_grad_ics: 8.082\n",
      "mean_grad_bcs: 0.376\n",
      "It: 11000| loss: 3.035e+00| loss_res: 1.786e+08| loss_f: 2.517e+00| loss_ics: 3.845e-02| loss_bcs: 4.791e-01| Time: 0.01\n",
      "mean_grad_res: 0.234\n",
      "mean_grad_ics: 1.707\n",
      "mean_grad_bcs: 1.307\n",
      "It: 11100| loss: 2.052e+00| loss_res: 1.371e+09| loss_f: 1.499e+00| loss_ics: 2.712e-01| loss_bcs: 2.815e-01| Time: 0.01\n",
      "mean_grad_res: 1.331\n",
      "mean_grad_ics: 6.917\n",
      "mean_grad_bcs: 1.003\n",
      "It: 11200| loss: 2.224e+00| loss_res: 2.634e+07| loss_f: 1.999e+00| loss_ics: 2.041e-02| loss_bcs: 2.049e-01| Time: 0.01\n",
      "mean_grad_res: 1.035\n",
      "mean_grad_ics: 0.162\n",
      "mean_grad_bcs: 0.207\n",
      "It: 11300| loss: 1.083e+00| loss_res: 1.295e+06| loss_f: 7.740e-01| loss_ics: 6.287e-02| loss_bcs: 2.461e-01| Time: 0.01\n",
      "mean_grad_res: 0.130\n",
      "mean_grad_ics: 1.852\n",
      "mean_grad_bcs: 0.710\n",
      "It: 11400| loss: 3.489e+00| loss_res: 4.203e+09| loss_f: 2.967e+00| loss_ics: 6.476e-02| loss_bcs: 4.569e-01| Time: 0.01\n",
      "mean_grad_res: 3.296\n",
      "mean_grad_ics: 2.987\n",
      "mean_grad_bcs: 1.097\n",
      "It: 11500| loss: 2.720e+00| loss_res: 1.687e+06| loss_f: 2.310e+00| loss_ics: 3.328e-02| loss_bcs: 3.772e-01| Time: 0.01\n",
      "mean_grad_res: 0.900\n",
      "mean_grad_ics: 2.368\n",
      "mean_grad_bcs: 0.653\n",
      "It: 11600| loss: 4.105e+00| loss_res: 9.295e+08| loss_f: 3.748e+00| loss_ics: 9.453e-02| loss_bcs: 2.631e-01| Time: 0.01\n",
      "mean_grad_res: 1.619\n",
      "mean_grad_ics: 4.708\n",
      "mean_grad_bcs: 0.222\n",
      "It: 11700| loss: 4.488e+00| loss_res: 1.849e+07| loss_f: 4.214e+00| loss_ics: 2.670e-02| loss_bcs: 2.466e-01| Time: 0.02\n",
      "mean_grad_res: 0.878\n",
      "mean_grad_ics: 3.770\n",
      "mean_grad_bcs: 0.323\n",
      "It: 11800| loss: 3.069e+00| loss_res: 1.331e+08| loss_f: 2.641e+00| loss_ics: 1.574e-01| loss_bcs: 2.707e-01| Time: 0.01\n",
      "mean_grad_res: 1.187\n",
      "mean_grad_ics: 3.161\n",
      "mean_grad_bcs: 0.514\n",
      "It: 11900| loss: 2.417e+00| loss_res: 7.167e+08| loss_f: 1.984e+00| loss_ics: 1.935e-01| loss_bcs: 2.393e-01| Time: 0.01\n",
      "mean_grad_res: 1.460\n",
      "mean_grad_ics: 9.359\n",
      "mean_grad_bcs: 0.911\n",
      "It: 12000| loss: 1.134e+01| loss_res: 2.606e+08| loss_f: 1.100e+01| loss_ics: 7.289e-02| loss_bcs: 2.653e-01| Time: 0.01\n",
      "mean_grad_res: 0.887\n",
      "mean_grad_ics: 0.768\n",
      "mean_grad_bcs: 0.524\n",
      "It: 12100| loss: 8.288e+00| loss_res: 8.574e+08| loss_f: 7.800e+00| loss_ics: 2.253e-01| loss_bcs: 2.622e-01| Time: 0.01\n",
      "mean_grad_res: 2.207\n",
      "mean_grad_ics: 2.446\n",
      "mean_grad_bcs: 1.119\n",
      "It: 12200| loss: 2.765e+00| loss_res: 1.017e+07| loss_f: 1.978e+00| loss_ics: 3.796e-01| loss_bcs: 4.072e-01| Time: 0.01\n",
      "mean_grad_res: 0.260\n",
      "mean_grad_ics: 9.952\n",
      "mean_grad_bcs: 0.799\n",
      "It: 12300| loss: 8.775e+00| loss_res: 6.028e+07| loss_f: 8.367e+00| loss_ics: 7.911e-02| loss_bcs: 3.284e-01| Time: 0.01\n",
      "mean_grad_res: 1.549\n",
      "mean_grad_ics: 2.291\n",
      "mean_grad_bcs: 0.540\n",
      "It: 12400| loss: 4.908e+00| loss_res: 5.253e+06| loss_f: 4.387e+00| loss_ics: 3.325e-02| loss_bcs: 4.881e-01| Time: 0.06\n",
      "mean_grad_res: 1.693\n",
      "mean_grad_ics: 4.393\n",
      "mean_grad_bcs: 3.872\n",
      "It: 12500| loss: 7.663e+00| loss_res: 2.917e+09| loss_f: 7.201e+00| loss_ics: 4.204e-02| loss_bcs: 4.193e-01| Time: 0.02\n",
      "mean_grad_res: 8.385\n",
      "mean_grad_ics: 1.582\n",
      "mean_grad_bcs: 3.155\n",
      "It: 12600| loss: 7.360e+00| loss_res: 1.441e+09| loss_f: 7.002e+00| loss_ics: 2.123e-02| loss_bcs: 3.364e-01| Time: 0.01\n",
      "mean_grad_res: 3.246\n",
      "mean_grad_ics: 3.823\n",
      "mean_grad_bcs: 0.590\n",
      "It: 12700| loss: 6.130e+00| loss_res: 1.140e+08| loss_f: 5.835e+00| loss_ics: 2.960e-02| loss_bcs: 2.652e-01| Time: 0.02\n",
      "mean_grad_res: 0.675\n",
      "mean_grad_ics: 0.610\n",
      "mean_grad_bcs: 0.844\n",
      "It: 12800| loss: 2.210e+00| loss_res: 6.401e+08| loss_f: 1.860e+00| loss_ics: 2.936e-02| loss_bcs: 3.204e-01| Time: 0.01\n",
      "mean_grad_res: 0.368\n",
      "mean_grad_ics: 1.827\n",
      "mean_grad_bcs: 1.410\n",
      "It: 12900| loss: 6.862e+00| loss_res: 6.922e+07| loss_f: 6.339e+00| loss_ics: 1.873e-01| loss_bcs: 3.351e-01| Time: 0.01\n",
      "mean_grad_res: 2.641\n",
      "mean_grad_ics: 8.239\n",
      "mean_grad_bcs: 3.116\n",
      "It: 13000| loss: 4.535e+00| loss_res: 1.666e+07| loss_f: 4.228e+00| loss_ics: 4.584e-02| loss_bcs: 2.612e-01| Time: 0.01\n",
      "mean_grad_res: 0.898\n",
      "mean_grad_ics: 6.417\n",
      "mean_grad_bcs: 1.587\n",
      "It: 13100| loss: 1.880e+00| loss_res: 7.966e+07| loss_f: 1.571e+00| loss_ics: 3.981e-02| loss_bcs: 2.691e-01| Time: 0.01\n",
      "mean_grad_res: 0.200\n",
      "mean_grad_ics: 1.740\n",
      "mean_grad_bcs: 1.658\n",
      "It: 13200| loss: 1.201e+00| loss_res: 1.737e+08| loss_f: 7.723e-01| loss_ics: 1.335e-01| loss_bcs: 2.951e-01| Time: 0.01\n",
      "mean_grad_res: 0.195\n",
      "mean_grad_ics: 3.913\n",
      "mean_grad_bcs: 1.023\n",
      "It: 13300| loss: 3.496e+00| loss_res: 6.998e+07| loss_f: 3.223e+00| loss_ics: 5.159e-02| loss_bcs: 2.211e-01| Time: 0.01\n",
      "mean_grad_res: 0.617\n",
      "mean_grad_ics: 2.299\n",
      "mean_grad_bcs: 0.753\n",
      "It: 13400| loss: 1.346e+01| loss_res: 1.514e+09| loss_f: 1.274e+01| loss_ics: 3.722e-01| loss_bcs: 3.540e-01| Time: 0.01\n",
      "mean_grad_res: 4.010\n",
      "mean_grad_ics: 8.272\n",
      "mean_grad_bcs: 0.486\n",
      "It: 13500| loss: 5.388e+00| loss_res: 1.688e+08| loss_f: 4.784e+00| loss_ics: 1.381e-01| loss_bcs: 4.663e-01| Time: 0.01\n",
      "mean_grad_res: 0.450\n",
      "mean_grad_ics: 5.550\n",
      "mean_grad_bcs: 3.216\n",
      "It: 13600| loss: 5.208e+00| loss_res: 1.624e+07| loss_f: 4.931e+00| loss_ics: 2.267e-02| loss_bcs: 2.541e-01| Time: 0.01\n",
      "mean_grad_res: 2.218\n",
      "mean_grad_ics: 3.531\n",
      "mean_grad_bcs: 0.676\n",
      "It: 13700| loss: 3.126e+00| loss_res: 9.192e+08| loss_f: 2.717e+00| loss_ics: 1.271e-01| loss_bcs: 2.815e-01| Time: 0.01\n",
      "mean_grad_res: 0.316\n",
      "mean_grad_ics: 5.125\n",
      "mean_grad_bcs: 0.995\n",
      "It: 13800| loss: 1.311e+00| loss_res: 2.388e+08| loss_f: 9.141e-01| loss_ics: 1.169e-01| loss_bcs: 2.804e-01| Time: 0.01\n",
      "mean_grad_res: 1.133\n",
      "mean_grad_ics: 2.976\n",
      "mean_grad_bcs: 0.912\n",
      "It: 13900| loss: 4.319e+00| loss_res: 3.247e+08| loss_f: 3.736e+00| loss_ics: 1.473e-01| loss_bcs: 4.359e-01| Time: 0.01\n",
      "mean_grad_res: 0.450\n",
      "mean_grad_ics: 7.027\n",
      "mean_grad_bcs: 2.765\n",
      "It: 14000| loss: 1.431e+00| loss_res: 5.249e+08| loss_f: 1.079e+00| loss_ics: 1.600e-01| loss_bcs: 1.916e-01| Time: 0.01\n",
      "mean_grad_res: 0.578\n",
      "mean_grad_ics: 4.900\n",
      "mean_grad_bcs: 0.832\n",
      "It: 14100| loss: 4.730e+00| loss_res: 1.132e+09| loss_f: 4.054e+00| loss_ics: 4.443e-01| loss_bcs: 2.322e-01| Time: 0.01\n",
      "mean_grad_res: 2.104\n",
      "mean_grad_ics: 19.737\n",
      "mean_grad_bcs: 0.541\n",
      "It: 14200| loss: 3.460e+00| loss_res: 3.408e+08| loss_f: 2.853e+00| loss_ics: 1.056e-01| loss_bcs: 5.010e-01| Time: 0.01\n",
      "mean_grad_res: 2.306\n",
      "mean_grad_ics: 6.249\n",
      "mean_grad_bcs: 1.987\n",
      "It: 14300| loss: 9.062e+00| loss_res: 4.147e+09| loss_f: 8.743e+00| loss_ics: 1.416e-01| loss_bcs: 1.772e-01| Time: 0.01\n",
      "mean_grad_res: 6.476\n",
      "mean_grad_ics: 3.823\n",
      "mean_grad_bcs: 0.636\n",
      "It: 14400| loss: 6.859e+00| loss_res: 2.484e+08| loss_f: 6.579e+00| loss_ics: 2.995e-02| loss_bcs: 2.506e-01| Time: 0.01\n",
      "mean_grad_res: 1.213\n",
      "mean_grad_ics: 4.620\n",
      "mean_grad_bcs: 1.593\n",
      "It: 14500| loss: 7.172e+00| loss_res: 2.758e+09| loss_f: 6.861e+00| loss_ics: 1.159e-01| loss_bcs: 1.949e-01| Time: 0.01\n",
      "mean_grad_res: 3.333\n",
      "mean_grad_ics: 3.396\n",
      "mean_grad_bcs: 0.528\n",
      "It: 14600| loss: 4.819e+00| loss_res: 1.354e+07| loss_f: 4.491e+00| loss_ics: 7.342e-02| loss_bcs: 2.546e-01| Time: 0.01\n",
      "mean_grad_res: 0.926\n",
      "mean_grad_ics: 7.981\n",
      "mean_grad_bcs: 0.722\n",
      "It: 14700| loss: 1.632e+00| loss_res: 3.234e+06| loss_f: 1.122e+00| loss_ics: 5.410e-02| loss_bcs: 4.562e-01| Time: 0.01\n",
      "mean_grad_res: 0.236\n",
      "mean_grad_ics: 2.190\n",
      "mean_grad_bcs: 6.811\n",
      "It: 14800| loss: 2.146e+00| loss_res: 3.148e+06| loss_f: 1.910e+00| loss_ics: 2.593e-02| loss_bcs: 2.104e-01| Time: 0.01\n",
      "mean_grad_res: 0.211\n",
      "mean_grad_ics: 1.491\n",
      "mean_grad_bcs: 0.389\n",
      "It: 14900| loss: 4.936e+00| loss_res: 3.071e+08| loss_f: 4.578e+00| loss_ics: 1.449e-01| loss_bcs: 2.135e-01| Time: 0.01\n",
      "mean_grad_res: 0.927\n",
      "mean_grad_ics: 1.878\n",
      "mean_grad_bcs: 0.725\n",
      "It: 15000| loss: 4.776e+00| loss_res: 4.145e+08| loss_f: 4.276e+00| loss_ics: 1.216e-01| loss_bcs: 3.781e-01| Time: 0.01\n",
      "mean_grad_res: 1.313\n",
      "mean_grad_ics: 3.064\n",
      "mean_grad_bcs: 0.952\n",
      "It: 15100| loss: 3.628e+00| loss_res: 1.901e+08| loss_f: 3.432e+00| loss_ics: 2.308e-02| loss_bcs: 1.731e-01| Time: 0.01\n",
      "mean_grad_res: 0.276\n",
      "mean_grad_ics: 3.396\n",
      "mean_grad_bcs: 0.293\n",
      "It: 15200| loss: 6.882e+00| loss_res: 4.726e+08| loss_f: 6.614e+00| loss_ics: 2.609e-02| loss_bcs: 2.423e-01| Time: 0.01\n",
      "mean_grad_res: 1.774\n",
      "mean_grad_ics: 1.824\n",
      "mean_grad_bcs: 0.668\n",
      "It: 15300| loss: 4.951e+00| loss_res: 9.569e+09| loss_f: 4.581e+00| loss_ics: 1.481e-01| loss_bcs: 2.215e-01| Time: 0.01\n",
      "mean_grad_res: 3.384\n",
      "mean_grad_ics: 3.910\n",
      "mean_grad_bcs: 0.872\n",
      "It: 15400| loss: 4.294e+00| loss_res: 2.274e+08| loss_f: 3.932e+00| loss_ics: 2.085e-02| loss_bcs: 3.409e-01| Time: 0.01\n",
      "mean_grad_res: 1.393\n",
      "mean_grad_ics: 3.359\n",
      "mean_grad_bcs: 2.690\n",
      "It: 15500| loss: 2.728e+00| loss_res: 3.521e+08| loss_f: 2.188e+00| loss_ics: 2.358e-01| loss_bcs: 3.047e-01| Time: 0.01\n",
      "mean_grad_res: 1.195\n",
      "mean_grad_ics: 5.495\n",
      "mean_grad_bcs: 1.096\n",
      "It: 15600| loss: 2.976e+00| loss_res: 3.237e+08| loss_f: 2.458e+00| loss_ics: 1.051e-01| loss_bcs: 4.131e-01| Time: 0.01\n",
      "mean_grad_res: 0.523\n",
      "mean_grad_ics: 1.765\n",
      "mean_grad_bcs: 1.811\n",
      "It: 15700| loss: 4.273e+00| loss_res: 1.033e+08| loss_f: 3.997e+00| loss_ics: 1.423e-02| loss_bcs: 2.625e-01| Time: 0.01\n",
      "mean_grad_res: 1.060\n",
      "mean_grad_ics: 3.149\n",
      "mean_grad_bcs: 0.444\n",
      "It: 15800| loss: 1.204e+00| loss_res: 9.135e+07| loss_f: 8.513e-01| loss_ics: 2.071e-02| loss_bcs: 3.323e-01| Time: 0.01\n",
      "mean_grad_res: 0.287\n",
      "mean_grad_ics: 1.892\n",
      "mean_grad_bcs: 2.647\n",
      "It: 15900| loss: 1.244e+00| loss_res: 8.278e+06| loss_f: 9.618e-01| loss_ics: 8.291e-02| loss_bcs: 1.989e-01| Time: 0.01\n",
      "mean_grad_res: 0.112\n",
      "mean_grad_ics: 5.168\n",
      "mean_grad_bcs: 0.724\n",
      "It: 16000| loss: 1.461e+00| loss_res: 9.444e+06| loss_f: 1.172e+00| loss_ics: 1.216e-02| loss_bcs: 2.774e-01| Time: 0.01\n",
      "mean_grad_res: 0.490\n",
      "mean_grad_ics: 6.474\n",
      "mean_grad_bcs: 1.907\n",
      "It: 16100| loss: 3.575e+00| loss_res: 3.602e+08| loss_f: 3.355e+00| loss_ics: 1.460e-02| loss_bcs: 2.047e-01| Time: 0.01\n",
      "mean_grad_res: 0.961\n",
      "mean_grad_ics: 1.535\n",
      "mean_grad_bcs: 0.528\n",
      "It: 16200| loss: 1.929e+00| loss_res: 2.917e+07| loss_f: 1.346e+00| loss_ics: 8.928e-02| loss_bcs: 4.933e-01| Time: 0.01\n",
      "mean_grad_res: 0.138\n",
      "mean_grad_ics: 8.232\n",
      "mean_grad_bcs: 0.691\n",
      "It: 16300| loss: 5.531e+00| loss_res: 2.806e+08| loss_f: 5.295e+00| loss_ics: 4.222e-02| loss_bcs: 1.930e-01| Time: 0.01\n",
      "mean_grad_res: 0.724\n",
      "mean_grad_ics: 3.444\n",
      "mean_grad_bcs: 0.403\n",
      "It: 16400| loss: 5.292e+00| loss_res: 2.188e+08| loss_f: 5.005e+00| loss_ics: 5.901e-02| loss_bcs: 2.281e-01| Time: 0.01\n",
      "mean_grad_res: 1.389\n",
      "mean_grad_ics: 3.104\n",
      "mean_grad_bcs: 0.841\n",
      "It: 16500| loss: 1.851e+00| loss_res: 1.148e+09| loss_f: 1.484e+00| loss_ics: 1.129e-01| loss_bcs: 2.540e-01| Time: 0.01\n",
      "mean_grad_res: 0.947\n",
      "mean_grad_ics: 5.316\n",
      "mean_grad_bcs: 2.192\n",
      "It: 16600| loss: 5.472e+00| loss_res: 1.125e+09| loss_f: 5.242e+00| loss_ics: 1.516e-02| loss_bcs: 2.149e-01| Time: 0.01\n",
      "mean_grad_res: 2.500\n",
      "mean_grad_ics: 3.747\n",
      "mean_grad_bcs: 0.657\n",
      "It: 16700| loss: 1.115e+00| loss_res: 1.813e+09| loss_f: 7.675e-01| loss_ics: 5.260e-02| loss_bcs: 2.954e-01| Time: 0.01\n",
      "mean_grad_res: 0.558\n",
      "mean_grad_ics: 2.782\n",
      "mean_grad_bcs: 0.903\n",
      "It: 16800| loss: 2.694e+00| loss_res: 3.563e+08| loss_f: 2.485e+00| loss_ics: 1.509e-02| loss_bcs: 1.939e-01| Time: 0.01\n",
      "mean_grad_res: 1.220\n",
      "mean_grad_ics: 2.512\n",
      "mean_grad_bcs: 0.814\n",
      "It: 16900| loss: 2.963e+00| loss_res: 1.192e+07| loss_f: 2.665e+00| loss_ics: 3.020e-02| loss_bcs: 2.677e-01| Time: 0.01\n",
      "mean_grad_res: 0.656\n",
      "mean_grad_ics: 3.579\n",
      "mean_grad_bcs: 1.727\n",
      "It: 17000| loss: 5.270e+00| loss_res: 1.311e+08| loss_f: 4.941e+00| loss_ics: 7.297e-02| loss_bcs: 2.565e-01| Time: 0.01\n",
      "mean_grad_res: 3.009\n",
      "mean_grad_ics: 3.314\n",
      "mean_grad_bcs: 3.522\n",
      "It: 17100| loss: 7.416e+00| loss_res: 5.446e+09| loss_f: 7.086e+00| loss_ics: 3.106e-02| loss_bcs: 2.989e-01| Time: 0.01\n",
      "mean_grad_res: 5.825\n",
      "mean_grad_ics: 2.540\n",
      "mean_grad_bcs: 2.018\n",
      "It: 17200| loss: 5.252e+00| loss_res: 1.246e+09| loss_f: 4.908e+00| loss_ics: 1.448e-01| loss_bcs: 1.995e-01| Time: 0.01\n",
      "mean_grad_res: 1.428\n",
      "mean_grad_ics: 1.333\n",
      "mean_grad_bcs: 0.969\n",
      "It: 17300| loss: 5.958e+00| loss_res: 4.557e+08| loss_f: 5.633e+00| loss_ics: 3.988e-02| loss_bcs: 2.850e-01| Time: 0.01\n",
      "mean_grad_res: 2.026\n",
      "mean_grad_ics: 3.574\n",
      "mean_grad_bcs: 0.818\n",
      "It: 17400| loss: 2.060e+00| loss_res: 3.585e+08| loss_f: 1.629e+00| loss_ics: 2.527e-01| loss_bcs: 1.783e-01| Time: 0.01\n",
      "mean_grad_res: 0.633\n",
      "mean_grad_ics: 2.280\n",
      "mean_grad_bcs: 0.662\n",
      "It: 17500| loss: 1.310e+00| loss_res: 2.536e+07| loss_f: 9.537e-01| loss_ics: 2.344e-02| loss_bcs: 3.324e-01| Time: 0.01\n",
      "mean_grad_res: 0.435\n",
      "mean_grad_ics: 4.548\n",
      "mean_grad_bcs: 1.771\n",
      "It: 17600| loss: 9.318e+00| loss_res: 1.482e+09| loss_f: 8.838e+00| loss_ics: 2.299e-01| loss_bcs: 2.500e-01| Time: 0.01\n",
      "mean_grad_res: 1.482\n",
      "mean_grad_ics: 2.753\n",
      "mean_grad_bcs: 1.064\n",
      "It: 17700| loss: 1.497e+00| loss_res: 6.331e+08| loss_f: 1.279e+00| loss_ics: 8.093e-02| loss_bcs: 1.363e-01| Time: 0.01\n",
      "mean_grad_res: 1.178\n",
      "mean_grad_ics: 5.443\n",
      "mean_grad_bcs: 0.398\n",
      "It: 17800| loss: 1.055e+01| loss_res: 3.245e+08| loss_f: 1.033e+01| loss_ics: 6.575e-02| loss_bcs: 1.514e-01| Time: 0.01\n",
      "mean_grad_res: 2.634\n",
      "mean_grad_ics: 6.273\n",
      "mean_grad_bcs: 0.336\n",
      "It: 17900| loss: 1.688e+00| loss_res: 1.236e+07| loss_f: 1.160e+00| loss_ics: 2.615e-01| loss_bcs: 2.670e-01| Time: 0.01\n",
      "mean_grad_res: 0.556\n",
      "mean_grad_ics: 2.661\n",
      "mean_grad_bcs: 1.581\n",
      "It: 18000| loss: 6.601e+00| loss_res: 3.791e+07| loss_f: 6.394e+00| loss_ics: 5.180e-02| loss_bcs: 1.554e-01| Time: 0.01\n",
      "mean_grad_res: 1.004\n",
      "mean_grad_ics: 4.550\n",
      "mean_grad_bcs: 0.492\n",
      "It: 18100| loss: 7.634e+00| loss_res: 6.007e+07| loss_f: 7.336e+00| loss_ics: 5.778e-02| loss_bcs: 2.398e-01| Time: 0.01\n",
      "mean_grad_res: 2.081\n",
      "mean_grad_ics: 7.157\n",
      "mean_grad_bcs: 0.936\n",
      "It: 18200| loss: 2.706e+00| loss_res: 2.264e+08| loss_f: 2.382e+00| loss_ics: 4.447e-02| loss_bcs: 2.796e-01| Time: 0.01\n",
      "mean_grad_res: 0.692\n",
      "mean_grad_ics: 3.614\n",
      "mean_grad_bcs: 0.394\n",
      "It: 18300| loss: 8.419e+00| loss_res: 5.017e+07| loss_f: 8.175e+00| loss_ics: 3.735e-02| loss_bcs: 2.066e-01| Time: 0.01\n",
      "mean_grad_res: 1.916\n",
      "mean_grad_ics: 5.775\n",
      "mean_grad_bcs: 0.783\n",
      "It: 18400| loss: 4.970e+00| loss_res: 1.185e+09| loss_f: 4.653e+00| loss_ics: 6.850e-02| loss_bcs: 2.488e-01| Time: 0.01\n",
      "mean_grad_res: 1.364\n",
      "mean_grad_ics: 6.382\n",
      "mean_grad_bcs: 2.042\n",
      "It: 18500| loss: 8.058e-01| loss_res: 5.517e+08| loss_f: 6.118e-01| loss_ics: 1.300e-02| loss_bcs: 1.810e-01| Time: 0.01\n",
      "mean_grad_res: 0.343\n",
      "mean_grad_ics: 4.870\n",
      "mean_grad_bcs: 0.559\n",
      "It: 18600| loss: 6.813e+00| loss_res: 9.645e+09| loss_f: 6.636e+00| loss_ics: 2.683e-02| loss_bcs: 1.497e-01| Time: 0.01\n",
      "mean_grad_res: 5.602\n",
      "mean_grad_ics: 4.627\n",
      "mean_grad_bcs: 0.466\n",
      "It: 18700| loss: 2.243e+00| loss_res: 3.139e+09| loss_f: 1.984e+00| loss_ics: 4.637e-02| loss_bcs: 2.123e-01| Time: 0.01\n",
      "mean_grad_res: 3.049\n",
      "mean_grad_ics: 18.739\n",
      "mean_grad_bcs: 1.540\n",
      "It: 18800| loss: 2.061e+00| loss_res: 7.553e+08| loss_f: 1.749e+00| loss_ics: 1.136e-01| loss_bcs: 1.981e-01| Time: 0.01\n",
      "mean_grad_res: 0.935\n",
      "mean_grad_ics: 3.869\n",
      "mean_grad_bcs: 0.739\n",
      "It: 18900| loss: 1.375e+00| loss_res: 5.360e+06| loss_f: 5.519e-01| loss_ics: 1.005e-01| loss_bcs: 7.230e-01| Time: 0.01\n",
      "mean_grad_res: 0.107\n",
      "mean_grad_ics: 9.441\n",
      "mean_grad_bcs: 6.233\n",
      "It: 19000| loss: 2.809e+00| loss_res: 1.300e+07| loss_f: 2.217e+00| loss_ics: 2.661e-01| loss_bcs: 3.250e-01| Time: 0.02\n",
      "mean_grad_res: 0.861\n",
      "mean_grad_ics: 2.332\n",
      "mean_grad_bcs: 2.296\n",
      "It: 19100| loss: 8.962e-01| loss_res: 1.755e+07| loss_f: 5.551e-01| loss_ics: 6.958e-02| loss_bcs: 2.715e-01| Time: 0.01\n",
      "mean_grad_res: 0.214\n",
      "mean_grad_ics: 1.345\n",
      "mean_grad_bcs: 2.035\n",
      "It: 19200| loss: 4.929e+00| loss_res: 1.999e+07| loss_f: 4.591e+00| loss_ics: 1.441e-02| loss_bcs: 3.230e-01| Time: 0.00\n",
      "mean_grad_res: 0.593\n",
      "mean_grad_ics: 3.729\n",
      "mean_grad_bcs: 2.169\n",
      "It: 19300| loss: 1.011e+01| loss_res: 5.253e+07| loss_f: 9.779e+00| loss_ics: 2.431e-02| loss_bcs: 3.049e-01| Time: 0.01\n",
      "mean_grad_res: 1.831\n",
      "mean_grad_ics: 4.449\n",
      "mean_grad_bcs: 1.843\n",
      "It: 19400| loss: 3.126e+00| loss_res: 1.463e+07| loss_f: 2.948e+00| loss_ics: 2.816e-02| loss_bcs: 1.497e-01| Time: 0.01\n",
      "mean_grad_res: 1.248\n",
      "mean_grad_ics: 4.225\n",
      "mean_grad_bcs: 1.081\n",
      "It: 19500| loss: 3.178e+00| loss_res: 7.864e+08| loss_f: 2.917e+00| loss_ics: 1.361e-02| loss_bcs: 2.468e-01| Time: 0.01\n",
      "mean_grad_res: 1.412\n",
      "mean_grad_ics: 4.242\n",
      "mean_grad_bcs: 1.240\n",
      "It: 19600| loss: 8.217e-01| loss_res: 8.846e+06| loss_f: 4.938e-01| loss_ics: 9.795e-02| loss_bcs: 2.299e-01| Time: 0.01\n",
      "mean_grad_res: 0.068\n",
      "mean_grad_ics: 7.056\n",
      "mean_grad_bcs: 1.585\n",
      "It: 19700| loss: 2.363e+00| loss_res: 7.387e+06| loss_f: 2.168e+00| loss_ics: 4.430e-02| loss_bcs: 1.502e-01| Time: 0.00\n",
      "mean_grad_res: 0.274\n",
      "mean_grad_ics: 2.115\n",
      "mean_grad_bcs: 0.822\n",
      "It: 19800| loss: 3.557e+00| loss_res: 1.935e+08| loss_f: 3.331e+00| loss_ics: 1.613e-02| loss_bcs: 2.107e-01| Time: 0.00\n",
      "mean_grad_res: 0.918\n",
      "mean_grad_ics: 1.681\n",
      "mean_grad_bcs: 1.160\n",
      "It: 19900| loss: 3.572e+00| loss_res: 6.195e+08| loss_f: 3.317e+00| loss_ics: 4.736e-02| loss_bcs: 2.074e-01| Time: 0.01\n",
      "mean_grad_res: 1.097\n",
      "mean_grad_ics: 6.074\n",
      "mean_grad_bcs: 0.610\n",
      "It: 20000| loss: 3.348e+00| loss_res: 1.138e+07| loss_f: 2.969e+00| loss_ics: 5.954e-02| loss_bcs: 3.193e-01| Time: 0.01\n",
      "mean_grad_res: 1.655\n",
      "mean_grad_ics: 3.886\n",
      "mean_grad_bcs: 2.273\n",
      "It: 20100| loss: 1.669e+00| loss_res: 6.158e+07| loss_f: 1.340e+00| loss_ics: 1.949e-02| loss_bcs: 3.096e-01| Time: 0.01\n",
      "mean_grad_res: 0.452\n",
      "mean_grad_ics: 2.962\n",
      "mean_grad_bcs: 0.986\n",
      "It: 20200| loss: 5.371e+00| loss_res: 9.715e+06| loss_f: 5.179e+00| loss_ics: 1.731e-02| loss_bcs: 1.748e-01| Time: 0.02\n",
      "mean_grad_res: 1.421\n",
      "mean_grad_ics: 4.093\n",
      "mean_grad_bcs: 0.873\n",
      "It: 20300| loss: 7.336e+00| loss_res: 1.913e+09| loss_f: 7.179e+00| loss_ics: 1.494e-02| loss_bcs: 1.417e-01| Time: 0.01\n",
      "mean_grad_res: 4.419\n",
      "mean_grad_ics: 5.886\n",
      "mean_grad_bcs: 1.189\n",
      "It: 20400| loss: 3.442e+00| loss_res: 2.808e+07| loss_f: 2.983e+00| loss_ics: 2.910e-01| loss_bcs: 1.681e-01| Time: 0.01\n",
      "mean_grad_res: 0.438\n",
      "mean_grad_ics: 12.636\n",
      "mean_grad_bcs: 0.628\n",
      "It: 20500| loss: 6.217e+00| loss_res: 4.005e+08| loss_f: 5.864e+00| loss_ics: 1.028e-01| loss_bcs: 2.508e-01| Time: 0.01\n",
      "mean_grad_res: 1.330\n",
      "mean_grad_ics: 3.766\n",
      "mean_grad_bcs: 1.254\n",
      "It: 20600| loss: 9.437e+00| loss_res: 2.135e+09| loss_f: 9.187e+00| loss_ics: 2.697e-02| loss_bcs: 2.226e-01| Time: 0.01\n",
      "mean_grad_res: 5.479\n",
      "mean_grad_ics: 5.210\n",
      "mean_grad_bcs: 1.141\n",
      "It: 20700| loss: 8.888e+00| loss_res: 2.162e+09| loss_f: 8.684e+00| loss_ics: 4.339e-02| loss_bcs: 1.606e-01| Time: 0.00\n",
      "mean_grad_res: 4.896\n",
      "mean_grad_ics: 1.220\n",
      "mean_grad_bcs: 0.541\n",
      "It: 20800| loss: 3.498e+00| loss_res: 4.556e+08| loss_f: 3.159e+00| loss_ics: 1.492e-01| loss_bcs: 1.902e-01| Time: 0.01\n",
      "mean_grad_res: 0.807\n",
      "mean_grad_ics: 17.365\n",
      "mean_grad_bcs: 1.214\n",
      "It: 20900| loss: 1.816e+00| loss_res: 7.545e+08| loss_f: 1.614e+00| loss_ics: 3.280e-02| loss_bcs: 1.697e-01| Time: 0.00\n",
      "mean_grad_res: 1.983\n",
      "mean_grad_ics: 8.851\n",
      "mean_grad_bcs: 0.652\n",
      "It: 21000| loss: 6.987e+00| loss_res: 3.580e+07| loss_f: 6.824e+00| loss_ics: 2.867e-02| loss_bcs: 1.340e-01| Time: 0.00\n",
      "mean_grad_res: 0.820\n",
      "mean_grad_ics: 0.859\n",
      "mean_grad_bcs: 0.715\n",
      "It: 21100| loss: 4.891e+00| loss_res: 5.071e+08| loss_f: 4.683e+00| loss_ics: 1.883e-02| loss_bcs: 1.888e-01| Time: 0.01\n",
      "mean_grad_res: 0.340\n",
      "mean_grad_ics: 3.383\n",
      "mean_grad_bcs: 1.422\n",
      "It: 21200| loss: 5.808e+00| loss_res: 6.414e+08| loss_f: 5.438e+00| loss_ics: 6.269e-02| loss_bcs: 3.075e-01| Time: 0.00\n",
      "mean_grad_res: 1.425\n",
      "mean_grad_ics: 1.235\n",
      "mean_grad_bcs: 1.950\n",
      "It: 21300| loss: 3.669e+00| loss_res: 4.744e+08| loss_f: 3.441e+00| loss_ics: 2.544e-02| loss_bcs: 2.027e-01| Time: 0.01\n",
      "mean_grad_res: 1.988\n",
      "mean_grad_ics: 13.498\n",
      "mean_grad_bcs: 1.430\n",
      "It: 21400| loss: 7.104e-01| loss_res: 1.334e+08| loss_f: 5.086e-01| loss_ics: 2.687e-02| loss_bcs: 1.749e-01| Time: 0.00\n",
      "mean_grad_res: 0.083\n",
      "mean_grad_ics: 0.942\n",
      "mean_grad_bcs: 0.837\n",
      "It: 21500| loss: 4.883e+00| loss_res: 3.644e+08| loss_f: 4.418e+00| loss_ics: 2.918e-01| loss_bcs: 1.726e-01| Time: 0.00\n",
      "mean_grad_res: 1.046\n",
      "mean_grad_ics: 21.293\n",
      "mean_grad_bcs: 0.799\n",
      "It: 21600| loss: 3.964e+00| loss_res: 6.760e+07| loss_f: 3.733e+00| loss_ics: 3.357e-02| loss_bcs: 1.975e-01| Time: 0.00\n",
      "mean_grad_res: 2.355\n",
      "mean_grad_ics: 2.344\n",
      "mean_grad_bcs: 0.832\n",
      "It: 21700| loss: 5.743e+00| loss_res: 1.293e+09| loss_f: 5.545e+00| loss_ics: 1.550e-02| loss_bcs: 1.830e-01| Time: 0.01\n",
      "mean_grad_res: 3.025\n",
      "mean_grad_ics: 4.412\n",
      "mean_grad_bcs: 0.685\n",
      "It: 21800| loss: 1.933e+00| loss_res: 3.626e+08| loss_f: 1.591e+00| loss_ics: 2.467e-01| loss_bcs: 9.611e-02| Time: 0.00\n",
      "mean_grad_res: 0.325\n",
      "mean_grad_ics: 15.655\n",
      "mean_grad_bcs: 0.872\n",
      "It: 21900| loss: 2.878e+00| loss_res: 2.145e+09| loss_f: 2.683e+00| loss_ics: 1.424e-02| loss_bcs: 1.802e-01| Time: 0.01\n",
      "mean_grad_res: 3.228\n",
      "mean_grad_ics: 5.882\n",
      "mean_grad_bcs: 1.163\n",
      "It: 22000| loss: 3.618e+00| loss_res: 7.418e+07| loss_f: 3.363e+00| loss_ics: 1.797e-02| loss_bcs: 2.372e-01| Time: 0.01\n",
      "mean_grad_res: 2.462\n",
      "mean_grad_ics: 4.341\n",
      "mean_grad_bcs: 1.100\n",
      "It: 22100| loss: 3.040e+00| loss_res: 4.230e+08| loss_f: 2.835e+00| loss_ics: 1.571e-02| loss_bcs: 1.902e-01| Time: 0.00\n",
      "mean_grad_res: 0.894\n",
      "mean_grad_ics: 4.580\n",
      "mean_grad_bcs: 1.208\n",
      "It: 22200| loss: 4.225e+00| loss_res: 2.150e+09| loss_f: 4.044e+00| loss_ics: 2.607e-02| loss_bcs: 1.558e-01| Time: 0.01\n",
      "mean_grad_res: 4.381\n",
      "mean_grad_ics: 8.037\n",
      "mean_grad_bcs: 0.750\n",
      "It: 22300| loss: 1.574e+00| loss_res: 1.163e+10| loss_f: 1.287e+00| loss_ics: 1.557e-02| loss_bcs: 2.714e-01| Time: 0.01\n",
      "mean_grad_res: 3.257\n",
      "mean_grad_ics: 4.408\n",
      "mean_grad_bcs: 3.617\n",
      "It: 22400| loss: 2.406e+00| loss_res: 9.962e+08| loss_f: 2.083e+00| loss_ics: 1.339e-01| loss_bcs: 1.892e-01| Time: 0.01\n",
      "mean_grad_res: 1.804\n",
      "mean_grad_ics: 2.989\n",
      "mean_grad_bcs: 0.717\n",
      "It: 22500| loss: 8.281e+00| loss_res: 7.973e+06| loss_f: 8.160e+00| loss_ics: 1.285e-02| loss_bcs: 1.076e-01| Time: 0.01\n",
      "mean_grad_res: 1.604\n",
      "mean_grad_ics: 4.003\n",
      "mean_grad_bcs: 0.523\n",
      "It: 22600| loss: 8.119e+00| loss_res: 1.685e+07| loss_f: 7.956e+00| loss_ics: 1.086e-02| loss_bcs: 1.525e-01| Time: 0.00\n",
      "mean_grad_res: 1.565\n",
      "mean_grad_ics: 2.381\n",
      "mean_grad_bcs: 1.167\n",
      "It: 22700| loss: 3.649e+00| loss_res: 2.895e+08| loss_f: 3.457e+00| loss_ics: 1.728e-02| loss_bcs: 1.747e-01| Time: 0.02\n",
      "mean_grad_res: 1.031\n",
      "mean_grad_ics: 1.297\n",
      "mean_grad_bcs: 2.833\n",
      "It: 22800| loss: 3.586e+00| loss_res: 1.896e+09| loss_f: 3.382e+00| loss_ics: 3.815e-02| loss_bcs: 1.660e-01| Time: 0.01\n",
      "mean_grad_res: 3.659\n",
      "mean_grad_ics: 7.281\n",
      "mean_grad_bcs: 0.714\n",
      "It: 22900| loss: 4.849e+00| loss_res: 1.589e+10| loss_f: 4.648e+00| loss_ics: 4.364e-02| loss_bcs: 1.580e-01| Time: 0.01\n",
      "mean_grad_res: 4.427\n",
      "mean_grad_ics: 1.336\n",
      "mean_grad_bcs: 0.477\n",
      "It: 23000| loss: 2.177e+00| loss_res: 9.718e+08| loss_f: 1.958e+00| loss_ics: 5.628e-02| loss_bcs: 1.625e-01| Time: 0.02\n",
      "mean_grad_res: 2.321\n",
      "mean_grad_ics: 7.791\n",
      "mean_grad_bcs: 1.050\n",
      "It: 23100| loss: 5.879e-01| loss_res: 1.192e+06| loss_f: 4.372e-01| loss_ics: 1.478e-02| loss_bcs: 1.359e-01| Time: 0.01\n",
      "mean_grad_res: 0.116\n",
      "mean_grad_ics: 1.559\n",
      "mean_grad_bcs: 1.283\n",
      "It: 23200| loss: 1.312e+00| loss_res: 1.825e+09| loss_f: 1.082e+00| loss_ics: 4.660e-02| loss_bcs: 1.835e-01| Time: 0.01\n",
      "mean_grad_res: 0.973\n",
      "mean_grad_ics: 7.533\n",
      "mean_grad_bcs: 3.264\n",
      "It: 23300| loss: 1.553e+00| loss_res: 1.040e+07| loss_f: 1.362e+00| loss_ics: 2.615e-02| loss_bcs: 1.654e-01| Time: 0.01\n",
      "mean_grad_res: 0.686\n",
      "mean_grad_ics: 25.060\n",
      "mean_grad_bcs: 1.736\n",
      "It: 23400| loss: 3.685e+00| loss_res: 5.382e+08| loss_f: 3.510e+00| loss_ics: 2.610e-02| loss_bcs: 1.489e-01| Time: 0.00\n",
      "mean_grad_res: 2.394\n",
      "mean_grad_ics: 2.744\n",
      "mean_grad_bcs: 1.328\n",
      "It: 23500| loss: 3.242e+00| loss_res: 8.362e+07| loss_f: 3.078e+00| loss_ics: 1.531e-02| loss_bcs: 1.492e-01| Time: 0.00\n",
      "mean_grad_res: 0.295\n",
      "mean_grad_ics: 3.281\n",
      "mean_grad_bcs: 0.503\n",
      "It: 23600| loss: 1.096e+00| loss_res: 2.282e+08| loss_f: 6.527e-01| loss_ics: 7.172e-02| loss_bcs: 3.720e-01| Time: 0.01\n",
      "mean_grad_res: 0.301\n",
      "mean_grad_ics: 5.512\n",
      "mean_grad_bcs: 10.256\n",
      "It: 23700| loss: 4.312e+00| loss_res: 4.146e+09| loss_f: 4.086e+00| loss_ics: 3.587e-02| loss_bcs: 1.905e-01| Time: 0.01\n",
      "mean_grad_res: 1.617\n",
      "mean_grad_ics: 1.269\n",
      "mean_grad_bcs: 1.939\n",
      "It: 23800| loss: 2.973e+00| loss_res: 5.771e+08| loss_f: 2.804e+00| loss_ics: 2.175e-02| loss_bcs: 1.477e-01| Time: 0.01\n",
      "mean_grad_res: 0.683\n",
      "mean_grad_ics: 2.378\n",
      "mean_grad_bcs: 2.139\n",
      "It: 23900| loss: 1.572e+00| loss_res: 9.970e+07| loss_f: 1.294e+00| loss_ics: 1.074e-01| loss_bcs: 1.716e-01| Time: 0.01\n",
      "mean_grad_res: 0.357\n",
      "mean_grad_ics: 5.761\n",
      "mean_grad_bcs: 0.999\n",
      "It: 24000| loss: 3.065e+00| loss_res: 2.543e+08| loss_f: 2.819e+00| loss_ics: 5.074e-02| loss_bcs: 1.958e-01| Time: 0.00\n",
      "mean_grad_res: 2.233\n",
      "mean_grad_ics: 18.721\n",
      "mean_grad_bcs: 1.926\n",
      "It: 24100| loss: 1.135e+00| loss_res: 4.838e+08| loss_f: 9.842e-01| loss_ics: 8.349e-03| loss_bcs: 1.428e-01| Time: 0.01\n",
      "mean_grad_res: 1.510\n",
      "mean_grad_ics: 1.494\n",
      "mean_grad_bcs: 1.218\n",
      "It: 24200| loss: 2.926e+00| loss_res: 2.063e+08| loss_f: 2.735e+00| loss_ics: 1.333e-02| loss_bcs: 1.774e-01| Time: 0.01\n",
      "mean_grad_res: 1.675\n",
      "mean_grad_ics: 1.048\n",
      "mean_grad_bcs: 2.338\n",
      "It: 24300| loss: 9.055e-01| loss_res: 9.483e+06| loss_f: 7.234e-01| loss_ics: 4.326e-02| loss_bcs: 1.389e-01| Time: 0.01\n",
      "mean_grad_res: 0.203\n",
      "mean_grad_ics: 2.400\n",
      "mean_grad_bcs: 1.107\n",
      "It: 24400| loss: 5.883e+00| loss_res: 8.252e+08| loss_f: 5.703e+00| loss_ics: 2.898e-02| loss_bcs: 1.513e-01| Time: 0.00\n",
      "mean_grad_res: 3.611\n",
      "mean_grad_ics: 8.207\n",
      "mean_grad_bcs: 3.275\n",
      "It: 24500| loss: 2.641e+00| loss_res: 4.127e+06| loss_f: 2.430e+00| loss_ics: 3.616e-02| loss_bcs: 1.754e-01| Time: 0.01\n",
      "mean_grad_res: 0.129\n",
      "mean_grad_ics: 2.834\n",
      "mean_grad_bcs: 1.066\n",
      "It: 24600| loss: 5.889e+00| loss_res: 4.213e+09| loss_f: 5.653e+00| loss_ics: 6.230e-02| loss_bcs: 1.735e-01| Time: 0.01\n",
      "mean_grad_res: 5.284\n",
      "mean_grad_ics: 8.132\n",
      "mean_grad_bcs: 0.804\n",
      "It: 24700| loss: 9.164e-01| loss_res: 2.524e+07| loss_f: 6.871e-01| loss_ics: 5.604e-02| loss_bcs: 1.733e-01| Time: 0.00\n",
      "mean_grad_res: 0.152\n",
      "mean_grad_ics: 4.812\n",
      "mean_grad_bcs: 0.952\n",
      "It: 24800| loss: 9.070e+00| loss_res: 2.515e+09| loss_f: 8.842e+00| loss_ics: 8.420e-03| loss_bcs: 2.202e-01| Time: 0.01\n",
      "mean_grad_res: 0.655\n",
      "mean_grad_ics: 5.794\n",
      "mean_grad_bcs: 1.640\n",
      "It: 24900| loss: 8.033e+00| loss_res: 7.782e+09| loss_f: 7.846e+00| loss_ics: 3.362e-02| loss_bcs: 1.525e-01| Time: 0.01\n",
      "mean_grad_res: 5.671\n",
      "mean_grad_ics: 4.421\n",
      "mean_grad_bcs: 1.233\n",
      "It: 25000| loss: 3.046e+00| loss_res: 2.543e+09| loss_f: 2.796e+00| loss_ics: 4.637e-02| loss_bcs: 2.039e-01| Time: 0.01\n",
      "mean_grad_res: 5.191\n",
      "mean_grad_ics: 5.192\n",
      "mean_grad_bcs: 1.861\n",
      "It: 25100| loss: 2.759e+00| loss_res: 4.624e+06| loss_f: 2.568e+00| loss_ics: 3.131e-02| loss_bcs: 1.593e-01| Time: 0.01\n",
      "mean_grad_res: 1.790\n",
      "mean_grad_ics: 3.025\n",
      "mean_grad_bcs: 0.577\n",
      "It: 25200| loss: 3.006e+00| loss_res: 1.534e+07| loss_f: 2.810e+00| loss_ics: 5.603e-02| loss_bcs: 1.400e-01| Time: 0.01\n",
      "mean_grad_res: 0.693\n",
      "mean_grad_ics: 6.272\n",
      "mean_grad_bcs: 1.761\n",
      "It: 25300| loss: 3.728e+00| loss_res: 2.741e+09| loss_f: 3.580e+00| loss_ics: 1.797e-02| loss_bcs: 1.304e-01| Time: 0.01\n",
      "mean_grad_res: 4.318\n",
      "mean_grad_ics: 9.193\n",
      "mean_grad_bcs: 0.537\n",
      "It: 25400| loss: 1.900e+00| loss_res: 7.466e+09| loss_f: 1.679e+00| loss_ics: 6.679e-02| loss_bcs: 1.546e-01| Time: 0.01\n",
      "mean_grad_res: 2.599\n",
      "mean_grad_ics: 11.699\n",
      "mean_grad_bcs: 0.729\n",
      "It: 25500| loss: 1.509e+00| loss_res: 9.343e+08| loss_f: 1.312e+00| loss_ics: 4.178e-02| loss_bcs: 1.547e-01| Time: 0.01\n",
      "mean_grad_res: 0.672\n",
      "mean_grad_ics: 2.172\n",
      "mean_grad_bcs: 0.799\n",
      "It: 25600| loss: 5.785e+00| loss_res: 8.151e+07| loss_f: 5.544e+00| loss_ics: 3.831e-02| loss_bcs: 2.024e-01| Time: 0.01\n",
      "mean_grad_res: 2.047\n",
      "mean_grad_ics: 5.265\n",
      "mean_grad_bcs: 1.795\n",
      "It: 25700| loss: 9.306e-01| loss_res: 2.482e+08| loss_f: 6.999e-01| loss_ics: 6.022e-02| loss_bcs: 1.705e-01| Time: 0.00\n",
      "mean_grad_res: 0.305\n",
      "mean_grad_ics: 8.205\n",
      "mean_grad_bcs: 0.834\n",
      "It: 25800| loss: 3.595e+00| loss_res: 2.529e+09| loss_f: 3.347e+00| loss_ics: 3.314e-02| loss_bcs: 2.154e-01| Time: 0.01\n",
      "mean_grad_res: 5.880\n",
      "mean_grad_ics: 5.656\n",
      "mean_grad_bcs: 2.225\n",
      "It: 25900| loss: 6.101e+00| loss_res: 9.663e+06| loss_f: 5.937e+00| loss_ics: 2.045e-02| loss_bcs: 1.436e-01| Time: 0.01\n",
      "mean_grad_res: 0.857\n",
      "mean_grad_ics: 2.414\n",
      "mean_grad_bcs: 0.473\n",
      "It: 26000| loss: 2.409e+00| loss_res: 1.449e+08| loss_f: 1.992e+00| loss_ics: 1.803e-01| loss_bcs: 2.368e-01| Time: 0.01\n",
      "mean_grad_res: 1.639\n",
      "mean_grad_ics: 10.316\n",
      "mean_grad_bcs: 1.499\n",
      "It: 26100| loss: 5.426e+00| loss_res: 8.439e+07| loss_f: 5.134e+00| loss_ics: 9.322e-02| loss_bcs: 1.981e-01| Time: 0.00\n",
      "mean_grad_res: 11.230\n",
      "mean_grad_ics: 5.387\n",
      "mean_grad_bcs: 2.455\n",
      "It: 26200| loss: 2.111e+00| loss_res: 9.648e+08| loss_f: 1.895e+00| loss_ics: 2.846e-02| loss_bcs: 1.876e-01| Time: 0.01\n",
      "mean_grad_res: 0.601\n",
      "mean_grad_ics: 2.966\n",
      "mean_grad_bcs: 1.126\n",
      "It: 26300| loss: 1.467e+00| loss_res: 8.662e+08| loss_f: 1.343e+00| loss_ics: 1.215e-02| loss_bcs: 1.118e-01| Time: 0.01\n",
      "mean_grad_res: 1.358\n",
      "mean_grad_ics: 4.765\n",
      "mean_grad_bcs: 0.723\n",
      "It: 26400| loss: 1.086e+00| loss_res: 9.981e+06| loss_f: 8.965e-01| loss_ics: 2.119e-02| loss_bcs: 1.687e-01| Time: 0.01\n",
      "mean_grad_res: 0.517\n",
      "mean_grad_ics: 2.865\n",
      "mean_grad_bcs: 1.165\n",
      "It: 26500| loss: 1.352e+00| loss_res: 6.155e+08| loss_f: 1.131e+00| loss_ics: 1.641e-02| loss_bcs: 2.043e-01| Time: 0.01\n",
      "mean_grad_res: 0.206\n",
      "mean_grad_ics: 5.243\n",
      "mean_grad_bcs: 3.108\n",
      "It: 26600| loss: 1.088e+01| loss_res: 5.571e+08| loss_f: 1.068e+01| loss_ics: 9.957e-03| loss_bcs: 1.899e-01| Time: 0.01\n",
      "mean_grad_res: 2.163\n",
      "mean_grad_ics: 4.502\n",
      "mean_grad_bcs: 5.893\n",
      "It: 26700| loss: 4.499e+00| loss_res: 2.445e+07| loss_f: 3.688e+00| loss_ics: 6.913e-01| loss_bcs: 1.203e-01| Time: 0.01\n",
      "mean_grad_res: 1.937\n",
      "mean_grad_ics: 16.251\n",
      "mean_grad_bcs: 2.288\n",
      "It: 26800| loss: 2.017e+00| loss_res: 1.190e+08| loss_f: 1.830e+00| loss_ics: 2.086e-02| loss_bcs: 1.661e-01| Time: 0.00\n",
      "mean_grad_res: 1.659\n",
      "mean_grad_ics: 4.033\n",
      "mean_grad_bcs: 1.470\n",
      "It: 26900| loss: 3.613e+00| loss_res: 6.637e+07| loss_f: 3.330e+00| loss_ics: 6.626e-02| loss_bcs: 2.169e-01| Time: 0.00\n",
      "mean_grad_res: 1.455\n",
      "mean_grad_ics: 6.636\n",
      "mean_grad_bcs: 2.930\n",
      "It: 27000| loss: 1.423e+00| loss_res: 2.542e+08| loss_f: 1.223e+00| loss_ics: 3.954e-02| loss_bcs: 1.599e-01| Time: 0.00\n",
      "mean_grad_res: 0.582\n",
      "mean_grad_ics: 6.218\n",
      "mean_grad_bcs: 1.551\n",
      "It: 27100| loss: 2.132e+00| loss_res: 1.019e+07| loss_f: 1.904e+00| loss_ics: 3.688e-02| loss_bcs: 1.907e-01| Time: 0.01\n",
      "mean_grad_res: 1.857\n",
      "mean_grad_ics: 1.597\n",
      "mean_grad_bcs: 1.735\n",
      "It: 27200| loss: 3.381e+00| loss_res: 1.041e+08| loss_f: 3.205e+00| loss_ics: 1.387e-02| loss_bcs: 1.624e-01| Time: 0.01\n",
      "mean_grad_res: 1.628\n",
      "mean_grad_ics: 4.252\n",
      "mean_grad_bcs: 1.144\n",
      "It: 27300| loss: 2.319e+00| loss_res: 1.333e+08| loss_f: 1.861e+00| loss_ics: 3.324e-02| loss_bcs: 4.242e-01| Time: 0.01\n",
      "mean_grad_res: 1.289\n",
      "mean_grad_ics: 5.220\n",
      "mean_grad_bcs: 8.534\n",
      "It: 27400| loss: 6.150e+00| loss_res: 2.514e+09| loss_f: 5.943e+00| loss_ics: 8.686e-03| loss_bcs: 1.982e-01| Time: 0.01\n",
      "mean_grad_res: 2.176\n",
      "mean_grad_ics: 0.742\n",
      "mean_grad_bcs: 2.274\n",
      "It: 27500| loss: 1.521e+00| loss_res: 5.739e+08| loss_f: 1.299e+00| loss_ics: 4.586e-02| loss_bcs: 1.759e-01| Time: 0.01\n",
      "mean_grad_res: 0.519\n",
      "mean_grad_ics: 10.181\n",
      "mean_grad_bcs: 1.398\n",
      "It: 27600| loss: 2.426e+00| loss_res: 5.710e+08| loss_f: 2.258e+00| loss_ics: 6.340e-02| loss_bcs: 1.047e-01| Time: 0.01\n",
      "mean_grad_res: 6.527\n",
      "mean_grad_ics: 14.458\n",
      "mean_grad_bcs: 0.593\n",
      "It: 27700| loss: 2.791e+00| loss_res: 2.811e+08| loss_f: 2.646e+00| loss_ics: 1.010e-02| loss_bcs: 1.346e-01| Time: 0.03\n",
      "mean_grad_res: 1.916\n",
      "mean_grad_ics: 2.249\n",
      "mean_grad_bcs: 1.110\n",
      "It: 27800| loss: 1.522e+00| loss_res: 6.212e+06| loss_f: 1.375e+00| loss_ics: 1.420e-02| loss_bcs: 1.325e-01| Time: 0.01\n",
      "mean_grad_res: 0.716\n",
      "mean_grad_ics: 7.426\n",
      "mean_grad_bcs: 0.664\n",
      "It: 27900| loss: 5.234e+00| loss_res: 1.198e+09| loss_f: 5.056e+00| loss_ics: 1.087e-02| loss_bcs: 1.670e-01| Time: 0.01\n",
      "mean_grad_res: 0.733\n",
      "mean_grad_ics: 1.576\n",
      "mean_grad_bcs: 0.526\n",
      "It: 28000| loss: 2.913e+00| loss_res: 1.423e+09| loss_f: 2.758e+00| loss_ics: 1.697e-02| loss_bcs: 1.373e-01| Time: 0.01\n",
      "mean_grad_res: 1.308\n",
      "mean_grad_ics: 3.877\n",
      "mean_grad_bcs: 0.212\n",
      "It: 28100| loss: 1.083e+01| loss_res: 3.722e+08| loss_f: 1.064e+01| loss_ics: 2.638e-02| loss_bcs: 1.676e-01| Time: 0.01\n",
      "mean_grad_res: 2.896\n",
      "mean_grad_ics: 15.092\n",
      "mean_grad_bcs: 1.117\n",
      "It: 28200| loss: 3.702e+00| loss_res: 3.481e+07| loss_f: 3.515e+00| loss_ics: 9.385e-03| loss_bcs: 1.782e-01| Time: 0.01\n",
      "mean_grad_res: 0.767\n",
      "mean_grad_ics: 2.821\n",
      "mean_grad_bcs: 1.852\n",
      "It: 28300| loss: 3.869e+00| loss_res: 8.443e+08| loss_f: 3.649e+00| loss_ics: 2.400e-02| loss_bcs: 1.957e-01| Time: 0.01\n",
      "mean_grad_res: 2.503\n",
      "mean_grad_ics: 2.305\n",
      "mean_grad_bcs: 1.744\n",
      "It: 28400| loss: 3.635e+00| loss_res: 1.472e+07| loss_f: 3.466e+00| loss_ics: 3.208e-02| loss_bcs: 1.373e-01| Time: 0.01\n",
      "mean_grad_res: 1.287\n",
      "mean_grad_ics: 6.960\n",
      "mean_grad_bcs: 1.204\n",
      "It: 28500| loss: 1.770e+00| loss_res: 9.647e+08| loss_f: 1.584e+00| loss_ics: 2.735e-02| loss_bcs: 1.582e-01| Time: 0.01\n",
      "mean_grad_res: 0.220\n",
      "mean_grad_ics: 4.343\n",
      "mean_grad_bcs: 1.092\n",
      "It: 28600| loss: 4.952e+00| loss_res: 1.060e+07| loss_f: 4.781e+00| loss_ics: 2.359e-02| loss_bcs: 1.474e-01| Time: 0.02\n",
      "mean_grad_res: 1.094\n",
      "mean_grad_ics: 4.410\n",
      "mean_grad_bcs: 0.431\n",
      "It: 28700| loss: 6.423e+00| loss_res: 7.775e+08| loss_f: 6.212e+00| loss_ics: 4.746e-02| loss_bcs: 1.637e-01| Time: 0.01\n",
      "mean_grad_res: 1.625\n",
      "mean_grad_ics: 3.697\n",
      "mean_grad_bcs: 1.371\n",
      "It: 28800| loss: 8.525e+00| loss_res: 5.854e+09| loss_f: 8.385e+00| loss_ics: 1.094e-02| loss_bcs: 1.287e-01| Time: 0.01\n",
      "mean_grad_res: 3.015\n",
      "mean_grad_ics: 4.663\n",
      "mean_grad_bcs: 0.591\n",
      "It: 28900| loss: 4.012e+00| loss_res: 1.206e+10| loss_f: 3.850e+00| loss_ics: 2.039e-02| loss_bcs: 1.415e-01| Time: 0.01\n",
      "mean_grad_res: 4.571\n",
      "mean_grad_ics: 3.841\n",
      "mean_grad_bcs: 2.295\n",
      "It: 29000| loss: 1.496e+00| loss_res: 5.619e+08| loss_f: 1.354e+00| loss_ics: 1.271e-02| loss_bcs: 1.294e-01| Time: 0.01\n",
      "mean_grad_res: 0.405\n",
      "mean_grad_ics: 4.384\n",
      "mean_grad_bcs: 1.163\n",
      "It: 29100| loss: 6.960e+00| loss_res: 7.685e+07| loss_f: 6.609e+00| loss_ics: 3.121e-02| loss_bcs: 3.205e-01| Time: 0.01\n",
      "mean_grad_res: 1.123\n",
      "mean_grad_ics: 6.104\n",
      "mean_grad_bcs: 5.328\n",
      "It: 29200| loss: 1.676e+00| loss_res: 7.059e+09| loss_f: 1.485e+00| loss_ics: 1.763e-02| loss_bcs: 1.736e-01| Time: 0.01\n",
      "mean_grad_res: 0.684\n",
      "mean_grad_ics: 4.265\n",
      "mean_grad_bcs: 0.621\n",
      "It: 29300| loss: 9.112e+00| loss_res: 1.370e+09| loss_f: 8.879e+00| loss_ics: 1.598e-02| loss_bcs: 2.168e-01| Time: 0.01\n",
      "mean_grad_res: 1.936\n",
      "mean_grad_ics: 6.521\n",
      "mean_grad_bcs: 2.799\n",
      "It: 29400| loss: 4.294e+00| loss_res: 3.029e+08| loss_f: 4.038e+00| loss_ics: 8.894e-02| loss_bcs: 1.671e-01| Time: 0.02\n",
      "mean_grad_res: 2.501\n",
      "mean_grad_ics: 2.728\n",
      "mean_grad_bcs: 2.290\n",
      "It: 29500| loss: 1.881e+00| loss_res: 3.358e+07| loss_f: 1.711e+00| loss_ics: 6.701e-02| loss_bcs: 1.038e-01| Time: 0.00\n",
      "mean_grad_res: 0.176\n",
      "mean_grad_ics: 3.897\n",
      "mean_grad_bcs: 0.954\n",
      "It: 29600| loss: 4.351e+00| loss_res: 1.522e+07| loss_f: 4.172e+00| loss_ics: 3.288e-02| loss_bcs: 1.458e-01| Time: 0.01\n",
      "mean_grad_res: 3.089\n",
      "mean_grad_ics: 5.618\n",
      "mean_grad_bcs: 1.107\n",
      "It: 29700| loss: 1.937e+00| loss_res: 2.018e+08| loss_f: 1.696e+00| loss_ics: 5.627e-03| loss_bcs: 2.358e-01| Time: 0.00\n",
      "mean_grad_res: 0.857\n",
      "mean_grad_ics: 2.359\n",
      "mean_grad_bcs: 2.839\n",
      "It: 29800| loss: 3.665e+00| loss_res: 1.864e+08| loss_f: 3.491e+00| loss_ics: 4.303e-02| loss_bcs: 1.309e-01| Time: 0.01\n",
      "mean_grad_res: 1.467\n",
      "mean_grad_ics: 21.554\n",
      "mean_grad_bcs: 0.904\n",
      "It: 29900| loss: 8.961e-01| loss_res: 6.552e+08| loss_f: 6.552e-01| loss_ics: 1.527e-02| loss_bcs: 2.256e-01| Time: 0.02\n",
      "mean_grad_res: 0.567\n",
      "mean_grad_ics: 0.674\n",
      "mean_grad_bcs: 2.964\n",
      "It: 30000| loss: 3.569e+00| loss_res: 1.719e+08| loss_f: 3.354e+00| loss_ics: 9.500e-03| loss_bcs: 2.055e-01| Time: 0.00\n",
      "mean_grad_res: 0.913\n",
      "mean_grad_ics: 3.054\n",
      "mean_grad_bcs: 2.362\n",
      "It: 30100| loss: 2.212e+00| loss_res: 1.947e+07| loss_f: 2.028e+00| loss_ics: 8.421e-03| loss_bcs: 1.757e-01| Time: 0.01\n",
      "mean_grad_res: 0.508\n",
      "mean_grad_ics: 0.858\n",
      "mean_grad_bcs: 0.814\n",
      "It: 30200| loss: 3.456e+00| loss_res: 1.171e+09| loss_f: 3.291e+00| loss_ics: 2.691e-02| loss_bcs: 1.386e-01| Time: 0.01\n",
      "mean_grad_res: 1.249\n",
      "mean_grad_ics: 2.595\n",
      "mean_grad_bcs: 0.848\n",
      "It: 30300| loss: 4.848e+00| loss_res: 2.795e+07| loss_f: 4.643e+00| loss_ics: 7.376e-02| loss_bcs: 1.320e-01| Time: 0.01\n",
      "mean_grad_res: 1.330\n",
      "mean_grad_ics: 3.826\n",
      "mean_grad_bcs: 0.711\n",
      "It: 30400| loss: 9.164e+00| loss_res: 5.076e+08| loss_f: 8.971e+00| loss_ics: 8.847e-03| loss_bcs: 1.838e-01| Time: 0.00\n",
      "mean_grad_res: 1.279\n",
      "mean_grad_ics: 3.025\n",
      "mean_grad_bcs: 1.937\n",
      "It: 30500| loss: 1.196e+00| loss_res: 1.766e+08| loss_f: 9.695e-01| loss_ics: 3.361e-02| loss_bcs: 1.929e-01| Time: 0.02\n",
      "mean_grad_res: 0.110\n",
      "mean_grad_ics: 7.923\n",
      "mean_grad_bcs: 3.007\n",
      "It: 30600| loss: 5.906e+00| loss_res: 5.103e+08| loss_f: 5.712e+00| loss_ics: 1.629e-02| loss_bcs: 1.779e-01| Time: 0.01\n",
      "mean_grad_res: 3.485\n",
      "mean_grad_ics: 4.525\n",
      "mean_grad_bcs: 0.912\n",
      "It: 30700| loss: 4.874e+00| loss_res: 5.168e+08| loss_f: 4.728e+00| loss_ics: 1.844e-02| loss_bcs: 1.274e-01| Time: 0.02\n",
      "mean_grad_res: 2.563\n",
      "mean_grad_ics: 14.026\n",
      "mean_grad_bcs: 0.942\n",
      "It: 30800| loss: 7.896e-01| loss_res: 3.015e+07| loss_f: 5.841e-01| loss_ics: 4.099e-02| loss_bcs: 1.645e-01| Time: 0.01\n",
      "mean_grad_res: 0.134\n",
      "mean_grad_ics: 6.985\n",
      "mean_grad_bcs: 0.807\n",
      "It: 30900| loss: 2.021e+00| loss_res: 1.188e+09| loss_f: 1.809e+00| loss_ics: 3.469e-02| loss_bcs: 1.772e-01| Time: 0.00\n",
      "mean_grad_res: 0.491\n",
      "mean_grad_ics: 2.951\n",
      "mean_grad_bcs: 2.442\n",
      "It: 31000| loss: 3.660e+00| loss_res: 5.678e+08| loss_f: 3.487e+00| loss_ics: 2.272e-02| loss_bcs: 1.498e-01| Time: 0.01\n",
      "mean_grad_res: 2.400\n",
      "mean_grad_ics: 3.297\n",
      "mean_grad_bcs: 1.270\n",
      "It: 31100| loss: 5.166e+00| loss_res: 1.047e+08| loss_f: 4.985e+00| loss_ics: 1.951e-02| loss_bcs: 1.613e-01| Time: 0.01\n",
      "mean_grad_res: 1.002\n",
      "mean_grad_ics: 1.105\n",
      "mean_grad_bcs: 2.061\n",
      "It: 31200| loss: 3.854e+00| loss_res: 1.315e+07| loss_f: 3.714e+00| loss_ics: 1.076e-02| loss_bcs: 1.292e-01| Time: 0.01\n",
      "mean_grad_res: 0.807\n",
      "mean_grad_ics: 3.676\n",
      "mean_grad_bcs: 0.623\n",
      "It: 31300| loss: 4.946e+00| loss_res: 2.003e+09| loss_f: 4.754e+00| loss_ics: 2.068e-02| loss_bcs: 1.718e-01| Time: 0.00\n",
      "mean_grad_res: 3.021\n",
      "mean_grad_ics: 3.610\n",
      "mean_grad_bcs: 2.862\n",
      "It: 31400| loss: 7.339e+00| loss_res: 3.051e+08| loss_f: 7.209e+00| loss_ics: 2.297e-02| loss_bcs: 1.073e-01| Time: 0.01\n",
      "mean_grad_res: 3.645\n",
      "mean_grad_ics: 8.153\n",
      "mean_grad_bcs: 1.577\n",
      "It: 31500| loss: 1.531e+00| loss_res: 3.506e+08| loss_f: 1.308e+00| loss_ics: 3.274e-02| loss_bcs: 1.900e-01| Time: 0.01\n",
      "mean_grad_res: 0.588\n",
      "mean_grad_ics: 6.241\n",
      "mean_grad_bcs: 1.744\n",
      "It: 31600| loss: 3.828e+00| loss_res: 7.669e+09| loss_f: 3.617e+00| loss_ics: 7.730e-02| loss_bcs: 1.330e-01| Time: 0.01\n",
      "mean_grad_res: 2.118\n",
      "mean_grad_ics: 14.965\n",
      "mean_grad_bcs: 0.705\n",
      "It: 31700| loss: 2.164e+00| loss_res: 5.919e+08| loss_f: 1.988e+00| loss_ics: 1.205e-02| loss_bcs: 1.634e-01| Time: 0.01\n",
      "mean_grad_res: 6.526\n",
      "mean_grad_ics: 8.821\n",
      "mean_grad_bcs: 1.728\n",
      "It: 31800| loss: 7.096e+00| loss_res: 1.513e+08| loss_f: 6.872e+00| loss_ics: 2.195e-02| loss_bcs: 2.012e-01| Time: 0.01\n",
      "mean_grad_res: 1.856\n",
      "mean_grad_ics: 1.806\n",
      "mean_grad_bcs: 2.165\n",
      "It: 31900| loss: 3.524e+00| loss_res: 1.163e+09| loss_f: 3.379e+00| loss_ics: 7.272e-03| loss_bcs: 1.370e-01| Time: 0.01\n",
      "mean_grad_res: 1.198\n",
      "mean_grad_ics: 1.599\n",
      "mean_grad_bcs: 1.288\n",
      "It: 32000| loss: 3.850e+00| loss_res: 9.518e+07| loss_f: 3.576e+00| loss_ics: 1.148e-02| loss_bcs: 2.618e-01| Time: 0.01\n",
      "mean_grad_res: 2.124\n",
      "mean_grad_ics: 0.599\n",
      "mean_grad_bcs: 6.306\n",
      "It: 32100| loss: 4.736e+00| loss_res: 1.526e+08| loss_f: 4.569e+00| loss_ics: 1.039e-02| loss_bcs: 1.569e-01| Time: 0.01\n",
      "mean_grad_res: 0.742\n",
      "mean_grad_ics: 4.087\n",
      "mean_grad_bcs: 1.102\n",
      "It: 32200| loss: 8.853e-01| loss_res: 6.287e+08| loss_f: 6.736e-01| loss_ics: 3.322e-02| loss_bcs: 1.785e-01| Time: 0.01\n",
      "mean_grad_res: 0.255\n",
      "mean_grad_ics: 2.387\n",
      "mean_grad_bcs: 1.436\n",
      "It: 32300| loss: 1.345e+01| loss_res: 1.455e+10| loss_f: 1.322e+01| loss_ics: 3.443e-02| loss_bcs: 1.962e-01| Time: 0.01\n",
      "mean_grad_res: 9.272\n",
      "mean_grad_ics: 3.287\n",
      "mean_grad_bcs: 1.842\n",
      "It: 32400| loss: 1.916e+00| loss_res: 2.884e+08| loss_f: 1.751e+00| loss_ics: 1.660e-02| loss_bcs: 1.489e-01| Time: 0.01\n",
      "mean_grad_res: 2.117\n",
      "mean_grad_ics: 5.819\n",
      "mean_grad_bcs: 1.260\n",
      "It: 32500| loss: 1.510e+00| loss_res: 5.368e+07| loss_f: 1.186e+00| loss_ics: 1.044e-01| loss_bcs: 2.196e-01| Time: 0.01\n",
      "mean_grad_res: 0.255\n",
      "mean_grad_ics: 7.977\n",
      "mean_grad_bcs: 2.753\n",
      "It: 32600| loss: 6.659e+00| loss_res: 2.844e+07| loss_f: 6.497e+00| loss_ics: 1.020e-02| loss_bcs: 1.520e-01| Time: 0.01\n",
      "mean_grad_res: 1.539\n",
      "mean_grad_ics: 1.908\n",
      "mean_grad_bcs: 1.160\n",
      "It: 32700| loss: 2.339e+00| loss_res: 4.937e+08| loss_f: 2.157e+00| loss_ics: 1.581e-02| loss_bcs: 1.663e-01| Time: 0.01\n",
      "mean_grad_res: 0.550\n",
      "mean_grad_ics: 2.478\n",
      "mean_grad_bcs: 3.465\n",
      "It: 32800| loss: 8.168e+00| loss_res: 4.475e+08| loss_f: 8.036e+00| loss_ics: 2.781e-02| loss_bcs: 1.041e-01| Time: 0.01\n",
      "mean_grad_res: 3.404\n",
      "mean_grad_ics: 6.427\n",
      "mean_grad_bcs: 0.677\n",
      "It: 32900| loss: 1.612e+00| loss_res: 1.070e+07| loss_f: 1.443e+00| loss_ics: 7.172e-03| loss_bcs: 1.623e-01| Time: 0.01\n",
      "mean_grad_res: 0.879\n",
      "mean_grad_ics: 1.786\n",
      "mean_grad_bcs: 1.939\n",
      "It: 33000| loss: 3.310e+00| loss_res: 2.658e+09| loss_f: 3.100e+00| loss_ics: 1.287e-02| loss_bcs: 1.971e-01| Time: 0.01\n",
      "mean_grad_res: 3.443\n",
      "mean_grad_ics: 4.881\n",
      "mean_grad_bcs: 4.990\n",
      "It: 33100| loss: 7.375e+00| loss_res: 5.322e+08| loss_f: 7.224e+00| loss_ics: 1.025e-02| loss_bcs: 1.404e-01| Time: 0.01\n",
      "mean_grad_res: 3.334\n",
      "mean_grad_ics: 4.076\n",
      "mean_grad_bcs: 1.491\n",
      "It: 33200| loss: 1.351e+00| loss_res: 4.831e+07| loss_f: 1.206e+00| loss_ics: 8.532e-03| loss_bcs: 1.360e-01| Time: 0.01\n",
      "mean_grad_res: 1.346\n",
      "mean_grad_ics: 5.610\n",
      "mean_grad_bcs: 1.519\n",
      "It: 33300| loss: 2.774e+00| loss_res: 2.766e+08| loss_f: 2.642e+00| loss_ics: 7.653e-03| loss_bcs: 1.239e-01| Time: 0.00\n",
      "mean_grad_res: 5.146\n",
      "mean_grad_ics: 0.928\n",
      "mean_grad_bcs: 2.325\n",
      "It: 33400| loss: 3.197e+00| loss_res: 3.582e+07| loss_f: 3.039e+00| loss_ics: 3.426e-02| loss_bcs: 1.228e-01| Time: 0.01\n",
      "mean_grad_res: 1.019\n",
      "mean_grad_ics: 6.188\n",
      "mean_grad_bcs: 0.905\n",
      "It: 33500| loss: 6.022e+00| loss_res: 2.614e+09| loss_f: 5.858e+00| loss_ics: 4.363e-02| loss_bcs: 1.204e-01| Time: 0.01\n",
      "mean_grad_res: 3.543\n",
      "mean_grad_ics: 6.855\n",
      "mean_grad_bcs: 0.621\n",
      "It: 33600| loss: 3.351e+00| loss_res: 1.878e+09| loss_f: 3.212e+00| loss_ics: 8.466e-03| loss_bcs: 1.310e-01| Time: 0.01\n",
      "mean_grad_res: 1.254\n",
      "mean_grad_ics: 3.884\n",
      "mean_grad_bcs: 3.159\n",
      "It: 33700| loss: 1.569e+00| loss_res: 2.906e+07| loss_f: 1.430e+00| loss_ics: 7.667e-03| loss_bcs: 1.315e-01| Time: 0.00\n",
      "mean_grad_res: 0.810\n",
      "mean_grad_ics: 1.457\n",
      "mean_grad_bcs: 1.238\n",
      "It: 33800| loss: 5.091e+00| loss_res: 1.751e+08| loss_f: 4.862e+00| loss_ics: 8.774e-02| loss_bcs: 1.416e-01| Time: 0.01\n",
      "mean_grad_res: 1.283\n",
      "mean_grad_ics: 5.966\n",
      "mean_grad_bcs: 2.140\n",
      "It: 33900| loss: 5.473e+00| loss_res: 6.004e+07| loss_f: 5.308e+00| loss_ics: 1.411e-02| loss_bcs: 1.512e-01| Time: 0.01\n",
      "mean_grad_res: 2.018\n",
      "mean_grad_ics: 6.780\n",
      "mean_grad_bcs: 2.076\n",
      "It: 34000| loss: 1.352e+00| loss_res: 5.354e+08| loss_f: 1.193e+00| loss_ics: 8.261e-03| loss_bcs: 1.505e-01| Time: 0.01\n",
      "mean_grad_res: 0.755\n",
      "mean_grad_ics: 2.148\n",
      "mean_grad_bcs: 2.163\n",
      "It: 34100| loss: 1.008e+00| loss_res: 5.938e+06| loss_f: 7.481e-01| loss_ics: 1.648e-01| loss_bcs: 9.492e-02| Time: 0.01\n",
      "mean_grad_res: 0.617\n",
      "mean_grad_ics: 17.761\n",
      "mean_grad_bcs: 0.707\n",
      "It: 34200| loss: 5.882e+00| loss_res: 2.844e+07| loss_f: 5.731e+00| loss_ics: 3.073e-02| loss_bcs: 1.202e-01| Time: 0.01\n",
      "mean_grad_res: 0.480\n",
      "mean_grad_ics: 2.118\n",
      "mean_grad_bcs: 0.822\n",
      "It: 34300| loss: 1.832e+00| loss_res: 1.260e+08| loss_f: 1.680e+00| loss_ics: 1.231e-02| loss_bcs: 1.392e-01| Time: 0.01\n",
      "mean_grad_res: 1.193\n",
      "mean_grad_ics: 1.804\n",
      "mean_grad_bcs: 1.624\n",
      "It: 34400| loss: 1.790e+00| loss_res: 3.772e+08| loss_f: 1.641e+00| loss_ics: 1.045e-02| loss_bcs: 1.388e-01| Time: 0.01\n",
      "mean_grad_res: 0.187\n",
      "mean_grad_ics: 0.744\n",
      "mean_grad_bcs: 1.371\n",
      "It: 34500| loss: 8.056e-01| loss_res: 1.080e+09| loss_f: 6.818e-01| loss_ics: 6.227e-03| loss_bcs: 1.176e-01| Time: 0.00\n",
      "mean_grad_res: 0.534\n",
      "mean_grad_ics: 2.475\n",
      "mean_grad_bcs: 0.292\n",
      "It: 34600| loss: 3.145e+00| loss_res: 1.672e+09| loss_f: 2.977e+00| loss_ics: 7.655e-02| loss_bcs: 9.150e-02| Time: 0.02\n",
      "mean_grad_res: 1.961\n",
      "mean_grad_ics: 8.932\n",
      "mean_grad_bcs: 0.766\n",
      "It: 34700| loss: 1.321e+00| loss_res: 4.647e+08| loss_f: 1.192e+00| loss_ics: 5.735e-03| loss_bcs: 1.230e-01| Time: 0.01\n",
      "mean_grad_res: 1.359\n",
      "mean_grad_ics: 1.011\n",
      "mean_grad_bcs: 0.816\n",
      "It: 34800| loss: 4.117e+00| loss_res: 6.404e+08| loss_f: 3.985e+00| loss_ics: 1.685e-02| loss_bcs: 1.146e-01| Time: 0.01\n",
      "mean_grad_res: 3.195\n",
      "mean_grad_ics: 4.423\n",
      "mean_grad_bcs: 1.179\n",
      "It: 34900| loss: 7.930e+00| loss_res: 9.620e+08| loss_f: 7.809e+00| loss_ics: 1.323e-02| loss_bcs: 1.080e-01| Time: 0.01\n",
      "mean_grad_res: 2.621\n",
      "mean_grad_ics: 3.634\n",
      "mean_grad_bcs: 1.618\n",
      "It: 35000| loss: 5.840e-01| loss_res: 1.765e+07| loss_f: 4.148e-01| loss_ics: 1.315e-02| loss_bcs: 1.561e-01| Time: 0.01\n",
      "mean_grad_res: 0.050\n",
      "mean_grad_ics: 5.226\n",
      "mean_grad_bcs: 2.218\n",
      "It: 35100| loss: 3.332e+00| loss_res: 2.083e+06| loss_f: 3.181e+00| loss_ics: 1.967e-02| loss_bcs: 1.320e-01| Time: 0.01\n",
      "mean_grad_res: 0.220\n",
      "mean_grad_ics: 5.840\n",
      "mean_grad_bcs: 1.824\n",
      "It: 35200| loss: 3.207e+00| loss_res: 7.574e+08| loss_f: 3.073e+00| loss_ics: 6.305e-03| loss_bcs: 1.277e-01| Time: 0.01\n",
      "mean_grad_res: 1.518\n",
      "mean_grad_ics: 2.014\n",
      "mean_grad_bcs: 1.055\n",
      "It: 35300| loss: 3.423e+00| loss_res: 3.849e+08| loss_f: 3.257e+00| loss_ics: 7.813e-03| loss_bcs: 1.578e-01| Time: 0.01\n",
      "mean_grad_res: 1.079\n",
      "mean_grad_ics: 1.571\n",
      "mean_grad_bcs: 3.633\n",
      "It: 35400| loss: 2.386e+00| loss_res: 2.023e+07| loss_f: 2.224e+00| loss_ics: 9.084e-03| loss_bcs: 1.538e-01| Time: 0.01\n",
      "mean_grad_res: 0.253\n",
      "mean_grad_ics: 0.822\n",
      "mean_grad_bcs: 2.192\n",
      "It: 35500| loss: 1.216e+00| loss_res: 1.514e+07| loss_f: 1.083e+00| loss_ics: 1.129e-02| loss_bcs: 1.222e-01| Time: 0.01\n",
      "mean_grad_res: 0.559\n",
      "mean_grad_ics: 1.881\n",
      "mean_grad_bcs: 0.413\n",
      "It: 35600| loss: 8.447e-01| loss_res: 2.614e+08| loss_f: 6.899e-01| loss_ics: 7.389e-03| loss_bcs: 1.475e-01| Time: 0.00\n",
      "mean_grad_res: 0.362\n",
      "mean_grad_ics: 3.189\n",
      "mean_grad_bcs: 3.277\n",
      "It: 35700| loss: 2.093e+00| loss_res: 3.015e+07| loss_f: 1.929e+00| loss_ics: 3.622e-02| loss_bcs: 1.271e-01| Time: 0.01\n",
      "mean_grad_res: 1.294\n",
      "mean_grad_ics: 3.177\n",
      "mean_grad_bcs: 0.690\n",
      "It: 35800| loss: 4.761e+00| loss_res: 6.609e+08| loss_f: 4.620e+00| loss_ics: 9.451e-03| loss_bcs: 1.313e-01| Time: 0.01\n",
      "mean_grad_res: 1.560\n",
      "mean_grad_ics: 0.836\n",
      "mean_grad_bcs: 1.252\n",
      "It: 35900| loss: 4.723e+00| loss_res: 1.400e+08| loss_f: 4.593e+00| loss_ics: 1.687e-02| loss_bcs: 1.132e-01| Time: 0.01\n",
      "mean_grad_res: 0.789\n",
      "mean_grad_ics: 4.633\n",
      "mean_grad_bcs: 0.384\n",
      "It: 36000| loss: 2.497e+00| loss_res: 1.830e+09| loss_f: 2.299e+00| loss_ics: 1.397e-02| loss_bcs: 1.836e-01| Time: 0.01\n",
      "mean_grad_res: 0.940\n",
      "mean_grad_ics: 5.688\n",
      "mean_grad_bcs: 3.271\n",
      "It: 36100| loss: 6.828e+00| loss_res: 2.974e+10| loss_f: 6.721e+00| loss_ics: 1.109e-02| loss_bcs: 9.584e-02| Time: 0.01\n",
      "mean_grad_res: 15.342\n",
      "mean_grad_ics: 4.968\n",
      "mean_grad_bcs: 1.408\n",
      "It: 36200| loss: 1.198e+01| loss_res: 4.801e+09| loss_f: 1.182e+01| loss_ics: 6.854e-03| loss_bcs: 1.511e-01| Time: 0.01\n",
      "mean_grad_res: 6.362\n",
      "mean_grad_ics: 2.273\n",
      "mean_grad_bcs: 5.662\n",
      "It: 36300| loss: 2.072e+00| loss_res: 1.390e+09| loss_f: 1.931e+00| loss_ics: 9.904e-03| loss_bcs: 1.307e-01| Time: 0.01\n",
      "mean_grad_res: 1.020\n",
      "mean_grad_ics: 5.129\n",
      "mean_grad_bcs: 1.161\n",
      "It: 36400| loss: 2.788e+00| loss_res: 1.431e+09| loss_f: 2.639e+00| loss_ics: 2.216e-02| loss_bcs: 1.273e-01| Time: 0.00\n",
      "mean_grad_res: 1.938\n",
      "mean_grad_ics: 1.662\n",
      "mean_grad_bcs: 1.940\n",
      "It: 36500| loss: 2.885e+00| loss_res: 1.154e+09| loss_f: 2.742e+00| loss_ics: 1.174e-02| loss_bcs: 1.309e-01| Time: 0.01\n",
      "mean_grad_res: 1.193\n",
      "mean_grad_ics: 2.468\n",
      "mean_grad_bcs: 1.746\n",
      "It: 36600| loss: 3.060e+00| loss_res: 1.815e+09| loss_f: 2.907e+00| loss_ics: 3.320e-02| loss_bcs: 1.198e-01| Time: 0.01\n",
      "mean_grad_res: 1.095\n",
      "mean_grad_ics: 2.023\n",
      "mean_grad_bcs: 1.616\n",
      "It: 36700| loss: 1.755e+00| loss_res: 1.299e+07| loss_f: 1.608e+00| loss_ics: 9.667e-03| loss_bcs: 1.373e-01| Time: 0.01\n",
      "mean_grad_res: 0.968\n",
      "mean_grad_ics: 4.845\n",
      "mean_grad_bcs: 1.814\n",
      "It: 36800| loss: 1.829e+00| loss_res: 9.317e+06| loss_f: 1.668e+00| loss_ics: 6.106e-03| loss_bcs: 1.550e-01| Time: 0.01\n",
      "mean_grad_res: 0.687\n",
      "mean_grad_ics: 4.872\n",
      "mean_grad_bcs: 0.688\n",
      "It: 36900| loss: 8.775e-01| loss_res: 7.614e+09| loss_f: 7.593e-01| loss_ics: 6.062e-03| loss_bcs: 1.121e-01| Time: 0.01\n",
      "mean_grad_res: 0.736\n",
      "mean_grad_ics: 4.314\n",
      "mean_grad_bcs: 1.414\n",
      "It: 37000| loss: 2.089e+00| loss_res: 5.995e+09| loss_f: 1.889e+00| loss_ics: 3.335e-02| loss_bcs: 1.662e-01| Time: 0.01\n",
      "mean_grad_res: 1.083\n",
      "mean_grad_ics: 4.208\n",
      "mean_grad_bcs: 1.748\n",
      "It: 37100| loss: 2.294e+00| loss_res: 2.688e+07| loss_f: 2.189e+00| loss_ics: 1.697e-02| loss_bcs: 8.791e-02| Time: 0.01\n",
      "mean_grad_res: 1.819\n",
      "mean_grad_ics: 1.588\n",
      "mean_grad_bcs: 0.656\n",
      "It: 37200| loss: 7.446e+00| loss_res: 6.634e+08| loss_f: 7.317e+00| loss_ics: 1.219e-02| loss_bcs: 1.172e-01| Time: 0.00\n",
      "mean_grad_res: 2.276\n",
      "mean_grad_ics: 2.427\n",
      "mean_grad_bcs: 0.347\n",
      "It: 37300| loss: 5.498e+00| loss_res: 2.145e+08| loss_f: 5.361e+00| loss_ics: 1.490e-02| loss_bcs: 1.216e-01| Time: 0.00\n",
      "mean_grad_res: 2.491\n",
      "mean_grad_ics: 2.740\n",
      "mean_grad_bcs: 0.913\n",
      "It: 37400| loss: 3.675e+00| loss_res: 1.265e+09| loss_f: 3.555e+00| loss_ics: 1.888e-02| loss_bcs: 1.016e-01| Time: 0.00\n",
      "mean_grad_res: 1.054\n",
      "mean_grad_ics: 1.332\n",
      "mean_grad_bcs: 1.351\n",
      "It: 37500| loss: 7.668e-01| loss_res: 6.134e+08| loss_f: 5.923e-01| loss_ics: 1.010e-02| loss_bcs: 1.644e-01| Time: 0.00\n",
      "mean_grad_res: 0.271\n",
      "mean_grad_ics: 3.678\n",
      "mean_grad_bcs: 0.703\n",
      "It: 37600| loss: 1.205e+00| loss_res: 3.697e+08| loss_f: 1.063e+00| loss_ics: 9.239e-03| loss_bcs: 1.334e-01| Time: 0.01\n",
      "mean_grad_res: 0.784\n",
      "mean_grad_ics: 2.976\n",
      "mean_grad_bcs: 0.739\n",
      "It: 37700| loss: 1.344e+00| loss_res: 3.385e+07| loss_f: 1.222e+00| loss_ics: 1.059e-02| loss_bcs: 1.113e-01| Time: 0.00\n",
      "mean_grad_res: 0.395\n",
      "mean_grad_ics: 4.487\n",
      "mean_grad_bcs: 0.272\n",
      "It: 37800| loss: 8.678e-01| loss_res: 7.838e+06| loss_f: 7.084e-01| loss_ics: 8.720e-03| loss_bcs: 1.507e-01| Time: 0.00\n",
      "mean_grad_res: 0.240\n",
      "mean_grad_ics: 2.722\n",
      "mean_grad_bcs: 1.636\n",
      "It: 37900| loss: 2.980e+00| loss_res: 3.779e+07| loss_f: 2.758e+00| loss_ics: 2.628e-02| loss_bcs: 1.962e-01| Time: 0.00\n",
      "mean_grad_res: 0.926\n",
      "mean_grad_ics: 5.896\n",
      "mean_grad_bcs: 4.128\n",
      "It: 38000| loss: 3.225e+00| loss_res: 1.093e+09| loss_f: 3.077e+00| loss_ics: 1.512e-02| loss_bcs: 1.335e-01| Time: 0.01\n",
      "mean_grad_res: 1.146\n",
      "mean_grad_ics: 6.215\n",
      "mean_grad_bcs: 1.040\n",
      "It: 38100| loss: 8.987e-01| loss_res: 1.549e+09| loss_f: 7.703e-01| loss_ics: 1.660e-02| loss_bcs: 1.118e-01| Time: 0.01\n",
      "mean_grad_res: 0.524\n",
      "mean_grad_ics: 4.773\n",
      "mean_grad_bcs: 1.355\n",
      "It: 38200| loss: 4.202e+00| loss_res: 9.692e+06| loss_f: 3.966e+00| loss_ics: 2.078e-02| loss_bcs: 2.152e-01| Time: 0.00\n",
      "mean_grad_res: 1.319\n",
      "mean_grad_ics: 6.086\n",
      "mean_grad_bcs: 5.805\n",
      "It: 38300| loss: 8.095e+00| loss_res: 1.163e+07| loss_f: 7.966e+00| loss_ics: 1.146e-02| loss_bcs: 1.182e-01| Time: 0.01\n",
      "mean_grad_res: 1.919\n",
      "mean_grad_ics: 2.850\n",
      "mean_grad_bcs: 0.872\n",
      "It: 38400| loss: 6.868e-01| loss_res: 2.614e+07| loss_f: 5.214e-01| loss_ics: 4.017e-02| loss_bcs: 1.252e-01| Time: 0.01\n",
      "mean_grad_res: 0.125\n",
      "mean_grad_ics: 3.058\n",
      "mean_grad_bcs: 1.998\n",
      "It: 38500| loss: 3.371e+00| loss_res: 2.361e+09| loss_f: 3.196e+00| loss_ics: 2.775e-02| loss_bcs: 1.470e-01| Time: 0.01\n",
      "mean_grad_res: 1.360\n",
      "mean_grad_ics: 16.509\n",
      "mean_grad_bcs: 3.952\n",
      "It: 38600| loss: 3.934e+00| loss_res: 1.363e+09| loss_f: 3.752e+00| loss_ics: 2.486e-02| loss_bcs: 1.569e-01| Time: 0.00\n",
      "mean_grad_res: 14.977\n",
      "mean_grad_ics: 4.794\n",
      "mean_grad_bcs: 3.594\n",
      "It: 38700| loss: 5.319e+00| loss_res: 4.890e+07| loss_f: 5.167e+00| loss_ics: 9.125e-03| loss_bcs: 1.436e-01| Time: 0.01\n",
      "mean_grad_res: 0.445\n",
      "mean_grad_ics: 1.475\n",
      "mean_grad_bcs: 2.773\n",
      "It: 38800| loss: 4.735e+00| loss_res: 5.759e+08| loss_f: 4.570e+00| loss_ics: 1.104e-02| loss_bcs: 1.542e-01| Time: 0.01\n",
      "mean_grad_res: 2.849\n",
      "mean_grad_ics: 6.104\n",
      "mean_grad_bcs: 1.982\n",
      "It: 38900| loss: 2.637e+00| loss_res: 6.693e+08| loss_f: 2.522e+00| loss_ics: 1.062e-02| loss_bcs: 1.049e-01| Time: 0.01\n",
      "mean_grad_res: 1.733\n",
      "mean_grad_ics: 2.257\n",
      "mean_grad_bcs: 1.190\n",
      "It: 39000| loss: 1.356e+00| loss_res: 4.604e+07| loss_f: 1.218e+00| loss_ics: 2.484e-02| loss_bcs: 1.133e-01| Time: 0.01\n",
      "mean_grad_res: 0.459\n",
      "mean_grad_ics: 4.106\n",
      "mean_grad_bcs: 0.353\n",
      "It: 39100| loss: 1.545e+01| loss_res: 3.853e+09| loss_f: 1.527e+01| loss_ics: 1.973e-02| loss_bcs: 1.583e-01| Time: 0.01\n",
      "mean_grad_res: 7.240\n",
      "mean_grad_ics: 1.837\n",
      "mean_grad_bcs: 0.738\n",
      "It: 39200| loss: 4.161e+00| loss_res: 6.284e+09| loss_f: 3.921e+00| loss_ics: 1.945e-02| loss_bcs: 2.201e-01| Time: 0.01\n",
      "mean_grad_res: 3.607\n",
      "mean_grad_ics: 7.094\n",
      "mean_grad_bcs: 4.943\n",
      "It: 39300| loss: 8.412e-01| loss_res: 3.457e+08| loss_f: 5.980e-01| loss_ics: 3.386e-02| loss_bcs: 2.094e-01| Time: 0.01\n",
      "mean_grad_res: 0.427\n",
      "mean_grad_ics: 7.432\n",
      "mean_grad_bcs: 3.219\n",
      "It: 39400| loss: 1.650e+00| loss_res: 5.781e+08| loss_f: 1.500e+00| loss_ics: 1.879e-02| loss_bcs: 1.308e-01| Time: 0.01\n",
      "mean_grad_res: 0.747\n",
      "mean_grad_ics: 4.863\n",
      "mean_grad_bcs: 0.464\n",
      "It: 39500| loss: 1.850e+00| loss_res: 1.414e+09| loss_f: 1.703e+00| loss_ics: 2.248e-02| loss_bcs: 1.247e-01| Time: 0.01\n",
      "mean_grad_res: 4.512\n",
      "mean_grad_ics: 5.655\n",
      "mean_grad_bcs: 0.830\n",
      "It: 39600| loss: 1.114e+01| loss_res: 3.674e+09| loss_f: 1.101e+01| loss_ics: 1.416e-02| loss_bcs: 1.144e-01| Time: 0.01\n",
      "mean_grad_res: 15.093\n",
      "mean_grad_ics: 2.544\n",
      "mean_grad_bcs: 1.615\n",
      "It: 39700| loss: 1.246e+00| loss_res: 1.711e+07| loss_f: 1.095e+00| loss_ics: 8.838e-03| loss_bcs: 1.418e-01| Time: 0.01\n",
      "mean_grad_res: 0.596\n",
      "mean_grad_ics: 2.802\n",
      "mean_grad_bcs: 1.016\n",
      "It: 39800| loss: 4.422e+00| loss_res: 4.709e+08| loss_f: 4.276e+00| loss_ics: 9.595e-03| loss_bcs: 1.370e-01| Time: 0.00\n",
      "mean_grad_res: 2.104\n",
      "mean_grad_ics: 3.015\n",
      "mean_grad_bcs: 1.123\n",
      "It: 39900| loss: 2.387e+00| loss_res: 2.262e+07| loss_f: 2.178e+00| loss_ics: 4.489e-02| loss_bcs: 1.635e-01| Time: 0.01\n",
      "mean_grad_res: 0.544\n",
      "mean_grad_ics: 6.980\n",
      "mean_grad_bcs: 2.119\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000| loss: 9.335e-01| loss_res: 3.224e+06| loss_f: 7.914e-01| loss_ics: 1.714e-02| loss_bcs: 1.250e-01| Time: 0.62\n",
      "mean_grad_res: 0.123\n",
      "mean_grad_ics: 9.419\n",
      "mean_grad_bcs: 1.935\n",
      "average lambda_bc1.3560e+00\n",
      "average lambda_bc3.7132e+00\n",
      "average lambda_res1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative L2 error_u: 7.61e+00\n",
      "Relative L2 error_f: 6.24e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Save uv NN parameters successfully in %s ...checkpoints/Jan-11-2024_18-13-04-025941_M2\n",
      "Final loss loss: 9.335278e-01\n",
      "Final loss loss_res: 3.223569e+06\n",
      "Final loss loss_f: 7.914183e-01\n",
      "Final loss loss_ics: 1.713917e-02\n",
      "Final loss loss_bcs: 1.249703e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "average of time_list: 298.3884103298187\n",
      "average of error_u_list: 7.610590456990409\n",
      "average of error_f_list: 623.6818394255982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters of equations\n",
    "alpha = -1.0\n",
    "beta = 0.0\n",
    "gamma = 1.0\n",
    "k = 3\n",
    "# Domain boundaries\n",
    "ics_coords = np.array([[0.0, 0.0], [0.0, 1.0]])\n",
    "bc1_coords = np.array([[0.0, 0.0], [1.0, 0.0]])\n",
    "bc2_coords = np.array([[0.0, 1.0], [1.0, 1.0]])\n",
    "dom_coords = np.array([[0.0, 0.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Define model\n",
    "layers = [2, 50, 50, 50, 50, 50, 1]\n",
    "mode = 'M2'          # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False  # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\" ]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "        # Create initial conditions samplers\n",
    "        ics_sampler = Sampler(2, ics_coords, lambda x: u(x), name='Initial Condition 1')\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x), name='Dirichlet BC2')\n",
    "        bcs_sampler = [bc1, bc2]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, alpha, beta, gamma, k), name='Forcing')\n",
    "        bcs_sampler = [bc1, bc2]\n",
    "\n",
    "        # [elapsed, error_u , error_f] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma ,k ,mode , stiff_ratio ,  X_star , u_star , f_star)\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "\n",
    "            model = Klein_Gordon(layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, mode, sess)\n",
    "\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                model.train(nIter=nIter )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                model.trainmb(nIter=nIter, batch_size=128)\n",
    "            else:\n",
    "                print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            f_pred = model.predict_r(X_star)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "            print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "            \n",
    "            model.print(\"average lambda_bc\" , np.average(model.mean_grad_bcs_log))\n",
    "            model.print(\"average lambda_bc\" , np.average(model.mean_grad_ics_log))\n",
    "            model.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close()  \n",
    "            model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( t , x , X_star , u_star , u_pred , f_star , f_pred)\n",
    "\n",
    "            time_list.append(elapsed)\n",
    "            error_u_list.append(error_u)\n",
    "            error_f_list.append(error_f)\n",
    "\n",
    "    # print(\"\\n\\nMethod: \", mtd)\n",
    "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    print(\"average of error_f_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,  error_f_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_Helmholtz_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "# Predictions\n",
    "u_pred = mode.predict_u(X_star)\n",
    "f_pred = mode.predict_r(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, F_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, F_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(F_star - F_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss\n",
    "loss_r = mode.loss_r_log\n",
    "loss_u = mode.loss_u_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_r, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_u, label='$\\mathcal{L}_{u}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adaptive Constant\n",
    "mean_grad_ics = mode.adaptive_constant_ics_log\n",
    "mean_grad_bcs = mode.adaptive_constant_bcs_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(mean_grad_ics, label='$\\lambda_{u_0}$')\n",
    "ax.plot(mean_grad_bcs, label='$\\lambda_{u_b}$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_ics = mode.dict_gradients_ics_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 8))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(2, 3, cnt)\n",
    "    gradients_ics = data_gradients_ics['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    sns.distplot(gradients_ics, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_0}\\mathcal{L}_{u_0}$')\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.get_legend().remove()\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.3, 0.01),\n",
    "                borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Eigenvalues of Hessian of losses if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = mode.eigenvalue_log\n",
    "    eigenvalues_ics_list = mode.eigenvalue_ics_log\n",
    "    eigenvalues_bcs_list = mode.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = mode.eigenvalue_res_log\n",
    "\n",
    "    eigenvalues_ics = eigenvalues_ics_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_ics, label='$\\mathcal{L}_{u_0}$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
