{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "######################################################################################################\n",
    "def u(x, a_1, a_2):\n",
    "    return np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_xx(x, a_1, a_2):\n",
    "    return - (a_1 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_yy(x, a_1, a_2):\n",
    "    return - (a_2 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "# Forcing\n",
    "def f(x, a_1, a_2, lam):\n",
    "    return u_xx(x, a_1, a_2) + u_yy(x, a_1, a_2) + lam * u(x, a_1, a_2)\n",
    "\n",
    "def operator(u, x1, x2, lam, sigma_x1=1.0, sigma_x2=1.0):\n",
    "    u_x1 = tf.gradients(u, x1)[0] / sigma_x1\n",
    "    u_x2 = tf.gradients(u, x2)[0] / sigma_x2\n",
    "    u_xx1 = tf.gradients(u_x1, x1)[0] / sigma_x1\n",
    "    u_xx2 = tf.gradients(u_x2, x2)[0] / sigma_x2\n",
    "    residual = u_xx1 + u_xx2 + lam * u\n",
    "    return residual\n",
    "#######################################################################################################\n",
    "\n",
    "class Helmholtz2D:\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess):\n",
    "        # Normalization constants\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x1, self.sigma_x1 = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x2, self.sigma_x2 = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Helmoholtz constant\n",
    "        self.lam = tf.constant(lam, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = mode\n",
    "\n",
    "        # Record stiff ratio\n",
    "        # self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # if mode in ['M3', 'M4']:\n",
    "        #     # Initialize encoder weights and biases\n",
    "        #     self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        #     self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x1_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # Define placeholder for adaptive constant\n",
    "        self.adaptive_constant_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred = self.net_u(self.x1_bc1_tf, self.x2_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.x1_bc2_tf, self.x2_bc2_tf)\n",
    "        self.u_bc3_pred = self.net_u(self.x1_bc3_tf, self.x2_bc3_tf)\n",
    "        self.u_bc4_pred = self.net_u(self.x1_bc4_tf, self.x2_bc4_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x1_u_tf, self.x2_u_tf)\n",
    "        self.r_pred = self.net_r(self.x1_r_tf, self.x2_r_tf)\n",
    "\n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_tf - self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_tf - self.u_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.u_bc3_tf - self.u_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.u_bc4_tf - self.u_bc4_pred))\n",
    "        self.loss_bcs = self.adaptive_constant_tf * (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        # Compute and store the adaptive constant\n",
    "        self.adpative_constant_log = []\n",
    "        self.adaptive_constant_list = []\n",
    "        \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.adaptive_constant = self.max_grad_res / self.mean_grad_bcs\n",
    "\n",
    "        # # Stiff Ratio\n",
    "        # if self.stiff_ratio:\n",
    "        #     self.Hessian, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "        #     self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "        #     self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "        #     self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "        #     self.eigenvalue_log = []\n",
    "        #     self.eigenvalue_bcs_log = []\n",
    "        #     self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "     # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,  self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P), dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),  dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),  dtype='float32')\n",
    "\n",
    "        return H, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self,size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, x1, x2):\n",
    "        u = self.forward_pass(tf.concat([x1, x2], 1))\n",
    "        return u\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x1, x2):\n",
    "        u = self.net_u(x1, x2)\n",
    "        residual = self.operator(u, x1, x2,\n",
    "                                 self.lam,\n",
    "                                 self.sigma_x1,\n",
    "                                 self.sigma_x2)\n",
    "        return residual\n",
    "\n",
    "    # Feed minibatch\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter=10000, batch_size=128):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            if self.stiff_ratio:\n",
    "                if it % 1000 == 0:\n",
    "                    print(\"Eigenvalues information stored ...\")\n",
    "                    eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "                                                                                   self.eigenvalues_bcs,\n",
    "                                                                                   self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # Log eigenvalues\n",
    "                    self.eigenvalue_log.append(eigenvalues)\n",
    "                    self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                if self.model in ['M2', 'M4']:\n",
    "                    adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                    self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta) \\\n",
    "                                                 + self.beta * self.adaptive_constant_val\n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' %\n",
    "                      (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # Store gradients\n",
    "            if it % 10000 == 0:\n",
    "                self.save_gradients(tf_dict)\n",
    "                print(\"Gradients information stored ...\")\n",
    "\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter , bcbatch_size , fbatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Fetch boundary mini-batches\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], bcbatch_size)\n",
    "        X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], bcbatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, fbatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.u_bc3_tf: u_bc3_batch,\n",
    "                    self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.u_bc4_tf: u_bc4_batch,\n",
    "                    self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                    self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                    }\n",
    "\n",
    "\n",
    "        for it in range(nIter):\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                # self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                #     self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "\n",
    "                # self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "  # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value ,  loss_bcs_value, loss_res_value = self.sess.run([self.loss, self.loss_bcs, self.loss_res] , tf_dict)\n",
    "\n",
    " \n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, elapsed))\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "                print('adaptive_constant_val: %f' % (self.adaptive_constant_val))\n",
    "\n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_u_tf: X_star[:, 0:1], self.x2_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_r_tf: X_star[:, 0:1], self.x2_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n",
    "\n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        #plt.show()\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "                \n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(r'$x_1$')\n",
    "        plt.ylabel(r'$x_2$')\n",
    "        plt.title('Exact $u(x)$')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(r'$x_1$')\n",
    "        plt.ylabel(r'$x_2$')\n",
    "        plt.title('Predicted $u(x)$')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(r'$x_1$')\n",
    "        plt.ylabel(r'$x_2$')\n",
    "        plt.title('Absolute error')\n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.pcolor(x1, x2, F_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(r'$x_1$')\n",
    "        plt.ylabel(r'$x_2$')\n",
    "        plt.title('Exact $u(x)$')\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.pcolor(x1, x2, F_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(r'$x_1$')\n",
    "        plt.ylabel(r'$x_2$')\n",
    "        plt.title('Predicted $u(x)$')\n",
    "\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.pcolor(x1, x2, np.abs(F_star - F_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(r'$x_1$')\n",
    "        plt.ylabel(r'$x_2$')\n",
    "        plt.title('Absolute error')\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, lam ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f ,  model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16158/2204119787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mcbar_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"each\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                      cbar_size=\"5%\", cbar_pad=0.0)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/mpl_toolkits/axes_grid1/axes_grid.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_geometry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1584x360 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_1 = plt.figure(1, figsize=(22, 5))\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                     label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                     cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                     cbar_size=\"5%\", cbar_pad=0.0)\n",
    "grid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "    from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "    ### Plot ###\n",
    "\n",
    "    # Exact solution & Predicted solution\n",
    "    # Exact soluton\n",
    "    U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "    F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "    # Predicted solution\n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "    F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "\n",
    "    fig_1 = plt.figure(1, figsize=(22, 5))\n",
    "    grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                     label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                     cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                     cbar_size=\"5%\", cbar_pad=0.0)\n",
    "   # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "    minmax_list = []\n",
    "    kwargs_list = []\n",
    "    for d in data:\n",
    "        # if(local):\n",
    "        #     minmax_list.append([np.min(d), np.max(d)])\n",
    "        # else:\n",
    "        minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "        kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60),\n",
    "            cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "    titles = []\n",
    "    data = []\n",
    "    \n",
    "    for ax, z, kwargs, minmax, title in zip(grid, data, kwargs_list, minmax_list, titles):\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.title('Exact $u(x)$')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.title('Predicted $u(x)$')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.title('Absolute error')\n",
    "\n",
    "    # fig_2 = plt.figure(1, figsize=(18, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.pcolor(x1, x2, F_star, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.title('Exact $u(x)$')\n",
    "\n",
    "    plt.subplot(1, 3,2)\n",
    "    plt.pcolor(x1, x2, F_pred, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.title('Predicted $u(x)$')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.pcolor(x1, x2, np.abs(F_star - F_pred), cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.title('Absolute error')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "plt_prediction( model , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16158/2627959977.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16158/2627959977.py:83: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16158/2627959977.py:84: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16158/2627959977.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16158/3650639197.py:280: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16158/3650639197.py:119: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 01:50:01.037124: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 01:50:01.062687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-12 01:50:01.063219: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5620fac87df0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-12 01:50:01.063234: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-12 01:50:01.066875: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_16158/3650639197.py:171: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16158/3650639197.py:173: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16158/3650639197.py:223: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "It: 0, Loss: 6.886e+03, Loss_bcs: 8.749e-02, Loss_res: 6.886e+03,Time: 1.52\n",
      "adaptive_constant_val: 7.099223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 10, Loss: 7.296e+03, Loss_bcs: 5.894e-01, Loss_res: 7.295e+03,Time: 0.01\n",
      "adaptive_constant_val: 7.851689\n",
      "It: 20, Loss: 8.069e+03, Loss_bcs: 3.448e-01, Loss_res: 8.068e+03,Time: 0.01\n",
      "adaptive_constant_val: 9.927831\n",
      "It: 30, Loss: 7.032e+03, Loss_bcs: 2.004e-01, Loss_res: 7.032e+03,Time: 0.01\n",
      "adaptive_constant_val: 11.735132\n",
      "It: 40, Loss: 8.077e+03, Loss_bcs: 4.340e-01, Loss_res: 8.077e+03,Time: 0.01\n",
      "adaptive_constant_val: 14.364941\n",
      "It: 50, Loss: 7.865e+03, Loss_bcs: 5.320e-01, Loss_res: 7.865e+03,Time: 0.01\n",
      "adaptive_constant_val: 14.207112\n",
      "It: 60, Loss: 7.719e+03, Loss_bcs: 5.676e-01, Loss_res: 7.719e+03,Time: 0.01\n",
      "adaptive_constant_val: 14.785601\n",
      "It: 70, Loss: 6.905e+03, Loss_bcs: 3.464e-01, Loss_res: 6.904e+03,Time: 0.01\n",
      "adaptive_constant_val: 26.296108\n",
      "It: 80, Loss: 6.061e+03, Loss_bcs: 9.222e-01, Loss_res: 6.060e+03,Time: 0.01\n",
      "adaptive_constant_val: 25.720902\n",
      "It: 90, Loss: 7.907e+03, Loss_bcs: 1.087e+00, Loss_res: 7.906e+03,Time: 0.01\n",
      "adaptive_constant_val: 27.667521\n",
      "It: 100, Loss: 7.660e+03, Loss_bcs: 1.976e+00, Loss_res: 7.658e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.122910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 110, Loss: 8.341e+03, Loss_bcs: 3.454e+00, Loss_res: 8.338e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.793460\n",
      "It: 120, Loss: 6.164e+03, Loss_bcs: 5.282e+00, Loss_res: 6.159e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.354971\n",
      "It: 130, Loss: 6.750e+03, Loss_bcs: 8.667e+00, Loss_res: 6.741e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.812446\n",
      "It: 140, Loss: 7.275e+03, Loss_bcs: 7.726e+00, Loss_res: 7.267e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.217734\n",
      "It: 150, Loss: 6.176e+03, Loss_bcs: 6.531e+00, Loss_res: 6.170e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.277033\n",
      "It: 160, Loss: 6.614e+03, Loss_bcs: 5.587e+00, Loss_res: 6.608e+03,Time: 0.01\n",
      "adaptive_constant_val: 33.258400\n",
      "It: 170, Loss: 6.249e+03, Loss_bcs: 6.365e+00, Loss_res: 6.242e+03,Time: 0.01\n",
      "adaptive_constant_val: 34.418056\n",
      "It: 180, Loss: 7.176e+03, Loss_bcs: 7.487e+00, Loss_res: 7.168e+03,Time: 0.01\n",
      "adaptive_constant_val: 32.577850\n",
      "It: 190, Loss: 7.324e+03, Loss_bcs: 6.807e+00, Loss_res: 7.317e+03,Time: 0.01\n",
      "adaptive_constant_val: 40.165985\n",
      "It: 200, Loss: 6.354e+03, Loss_bcs: 9.051e+00, Loss_res: 6.345e+03,Time: 0.01\n",
      "adaptive_constant_val: 41.177889\n",
      "It: 210, Loss: 6.302e+03, Loss_bcs: 1.077e+01, Loss_res: 6.292e+03,Time: 0.01\n",
      "adaptive_constant_val: 41.321635\n",
      "It: 220, Loss: 7.373e+03, Loss_bcs: 1.015e+01, Loss_res: 7.362e+03,Time: 0.01\n",
      "adaptive_constant_val: 39.933713\n",
      "It: 230, Loss: 7.159e+03, Loss_bcs: 9.728e+00, Loss_res: 7.150e+03,Time: 0.01\n",
      "adaptive_constant_val: 42.265773\n",
      "It: 240, Loss: 7.309e+03, Loss_bcs: 1.125e+01, Loss_res: 7.298e+03,Time: 0.01\n",
      "adaptive_constant_val: 41.196000\n",
      "It: 250, Loss: 7.210e+03, Loss_bcs: 1.057e+01, Loss_res: 7.200e+03,Time: 0.01\n",
      "adaptive_constant_val: 38.144885\n",
      "It: 260, Loss: 5.308e+03, Loss_bcs: 8.750e+00, Loss_res: 5.300e+03,Time: 0.01\n",
      "adaptive_constant_val: 36.685741\n",
      "It: 270, Loss: 7.666e+03, Loss_bcs: 1.495e+01, Loss_res: 7.651e+03,Time: 0.01\n",
      "adaptive_constant_val: 38.972133\n",
      "It: 280, Loss: 6.290e+03, Loss_bcs: 1.607e+01, Loss_res: 6.274e+03,Time: 0.01\n",
      "adaptive_constant_val: 38.644828\n",
      "It: 290, Loss: 7.870e+03, Loss_bcs: 1.441e+01, Loss_res: 7.856e+03,Time: 0.01\n",
      "adaptive_constant_val: 40.072428\n",
      "It: 300, Loss: 6.429e+03, Loss_bcs: 1.465e+01, Loss_res: 6.415e+03,Time: 0.01\n",
      "adaptive_constant_val: 47.247984\n",
      "It: 310, Loss: 5.918e+03, Loss_bcs: 1.747e+01, Loss_res: 5.901e+03,Time: 0.01\n",
      "adaptive_constant_val: 51.017859\n",
      "It: 320, Loss: 6.159e+03, Loss_bcs: 1.324e+01, Loss_res: 6.146e+03,Time: 0.01\n",
      "adaptive_constant_val: 65.370428\n",
      "It: 330, Loss: 7.142e+03, Loss_bcs: 1.522e+01, Loss_res: 7.126e+03,Time: 0.01\n",
      "adaptive_constant_val: 70.928287\n",
      "It: 340, Loss: 5.796e+03, Loss_bcs: 1.832e+01, Loss_res: 5.778e+03,Time: 0.01\n",
      "adaptive_constant_val: 81.416993\n",
      "It: 350, Loss: 6.397e+03, Loss_bcs: 1.993e+01, Loss_res: 6.377e+03,Time: 0.01\n",
      "adaptive_constant_val: 84.821669\n",
      "It: 360, Loss: 5.849e+03, Loss_bcs: 2.223e+01, Loss_res: 5.827e+03,Time: 0.01\n",
      "adaptive_constant_val: 81.727306\n",
      "It: 370, Loss: 7.034e+03, Loss_bcs: 2.482e+01, Loss_res: 7.010e+03,Time: 0.01\n",
      "adaptive_constant_val: 90.923461\n",
      "It: 380, Loss: 5.053e+03, Loss_bcs: 3.258e+01, Loss_res: 5.021e+03,Time: 0.01\n",
      "adaptive_constant_val: 99.974839\n",
      "It: 390, Loss: 4.915e+03, Loss_bcs: 4.064e+01, Loss_res: 4.874e+03,Time: 0.01\n",
      "adaptive_constant_val: 110.047877\n",
      "It: 400, Loss: 4.844e+03, Loss_bcs: 4.336e+01, Loss_res: 4.801e+03,Time: 0.01\n",
      "adaptive_constant_val: 112.418358\n",
      "It: 410, Loss: 5.780e+03, Loss_bcs: 5.078e+01, Loss_res: 5.729e+03,Time: 0.01\n",
      "adaptive_constant_val: 110.089560\n",
      "It: 420, Loss: 5.631e+03, Loss_bcs: 4.513e+01, Loss_res: 5.586e+03,Time: 0.01\n",
      "adaptive_constant_val: 104.405225\n",
      "It: 430, Loss: 6.224e+03, Loss_bcs: 4.968e+01, Loss_res: 6.174e+03,Time: 0.01\n",
      "adaptive_constant_val: 101.323928\n",
      "It: 440, Loss: 5.466e+03, Loss_bcs: 4.442e+01, Loss_res: 5.422e+03,Time: 0.01\n",
      "adaptive_constant_val: 96.686703\n",
      "It: 450, Loss: 5.538e+03, Loss_bcs: 3.964e+01, Loss_res: 5.498e+03,Time: 0.01\n",
      "adaptive_constant_val: 93.723920\n",
      "It: 460, Loss: 4.299e+03, Loss_bcs: 4.662e+01, Loss_res: 4.252e+03,Time: 0.01\n",
      "adaptive_constant_val: 88.470164\n",
      "It: 470, Loss: 5.833e+03, Loss_bcs: 6.025e+01, Loss_res: 5.773e+03,Time: 0.01\n",
      "adaptive_constant_val: 88.088413\n",
      "It: 480, Loss: 3.902e+03, Loss_bcs: 6.406e+01, Loss_res: 3.838e+03,Time: 0.01\n",
      "adaptive_constant_val: 85.288300\n",
      "It: 490, Loss: 4.928e+03, Loss_bcs: 5.802e+01, Loss_res: 4.870e+03,Time: 0.01\n",
      "adaptive_constant_val: 82.975553\n",
      "It: 500, Loss: 4.935e+03, Loss_bcs: 6.229e+01, Loss_res: 4.873e+03,Time: 0.01\n",
      "adaptive_constant_val: 80.165180\n",
      "It: 510, Loss: 4.150e+03, Loss_bcs: 4.773e+01, Loss_res: 4.103e+03,Time: 0.01\n",
      "adaptive_constant_val: 83.459728\n",
      "It: 520, Loss: 4.324e+03, Loss_bcs: 5.110e+01, Loss_res: 4.273e+03,Time: 0.01\n",
      "adaptive_constant_val: 86.082479\n",
      "It: 530, Loss: 4.470e+03, Loss_bcs: 4.416e+01, Loss_res: 4.426e+03,Time: 0.01\n",
      "adaptive_constant_val: 88.158314\n",
      "It: 540, Loss: 3.501e+03, Loss_bcs: 4.627e+01, Loss_res: 3.455e+03,Time: 0.01\n",
      "adaptive_constant_val: 90.403094\n",
      "It: 550, Loss: 4.102e+03, Loss_bcs: 4.742e+01, Loss_res: 4.055e+03,Time: 0.01\n",
      "adaptive_constant_val: 89.336355\n",
      "It: 560, Loss: 3.901e+03, Loss_bcs: 4.750e+01, Loss_res: 3.853e+03,Time: 0.01\n",
      "adaptive_constant_val: 84.679209\n",
      "It: 570, Loss: 3.465e+03, Loss_bcs: 4.278e+01, Loss_res: 3.422e+03,Time: 0.01\n",
      "adaptive_constant_val: 83.965642\n",
      "It: 580, Loss: 3.593e+03, Loss_bcs: 3.798e+01, Loss_res: 3.555e+03,Time: 0.01\n",
      "adaptive_constant_val: 84.760531\n",
      "It: 590, Loss: 4.043e+03, Loss_bcs: 5.054e+01, Loss_res: 3.992e+03,Time: 0.01\n",
      "adaptive_constant_val: 84.689019\n",
      "It: 600, Loss: 2.897e+03, Loss_bcs: 5.327e+01, Loss_res: 2.844e+03,Time: 0.01\n",
      "adaptive_constant_val: 81.585836\n",
      "It: 610, Loss: 2.711e+03, Loss_bcs: 5.603e+01, Loss_res: 2.655e+03,Time: 0.01\n",
      "adaptive_constant_val: 82.525425\n",
      "It: 620, Loss: 2.188e+03, Loss_bcs: 5.204e+01, Loss_res: 2.136e+03,Time: 0.01\n",
      "adaptive_constant_val: 89.751811\n",
      "It: 630, Loss: 3.115e+03, Loss_bcs: 6.276e+01, Loss_res: 3.052e+03,Time: 0.01\n",
      "adaptive_constant_val: 85.172975\n",
      "It: 640, Loss: 2.174e+03, Loss_bcs: 5.657e+01, Loss_res: 2.118e+03,Time: 0.01\n",
      "adaptive_constant_val: 98.491656\n",
      "It: 650, Loss: 1.852e+03, Loss_bcs: 5.891e+01, Loss_res: 1.793e+03,Time: 0.01\n",
      "adaptive_constant_val: 94.473224\n",
      "It: 660, Loss: 2.275e+03, Loss_bcs: 7.110e+01, Loss_res: 2.204e+03,Time: 0.01\n",
      "adaptive_constant_val: 95.391271\n",
      "It: 670, Loss: 2.045e+03, Loss_bcs: 6.869e+01, Loss_res: 1.977e+03,Time: 0.01\n",
      "adaptive_constant_val: 94.496297\n",
      "It: 680, Loss: 1.694e+03, Loss_bcs: 6.279e+01, Loss_res: 1.631e+03,Time: 0.01\n",
      "adaptive_constant_val: 90.268845\n",
      "It: 690, Loss: 1.816e+03, Loss_bcs: 6.404e+01, Loss_res: 1.751e+03,Time: 0.01\n",
      "adaptive_constant_val: 94.899416\n",
      "It: 700, Loss: 1.969e+03, Loss_bcs: 6.428e+01, Loss_res: 1.905e+03,Time: 0.01\n",
      "adaptive_constant_val: 98.001699\n",
      "It: 710, Loss: 2.304e+03, Loss_bcs: 8.289e+01, Loss_res: 2.221e+03,Time: 0.01\n",
      "adaptive_constant_val: 101.816120\n",
      "It: 720, Loss: 1.551e+03, Loss_bcs: 8.904e+01, Loss_res: 1.462e+03,Time: 0.01\n",
      "adaptive_constant_val: 97.678378\n",
      "It: 730, Loss: 1.650e+03, Loss_bcs: 9.015e+01, Loss_res: 1.560e+03,Time: 0.01\n",
      "adaptive_constant_val: 94.653662\n",
      "It: 740, Loss: 1.677e+03, Loss_bcs: 9.215e+01, Loss_res: 1.585e+03,Time: 0.01\n",
      "adaptive_constant_val: 100.410736\n",
      "It: 750, Loss: 1.569e+03, Loss_bcs: 9.432e+01, Loss_res: 1.475e+03,Time: 0.02\n",
      "adaptive_constant_val: 105.048922\n",
      "It: 760, Loss: 1.231e+03, Loss_bcs: 1.155e+02, Loss_res: 1.115e+03,Time: 0.01\n",
      "adaptive_constant_val: 98.232364\n",
      "It: 770, Loss: 1.186e+03, Loss_bcs: 1.209e+02, Loss_res: 1.065e+03,Time: 0.01\n",
      "adaptive_constant_val: 93.756414\n",
      "It: 780, Loss: 1.761e+03, Loss_bcs: 1.079e+02, Loss_res: 1.653e+03,Time: 0.01\n",
      "adaptive_constant_val: 89.457674\n",
      "It: 790, Loss: 1.224e+03, Loss_bcs: 9.707e+01, Loss_res: 1.127e+03,Time: 0.01\n",
      "adaptive_constant_val: 88.446552\n",
      "It: 800, Loss: 1.513e+03, Loss_bcs: 1.045e+02, Loss_res: 1.409e+03,Time: 0.01\n",
      "adaptive_constant_val: 83.364907\n",
      "It: 810, Loss: 1.094e+03, Loss_bcs: 1.030e+02, Loss_res: 9.906e+02,Time: 0.01\n",
      "adaptive_constant_val: 88.202737\n",
      "It: 820, Loss: 9.020e+02, Loss_bcs: 1.101e+02, Loss_res: 7.919e+02,Time: 0.01\n",
      "adaptive_constant_val: 91.291589\n",
      "It: 830, Loss: 9.462e+02, Loss_bcs: 1.112e+02, Loss_res: 8.350e+02,Time: 0.01\n",
      "adaptive_constant_val: 87.503036\n",
      "It: 840, Loss: 1.256e+03, Loss_bcs: 9.675e+01, Loss_res: 1.159e+03,Time: 0.01\n",
      "adaptive_constant_val: 83.775109\n",
      "It: 850, Loss: 7.207e+02, Loss_bcs: 1.023e+02, Loss_res: 6.184e+02,Time: 0.01\n",
      "adaptive_constant_val: 78.659098\n",
      "It: 860, Loss: 9.129e+02, Loss_bcs: 8.811e+01, Loss_res: 8.248e+02,Time: 0.02\n",
      "adaptive_constant_val: 76.885959\n",
      "It: 870, Loss: 1.030e+03, Loss_bcs: 8.719e+01, Loss_res: 9.427e+02,Time: 0.01\n",
      "adaptive_constant_val: 78.842030\n",
      "It: 880, Loss: 1.111e+03, Loss_bcs: 8.185e+01, Loss_res: 1.029e+03,Time: 0.01\n",
      "adaptive_constant_val: 81.495017\n",
      "It: 890, Loss: 8.574e+02, Loss_bcs: 9.075e+01, Loss_res: 7.667e+02,Time: 0.01\n",
      "adaptive_constant_val: 78.286586\n",
      "It: 900, Loss: 7.405e+02, Loss_bcs: 8.339e+01, Loss_res: 6.571e+02,Time: 0.01\n",
      "adaptive_constant_val: 77.130268\n",
      "It: 910, Loss: 9.065e+02, Loss_bcs: 8.071e+01, Loss_res: 8.258e+02,Time: 0.01\n",
      "adaptive_constant_val: 74.833893\n",
      "It: 920, Loss: 7.797e+02, Loss_bcs: 9.275e+01, Loss_res: 6.870e+02,Time: 0.01\n",
      "adaptive_constant_val: 78.633938\n",
      "It: 930, Loss: 9.280e+02, Loss_bcs: 9.032e+01, Loss_res: 8.377e+02,Time: 0.01\n",
      "adaptive_constant_val: 75.161484\n",
      "It: 940, Loss: 9.008e+02, Loss_bcs: 7.914e+01, Loss_res: 8.217e+02,Time: 0.01\n",
      "adaptive_constant_val: 80.101925\n",
      "It: 950, Loss: 7.164e+02, Loss_bcs: 8.779e+01, Loss_res: 6.286e+02,Time: 0.01\n",
      "adaptive_constant_val: 81.040546\n",
      "It: 960, Loss: 8.296e+02, Loss_bcs: 8.714e+01, Loss_res: 7.424e+02,Time: 0.01\n",
      "adaptive_constant_val: 78.741794\n",
      "It: 970, Loss: 6.476e+02, Loss_bcs: 9.143e+01, Loss_res: 5.561e+02,Time: 0.01\n",
      "adaptive_constant_val: 82.008559\n",
      "It: 980, Loss: 8.119e+02, Loss_bcs: 8.776e+01, Loss_res: 7.241e+02,Time: 0.01\n",
      "adaptive_constant_val: 87.856269\n",
      "It: 990, Loss: 7.165e+02, Loss_bcs: 9.834e+01, Loss_res: 6.182e+02,Time: 0.01\n",
      "adaptive_constant_val: 88.765193\n",
      "It: 1000, Loss: 6.192e+02, Loss_bcs: 9.339e+01, Loss_res: 5.258e+02,Time: 0.01\n",
      "adaptive_constant_val: 87.160285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 1010, Loss: 7.714e+02, Loss_bcs: 7.113e+01, Loss_res: 7.003e+02,Time: 0.01\n",
      "adaptive_constant_val: 87.375224\n",
      "It: 1020, Loss: 7.723e+02, Loss_bcs: 8.786e+01, Loss_res: 6.844e+02,Time: 0.01\n",
      "adaptive_constant_val: 86.655272\n",
      "It: 1030, Loss: 7.958e+02, Loss_bcs: 8.979e+01, Loss_res: 7.060e+02,Time: 0.01\n",
      "adaptive_constant_val: 91.301897\n",
      "It: 1040, Loss: 7.178e+02, Loss_bcs: 9.353e+01, Loss_res: 6.242e+02,Time: 0.01\n",
      "adaptive_constant_val: 91.250773\n",
      "It: 1050, Loss: 5.539e+02, Loss_bcs: 8.063e+01, Loss_res: 4.733e+02,Time: 0.01\n",
      "adaptive_constant_val: 89.914618\n",
      "It: 1060, Loss: 7.174e+02, Loss_bcs: 9.342e+01, Loss_res: 6.240e+02,Time: 0.02\n",
      "adaptive_constant_val: 92.312680\n",
      "It: 1070, Loss: 6.817e+02, Loss_bcs: 8.104e+01, Loss_res: 6.007e+02,Time: 0.01\n",
      "adaptive_constant_val: 91.923598\n",
      "It: 1080, Loss: 5.944e+02, Loss_bcs: 7.633e+01, Loss_res: 5.180e+02,Time: 0.02\n",
      "adaptive_constant_val: 98.598837\n",
      "It: 1090, Loss: 6.261e+02, Loss_bcs: 7.946e+01, Loss_res: 5.466e+02,Time: 0.01\n",
      "adaptive_constant_val: 91.969586\n",
      "It: 1100, Loss: 6.324e+02, Loss_bcs: 8.282e+01, Loss_res: 5.496e+02,Time: 0.01\n",
      "adaptive_constant_val: 89.235181\n",
      "It: 1110, Loss: 6.710e+02, Loss_bcs: 7.073e+01, Loss_res: 6.003e+02,Time: 0.01\n",
      "adaptive_constant_val: 91.260293\n",
      "It: 1120, Loss: 5.137e+02, Loss_bcs: 7.621e+01, Loss_res: 4.375e+02,Time: 0.01\n",
      "adaptive_constant_val: 85.048231\n",
      "It: 1130, Loss: 5.519e+02, Loss_bcs: 6.720e+01, Loss_res: 4.847e+02,Time: 0.01\n",
      "adaptive_constant_val: 80.347209\n",
      "It: 1140, Loss: 5.523e+02, Loss_bcs: 6.804e+01, Loss_res: 4.842e+02,Time: 0.01\n",
      "adaptive_constant_val: 79.126758\n",
      "It: 1150, Loss: 5.117e+02, Loss_bcs: 8.278e+01, Loss_res: 4.290e+02,Time: 0.01\n",
      "adaptive_constant_val: 85.306926\n",
      "It: 1160, Loss: 7.401e+02, Loss_bcs: 6.602e+01, Loss_res: 6.741e+02,Time: 0.01\n",
      "adaptive_constant_val: 80.911727\n",
      "It: 1170, Loss: 5.212e+02, Loss_bcs: 6.289e+01, Loss_res: 4.583e+02,Time: 0.01\n",
      "adaptive_constant_val: 79.045746\n",
      "It: 1180, Loss: 4.688e+02, Loss_bcs: 6.485e+01, Loss_res: 4.040e+02,Time: 0.01\n",
      "adaptive_constant_val: 78.267158\n",
      "It: 1190, Loss: 3.766e+02, Loss_bcs: 6.370e+01, Loss_res: 3.129e+02,Time: 0.01\n",
      "adaptive_constant_val: 74.939698\n",
      "It: 1200, Loss: 3.679e+02, Loss_bcs: 6.033e+01, Loss_res: 3.075e+02,Time: 0.01\n",
      "adaptive_constant_val: 71.828158\n",
      "It: 1210, Loss: 5.057e+02, Loss_bcs: 5.681e+01, Loss_res: 4.489e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.252308\n",
      "It: 1220, Loss: 4.744e+02, Loss_bcs: 5.325e+01, Loss_res: 4.212e+02,Time: 0.01\n",
      "adaptive_constant_val: 69.384694\n",
      "It: 1230, Loss: 3.703e+02, Loss_bcs: 6.362e+01, Loss_res: 3.067e+02,Time: 0.01\n",
      "adaptive_constant_val: 65.594218\n",
      "It: 1240, Loss: 4.202e+02, Loss_bcs: 5.379e+01, Loss_res: 3.664e+02,Time: 0.01\n",
      "adaptive_constant_val: 64.897528\n",
      "It: 1250, Loss: 5.264e+02, Loss_bcs: 5.561e+01, Loss_res: 4.708e+02,Time: 0.01\n",
      "adaptive_constant_val: 62.925545\n",
      "It: 1260, Loss: 5.436e+02, Loss_bcs: 5.601e+01, Loss_res: 4.876e+02,Time: 0.01\n",
      "adaptive_constant_val: 61.181941\n",
      "It: 1270, Loss: 5.325e+02, Loss_bcs: 5.073e+01, Loss_res: 4.818e+02,Time: 0.01\n",
      "adaptive_constant_val: 66.065295\n",
      "It: 1280, Loss: 4.184e+02, Loss_bcs: 6.341e+01, Loss_res: 3.550e+02,Time: 0.01\n",
      "adaptive_constant_val: 64.524327\n",
      "It: 1290, Loss: 4.211e+02, Loss_bcs: 5.744e+01, Loss_res: 3.637e+02,Time: 0.01\n",
      "adaptive_constant_val: 63.004261\n",
      "It: 1300, Loss: 3.211e+02, Loss_bcs: 5.517e+01, Loss_res: 2.660e+02,Time: 0.01\n",
      "adaptive_constant_val: 60.834929\n",
      "It: 1310, Loss: 3.882e+02, Loss_bcs: 5.564e+01, Loss_res: 3.326e+02,Time: 0.01\n",
      "adaptive_constant_val: 64.518101\n",
      "It: 1320, Loss: 4.396e+02, Loss_bcs: 5.710e+01, Loss_res: 3.825e+02,Time: 0.01\n",
      "adaptive_constant_val: 67.035234\n",
      "It: 1330, Loss: 2.457e+02, Loss_bcs: 5.170e+01, Loss_res: 1.940e+02,Time: 0.00\n",
      "adaptive_constant_val: 63.817077\n",
      "It: 1340, Loss: 3.630e+02, Loss_bcs: 4.589e+01, Loss_res: 3.171e+02,Time: 0.01\n",
      "adaptive_constant_val: 63.995782\n",
      "It: 1350, Loss: 2.501e+02, Loss_bcs: 5.307e+01, Loss_res: 1.970e+02,Time: 0.01\n",
      "adaptive_constant_val: 62.881515\n",
      "It: 1360, Loss: 4.100e+02, Loss_bcs: 5.259e+01, Loss_res: 3.574e+02,Time: 0.01\n",
      "adaptive_constant_val: 66.413377\n",
      "It: 1370, Loss: 2.880e+02, Loss_bcs: 5.660e+01, Loss_res: 2.314e+02,Time: 0.01\n",
      "adaptive_constant_val: 64.117862\n",
      "It: 1380, Loss: 2.111e+02, Loss_bcs: 4.239e+01, Loss_res: 1.687e+02,Time: 0.01\n",
      "adaptive_constant_val: 62.542163\n",
      "It: 1390, Loss: 3.201e+02, Loss_bcs: 4.189e+01, Loss_res: 2.782e+02,Time: 0.01\n",
      "adaptive_constant_val: 64.007737\n",
      "It: 1400, Loss: 2.717e+02, Loss_bcs: 4.149e+01, Loss_res: 2.303e+02,Time: 0.01\n",
      "adaptive_constant_val: 65.066045\n",
      "It: 1410, Loss: 2.154e+02, Loss_bcs: 3.445e+01, Loss_res: 1.810e+02,Time: 0.01\n",
      "adaptive_constant_val: 67.733188\n",
      "It: 1420, Loss: 2.228e+02, Loss_bcs: 3.316e+01, Loss_res: 1.896e+02,Time: 0.01\n",
      "adaptive_constant_val: 72.524713\n",
      "It: 1430, Loss: 2.146e+02, Loss_bcs: 3.854e+01, Loss_res: 1.761e+02,Time: 0.01\n",
      "adaptive_constant_val: 70.711582\n",
      "It: 1440, Loss: 2.698e+02, Loss_bcs: 4.276e+01, Loss_res: 2.271e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.748217\n",
      "It: 1450, Loss: 2.713e+02, Loss_bcs: 3.911e+01, Loss_res: 2.322e+02,Time: 0.01\n",
      "adaptive_constant_val: 66.674165\n",
      "It: 1460, Loss: 2.374e+02, Loss_bcs: 4.048e+01, Loss_res: 1.970e+02,Time: 0.01\n",
      "adaptive_constant_val: 64.626463\n",
      "It: 1470, Loss: 1.994e+02, Loss_bcs: 3.447e+01, Loss_res: 1.650e+02,Time: 0.01\n",
      "adaptive_constant_val: 66.950490\n",
      "It: 1480, Loss: 2.610e+02, Loss_bcs: 3.742e+01, Loss_res: 2.236e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.331962\n",
      "It: 1490, Loss: 1.848e+02, Loss_bcs: 3.335e+01, Loss_res: 1.514e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.932599\n",
      "It: 1500, Loss: 2.337e+02, Loss_bcs: 3.551e+01, Loss_res: 1.982e+02,Time: 0.01\n",
      "adaptive_constant_val: 79.617450\n",
      "It: 1510, Loss: 1.893e+02, Loss_bcs: 3.342e+01, Loss_res: 1.559e+02,Time: 0.01\n",
      "adaptive_constant_val: 79.781536\n",
      "It: 1520, Loss: 2.231e+02, Loss_bcs: 2.844e+01, Loss_res: 1.946e+02,Time: 0.01\n",
      "adaptive_constant_val: 78.675315\n",
      "It: 1530, Loss: 1.673e+02, Loss_bcs: 3.350e+01, Loss_res: 1.338e+02,Time: 0.01\n",
      "adaptive_constant_val: 73.443702\n",
      "It: 1540, Loss: 1.775e+02, Loss_bcs: 2.680e+01, Loss_res: 1.507e+02,Time: 0.01\n",
      "adaptive_constant_val: 74.359851\n",
      "It: 1550, Loss: 1.540e+02, Loss_bcs: 3.212e+01, Loss_res: 1.218e+02,Time: 0.01\n",
      "adaptive_constant_val: 71.933821\n",
      "It: 1560, Loss: 1.918e+02, Loss_bcs: 3.114e+01, Loss_res: 1.607e+02,Time: 0.01\n",
      "adaptive_constant_val: 69.600501\n",
      "It: 1570, Loss: 1.702e+02, Loss_bcs: 2.553e+01, Loss_res: 1.446e+02,Time: 0.01\n",
      "adaptive_constant_val: 65.911565\n",
      "It: 1580, Loss: 1.989e+02, Loss_bcs: 2.423e+01, Loss_res: 1.746e+02,Time: 0.01\n",
      "adaptive_constant_val: 82.558046\n",
      "It: 1590, Loss: 2.021e+02, Loss_bcs: 3.946e+01, Loss_res: 1.627e+02,Time: 0.01\n",
      "adaptive_constant_val: 80.340558\n",
      "It: 1600, Loss: 2.023e+02, Loss_bcs: 2.792e+01, Loss_res: 1.743e+02,Time: 0.01\n",
      "adaptive_constant_val: 75.593258\n",
      "It: 1610, Loss: 1.757e+02, Loss_bcs: 2.521e+01, Loss_res: 1.505e+02,Time: 0.01\n",
      "adaptive_constant_val: 73.408322\n",
      "It: 1620, Loss: 1.548e+02, Loss_bcs: 2.513e+01, Loss_res: 1.296e+02,Time: 0.01\n",
      "adaptive_constant_val: 73.964797\n",
      "It: 1630, Loss: 2.107e+02, Loss_bcs: 2.349e+01, Loss_res: 1.873e+02,Time: 0.01\n",
      "adaptive_constant_val: 73.717858\n",
      "It: 1640, Loss: 1.622e+02, Loss_bcs: 2.198e+01, Loss_res: 1.402e+02,Time: 0.01\n",
      "adaptive_constant_val: 78.832612\n",
      "It: 1650, Loss: 1.858e+02, Loss_bcs: 2.467e+01, Loss_res: 1.611e+02,Time: 0.01\n",
      "adaptive_constant_val: 75.223824\n",
      "It: 1660, Loss: 1.502e+02, Loss_bcs: 2.845e+01, Loss_res: 1.217e+02,Time: 0.01\n",
      "adaptive_constant_val: 69.836667\n",
      "It: 1670, Loss: 1.601e+02, Loss_bcs: 2.517e+01, Loss_res: 1.349e+02,Time: 0.00\n",
      "adaptive_constant_val: 66.349499\n",
      "It: 1680, Loss: 1.091e+02, Loss_bcs: 2.490e+01, Loss_res: 8.422e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.524858\n",
      "It: 1690, Loss: 1.513e+02, Loss_bcs: 2.320e+01, Loss_res: 1.281e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.370457\n",
      "It: 1700, Loss: 1.441e+02, Loss_bcs: 2.201e+01, Loss_res: 1.221e+02,Time: 0.00\n",
      "adaptive_constant_val: 66.003211\n",
      "It: 1710, Loss: 1.365e+02, Loss_bcs: 2.046e+01, Loss_res: 1.160e+02,Time: 0.01\n",
      "adaptive_constant_val: 71.580727\n",
      "It: 1720, Loss: 1.419e+02, Loss_bcs: 2.381e+01, Loss_res: 1.181e+02,Time: 0.01\n",
      "adaptive_constant_val: 70.855372\n",
      "It: 1730, Loss: 1.068e+02, Loss_bcs: 2.398e+01, Loss_res: 8.280e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.053180\n",
      "It: 1740, Loss: 1.254e+02, Loss_bcs: 2.255e+01, Loss_res: 1.029e+02,Time: 0.01\n",
      "adaptive_constant_val: 65.982641\n",
      "It: 1750, Loss: 1.238e+02, Loss_bcs: 2.117e+01, Loss_res: 1.026e+02,Time: 0.01\n",
      "adaptive_constant_val: 63.576858\n",
      "It: 1760, Loss: 1.052e+02, Loss_bcs: 2.149e+01, Loss_res: 8.369e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.601494\n",
      "It: 1770, Loss: 1.134e+02, Loss_bcs: 2.141e+01, Loss_res: 9.198e+01,Time: 0.00\n",
      "adaptive_constant_val: 69.241794\n",
      "It: 1780, Loss: 1.114e+02, Loss_bcs: 2.212e+01, Loss_res: 8.928e+01,Time: 0.00\n",
      "adaptive_constant_val: 69.159883\n",
      "It: 1790, Loss: 1.684e+02, Loss_bcs: 2.047e+01, Loss_res: 1.480e+02,Time: 0.01\n",
      "adaptive_constant_val: 74.462348\n",
      "It: 1800, Loss: 1.145e+02, Loss_bcs: 2.124e+01, Loss_res: 9.324e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.644615\n",
      "It: 1810, Loss: 1.614e+02, Loss_bcs: 2.112e+01, Loss_res: 1.403e+02,Time: 0.01\n",
      "adaptive_constant_val: 81.897879\n",
      "It: 1820, Loss: 1.148e+02, Loss_bcs: 1.759e+01, Loss_res: 9.717e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.618593\n",
      "It: 1830, Loss: 1.202e+02, Loss_bcs: 2.248e+01, Loss_res: 9.769e+01,Time: 0.01\n",
      "adaptive_constant_val: 78.960522\n",
      "It: 1840, Loss: 1.353e+02, Loss_bcs: 2.103e+01, Loss_res: 1.143e+02,Time: 0.01\n",
      "adaptive_constant_val: 76.957888\n",
      "It: 1850, Loss: 9.690e+01, Loss_bcs: 2.178e+01, Loss_res: 7.512e+01,Time: 0.01\n",
      "adaptive_constant_val: 74.297757\n",
      "It: 1860, Loss: 1.002e+02, Loss_bcs: 2.286e+01, Loss_res: 7.734e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.092390\n",
      "It: 1870, Loss: 1.634e+02, Loss_bcs: 2.301e+01, Loss_res: 1.404e+02,Time: 0.01\n",
      "adaptive_constant_val: 68.512979\n",
      "It: 1880, Loss: 1.160e+02, Loss_bcs: 2.110e+01, Loss_res: 9.485e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.876739\n",
      "It: 1890, Loss: 1.140e+02, Loss_bcs: 2.008e+01, Loss_res: 9.396e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.791596\n",
      "It: 1900, Loss: 1.384e+02, Loss_bcs: 2.158e+01, Loss_res: 1.168e+02,Time: 0.01\n",
      "adaptive_constant_val: 76.539559\n",
      "It: 1910, Loss: 1.336e+02, Loss_bcs: 2.223e+01, Loss_res: 1.114e+02,Time: 0.01\n",
      "adaptive_constant_val: 73.254555\n",
      "It: 1920, Loss: 1.893e+02, Loss_bcs: 2.224e+01, Loss_res: 1.671e+02,Time: 0.01\n",
      "adaptive_constant_val: 73.337871\n",
      "It: 1930, Loss: 1.119e+02, Loss_bcs: 2.272e+01, Loss_res: 8.921e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.144872\n",
      "It: 1940, Loss: 1.072e+02, Loss_bcs: 2.218e+01, Loss_res: 8.502e+01,Time: 0.01\n",
      "adaptive_constant_val: 75.060155\n",
      "It: 1950, Loss: 1.049e+02, Loss_bcs: 1.920e+01, Loss_res: 8.571e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.324058\n",
      "It: 1960, Loss: 1.299e+02, Loss_bcs: 1.856e+01, Loss_res: 1.114e+02,Time: 0.01\n",
      "adaptive_constant_val: 76.250009\n",
      "It: 1970, Loss: 1.033e+02, Loss_bcs: 2.195e+01, Loss_res: 8.138e+01,Time: 0.00\n",
      "adaptive_constant_val: 72.722599\n",
      "It: 1980, Loss: 1.188e+02, Loss_bcs: 1.816e+01, Loss_res: 1.006e+02,Time: 0.01\n",
      "adaptive_constant_val: 70.881413\n",
      "It: 1990, Loss: 1.004e+02, Loss_bcs: 1.782e+01, Loss_res: 8.256e+01,Time: 0.00\n",
      "adaptive_constant_val: 67.064826\n",
      "It: 2000, Loss: 1.102e+02, Loss_bcs: 1.536e+01, Loss_res: 9.481e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.955602\n",
      "It: 2010, Loss: 1.404e+02, Loss_bcs: 1.755e+01, Loss_res: 1.228e+02,Time: 0.01\n",
      "adaptive_constant_val: 66.894567\n",
      "It: 2020, Loss: 9.068e+01, Loss_bcs: 1.881e+01, Loss_res: 7.187e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.252984\n",
      "It: 2030, Loss: 9.351e+01, Loss_bcs: 1.940e+01, Loss_res: 7.411e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.493940\n",
      "It: 2040, Loss: 1.157e+02, Loss_bcs: 1.984e+01, Loss_res: 9.585e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.702830\n",
      "It: 2050, Loss: 1.427e+02, Loss_bcs: 2.165e+01, Loss_res: 1.210e+02,Time: 0.01\n",
      "adaptive_constant_val: 67.692617\n",
      "It: 2060, Loss: 9.253e+01, Loss_bcs: 2.110e+01, Loss_res: 7.143e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.388049\n",
      "It: 2070, Loss: 9.716e+01, Loss_bcs: 1.843e+01, Loss_res: 7.873e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.296125\n",
      "It: 2080, Loss: 1.284e+02, Loss_bcs: 1.906e+01, Loss_res: 1.093e+02,Time: 0.01\n",
      "adaptive_constant_val: 73.567770\n",
      "It: 2090, Loss: 7.191e+01, Loss_bcs: 1.791e+01, Loss_res: 5.400e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.810979\n",
      "It: 2100, Loss: 9.999e+01, Loss_bcs: 1.793e+01, Loss_res: 8.207e+01,Time: 0.01\n",
      "adaptive_constant_val: 65.836647\n",
      "It: 2110, Loss: 9.843e+01, Loss_bcs: 1.642e+01, Loss_res: 8.200e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.560413\n",
      "It: 2120, Loss: 1.095e+02, Loss_bcs: 1.974e+01, Loss_res: 8.972e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.720693\n",
      "It: 2130, Loss: 1.114e+02, Loss_bcs: 1.427e+01, Loss_res: 9.717e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.891121\n",
      "It: 2140, Loss: 9.403e+01, Loss_bcs: 1.806e+01, Loss_res: 7.597e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.454587\n",
      "It: 2150, Loss: 1.060e+02, Loss_bcs: 1.697e+01, Loss_res: 8.905e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.981135\n",
      "It: 2160, Loss: 1.238e+02, Loss_bcs: 1.584e+01, Loss_res: 1.079e+02,Time: 0.01\n",
      "adaptive_constant_val: 72.838639\n",
      "It: 2170, Loss: 9.863e+01, Loss_bcs: 1.447e+01, Loss_res: 8.416e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.192024\n",
      "It: 2180, Loss: 7.897e+01, Loss_bcs: 1.620e+01, Loss_res: 6.277e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.418443\n",
      "It: 2190, Loss: 1.228e+02, Loss_bcs: 1.906e+01, Loss_res: 1.037e+02,Time: 0.01\n",
      "adaptive_constant_val: 66.350987\n",
      "It: 2200, Loss: 7.578e+01, Loss_bcs: 1.576e+01, Loss_res: 6.002e+01,Time: 0.05\n",
      "adaptive_constant_val: 65.923363\n",
      "It: 2210, Loss: 6.285e+01, Loss_bcs: 1.391e+01, Loss_res: 4.894e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.715172\n",
      "It: 2220, Loss: 1.186e+02, Loss_bcs: 1.495e+01, Loss_res: 1.036e+02,Time: 0.01\n",
      "adaptive_constant_val: 82.286844\n",
      "It: 2230, Loss: 1.186e+02, Loss_bcs: 1.815e+01, Loss_res: 1.004e+02,Time: 0.01\n",
      "adaptive_constant_val: 86.247852\n",
      "It: 2240, Loss: 7.449e+01, Loss_bcs: 1.795e+01, Loss_res: 5.655e+01,Time: 0.01\n",
      "adaptive_constant_val: 82.462177\n",
      "It: 2250, Loss: 1.029e+02, Loss_bcs: 1.881e+01, Loss_res: 8.407e+01,Time: 0.01\n",
      "adaptive_constant_val: 88.448088\n",
      "It: 2260, Loss: 9.765e+01, Loss_bcs: 1.887e+01, Loss_res: 7.878e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.685516\n",
      "It: 2270, Loss: 9.668e+01, Loss_bcs: 1.636e+01, Loss_res: 8.032e+01,Time: 0.01\n",
      "adaptive_constant_val: 78.596508\n",
      "It: 2280, Loss: 1.067e+02, Loss_bcs: 1.823e+01, Loss_res: 8.844e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.199744\n",
      "It: 2290, Loss: 1.287e+02, Loss_bcs: 1.692e+01, Loss_res: 1.118e+02,Time: 0.00\n",
      "adaptive_constant_val: 82.183499\n",
      "It: 2300, Loss: 8.389e+01, Loss_bcs: 1.462e+01, Loss_res: 6.927e+01,Time: 0.01\n",
      "adaptive_constant_val: 81.598258\n",
      "It: 2310, Loss: 8.006e+01, Loss_bcs: 1.675e+01, Loss_res: 6.331e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.801531\n",
      "It: 2320, Loss: 1.122e+02, Loss_bcs: 1.504e+01, Loss_res: 9.720e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.928903\n",
      "It: 2330, Loss: 6.274e+01, Loss_bcs: 1.620e+01, Loss_res: 4.653e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.860347\n",
      "It: 2340, Loss: 8.516e+01, Loss_bcs: 1.432e+01, Loss_res: 7.084e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.797117\n",
      "It: 2350, Loss: 7.342e+01, Loss_bcs: 1.554e+01, Loss_res: 5.788e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.280522\n",
      "It: 2360, Loss: 9.947e+01, Loss_bcs: 1.405e+01, Loss_res: 8.542e+01,Time: 0.00\n",
      "adaptive_constant_val: 68.160804\n",
      "It: 2370, Loss: 8.659e+01, Loss_bcs: 1.355e+01, Loss_res: 7.304e+01,Time: 0.01\n",
      "adaptive_constant_val: 65.914489\n",
      "It: 2380, Loss: 6.813e+01, Loss_bcs: 1.475e+01, Loss_res: 5.337e+01,Time: 0.01\n",
      "adaptive_constant_val: 63.663130\n",
      "It: 2390, Loss: 7.847e+01, Loss_bcs: 1.306e+01, Loss_res: 6.542e+01,Time: 0.01\n",
      "adaptive_constant_val: 61.788080\n",
      "It: 2400, Loss: 6.367e+01, Loss_bcs: 1.334e+01, Loss_res: 5.033e+01,Time: 0.01\n",
      "adaptive_constant_val: 60.360825\n",
      "It: 2410, Loss: 6.669e+01, Loss_bcs: 1.300e+01, Loss_res: 5.369e+01,Time: 0.01\n",
      "adaptive_constant_val: 64.176384\n",
      "It: 2420, Loss: 7.985e+01, Loss_bcs: 1.434e+01, Loss_res: 6.551e+01,Time: 0.00\n",
      "adaptive_constant_val: 63.553471\n",
      "It: 2430, Loss: 8.714e+01, Loss_bcs: 1.532e+01, Loss_res: 7.182e+01,Time: 0.00\n",
      "adaptive_constant_val: 63.923539\n",
      "It: 2440, Loss: 9.707e+01, Loss_bcs: 1.260e+01, Loss_res: 8.448e+01,Time: 0.00\n",
      "adaptive_constant_val: 69.554306\n",
      "It: 2450, Loss: 6.667e+01, Loss_bcs: 1.463e+01, Loss_res: 5.204e+01,Time: 0.00\n",
      "adaptive_constant_val: 69.188610\n",
      "It: 2460, Loss: 6.722e+01, Loss_bcs: 1.295e+01, Loss_res: 5.427e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.861952\n",
      "It: 2470, Loss: 6.596e+01, Loss_bcs: 1.403e+01, Loss_res: 5.194e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.058158\n",
      "It: 2480, Loss: 6.885e+01, Loss_bcs: 1.541e+01, Loss_res: 5.344e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.767450\n",
      "It: 2490, Loss: 8.682e+01, Loss_bcs: 1.325e+01, Loss_res: 7.357e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.601617\n",
      "It: 2500, Loss: 8.460e+01, Loss_bcs: 1.555e+01, Loss_res: 6.905e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.318838\n",
      "It: 2510, Loss: 8.212e+01, Loss_bcs: 1.471e+01, Loss_res: 6.742e+01,Time: 0.00\n",
      "adaptive_constant_val: 66.220096\n",
      "It: 2520, Loss: 6.619e+01, Loss_bcs: 1.520e+01, Loss_res: 5.099e+01,Time: 0.00\n",
      "adaptive_constant_val: 64.870988\n",
      "It: 2530, Loss: 8.827e+01, Loss_bcs: 1.319e+01, Loss_res: 7.508e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.628384\n",
      "It: 2540, Loss: 7.241e+01, Loss_bcs: 1.289e+01, Loss_res: 5.952e+01,Time: 0.00\n",
      "adaptive_constant_val: 68.463641\n",
      "It: 2550, Loss: 6.974e+01, Loss_bcs: 1.479e+01, Loss_res: 5.495e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.064194\n",
      "It: 2560, Loss: 6.274e+01, Loss_bcs: 1.495e+01, Loss_res: 4.779e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.910171\n",
      "It: 2570, Loss: 5.385e+01, Loss_bcs: 1.256e+01, Loss_res: 4.129e+01,Time: 0.01\n",
      "adaptive_constant_val: 74.279027\n",
      "It: 2580, Loss: 6.108e+01, Loss_bcs: 1.286e+01, Loss_res: 4.822e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.184737\n",
      "It: 2590, Loss: 9.675e+01, Loss_bcs: 1.239e+01, Loss_res: 8.435e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.170233\n",
      "It: 2600, Loss: 6.542e+01, Loss_bcs: 1.140e+01, Loss_res: 5.402e+01,Time: 0.00\n",
      "adaptive_constant_val: 66.681193\n",
      "It: 2610, Loss: 7.520e+01, Loss_bcs: 1.242e+01, Loss_res: 6.278e+01,Time: 0.01\n",
      "adaptive_constant_val: 65.911004\n",
      "It: 2620, Loss: 5.714e+01, Loss_bcs: 1.054e+01, Loss_res: 4.659e+01,Time: 0.01\n",
      "adaptive_constant_val: 63.508196\n",
      "It: 2630, Loss: 6.916e+01, Loss_bcs: 1.174e+01, Loss_res: 5.741e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.063915\n",
      "It: 2640, Loss: 7.056e+01, Loss_bcs: 1.153e+01, Loss_res: 5.903e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.234637\n",
      "It: 2650, Loss: 5.352e+01, Loss_bcs: 1.162e+01, Loss_res: 4.190e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.096178\n",
      "It: 2660, Loss: 7.313e+01, Loss_bcs: 1.004e+01, Loss_res: 6.309e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.045557\n",
      "It: 2670, Loss: 8.056e+01, Loss_bcs: 1.138e+01, Loss_res: 6.919e+01,Time: 0.00\n",
      "adaptive_constant_val: 69.217742\n",
      "It: 2680, Loss: 5.614e+01, Loss_bcs: 9.617e+00, Loss_res: 4.652e+01,Time: 0.00\n",
      "adaptive_constant_val: 66.522307\n",
      "It: 2690, Loss: 7.503e+01, Loss_bcs: 1.274e+01, Loss_res: 6.230e+01,Time: 0.01\n",
      "adaptive_constant_val: 65.024914\n",
      "It: 2700, Loss: 5.931e+01, Loss_bcs: 1.163e+01, Loss_res: 4.767e+01,Time: 0.00\n",
      "adaptive_constant_val: 62.431259\n",
      "It: 2710, Loss: 6.894e+01, Loss_bcs: 1.060e+01, Loss_res: 5.834e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.640211\n",
      "It: 2720, Loss: 8.570e+01, Loss_bcs: 1.012e+01, Loss_res: 7.559e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.313202\n",
      "It: 2730, Loss: 6.485e+01, Loss_bcs: 1.243e+01, Loss_res: 5.243e+01,Time: 0.01\n",
      "adaptive_constant_val: 80.812204\n",
      "It: 2740, Loss: 6.024e+01, Loss_bcs: 1.305e+01, Loss_res: 4.719e+01,Time: 0.00\n",
      "adaptive_constant_val: 76.838320\n",
      "It: 2750, Loss: 8.484e+01, Loss_bcs: 1.082e+01, Loss_res: 7.403e+01,Time: 0.00\n",
      "adaptive_constant_val: 75.905557\n",
      "It: 2760, Loss: 7.624e+01, Loss_bcs: 1.295e+01, Loss_res: 6.328e+01,Time: 0.01\n",
      "adaptive_constant_val: 75.724093\n",
      "It: 2770, Loss: 5.846e+01, Loss_bcs: 1.296e+01, Loss_res: 4.550e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.695451\n",
      "It: 2780, Loss: 4.601e+01, Loss_bcs: 1.307e+01, Loss_res: 3.293e+01,Time: 0.00\n",
      "adaptive_constant_val: 69.254972\n",
      "It: 2790, Loss: 7.236e+01, Loss_bcs: 1.378e+01, Loss_res: 5.858e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.473307\n",
      "It: 2800, Loss: 8.476e+01, Loss_bcs: 1.147e+01, Loss_res: 7.329e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.163982\n",
      "It: 2810, Loss: 6.178e+01, Loss_bcs: 1.364e+01, Loss_res: 4.814e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.499228\n",
      "It: 2820, Loss: 5.020e+01, Loss_bcs: 1.337e+01, Loss_res: 3.683e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.496625\n",
      "It: 2830, Loss: 5.959e+01, Loss_bcs: 1.124e+01, Loss_res: 4.836e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.163298\n",
      "It: 2840, Loss: 4.619e+01, Loss_bcs: 9.519e+00, Loss_res: 3.667e+01,Time: 0.00\n",
      "adaptive_constant_val: 67.854752\n",
      "It: 2850, Loss: 6.372e+01, Loss_bcs: 1.286e+01, Loss_res: 5.085e+01,Time: 0.01\n",
      "adaptive_constant_val: 74.503500\n",
      "It: 2860, Loss: 4.616e+01, Loss_bcs: 1.126e+01, Loss_res: 3.490e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.863724\n",
      "It: 2870, Loss: 6.244e+01, Loss_bcs: 1.227e+01, Loss_res: 5.017e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.954183\n",
      "It: 2880, Loss: 7.069e+01, Loss_bcs: 9.637e+00, Loss_res: 6.105e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.619519\n",
      "It: 2890, Loss: 5.300e+01, Loss_bcs: 9.968e+00, Loss_res: 4.303e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.237791\n",
      "It: 2900, Loss: 5.155e+01, Loss_bcs: 1.055e+01, Loss_res: 4.100e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.630702\n",
      "It: 2910, Loss: 5.454e+01, Loss_bcs: 9.376e+00, Loss_res: 4.516e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.216459\n",
      "It: 2920, Loss: 5.629e+01, Loss_bcs: 9.813e+00, Loss_res: 4.648e+01,Time: 0.00\n",
      "adaptive_constant_val: 72.797337\n",
      "It: 2930, Loss: 3.731e+01, Loss_bcs: 8.553e+00, Loss_res: 2.876e+01,Time: 0.00\n",
      "adaptive_constant_val: 72.597853\n",
      "It: 2940, Loss: 5.011e+01, Loss_bcs: 9.647e+00, Loss_res: 4.046e+01,Time: 0.02\n",
      "adaptive_constant_val: 72.437424\n",
      "It: 2950, Loss: 4.875e+01, Loss_bcs: 8.541e+00, Loss_res: 4.021e+01,Time: 0.00\n",
      "adaptive_constant_val: 74.906048\n",
      "It: 2960, Loss: 4.588e+01, Loss_bcs: 9.197e+00, Loss_res: 3.668e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.118041\n",
      "It: 2970, Loss: 4.861e+01, Loss_bcs: 1.046e+01, Loss_res: 3.814e+01,Time: 0.00\n",
      "adaptive_constant_val: 70.193648\n",
      "It: 2980, Loss: 5.096e+01, Loss_bcs: 9.356e+00, Loss_res: 4.161e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.683236\n",
      "It: 2990, Loss: 3.889e+01, Loss_bcs: 7.734e+00, Loss_res: 3.116e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.972477\n",
      "It: 3000, Loss: 6.428e+01, Loss_bcs: 1.050e+01, Loss_res: 5.378e+01,Time: 0.01\n",
      "adaptive_constant_val: 77.421112\n",
      "It: 3010, Loss: 7.341e+01, Loss_bcs: 1.042e+01, Loss_res: 6.299e+01,Time: 0.00\n",
      "adaptive_constant_val: 78.869826\n",
      "It: 3020, Loss: 3.986e+01, Loss_bcs: 9.613e+00, Loss_res: 3.025e+01,Time: 0.00\n",
      "adaptive_constant_val: 73.384002\n",
      "It: 3030, Loss: 3.939e+01, Loss_bcs: 8.106e+00, Loss_res: 3.129e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.409889\n",
      "It: 3040, Loss: 3.281e+01, Loss_bcs: 7.949e+00, Loss_res: 2.486e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.585795\n",
      "It: 3050, Loss: 4.447e+01, Loss_bcs: 7.326e+00, Loss_res: 3.714e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.383171\n",
      "It: 3060, Loss: 5.329e+01, Loss_bcs: 1.093e+01, Loss_res: 4.236e+01,Time: 0.01\n",
      "adaptive_constant_val: 76.559216\n",
      "It: 3070, Loss: 4.036e+01, Loss_bcs: 1.049e+01, Loss_res: 2.988e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.726705\n",
      "It: 3080, Loss: 3.683e+01, Loss_bcs: 9.529e+00, Loss_res: 2.730e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.790628\n",
      "It: 3090, Loss: 5.094e+01, Loss_bcs: 8.186e+00, Loss_res: 4.275e+01,Time: 0.01\n",
      "adaptive_constant_val: 77.121494\n",
      "It: 3100, Loss: 5.751e+01, Loss_bcs: 7.680e+00, Loss_res: 4.983e+01,Time: 0.01\n",
      "adaptive_constant_val: 83.785064\n",
      "It: 3110, Loss: 5.094e+01, Loss_bcs: 8.798e+00, Loss_res: 4.214e+01,Time: 0.00\n",
      "adaptive_constant_val: 85.748243\n",
      "It: 3120, Loss: 3.889e+01, Loss_bcs: 7.959e+00, Loss_res: 3.093e+01,Time: 0.00\n",
      "adaptive_constant_val: 81.523924\n",
      "It: 3130, Loss: 5.275e+01, Loss_bcs: 8.765e+00, Loss_res: 4.399e+01,Time: 0.01\n",
      "adaptive_constant_val: 82.721908\n",
      "It: 3140, Loss: 4.378e+01, Loss_bcs: 7.659e+00, Loss_res: 3.612e+01,Time: 0.00\n",
      "adaptive_constant_val: 88.437928\n",
      "It: 3150, Loss: 5.480e+01, Loss_bcs: 8.287e+00, Loss_res: 4.652e+01,Time: 0.00\n",
      "adaptive_constant_val: 97.863689\n",
      "It: 3160, Loss: 4.030e+01, Loss_bcs: 1.007e+01, Loss_res: 3.023e+01,Time: 0.01\n",
      "adaptive_constant_val: 91.665426\n",
      "It: 3170, Loss: 4.211e+01, Loss_bcs: 9.341e+00, Loss_res: 3.277e+01,Time: 0.00\n",
      "adaptive_constant_val: 86.697287\n",
      "It: 3180, Loss: 3.465e+01, Loss_bcs: 8.374e+00, Loss_res: 2.628e+01,Time: 0.00\n",
      "adaptive_constant_val: 83.853112\n",
      "It: 3190, Loss: 4.404e+01, Loss_bcs: 8.417e+00, Loss_res: 3.562e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.442129\n",
      "It: 3200, Loss: 4.446e+01, Loss_bcs: 7.954e+00, Loss_res: 3.651e+01,Time: 0.01\n",
      "adaptive_constant_val: 75.654206\n",
      "It: 3210, Loss: 4.869e+01, Loss_bcs: 7.298e+00, Loss_res: 4.140e+01,Time: 0.00\n",
      "adaptive_constant_val: 75.980306\n",
      "It: 3220, Loss: 4.994e+01, Loss_bcs: 9.630e+00, Loss_res: 4.031e+01,Time: 0.00\n",
      "adaptive_constant_val: 74.648828\n",
      "It: 3230, Loss: 4.256e+01, Loss_bcs: 8.985e+00, Loss_res: 3.357e+01,Time: 0.01\n",
      "adaptive_constant_val: 72.728005\n",
      "It: 3240, Loss: 4.561e+01, Loss_bcs: 8.171e+00, Loss_res: 3.743e+01,Time: 0.01\n",
      "adaptive_constant_val: 72.882505\n",
      "It: 3250, Loss: 3.756e+01, Loss_bcs: 7.676e+00, Loss_res: 2.988e+01,Time: 0.00\n",
      "adaptive_constant_val: 71.401124\n",
      "It: 3260, Loss: 3.852e+01, Loss_bcs: 8.124e+00, Loss_res: 3.040e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.347673\n",
      "It: 3270, Loss: 5.315e+01, Loss_bcs: 8.945e+00, Loss_res: 4.421e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.296873\n",
      "It: 3280, Loss: 3.117e+01, Loss_bcs: 7.497e+00, Loss_res: 2.368e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.923990\n",
      "It: 3290, Loss: 4.587e+01, Loss_bcs: 7.751e+00, Loss_res: 3.812e+01,Time: 0.00\n",
      "adaptive_constant_val: 76.756838\n",
      "It: 3300, Loss: 3.833e+01, Loss_bcs: 8.985e+00, Loss_res: 2.934e+01,Time: 0.01\n",
      "adaptive_constant_val: 73.276953\n",
      "It: 3310, Loss: 3.121e+01, Loss_bcs: 7.386e+00, Loss_res: 2.382e+01,Time: 0.00\n",
      "adaptive_constant_val: 71.855386\n",
      "It: 3320, Loss: 3.065e+01, Loss_bcs: 7.426e+00, Loss_res: 2.322e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.824207\n",
      "It: 3330, Loss: 3.136e+01, Loss_bcs: 7.620e+00, Loss_res: 2.374e+01,Time: 0.01\n",
      "adaptive_constant_val: 65.931816\n",
      "It: 3340, Loss: 4.357e+01, Loss_bcs: 8.172e+00, Loss_res: 3.539e+01,Time: 0.00\n",
      "adaptive_constant_val: 63.798160\n",
      "It: 3350, Loss: 5.299e+01, Loss_bcs: 7.223e+00, Loss_res: 4.577e+01,Time: 0.01\n",
      "adaptive_constant_val: 60.388103\n",
      "It: 3360, Loss: 4.365e+01, Loss_bcs: 6.165e+00, Loss_res: 3.748e+01,Time: 0.01\n",
      "adaptive_constant_val: 81.120140\n",
      "It: 3370, Loss: 4.956e+01, Loss_bcs: 8.711e+00, Loss_res: 4.085e+01,Time: 0.01\n",
      "adaptive_constant_val: 79.987639\n",
      "It: 3380, Loss: 4.094e+01, Loss_bcs: 9.354e+00, Loss_res: 3.159e+01,Time: 0.00\n",
      "adaptive_constant_val: 80.518590\n",
      "It: 3390, Loss: 2.918e+01, Loss_bcs: 7.687e+00, Loss_res: 2.149e+01,Time: 0.00\n",
      "adaptive_constant_val: 76.907156\n",
      "It: 3400, Loss: 3.126e+01, Loss_bcs: 8.643e+00, Loss_res: 2.262e+01,Time: 0.01\n",
      "adaptive_constant_val: 77.196700\n",
      "It: 3410, Loss: 3.439e+01, Loss_bcs: 7.132e+00, Loss_res: 2.725e+01,Time: 0.01\n",
      "adaptive_constant_val: 75.924561\n",
      "It: 3420, Loss: 3.860e+01, Loss_bcs: 7.783e+00, Loss_res: 3.082e+01,Time: 0.02\n",
      "adaptive_constant_val: 73.731913\n",
      "It: 3430, Loss: 4.156e+01, Loss_bcs: 9.193e+00, Loss_res: 3.237e+01,Time: 0.01\n",
      "adaptive_constant_val: 72.094575\n",
      "It: 3440, Loss: 3.005e+01, Loss_bcs: 8.782e+00, Loss_res: 2.127e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.708545\n",
      "It: 3450, Loss: 4.108e+01, Loss_bcs: 7.668e+00, Loss_res: 3.342e+01,Time: 0.01\n",
      "adaptive_constant_val: 64.342154\n",
      "It: 3460, Loss: 4.147e+01, Loss_bcs: 7.259e+00, Loss_res: 3.421e+01,Time: 0.01\n",
      "adaptive_constant_val: 61.018868\n",
      "It: 3470, Loss: 3.771e+01, Loss_bcs: 6.822e+00, Loss_res: 3.089e+01,Time: 0.01\n",
      "adaptive_constant_val: 59.981487\n",
      "It: 3480, Loss: 3.898e+01, Loss_bcs: 7.432e+00, Loss_res: 3.155e+01,Time: 0.00\n",
      "adaptive_constant_val: 58.109210\n",
      "It: 3490, Loss: 2.696e+01, Loss_bcs: 6.801e+00, Loss_res: 2.016e+01,Time: 0.01\n",
      "adaptive_constant_val: 58.848072\n",
      "It: 3500, Loss: 3.493e+01, Loss_bcs: 7.102e+00, Loss_res: 2.783e+01,Time: 0.01\n",
      "adaptive_constant_val: 60.336118\n",
      "It: 3510, Loss: 3.369e+01, Loss_bcs: 6.961e+00, Loss_res: 2.673e+01,Time: 0.01\n",
      "adaptive_constant_val: 60.935637\n",
      "It: 3520, Loss: 3.278e+01, Loss_bcs: 6.614e+00, Loss_res: 2.616e+01,Time: 0.00\n",
      "adaptive_constant_val: 66.413899\n",
      "It: 3530, Loss: 3.038e+01, Loss_bcs: 6.734e+00, Loss_res: 2.365e+01,Time: 0.00\n",
      "adaptive_constant_val: 63.201239\n",
      "It: 3540, Loss: 3.432e+01, Loss_bcs: 6.134e+00, Loss_res: 2.819e+01,Time: 0.00\n",
      "adaptive_constant_val: 62.598309\n",
      "It: 3550, Loss: 3.668e+01, Loss_bcs: 7.089e+00, Loss_res: 2.959e+01,Time: 0.01\n",
      "adaptive_constant_val: 70.734954\n",
      "It: 3560, Loss: 3.680e+01, Loss_bcs: 6.972e+00, Loss_res: 2.982e+01,Time: 0.00\n",
      "adaptive_constant_val: 67.427301\n",
      "It: 3570, Loss: 3.306e+01, Loss_bcs: 7.685e+00, Loss_res: 2.538e+01,Time: 0.00\n",
      "adaptive_constant_val: 63.003547\n",
      "It: 3580, Loss: 2.826e+01, Loss_bcs: 7.508e+00, Loss_res: 2.076e+01,Time: 0.01\n",
      "adaptive_constant_val: 65.341184\n",
      "It: 3590, Loss: 2.935e+01, Loss_bcs: 7.675e+00, Loss_res: 2.167e+01,Time: 0.01\n",
      "adaptive_constant_val: 63.667062\n",
      "It: 3600, Loss: 3.642e+01, Loss_bcs: 6.863e+00, Loss_res: 2.955e+01,Time: 0.00\n",
      "adaptive_constant_val: 65.216258\n",
      "It: 3610, Loss: 2.806e+01, Loss_bcs: 7.982e+00, Loss_res: 2.008e+01,Time: 0.01\n",
      "adaptive_constant_val: 64.717923\n",
      "It: 3620, Loss: 3.825e+01, Loss_bcs: 7.396e+00, Loss_res: 3.085e+01,Time: 0.00\n",
      "adaptive_constant_val: 67.120127\n",
      "It: 3630, Loss: 2.754e+01, Loss_bcs: 7.893e+00, Loss_res: 1.965e+01,Time: 0.01\n",
      "adaptive_constant_val: 62.745908\n",
      "It: 3640, Loss: 2.977e+01, Loss_bcs: 7.856e+00, Loss_res: 2.191e+01,Time: 0.00\n",
      "adaptive_constant_val: 60.296854\n",
      "It: 3650, Loss: 3.124e+01, Loss_bcs: 6.990e+00, Loss_res: 2.425e+01,Time: 0.01\n",
      "adaptive_constant_val: 59.854614\n",
      "It: 3660, Loss: 3.245e+01, Loss_bcs: 7.008e+00, Loss_res: 2.544e+01,Time: 0.00\n",
      "adaptive_constant_val: 57.834019\n",
      "It: 3670, Loss: 2.659e+01, Loss_bcs: 6.846e+00, Loss_res: 1.974e+01,Time: 0.01\n",
      "adaptive_constant_val: 58.700595\n",
      "It: 3680, Loss: 3.255e+01, Loss_bcs: 7.321e+00, Loss_res: 2.523e+01,Time: 0.01\n",
      "adaptive_constant_val: 61.183460\n",
      "It: 3690, Loss: 4.298e+01, Loss_bcs: 6.516e+00, Loss_res: 3.647e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.599808\n",
      "It: 3700, Loss: 3.490e+01, Loss_bcs: 6.816e+00, Loss_res: 2.808e+01,Time: 0.02\n",
      "adaptive_constant_val: 69.624701\n",
      "It: 3710, Loss: 3.068e+01, Loss_bcs: 7.380e+00, Loss_res: 2.330e+01,Time: 0.01\n",
      "adaptive_constant_val: 72.685554\n",
      "It: 3720, Loss: 2.538e+01, Loss_bcs: 6.711e+00, Loss_res: 1.867e+01,Time: 0.01\n",
      "adaptive_constant_val: 69.466826\n",
      "It: 3730, Loss: 3.476e+01, Loss_bcs: 8.419e+00, Loss_res: 2.635e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.217846\n",
      "It: 3740, Loss: 3.494e+01, Loss_bcs: 7.055e+00, Loss_res: 2.789e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.644051\n",
      "It: 3750, Loss: 2.929e+01, Loss_bcs: 7.149e+00, Loss_res: 2.214e+01,Time: 0.00\n",
      "adaptive_constant_val: 62.081761\n",
      "It: 3760, Loss: 2.998e+01, Loss_bcs: 7.977e+00, Loss_res: 2.200e+01,Time: 0.01\n",
      "adaptive_constant_val: 63.154345\n",
      "It: 3770, Loss: 3.281e+01, Loss_bcs: 7.825e+00, Loss_res: 2.499e+01,Time: 0.01\n",
      "adaptive_constant_val: 63.197165\n",
      "It: 3780, Loss: 3.788e+01, Loss_bcs: 8.238e+00, Loss_res: 2.964e+01,Time: 0.00\n",
      "adaptive_constant_val: 68.733397\n",
      "It: 3790, Loss: 3.416e+01, Loss_bcs: 7.918e+00, Loss_res: 2.624e+01,Time: 0.00\n",
      "adaptive_constant_val: 69.212765\n",
      "It: 3800, Loss: 3.602e+01, Loss_bcs: 7.435e+00, Loss_res: 2.859e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.686482\n",
      "It: 3810, Loss: 3.611e+01, Loss_bcs: 7.554e+00, Loss_res: 2.855e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.688184\n",
      "It: 3820, Loss: 2.994e+01, Loss_bcs: 8.697e+00, Loss_res: 2.125e+01,Time: 0.00\n",
      "adaptive_constant_val: 69.501908\n",
      "It: 3830, Loss: 3.090e+01, Loss_bcs: 8.302e+00, Loss_res: 2.260e+01,Time: 0.01\n",
      "adaptive_constant_val: 71.938543\n",
      "It: 3840, Loss: 2.970e+01, Loss_bcs: 8.168e+00, Loss_res: 2.153e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.795087\n",
      "It: 3850, Loss: 2.709e+01, Loss_bcs: 6.268e+00, Loss_res: 2.083e+01,Time: 0.01\n",
      "adaptive_constant_val: 67.546312\n",
      "It: 3860, Loss: 3.346e+01, Loss_bcs: 8.419e+00, Loss_res: 2.504e+01,Time: 0.00\n",
      "adaptive_constant_val: 65.843523\n",
      "It: 3870, Loss: 2.779e+01, Loss_bcs: 6.951e+00, Loss_res: 2.084e+01,Time: 0.00\n",
      "adaptive_constant_val: 62.626934\n",
      "It: 3880, Loss: 2.489e+01, Loss_bcs: 7.195e+00, Loss_res: 1.770e+01,Time: 0.01\n",
      "adaptive_constant_val: 60.063777\n",
      "It: 3890, Loss: 2.588e+01, Loss_bcs: 5.888e+00, Loss_res: 1.999e+01,Time: 0.01\n",
      "adaptive_constant_val: 57.713541\n",
      "It: 3900, Loss: 3.251e+01, Loss_bcs: 5.960e+00, Loss_res: 2.655e+01,Time: 0.01\n",
      "adaptive_constant_val: 58.788192\n",
      "It: 3910, Loss: 2.295e+01, Loss_bcs: 6.553e+00, Loss_res: 1.639e+01,Time: 0.00\n",
      "adaptive_constant_val: 57.208014\n",
      "It: 3920, Loss: 2.773e+01, Loss_bcs: 6.649e+00, Loss_res: 2.108e+01,Time: 0.01\n",
      "adaptive_constant_val: 57.434411\n",
      "It: 3930, Loss: 2.400e+01, Loss_bcs: 6.090e+00, Loss_res: 1.791e+01,Time: 0.01\n",
      "adaptive_constant_val: 55.828385\n",
      "It: 3940, Loss: 2.176e+01, Loss_bcs: 5.607e+00, Loss_res: 1.615e+01,Time: 0.01\n",
      "adaptive_constant_val: 66.754955\n",
      "It: 3950, Loss: 2.389e+01, Loss_bcs: 6.928e+00, Loss_res: 1.696e+01,Time: 0.00\n",
      "adaptive_constant_val: 63.149219\n",
      "It: 3960, Loss: 2.647e+01, Loss_bcs: 6.914e+00, Loss_res: 1.955e+01,Time: 0.01\n",
      "adaptive_constant_val: 68.844165\n",
      "It: 3970, Loss: 2.794e+01, Loss_bcs: 7.061e+00, Loss_res: 2.088e+01,Time: 0.00\n",
      "adaptive_constant_val: 65.797643\n",
      "It: 3980, Loss: 3.251e+01, Loss_bcs: 6.648e+00, Loss_res: 2.586e+01,Time: 0.00\n",
      "adaptive_constant_val: 63.457119\n",
      "It: 3990, Loss: 2.781e+01, Loss_bcs: 6.622e+00, Loss_res: 2.119e+01,Time: 0.00\n",
      "adaptive_constant_val: 64.931896\n",
      "It: 4000, Loss: 3.638e+01, Loss_bcs: 7.192e+00, Loss_res: 2.919e+01,Time: 0.00\n",
      "adaptive_constant_val: 78.239216\n",
      "Relative L2 error_u: 2.69e-01\n",
      "Relative L2 error_f: 6.18e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-12-2023_01-50-01-067300_M2\n",
      "Final loss total loss: 3.287311e+01\n",
      "Final loss loss_res: 2.580956e+01\n",
      "Final loss loss_bc1: 5.888937e-02\n",
      "Final loss loss_bc2: 5.085504e-03\n",
      "Final loss loss_bc3: 2.854954e-02\n",
      "Final loss loss_bc4: 1.625958e-02\n",
      "average lambda_bc7.1102e+01\n",
      "average lambda_res1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Method:  mini_batch\n",
      "\n",
      "average of time_list: 42.93175745010376\n",
      "average of error_u_list: 0.2688071619467761\n",
      "average of error_v_list: 0.06181532358320891\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './M1_dataset/NS_model_M2_result_mb500_fb5000_500_1.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './M1_dataset/NS_model_M2_result_mb500_fb5000_500_1.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16158/2627959977.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavemat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./M1_dataset/NS_model_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_result_mb\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbcbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_fb\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mubatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbcbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mresult_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36msavemat\u001b[0;34m(file_name, mdict, appendmat, format, long_field_names, do_compression, oned_as)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0msavemat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matlab_matrix.mat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'4'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlong_field_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './M1_dataset/NS_model_M2_result_mb500_fb5000_500_1.mat'"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =4001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "a_1 = 1\n",
    "a_2 = 4\n",
    "\n",
    "# Parameter\n",
    "lam = 1.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[-1.0, -1.0], [1.0, -1.0]])\n",
    "bc2_coords = np.array([[1.0, -1.0], [1.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "bc4_coords = np.array([[-1.0, 1.0], [-1.0, -1.0]])\n",
    "\n",
    "dom_coords = np.array([[-1.0, -1.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "x1 = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x2 = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star, a_1, a_2)\n",
    "f_star = f(X_star, a_1, a_2, lam)\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = None\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'            # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False    # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, a_1, a_2, lam), name='Forcing')\n",
    "\n",
    "        # [elapsed, error_u , error_f ,  mode] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , \n",
    "        #                                                                stiff_ratio , X_star ,u_star , f_star , nIter ,bcbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    " #def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                model.trainmb(nIter, batch_size=mbbatch_size )\n",
    "            else:\n",
    "                print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            f_pred = model.predict_r(X_star)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "            print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "            model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( x1 , x2 , X_star , u_star , u_pred , f_star , f_pred)\n",
    "\n",
    "            model.print(\"average lambda_bc\" , np.average(model.adpative_constant_log))\n",
    "            model.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "            time_list.append(elapsed)\n",
    "            error_u_list.append(error_u)\n",
    "            error_f_list.append(error_f)\n",
    "\n",
    "    print(\"\\n\\nMethod: \", mtd)\n",
    "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    print(\"average of error_v_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_f_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(\"./M1_dataset/NS_model_\"+mode+\"_result_mb\"+str(bcbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### database is a vailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21548/2533309064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predicted solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mU_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mF_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u_pred' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot ###\n",
    "\n",
    "# Exact solution & Predicted solution\n",
    "# Exact soluton\n",
    "U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "# Predicted solution\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Exact $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Predicted $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual loss & Boundary loss\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "gradients_res_list = []\n",
    "gradients_bcs_list = []\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "for j in range(num_hidden_layers):\n",
    "    gradient_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradient_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    gradients_res_list.append(gradient_res)\n",
    "    gradients_bcs_list.append(gradient_bcs)\n",
    "\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim([-3.0, 3.0])\n",
    "    ax.set_ylim([0,100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = mode.eigenvalue_log\n",
    "    eigenvalues_bcs_list = mode.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = mode.eigenvalue_res_log\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
