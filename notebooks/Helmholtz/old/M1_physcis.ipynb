{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "######################################################################################################\n",
    "def u(x, a_1, a_2):\n",
    "    return np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_xx(x, a_1, a_2):\n",
    "    return - (a_1 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_yy(x, a_1, a_2):\n",
    "    return - (a_2 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "# Forcing\n",
    "def f(x, a_1, a_2, lam):\n",
    "    return u_xx(x, a_1, a_2) + u_yy(x, a_1, a_2) + lam * u(x, a_1, a_2)\n",
    "\n",
    "def operator(u, x1, x2, lam, sigma_x1=1.0, sigma_x2=1.0):\n",
    "    u_x1 = tf.gradients(u, x1)[0] / sigma_x1\n",
    "    u_x2 = tf.gradients(u, x2)[0] / sigma_x2\n",
    "    u_xx1 = tf.gradients(u_x1, x1)[0] / sigma_x1\n",
    "    u_xx2 = tf.gradients(u_x2, x2)[0] / sigma_x2\n",
    "    residual = u_xx1 + u_xx2 + lam * u\n",
    "    return residual\n",
    "#######################################################################################################\n",
    "\n",
    "class Helmholtz2D:\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess):\n",
    "        # Normalization constants\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x1, self.sigma_x1 = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x2, self.sigma_x2 = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Helmoholtz constant\n",
    "        self.lam = tf.constant(lam, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = mode\n",
    "\n",
    "        # Record stiff ratio\n",
    "        # self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # if mode in ['M3', 'M4']:\n",
    "        #     # Initialize encoder weights and biases\n",
    "        #     self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        #     self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x1_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf    = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # Define placeholder for adaptive constant\n",
    "        self.adaptive_constant_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred = self.net_u(self.x1_bc1_tf, self.x2_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.x1_bc2_tf, self.x2_bc2_tf)\n",
    "        self.u_bc3_pred = self.net_u(self.x1_bc3_tf, self.x2_bc3_tf)\n",
    "        self.u_bc4_pred = self.net_u(self.x1_bc4_tf, self.x2_bc4_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x1_u_tf, self.x2_u_tf)\n",
    "        self.r_pred = self.net_r(self.x1_r_tf, self.x2_r_tf)\n",
    "\n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_tf - self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_tf - self.u_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.u_bc3_tf - self.u_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.u_bc4_tf - self.u_bc4_pred))\n",
    "        self.loss_bcs = self.adaptive_constant_tf * (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = 500 * self.loss_res + self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        # Compute and store the adaptive constant\n",
    "        self.adpative_constant_log = []\n",
    "        \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        \n",
    "        self.max_grad_res_log = []\n",
    "        self.mean_grad_bcs_log = []\n",
    "    \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_mean(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        \n",
    "        self.mean_grad_res = tf.reduce_mean(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.adaptive_constant = self.mean_grad_res / self.mean_grad_bcs\n",
    "\n",
    "        # # Stiff Ratio\n",
    "        # if self.stiff_ratio:\n",
    "        #     self.Hessian, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "        #     self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "        #     self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "        #     self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "        #     self.eigenvalue_log = []\n",
    "        #     self.eigenvalue_bcs_log = []\n",
    "        #     self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "     # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,  self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P), dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),  dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),  dtype='float32')\n",
    "\n",
    "        return H, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self,size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, x1, x2):\n",
    "        u = self.forward_pass(tf.concat([x1, x2], 1))\n",
    "        return u\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x1, x2):\n",
    "        u = self.net_u(x1, x2)\n",
    "        residual = self.operator(u, x1, x2,  self.lam, self.sigma_x1,  self.sigma_x2)\n",
    "        return residual\n",
    "\n",
    "    # Feed minibatch\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter , bcbatch_size , fbatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Fetch boundary mini-batches\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], bcbatch_size)\n",
    "        X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], bcbatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, fbatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.u_bc3_tf: u_bc3_batch,\n",
    "                    self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.u_bc4_tf: u_bc4_batch,\n",
    "                    self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                    self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                    }\n",
    "\n",
    "\n",
    "        for it in range(nIter):\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                # self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                #     self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "\n",
    "                # self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "  # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], \n",
    "                       self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss ,  loss_bcs, loss_res = self.sess.run([self.loss, self.loss_bcs, self.loss_res] , tf_dict)\n",
    "\n",
    " \n",
    "                self.print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e,Time: %.2f' % (it, loss, loss_bcs, loss_res, elapsed))\n",
    "\n",
    "                mean_grad_res , mean_grad_bcs = self.sess.run([self.mean_grad_res , self.mean_grad_bcs], tf_dict)\n",
    "                self.max_grad_res_log.append(mean_grad_res)\n",
    "                self.mean_grad_bcs_log.append(mean_grad_bcs)\n",
    "\n",
    "                self.print('mean_grad_res: %f' % (mean_grad_res))\n",
    "                self.print('mean_grad_bcs: %f' % (mean_grad_bcs))\n",
    "\n",
    "            if it % 10000 == 0:\n",
    "                # adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                self.adaptive_constant_val = 1000 # * adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "                # self.print('adaptive_constant_val: %f' % (self.adaptive_constant_val))\n",
    "\n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_u_tf: X_star[:, 0:1], self.x2_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_r_tf: X_star[:, 0:1], self.x2_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n",
    "\n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        # shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $f(x)$' , 'Predicted $f(x)$' , 'Absolute error']\n",
    "        data = [U_star , U_pred ,  np.abs(U_star - U_pred) , F_star , F_pred ,  np.abs(F_star - F_pred) ]\n",
    "        \n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        kwargs_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60),\n",
    "                cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, kwargs, minmax, title in zip(grid, data, kwargs_list, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "                #  print( z[timeStp,:,:])\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fontsize = 17\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([16,8])\n",
    "        ax.semilogy(self.max_grad_res_log, label=r'$\\bar{\\nabla_\\theta {u_{res}}}$' , color = 'tab:green')\n",
    "        ax.semilogy(self.mean_grad_bcs_log, label=r'$\\bar{\\nabla_\\theta {u_{bc}}}$' , color = 'tab:blue')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
    "        ax.tick_params(labelsize=fontsize)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
    "\n",
    "        ax2 = ax.twinx() \n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_size_inches([15,8])\n",
    "    \n",
    "        ax2.semilogy(self.adpative_constant_log, label=r'$\\bar{\\lambda_{bc}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
    "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
    "        ax2.tick_params(labelsize=fontsize)\n",
    "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = os.path.join(self.dirname,'lambda_history.png')\n",
    "        plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, lam ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f ,  model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19687/71431847.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19687/71431847.py:83: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19687/71431847.py:84: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19687/71431847.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19687/715086085.py:283: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19687/715086085.py:120: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 23:34:46.929981: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-09 23:34:46.957129: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2024-01-09 23:34:46.957563: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557da1e14080 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-09 23:34:46.957578: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-01-09 23:34:46.965268: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_19687/715086085.py:172: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19687/715086085.py:174: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19687/715086085.py:226: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 1.297e-01, Loss_bcs: 9.171e-02, Loss_res: 3.796e-02,Time: 1.50\n",
      "mean_grad_res: 0.017573\n",
      "mean_grad_bcs: 0.076702\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, Loss: 6.468e-02, Loss_bcs: 4.842e-02, Loss_res: 1.625e-02,Time: 0.01\n",
      "mean_grad_res: 0.007960\n",
      "mean_grad_bcs: 0.365481\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, Loss: 2.550e-02, Loss_bcs: 1.729e-02, Loss_res: 8.213e-03,Time: 0.01\n",
      "mean_grad_res: 0.004867\n",
      "mean_grad_bcs: 0.189095\n",
      "It: 300, Loss: 1.244e-02, Loss_bcs: 6.531e-03, Loss_res: 5.906e-03,Time: 0.01\n",
      "mean_grad_res: 0.004438\n",
      "mean_grad_bcs: 0.171527\n",
      "It: 400, Loss: 7.278e-03, Loss_bcs: 3.566e-03, Loss_res: 3.712e-03,Time: 0.00\n",
      "mean_grad_res: 0.003408\n",
      "mean_grad_bcs: 0.185475\n",
      "It: 500, Loss: 9.193e-03, Loss_bcs: 6.350e-03, Loss_res: 2.842e-03,Time: 0.00\n",
      "mean_grad_res: 0.003324\n",
      "mean_grad_bcs: 0.657734\n",
      "It: 600, Loss: 1.006e-02, Loss_bcs: 8.219e-03, Loss_res: 1.846e-03,Time: 0.01\n",
      "mean_grad_res: 0.002569\n",
      "mean_grad_bcs: 0.817537\n",
      "It: 700, Loss: 6.683e-03, Loss_bcs: 5.059e-03, Loss_res: 1.624e-03,Time: 0.01\n",
      "mean_grad_res: 0.002438\n",
      "mean_grad_bcs: 0.647395\n",
      "It: 800, Loss: 1.581e-02, Loss_bcs: 1.466e-02, Loss_res: 1.144e-03,Time: 0.01\n",
      "mean_grad_res: 0.001883\n",
      "mean_grad_bcs: 1.215776\n",
      "It: 900, Loss: 2.307e-02, Loss_bcs: 2.214e-02, Loss_res: 9.297e-04,Time: 0.01\n",
      "mean_grad_res: 0.001825\n",
      "mean_grad_bcs: 1.476471\n",
      "It: 1000, Loss: 1.827e-02, Loss_bcs: 1.758e-02, Loss_res: 6.952e-04,Time: 0.00\n",
      "mean_grad_res: 0.001526\n",
      "mean_grad_bcs: 1.284423\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, Loss: 6.667e-03, Loss_bcs: 6.005e-03, Loss_res: 6.616e-04,Time: 0.00\n",
      "mean_grad_res: 0.001475\n",
      "mean_grad_bcs: 0.744696\n",
      "It: 1200, Loss: 9.789e-03, Loss_bcs: 9.326e-03, Loss_res: 4.628e-04,Time: 0.01\n",
      "mean_grad_res: 0.001209\n",
      "mean_grad_bcs: 0.951296\n",
      "It: 1300, Loss: 7.037e-04, Loss_bcs: 2.454e-04, Loss_res: 4.583e-04,Time: 0.00\n",
      "mean_grad_res: 0.001167\n",
      "mean_grad_bcs: 0.082939\n",
      "It: 1400, Loss: 6.209e-04, Loss_bcs: 2.055e-04, Loss_res: 4.154e-04,Time: 0.00\n",
      "mean_grad_res: 0.001201\n",
      "mean_grad_bcs: 0.080927\n",
      "It: 1500, Loss: 2.600e-03, Loss_bcs: 2.181e-03, Loss_res: 4.186e-04,Time: 0.00\n",
      "mean_grad_res: 0.001154\n",
      "mean_grad_bcs: 0.447051\n",
      "It: 1600, Loss: 4.527e-03, Loss_bcs: 4.256e-03, Loss_res: 2.715e-04,Time: 0.01\n",
      "mean_grad_res: 0.000812\n",
      "mean_grad_bcs: 0.625926\n",
      "It: 1700, Loss: 2.328e-02, Loss_bcs: 2.303e-02, Loss_res: 2.463e-04,Time: 0.01\n",
      "mean_grad_res: 0.000838\n",
      "mean_grad_bcs: 1.452453\n",
      "It: 1800, Loss: 3.999e-04, Loss_bcs: 1.453e-04, Loss_res: 2.546e-04,Time: 0.01\n",
      "mean_grad_res: 0.000840\n",
      "mean_grad_bcs: 0.082423\n",
      "It: 1900, Loss: 3.235e-04, Loss_bcs: 1.064e-04, Loss_res: 2.171e-04,Time: 0.01\n",
      "mean_grad_res: 0.000870\n",
      "mean_grad_bcs: 0.056453\n",
      "It: 2000, Loss: 1.017e-03, Loss_bcs: 8.104e-04, Loss_res: 2.069e-04,Time: 0.01\n",
      "mean_grad_res: 0.000715\n",
      "mean_grad_bcs: 0.262404\n",
      "It: 2100, Loss: 1.475e-02, Loss_bcs: 1.461e-02, Loss_res: 1.469e-04,Time: 0.00\n",
      "mean_grad_res: 0.000546\n",
      "mean_grad_bcs: 1.141106\n",
      "It: 2200, Loss: 2.881e-04, Loss_bcs: 1.415e-04, Loss_res: 1.466e-04,Time: 0.01\n",
      "mean_grad_res: 0.000620\n",
      "mean_grad_bcs: 0.065964\n",
      "It: 2300, Loss: 1.095e-03, Loss_bcs: 9.496e-04, Loss_res: 1.453e-04,Time: 0.00\n",
      "mean_grad_res: 0.000500\n",
      "mean_grad_bcs: 0.276053\n",
      "It: 2400, Loss: 3.815e-03, Loss_bcs: 3.684e-03, Loss_res: 1.311e-04,Time: 0.02\n",
      "mean_grad_res: 0.000484\n",
      "mean_grad_bcs: 0.561636\n",
      "It: 2500, Loss: 1.391e-04, Loss_bcs: 3.530e-05, Loss_res: 1.038e-04,Time: 0.00\n",
      "mean_grad_res: 0.000431\n",
      "mean_grad_bcs: 0.009644\n",
      "It: 2600, Loss: 7.873e-03, Loss_bcs: 7.773e-03, Loss_res: 1.004e-04,Time: 0.00\n",
      "mean_grad_res: 0.000404\n",
      "mean_grad_bcs: 0.628890\n",
      "It: 2700, Loss: 2.019e-04, Loss_bcs: 8.857e-05, Loss_res: 1.134e-04,Time: 0.00\n",
      "mean_grad_res: 0.000512\n",
      "mean_grad_bcs: 0.071579\n",
      "It: 2800, Loss: 1.365e-03, Loss_bcs: 1.277e-03, Loss_res: 8.781e-05,Time: 0.00\n",
      "mean_grad_res: 0.000333\n",
      "mean_grad_bcs: 0.230334\n",
      "It: 2900, Loss: 1.223e-04, Loss_bcs: 2.611e-05, Loss_res: 9.616e-05,Time: 0.00\n",
      "mean_grad_res: 0.000348\n",
      "mean_grad_bcs: 0.018657\n",
      "It: 3000, Loss: 3.609e-02, Loss_bcs: 3.601e-02, Loss_res: 8.627e-05,Time: 0.00\n",
      "mean_grad_res: 0.000349\n",
      "mean_grad_bcs: 1.732638\n",
      "It: 3100, Loss: 1.576e-04, Loss_bcs: 7.756e-05, Loss_res: 8.005e-05,Time: 0.00\n",
      "mean_grad_res: 0.000337\n",
      "mean_grad_bcs: 0.043529\n",
      "It: 3200, Loss: 1.180e-04, Loss_bcs: 4.386e-05, Loss_res: 7.411e-05,Time: 0.00\n",
      "mean_grad_res: 0.000268\n",
      "mean_grad_bcs: 0.040985\n",
      "It: 3300, Loss: 4.212e-04, Loss_bcs: 3.304e-04, Loss_res: 9.073e-05,Time: 0.01\n",
      "mean_grad_res: 0.000325\n",
      "mean_grad_bcs: 0.151115\n",
      "It: 3400, Loss: 8.006e-03, Loss_bcs: 7.925e-03, Loss_res: 8.165e-05,Time: 0.01\n",
      "mean_grad_res: 0.000376\n",
      "mean_grad_bcs: 0.709836\n",
      "It: 3500, Loss: 2.221e-04, Loss_bcs: 1.585e-04, Loss_res: 6.367e-05,Time: 0.01\n",
      "mean_grad_res: 0.000272\n",
      "mean_grad_bcs: 0.093492\n",
      "It: 3600, Loss: 1.952e-03, Loss_bcs: 1.889e-03, Loss_res: 6.280e-05,Time: 0.01\n",
      "mean_grad_res: 0.000269\n",
      "mean_grad_bcs: 0.347957\n",
      "It: 3700, Loss: 2.203e-02, Loss_bcs: 2.196e-02, Loss_res: 6.175e-05,Time: 0.01\n",
      "mean_grad_res: 0.000287\n",
      "mean_grad_bcs: 0.813198\n",
      "It: 3800, Loss: 6.305e-05, Loss_bcs: 2.051e-05, Loss_res: 4.253e-05,Time: 0.01\n",
      "mean_grad_res: 0.000227\n",
      "mean_grad_bcs: 0.017072\n",
      "It: 3900, Loss: 4.479e-02, Loss_bcs: 4.475e-02, Loss_res: 3.858e-05,Time: 0.01\n",
      "mean_grad_res: 0.000159\n",
      "mean_grad_bcs: 1.712113\n",
      "It: 4000, Loss: 1.294e-04, Loss_bcs: 7.169e-05, Loss_res: 5.768e-05,Time: 0.01\n",
      "mean_grad_res: 0.000346\n",
      "mean_grad_bcs: 0.061262\n",
      "It: 4100, Loss: 1.081e-04, Loss_bcs: 4.732e-05, Loss_res: 6.079e-05,Time: 0.00\n",
      "mean_grad_res: 0.000278\n",
      "mean_grad_bcs: 0.037282\n",
      "It: 4200, Loss: 1.520e-02, Loss_bcs: 1.513e-02, Loss_res: 6.804e-05,Time: 0.01\n",
      "mean_grad_res: 0.000356\n",
      "mean_grad_bcs: 0.953189\n",
      "It: 4300, Loss: 2.464e-04, Loss_bcs: 1.795e-04, Loss_res: 6.688e-05,Time: 0.01\n",
      "mean_grad_res: 0.000312\n",
      "mean_grad_bcs: 0.109078\n",
      "It: 4400, Loss: 3.605e-02, Loss_bcs: 3.600e-02, Loss_res: 5.353e-05,Time: 0.00\n",
      "mean_grad_res: 0.000266\n",
      "mean_grad_bcs: 1.508045\n",
      "It: 4500, Loss: 1.849e-04, Loss_bcs: 1.348e-04, Loss_res: 5.010e-05,Time: 0.00\n",
      "mean_grad_res: 0.000343\n",
      "mean_grad_bcs: 0.092258\n",
      "It: 4600, Loss: 5.059e-03, Loss_bcs: 5.020e-03, Loss_res: 3.933e-05,Time: 0.01\n",
      "mean_grad_res: 0.000206\n",
      "mean_grad_bcs: 0.585562\n",
      "It: 4700, Loss: 1.501e-03, Loss_bcs: 1.465e-03, Loss_res: 3.626e-05,Time: 0.00\n",
      "mean_grad_res: 0.000193\n",
      "mean_grad_bcs: 0.312046\n",
      "It: 4800, Loss: 2.707e-04, Loss_bcs: 2.129e-04, Loss_res: 5.778e-05,Time: 0.00\n",
      "mean_grad_res: 0.000243\n",
      "mean_grad_bcs: 0.110030\n",
      "It: 4900, Loss: 8.594e-05, Loss_bcs: 3.534e-05, Loss_res: 5.060e-05,Time: 0.00\n",
      "mean_grad_res: 0.000272\n",
      "mean_grad_bcs: 0.034459\n",
      "It: 5000, Loss: 1.756e-03, Loss_bcs: 1.688e-03, Loss_res: 6.799e-05,Time: 0.01\n",
      "mean_grad_res: 0.000337\n",
      "mean_grad_bcs: 0.116629\n",
      "It: 5100, Loss: 2.270e-02, Loss_bcs: 2.265e-02, Loss_res: 5.043e-05,Time: 0.01\n",
      "mean_grad_res: 0.000243\n",
      "mean_grad_bcs: 1.279176\n",
      "It: 5200, Loss: 8.131e-04, Loss_bcs: 7.744e-04, Loss_res: 3.874e-05,Time: 0.01\n",
      "mean_grad_res: 0.000173\n",
      "mean_grad_bcs: 0.229319\n",
      "It: 5300, Loss: 1.085e-03, Loss_bcs: 1.039e-03, Loss_res: 4.592e-05,Time: 0.01\n",
      "mean_grad_res: 0.000243\n",
      "mean_grad_bcs: 0.257804\n",
      "It: 5400, Loss: 7.785e-03, Loss_bcs: 7.748e-03, Loss_res: 3.772e-05,Time: 0.01\n",
      "mean_grad_res: 0.000199\n",
      "mean_grad_bcs: 0.695764\n",
      "It: 5500, Loss: 7.432e-04, Loss_bcs: 6.957e-04, Loss_res: 4.748e-05,Time: 0.00\n",
      "mean_grad_res: 0.000238\n",
      "mean_grad_bcs: 0.207684\n",
      "It: 5600, Loss: 1.835e-03, Loss_bcs: 1.797e-03, Loss_res: 3.846e-05,Time: 0.01\n",
      "mean_grad_res: 0.000180\n",
      "mean_grad_bcs: 0.288658\n",
      "It: 5700, Loss: 1.916e-02, Loss_bcs: 1.911e-02, Loss_res: 4.990e-05,Time: 0.00\n",
      "mean_grad_res: 0.000270\n",
      "mean_grad_bcs: 0.955289\n",
      "It: 5800, Loss: 9.573e-04, Loss_bcs: 9.157e-04, Loss_res: 4.163e-05,Time: 0.00\n",
      "mean_grad_res: 0.000202\n",
      "mean_grad_bcs: 0.237448\n",
      "It: 5900, Loss: 5.506e-02, Loss_bcs: 5.501e-02, Loss_res: 4.754e-05,Time: 0.00\n",
      "mean_grad_res: 0.000348\n",
      "mean_grad_bcs: 1.901916\n",
      "It: 6000, Loss: 7.731e-05, Loss_bcs: 3.314e-05, Loss_res: 4.417e-05,Time: 0.00\n",
      "mean_grad_res: 0.000245\n",
      "mean_grad_bcs: 0.034050\n",
      "It: 6100, Loss: 4.170e-04, Loss_bcs: 3.832e-04, Loss_res: 3.376e-05,Time: 0.00\n",
      "mean_grad_res: 0.000182\n",
      "mean_grad_bcs: 0.161664\n",
      "It: 6200, Loss: 2.739e-02, Loss_bcs: 2.735e-02, Loss_res: 3.420e-05,Time: 0.01\n",
      "mean_grad_res: 0.000167\n",
      "mean_grad_bcs: 1.345865\n",
      "It: 6300, Loss: 1.486e-03, Loss_bcs: 1.451e-03, Loss_res: 3.521e-05,Time: 0.00\n",
      "mean_grad_res: 0.000165\n",
      "mean_grad_bcs: 0.256729\n",
      "It: 6400, Loss: 8.528e-03, Loss_bcs: 8.490e-03, Loss_res: 3.764e-05,Time: 0.00\n",
      "mean_grad_res: 0.000179\n",
      "mean_grad_bcs: 0.598129\n",
      "It: 6500, Loss: 1.001e-02, Loss_bcs: 9.976e-03, Loss_res: 3.920e-05,Time: 0.00\n",
      "mean_grad_res: 0.000192\n",
      "mean_grad_bcs: 0.806372\n",
      "It: 6600, Loss: 1.582e-03, Loss_bcs: 1.542e-03, Loss_res: 4.013e-05,Time: 0.00\n",
      "mean_grad_res: 0.000187\n",
      "mean_grad_bcs: 0.314927\n",
      "It: 6700, Loss: 3.774e-02, Loss_bcs: 3.770e-02, Loss_res: 4.471e-05,Time: 0.01\n",
      "mean_grad_res: 0.000222\n",
      "mean_grad_bcs: 1.589665\n",
      "It: 6800, Loss: 9.280e-04, Loss_bcs: 8.937e-04, Loss_res: 3.434e-05,Time: 0.01\n",
      "mean_grad_res: 0.000178\n",
      "mean_grad_bcs: 0.231672\n",
      "It: 6900, Loss: 1.275e-03, Loss_bcs: 1.241e-03, Loss_res: 3.321e-05,Time: 0.01\n",
      "mean_grad_res: 0.000171\n",
      "mean_grad_bcs: 0.288026\n",
      "It: 7000, Loss: 8.879e-04, Loss_bcs: 8.274e-04, Loss_res: 6.045e-05,Time: 0.00\n",
      "mean_grad_res: 0.000331\n",
      "mean_grad_bcs: 0.203561\n",
      "It: 7100, Loss: 4.687e-05, Loss_bcs: 1.024e-05, Loss_res: 3.663e-05,Time: 0.00\n",
      "mean_grad_res: 0.000179\n",
      "mean_grad_bcs: 0.011141\n",
      "It: 7200, Loss: 5.415e-05, Loss_bcs: 1.985e-05, Loss_res: 3.430e-05,Time: 0.00\n",
      "mean_grad_res: 0.000158\n",
      "mean_grad_bcs: 0.023390\n",
      "It: 7300, Loss: 4.086e-04, Loss_bcs: 3.801e-04, Loss_res: 2.848e-05,Time: 0.01\n",
      "mean_grad_res: 0.000143\n",
      "mean_grad_bcs: 0.142044\n",
      "It: 7400, Loss: 4.149e-05, Loss_bcs: 1.068e-05, Loss_res: 3.081e-05,Time: 0.01\n",
      "mean_grad_res: 0.000151\n",
      "mean_grad_bcs: 0.005828\n",
      "It: 7500, Loss: 4.817e-05, Loss_bcs: 2.275e-05, Loss_res: 2.541e-05,Time: 0.00\n",
      "mean_grad_res: 0.000136\n",
      "mean_grad_bcs: 0.024674\n",
      "It: 7600, Loss: 7.606e-05, Loss_bcs: 4.553e-05, Loss_res: 3.053e-05,Time: 0.00\n",
      "mean_grad_res: 0.000151\n",
      "mean_grad_bcs: 0.046291\n",
      "It: 7700, Loss: 7.288e-04, Loss_bcs: 6.870e-04, Loss_res: 4.188e-05,Time: 0.00\n",
      "mean_grad_res: 0.000198\n",
      "mean_grad_bcs: 0.200617\n",
      "It: 7800, Loss: 4.697e-05, Loss_bcs: 1.678e-05, Loss_res: 3.018e-05,Time: 0.00\n",
      "mean_grad_res: 0.000150\n",
      "mean_grad_bcs: 0.020567\n",
      "It: 7900, Loss: 7.173e-05, Loss_bcs: 4.342e-05, Loss_res: 2.831e-05,Time: 0.00\n",
      "mean_grad_res: 0.000147\n",
      "mean_grad_bcs: 0.042007\n",
      "It: 8000, Loss: 2.234e-02, Loss_bcs: 2.231e-02, Loss_res: 2.891e-05,Time: 0.00\n",
      "mean_grad_res: 0.000171\n",
      "mean_grad_bcs: 1.196786\n",
      "It: 8100, Loss: 5.082e-05, Loss_bcs: 2.709e-05, Loss_res: 2.373e-05,Time: 0.00\n",
      "mean_grad_res: 0.000129\n",
      "mean_grad_bcs: 0.028764\n",
      "It: 8200, Loss: 4.172e-04, Loss_bcs: 3.850e-04, Loss_res: 3.220e-05,Time: 0.00\n",
      "mean_grad_res: 0.000159\n",
      "mean_grad_bcs: 0.135060\n",
      "It: 8300, Loss: 8.395e-05, Loss_bcs: 6.115e-05, Loss_res: 2.281e-05,Time: 0.00\n",
      "mean_grad_res: 0.000124\n",
      "mean_grad_bcs: 0.036615\n",
      "It: 8400, Loss: 3.634e-04, Loss_bcs: 3.377e-04, Loss_res: 2.564e-05,Time: 0.00\n",
      "mean_grad_res: 0.000130\n",
      "mean_grad_bcs: 0.130377\n",
      "It: 8500, Loss: 7.629e-03, Loss_bcs: 7.595e-03, Loss_res: 3.391e-05,Time: 0.00\n",
      "mean_grad_res: 0.000180\n",
      "mean_grad_bcs: 0.619057\n",
      "It: 8600, Loss: 1.111e-04, Loss_bcs: 8.724e-05, Loss_res: 2.384e-05,Time: 0.00\n",
      "mean_grad_res: 0.000125\n",
      "mean_grad_bcs: 0.066070\n",
      "It: 8700, Loss: 5.972e-03, Loss_bcs: 5.949e-03, Loss_res: 2.331e-05,Time: 0.00\n",
      "mean_grad_res: 0.000119\n",
      "mean_grad_bcs: 0.585718\n",
      "It: 8800, Loss: 1.397e-03, Loss_bcs: 1.369e-03, Loss_res: 2.782e-05,Time: 0.00\n",
      "mean_grad_res: 0.000155\n",
      "mean_grad_bcs: 0.234813\n",
      "It: 8900, Loss: 1.874e-04, Loss_bcs: 1.637e-04, Loss_res: 2.372e-05,Time: 0.01\n",
      "mean_grad_res: 0.000144\n",
      "mean_grad_bcs: 0.070283\n",
      "It: 9000, Loss: 2.516e-02, Loss_bcs: 2.513e-02, Loss_res: 2.825e-05,Time: 0.01\n",
      "mean_grad_res: 0.000153\n",
      "mean_grad_bcs: 1.201771\n",
      "It: 9100, Loss: 6.239e-04, Loss_bcs: 5.938e-04, Loss_res: 3.017e-05,Time: 0.01\n",
      "mean_grad_res: 0.000153\n",
      "mean_grad_bcs: 0.178695\n",
      "It: 9200, Loss: 9.288e-05, Loss_bcs: 6.428e-05, Loss_res: 2.859e-05,Time: 0.00\n",
      "mean_grad_res: 0.000155\n",
      "mean_grad_bcs: 0.057925\n",
      "It: 9300, Loss: 5.895e-04, Loss_bcs: 5.620e-04, Loss_res: 2.751e-05,Time: 0.00\n",
      "mean_grad_res: 0.000152\n",
      "mean_grad_bcs: 0.171221\n",
      "It: 9400, Loss: 3.631e-03, Loss_bcs: 3.595e-03, Loss_res: 3.566e-05,Time: 0.00\n",
      "mean_grad_res: 0.000181\n",
      "mean_grad_bcs: 0.371766\n",
      "It: 9500, Loss: 1.801e-04, Loss_bcs: 1.530e-04, Loss_res: 2.714e-05,Time: 0.01\n",
      "mean_grad_res: 0.000146\n",
      "mean_grad_bcs: 0.068557\n",
      "It: 9600, Loss: 3.891e-04, Loss_bcs: 3.631e-04, Loss_res: 2.602e-05,Time: 0.01\n",
      "mean_grad_res: 0.000147\n",
      "mean_grad_bcs: 0.140935\n",
      "It: 9700, Loss: 1.424e-02, Loss_bcs: 1.421e-02, Loss_res: 2.848e-05,Time: 0.01\n",
      "mean_grad_res: 0.000147\n",
      "mean_grad_bcs: 0.923279\n",
      "It: 9800, Loss: 7.986e-05, Loss_bcs: 5.383e-05, Loss_res: 2.603e-05,Time: 0.00\n",
      "mean_grad_res: 0.000143\n",
      "mean_grad_bcs: 0.051150\n",
      "It: 9900, Loss: 1.203e-04, Loss_bcs: 9.355e-05, Loss_res: 2.679e-05,Time: 0.00\n",
      "mean_grad_res: 0.000146\n",
      "mean_grad_bcs: 0.055595\n",
      "It: 10000, Loss: 4.033e-05, Loss_bcs: 1.539e-05, Loss_res: 2.494e-05,Time: 0.00\n",
      "mean_grad_res: 0.000137\n",
      "mean_grad_bcs: 0.021688\n",
      "It: 10100, Loss: 2.003e-02, Loss_bcs: 1.999e-02, Loss_res: 3.819e-05,Time: 0.00\n",
      "mean_grad_res: 0.000214\n",
      "mean_grad_bcs: 1.017791\n",
      "It: 10200, Loss: 3.544e-05, Loss_bcs: 9.574e-06, Loss_res: 2.587e-05,Time: 0.00\n",
      "mean_grad_res: 0.000139\n",
      "mean_grad_bcs: 0.012720\n",
      "It: 10300, Loss: 3.385e-05, Loss_bcs: 1.039e-05, Loss_res: 2.346e-05,Time: 0.01\n",
      "mean_grad_res: 0.000128\n",
      "mean_grad_bcs: 0.016425\n",
      "It: 10400, Loss: 4.207e-02, Loss_bcs: 4.205e-02, Loss_res: 1.907e-05,Time: 0.00\n",
      "mean_grad_res: 0.000124\n",
      "mean_grad_bcs: 1.547466\n",
      "It: 10500, Loss: 6.669e-05, Loss_bcs: 4.358e-05, Loss_res: 2.310e-05,Time: 0.00\n",
      "mean_grad_res: 0.000127\n",
      "mean_grad_bcs: 0.045220\n",
      "It: 10600, Loss: 6.614e-04, Loss_bcs: 6.424e-04, Loss_res: 1.894e-05,Time: 0.00\n",
      "mean_grad_res: 0.000115\n",
      "mean_grad_bcs: 0.151540\n",
      "It: 10700, Loss: 4.203e-04, Loss_bcs: 3.998e-04, Loss_res: 2.052e-05,Time: 0.00\n",
      "mean_grad_res: 0.000114\n",
      "mean_grad_bcs: 0.126524\n",
      "It: 10800, Loss: 1.749e-02, Loss_bcs: 1.746e-02, Loss_res: 2.957e-05,Time: 0.00\n",
      "mean_grad_res: 0.000163\n",
      "mean_grad_bcs: 0.957238\n",
      "It: 10900, Loss: 5.819e-05, Loss_bcs: 3.328e-05, Loss_res: 2.491e-05,Time: 0.00\n",
      "mean_grad_res: 0.000144\n",
      "mean_grad_bcs: 0.038927\n",
      "It: 11000, Loss: 1.363e-02, Loss_bcs: 1.361e-02, Loss_res: 2.227e-05,Time: 0.00\n",
      "mean_grad_res: 0.000125\n",
      "mean_grad_bcs: 0.853101\n",
      "It: 11100, Loss: 3.563e-05, Loss_bcs: 1.298e-05, Loss_res: 2.265e-05,Time: 0.01\n",
      "mean_grad_res: 0.000123\n",
      "mean_grad_bcs: 0.021248\n",
      "It: 11200, Loss: 4.244e-05, Loss_bcs: 2.108e-05, Loss_res: 2.136e-05,Time: 0.00\n",
      "mean_grad_res: 0.000129\n",
      "mean_grad_bcs: 0.029665\n",
      "It: 11300, Loss: 1.092e-03, Loss_bcs: 1.072e-03, Loss_res: 1.984e-05,Time: 0.00\n",
      "mean_grad_res: 0.000115\n",
      "mean_grad_bcs: 0.196781\n",
      "It: 11400, Loss: 4.004e-05, Loss_bcs: 1.730e-05, Loss_res: 2.274e-05,Time: 0.01\n",
      "mean_grad_res: 0.000122\n",
      "mean_grad_bcs: 0.023746\n",
      "It: 11500, Loss: 4.345e-05, Loss_bcs: 1.755e-05, Loss_res: 2.591e-05,Time: 0.01\n",
      "mean_grad_res: 0.000140\n",
      "mean_grad_bcs: 0.025654\n",
      "It: 11600, Loss: 2.265e-04, Loss_bcs: 2.063e-04, Loss_res: 2.027e-05,Time: 0.01\n",
      "mean_grad_res: 0.000116\n",
      "mean_grad_bcs: 0.088092\n",
      "It: 11700, Loss: 3.823e-03, Loss_bcs: 3.803e-03, Loss_res: 2.037e-05,Time: 0.01\n",
      "mean_grad_res: 0.000121\n",
      "mean_grad_bcs: 0.450954\n",
      "It: 11800, Loss: 3.138e-05, Loss_bcs: 1.106e-05, Loss_res: 2.032e-05,Time: 0.01\n",
      "mean_grad_res: 0.000116\n",
      "mean_grad_bcs: 0.007780\n",
      "It: 11900, Loss: 1.703e-03, Loss_bcs: 1.681e-03, Loss_res: 2.114e-05,Time: 0.01\n",
      "mean_grad_res: 0.000128\n",
      "mean_grad_bcs: 0.256255\n",
      "It: 12000, Loss: 1.728e-04, Loss_bcs: 1.485e-04, Loss_res: 2.434e-05,Time: 0.01\n",
      "mean_grad_res: 0.000134\n",
      "mean_grad_bcs: 0.074866\n",
      "It: 12100, Loss: 5.882e-04, Loss_bcs: 5.707e-04, Loss_res: 1.749e-05,Time: 0.00\n",
      "mean_grad_res: 0.000116\n",
      "mean_grad_bcs: 0.164240\n",
      "It: 12200, Loss: 1.345e-04, Loss_bcs: 1.163e-04, Loss_res: 1.822e-05,Time: 0.01\n",
      "mean_grad_res: 0.000105\n",
      "mean_grad_bcs: 0.064949\n",
      "It: 12300, Loss: 3.159e-04, Loss_bcs: 2.937e-04, Loss_res: 2.219e-05,Time: 0.01\n",
      "mean_grad_res: 0.000132\n",
      "mean_grad_bcs: 0.123025\n",
      "It: 12400, Loss: 1.304e-03, Loss_bcs: 1.282e-03, Loss_res: 2.106e-05,Time: 0.00\n",
      "mean_grad_res: 0.000136\n",
      "mean_grad_bcs: 0.242415\n",
      "It: 12500, Loss: 5.728e-03, Loss_bcs: 5.710e-03, Loss_res: 1.790e-05,Time: 0.01\n",
      "mean_grad_res: 0.000110\n",
      "mean_grad_bcs: 0.570494\n",
      "It: 12600, Loss: 1.020e-04, Loss_bcs: 8.307e-05, Loss_res: 1.895e-05,Time: 0.00\n",
      "mean_grad_res: 0.000113\n",
      "mean_grad_bcs: 0.055764\n",
      "It: 12700, Loss: 2.231e-03, Loss_bcs: 2.216e-03, Loss_res: 1.421e-05,Time: 0.01\n",
      "mean_grad_res: 0.000089\n",
      "mean_grad_bcs: 0.278402\n",
      "It: 12800, Loss: 5.290e-05, Loss_bcs: 3.414e-05, Loss_res: 1.876e-05,Time: 0.01\n",
      "mean_grad_res: 0.000112\n",
      "mean_grad_bcs: 0.036998\n",
      "It: 12900, Loss: 3.158e-03, Loss_bcs: 3.137e-03, Loss_res: 2.062e-05,Time: 0.00\n",
      "mean_grad_res: 0.000120\n",
      "mean_grad_bcs: 0.336961\n",
      "It: 13000, Loss: 1.480e-03, Loss_bcs: 1.459e-03, Loss_res: 2.034e-05,Time: 0.00\n",
      "mean_grad_res: 0.000120\n",
      "mean_grad_bcs: 0.270088\n",
      "It: 13100, Loss: 7.897e-04, Loss_bcs: 7.680e-04, Loss_res: 2.171e-05,Time: 0.01\n",
      "mean_grad_res: 0.000127\n",
      "mean_grad_bcs: 0.184463\n",
      "It: 13200, Loss: 8.370e-05, Loss_bcs: 6.686e-05, Loss_res: 1.684e-05,Time: 0.01\n",
      "mean_grad_res: 0.000101\n",
      "mean_grad_bcs: 0.047743\n",
      "It: 13300, Loss: 3.013e-05, Loss_bcs: 8.606e-06, Loss_res: 2.153e-05,Time: 0.00\n",
      "mean_grad_res: 0.000122\n",
      "mean_grad_bcs: 0.014319\n",
      "It: 13400, Loss: 6.482e-03, Loss_bcs: 6.465e-03, Loss_res: 1.678e-05,Time: 0.01\n",
      "mean_grad_res: 0.000114\n",
      "mean_grad_bcs: 0.486513\n",
      "It: 13500, Loss: 8.115e-05, Loss_bcs: 6.493e-05, Loss_res: 1.623e-05,Time: 0.01\n",
      "mean_grad_res: 0.000103\n",
      "mean_grad_bcs: 0.045275\n",
      "It: 13600, Loss: 4.454e-03, Loss_bcs: 4.438e-03, Loss_res: 1.582e-05,Time: 0.00\n",
      "mean_grad_res: 0.000108\n",
      "mean_grad_bcs: 0.464928\n",
      "It: 13700, Loss: 2.818e-05, Loss_bcs: 1.090e-05, Loss_res: 1.728e-05,Time: 0.01\n",
      "mean_grad_res: 0.000104\n",
      "mean_grad_bcs: 0.014913\n",
      "It: 13800, Loss: 7.159e-03, Loss_bcs: 7.135e-03, Loss_res: 2.407e-05,Time: 0.01\n",
      "mean_grad_res: 0.000149\n",
      "mean_grad_bcs: 0.499603\n",
      "It: 13900, Loss: 9.562e-05, Loss_bcs: 8.131e-05, Loss_res: 1.431e-05,Time: 0.01\n",
      "mean_grad_res: 0.000089\n",
      "mean_grad_bcs: 0.056214\n",
      "It: 14000, Loss: 6.927e-04, Loss_bcs: 6.789e-04, Loss_res: 1.385e-05,Time: 0.00\n",
      "mean_grad_res: 0.000087\n",
      "mean_grad_bcs: 0.188599\n",
      "It: 14100, Loss: 2.106e-04, Loss_bcs: 1.963e-04, Loss_res: 1.428e-05,Time: 0.01\n",
      "mean_grad_res: 0.000093\n",
      "mean_grad_bcs: 0.098318\n",
      "It: 14200, Loss: 6.875e-04, Loss_bcs: 6.716e-04, Loss_res: 1.588e-05,Time: 0.01\n",
      "mean_grad_res: 0.000101\n",
      "mean_grad_bcs: 0.163914\n",
      "It: 14300, Loss: 3.583e-04, Loss_bcs: 3.416e-04, Loss_res: 1.671e-05,Time: 0.00\n",
      "mean_grad_res: 0.000107\n",
      "mean_grad_bcs: 0.111157\n",
      "It: 14400, Loss: 1.332e-02, Loss_bcs: 1.330e-02, Loss_res: 1.898e-05,Time: 0.01\n",
      "mean_grad_res: 0.000115\n",
      "mean_grad_bcs: 0.785019\n",
      "It: 14500, Loss: 5.787e-04, Loss_bcs: 5.645e-04, Loss_res: 1.419e-05,Time: 0.00\n",
      "mean_grad_res: 0.000104\n",
      "mean_grad_bcs: 0.168766\n",
      "It: 14600, Loss: 2.521e-04, Loss_bcs: 2.354e-04, Loss_res: 1.678e-05,Time: 0.01\n",
      "mean_grad_res: 0.000105\n",
      "mean_grad_bcs: 0.105092\n",
      "It: 14700, Loss: 1.178e-03, Loss_bcs: 1.165e-03, Loss_res: 1.363e-05,Time: 0.00\n",
      "mean_grad_res: 0.000084\n",
      "mean_grad_bcs: 0.219525\n",
      "It: 14800, Loss: 3.082e-04, Loss_bcs: 2.928e-04, Loss_res: 1.546e-05,Time: 0.00\n",
      "mean_grad_res: 0.000111\n",
      "mean_grad_bcs: 0.119702\n",
      "It: 14900, Loss: 5.729e-05, Loss_bcs: 4.370e-05, Loss_res: 1.359e-05,Time: 0.01\n",
      "mean_grad_res: 0.000090\n",
      "mean_grad_bcs: 0.035280\n",
      "It: 15000, Loss: 4.097e-03, Loss_bcs: 4.076e-03, Loss_res: 2.069e-05,Time: 0.01\n",
      "mean_grad_res: 0.000129\n",
      "mean_grad_bcs: 0.414288\n",
      "It: 15100, Loss: 3.738e-05, Loss_bcs: 1.796e-05, Loss_res: 1.942e-05,Time: 0.01\n",
      "mean_grad_res: 0.000126\n",
      "mean_grad_bcs: 0.022206\n",
      "It: 15200, Loss: 6.417e-03, Loss_bcs: 6.405e-03, Loss_res: 1.272e-05,Time: 0.01\n",
      "mean_grad_res: 0.000082\n",
      "mean_grad_bcs: 0.490246\n",
      "It: 15300, Loss: 3.544e-04, Loss_bcs: 3.376e-04, Loss_res: 1.676e-05,Time: 0.01\n",
      "mean_grad_res: 0.000107\n",
      "mean_grad_bcs: 0.131866\n",
      "It: 15400, Loss: 5.552e-05, Loss_bcs: 4.091e-05, Loss_res: 1.461e-05,Time: 0.01\n",
      "mean_grad_res: 0.000095\n",
      "mean_grad_bcs: 0.044940\n",
      "It: 15500, Loss: 2.629e-05, Loss_bcs: 1.088e-05, Loss_res: 1.541e-05,Time: 0.01\n",
      "mean_grad_res: 0.000096\n",
      "mean_grad_bcs: 0.017200\n",
      "It: 15600, Loss: 2.820e-05, Loss_bcs: 1.050e-05, Loss_res: 1.770e-05,Time: 0.01\n",
      "mean_grad_res: 0.000125\n",
      "mean_grad_bcs: 0.015486\n",
      "It: 15700, Loss: 4.204e-03, Loss_bcs: 4.190e-03, Loss_res: 1.445e-05,Time: 0.01\n",
      "mean_grad_res: 0.000106\n",
      "mean_grad_bcs: 0.450347\n",
      "It: 15800, Loss: 1.050e-04, Loss_bcs: 8.957e-05, Loss_res: 1.545e-05,Time: 0.00\n",
      "mean_grad_res: 0.000103\n",
      "mean_grad_bcs: 0.066833\n",
      "It: 15900, Loss: 3.587e-03, Loss_bcs: 3.571e-03, Loss_res: 1.611e-05,Time: 0.00\n",
      "mean_grad_res: 0.000109\n",
      "mean_grad_bcs: 0.382917\n",
      "It: 16000, Loss: 4.890e-04, Loss_bcs: 4.756e-04, Loss_res: 1.343e-05,Time: 0.00\n",
      "mean_grad_res: 0.000097\n",
      "mean_grad_bcs: 0.153162\n",
      "It: 16100, Loss: 7.853e-05, Loss_bcs: 6.637e-05, Loss_res: 1.216e-05,Time: 0.01\n",
      "mean_grad_res: 0.000083\n",
      "mean_grad_bcs: 0.016173\n",
      "It: 16200, Loss: 8.816e-04, Loss_bcs: 8.690e-04, Loss_res: 1.256e-05,Time: 0.01\n",
      "mean_grad_res: 0.000085\n",
      "mean_grad_bcs: 0.200210\n",
      "It: 16300, Loss: 1.009e-04, Loss_bcs: 8.690e-05, Loss_res: 1.399e-05,Time: 0.01\n",
      "mean_grad_res: 0.000094\n",
      "mean_grad_bcs: 0.052705\n",
      "It: 16400, Loss: 1.077e-03, Loss_bcs: 1.066e-03, Loss_res: 1.135e-05,Time: 0.01\n",
      "mean_grad_res: 0.000076\n",
      "mean_grad_bcs: 0.191038\n",
      "It: 16500, Loss: 1.299e-04, Loss_bcs: 1.197e-04, Loss_res: 1.021e-05,Time: 0.00\n",
      "mean_grad_res: 0.000070\n",
      "mean_grad_bcs: 0.067704\n",
      "It: 16600, Loss: 1.832e-03, Loss_bcs: 1.819e-03, Loss_res: 1.246e-05,Time: 0.01\n",
      "mean_grad_res: 0.000089\n",
      "mean_grad_bcs: 0.243830\n",
      "It: 16700, Loss: 1.355e-04, Loss_bcs: 1.202e-04, Loss_res: 1.526e-05,Time: 0.01\n",
      "mean_grad_res: 0.000113\n",
      "mean_grad_bcs: 0.076765\n",
      "It: 16800, Loss: 2.356e-03, Loss_bcs: 2.341e-03, Loss_res: 1.479e-05,Time: 0.00\n",
      "mean_grad_res: 0.000103\n",
      "mean_grad_bcs: 0.312519\n",
      "It: 16900, Loss: 2.314e-05, Loss_bcs: 9.108e-06, Loss_res: 1.403e-05,Time: 0.00\n",
      "mean_grad_res: 0.000093\n",
      "mean_grad_bcs: 0.012980\n",
      "It: 17000, Loss: 3.119e-05, Loss_bcs: 2.153e-05, Loss_res: 9.662e-06,Time: 0.01\n",
      "mean_grad_res: 0.000074\n",
      "mean_grad_bcs: 0.029361\n",
      "It: 17100, Loss: 5.577e-04, Loss_bcs: 5.419e-04, Loss_res: 1.580e-05,Time: 0.01\n",
      "mean_grad_res: 0.000102\n",
      "mean_grad_bcs: 0.160376\n",
      "It: 17200, Loss: 8.609e-04, Loss_bcs: 8.475e-04, Loss_res: 1.343e-05,Time: 0.01\n",
      "mean_grad_res: 0.000096\n",
      "mean_grad_bcs: 0.181672\n",
      "It: 17300, Loss: 7.363e-03, Loss_bcs: 7.349e-03, Loss_res: 1.405e-05,Time: 0.01\n",
      "mean_grad_res: 0.000101\n",
      "mean_grad_bcs: 0.529617\n",
      "It: 17400, Loss: 1.944e-05, Loss_bcs: 8.310e-06, Loss_res: 1.113e-05,Time: 0.00\n",
      "mean_grad_res: 0.000079\n",
      "mean_grad_bcs: 0.014677\n",
      "It: 17500, Loss: 1.567e-05, Loss_bcs: 3.285e-06, Loss_res: 1.239e-05,Time: 0.01\n",
      "mean_grad_res: 0.000092\n",
      "mean_grad_bcs: 0.001187\n",
      "It: 17600, Loss: 1.528e-03, Loss_bcs: 1.516e-03, Loss_res: 1.156e-05,Time: 0.01\n",
      "mean_grad_res: 0.000091\n",
      "mean_grad_bcs: 0.271016\n",
      "It: 17700, Loss: 4.374e-05, Loss_bcs: 3.127e-05, Loss_res: 1.247e-05,Time: 0.01\n",
      "mean_grad_res: 0.000088\n",
      "mean_grad_bcs: 0.030749\n",
      "It: 17800, Loss: 3.251e-04, Loss_bcs: 3.103e-04, Loss_res: 1.480e-05,Time: 0.01\n",
      "mean_grad_res: 0.000108\n",
      "mean_grad_bcs: 0.101487\n",
      "It: 17900, Loss: 3.367e-05, Loss_bcs: 2.497e-05, Loss_res: 8.696e-06,Time: 0.00\n",
      "mean_grad_res: 0.000066\n",
      "mean_grad_bcs: 0.025425\n",
      "It: 18000, Loss: 1.565e-05, Loss_bcs: 3.826e-06, Loss_res: 1.183e-05,Time: 0.01\n",
      "mean_grad_res: 0.000093\n",
      "mean_grad_bcs: 0.004722\n",
      "It: 18100, Loss: 7.686e-04, Loss_bcs: 7.568e-04, Loss_res: 1.183e-05,Time: 0.00\n",
      "mean_grad_res: 0.000091\n",
      "mean_grad_bcs: 0.170180\n",
      "It: 18200, Loss: 6.554e-04, Loss_bcs: 6.416e-04, Loss_res: 1.386e-05,Time: 0.00\n",
      "mean_grad_res: 0.000111\n",
      "mean_grad_bcs: 0.181414\n",
      "It: 18300, Loss: 1.290e-04, Loss_bcs: 1.180e-04, Loss_res: 1.100e-05,Time: 0.00\n",
      "mean_grad_res: 0.000080\n",
      "mean_grad_bcs: 0.066716\n",
      "It: 18400, Loss: 9.039e-04, Loss_bcs: 8.898e-04, Loss_res: 1.411e-05,Time: 0.00\n",
      "mean_grad_res: 0.000103\n",
      "mean_grad_bcs: 0.170866\n",
      "It: 18500, Loss: 4.681e-02, Loss_bcs: 4.680e-02, Loss_res: 1.358e-05,Time: 0.01\n",
      "mean_grad_res: 0.000091\n",
      "mean_grad_bcs: 1.522042\n",
      "It: 18600, Loss: 1.863e-05, Loss_bcs: 7.370e-06, Loss_res: 1.127e-05,Time: 0.01\n",
      "mean_grad_res: 0.000079\n",
      "mean_grad_bcs: 0.011496\n",
      "It: 18700, Loss: 1.787e-05, Loss_bcs: 7.477e-06, Loss_res: 1.040e-05,Time: 0.00\n",
      "mean_grad_res: 0.000085\n",
      "mean_grad_bcs: 0.014216\n",
      "It: 18800, Loss: 2.478e-03, Loss_bcs: 2.465e-03, Loss_res: 1.288e-05,Time: 0.00\n",
      "mean_grad_res: 0.000088\n",
      "mean_grad_bcs: 0.328142\n",
      "It: 18900, Loss: 3.424e-05, Loss_bcs: 2.473e-05, Loss_res: 9.511e-06,Time: 0.00\n",
      "mean_grad_res: 0.000087\n",
      "mean_grad_bcs: 0.027485\n",
      "It: 19000, Loss: 1.052e-03, Loss_bcs: 1.042e-03, Loss_res: 1.044e-05,Time: 0.00\n",
      "mean_grad_res: 0.000079\n",
      "mean_grad_bcs: 0.185856\n",
      "It: 19100, Loss: 3.340e-05, Loss_bcs: 2.322e-05, Loss_res: 1.019e-05,Time: 0.01\n",
      "mean_grad_res: 0.000085\n",
      "mean_grad_bcs: 0.031599\n",
      "It: 19200, Loss: 3.989e-05, Loss_bcs: 2.979e-05, Loss_res: 1.011e-05,Time: 0.00\n",
      "mean_grad_res: 0.000079\n",
      "mean_grad_bcs: 0.032375\n",
      "It: 19300, Loss: 1.843e-04, Loss_bcs: 1.734e-04, Loss_res: 1.091e-05,Time: 0.00\n",
      "mean_grad_res: 0.000078\n",
      "mean_grad_bcs: 0.086052\n",
      "It: 19400, Loss: 7.678e-05, Loss_bcs: 6.642e-05, Loss_res: 1.036e-05,Time: 0.01\n",
      "mean_grad_res: 0.000080\n",
      "mean_grad_bcs: 0.045569\n",
      "It: 19500, Loss: 1.788e-05, Loss_bcs: 7.342e-06, Loss_res: 1.054e-05,Time: 0.01\n",
      "mean_grad_res: 0.000080\n",
      "mean_grad_bcs: 0.012825\n",
      "It: 19600, Loss: 1.725e-03, Loss_bcs: 1.713e-03, Loss_res: 1.210e-05,Time: 0.01\n",
      "mean_grad_res: 0.000091\n",
      "mean_grad_bcs: 0.281103\n",
      "It: 19700, Loss: 1.951e-05, Loss_bcs: 6.870e-06, Loss_res: 1.264e-05,Time: 0.00\n",
      "mean_grad_res: 0.000097\n",
      "mean_grad_bcs: 0.012721\n",
      "It: 19800, Loss: 2.946e-05, Loss_bcs: 1.631e-05, Loss_res: 1.315e-05,Time: 0.01\n",
      "mean_grad_res: 0.000091\n",
      "mean_grad_bcs: 0.024752\n",
      "It: 19900, Loss: 9.684e-05, Loss_bcs: 8.409e-05, Loss_res: 1.275e-05,Time: 0.01\n",
      "mean_grad_res: 0.000104\n",
      "mean_grad_bcs: 0.049721\n",
      "It: 20000, Loss: 8.836e-04, Loss_bcs: 8.763e-04, Loss_res: 7.229e-06,Time: 0.01\n",
      "mean_grad_res: 0.000056\n",
      "mean_grad_bcs: 0.181506\n",
      "It: 20100, Loss: 1.836e-05, Loss_bcs: 8.115e-06, Loss_res: 1.024e-05,Time: 0.00\n",
      "mean_grad_res: 0.000082\n",
      "mean_grad_bcs: 0.006498\n",
      "It: 20200, Loss: 8.008e-05, Loss_bcs: 6.903e-05, Loss_res: 1.105e-05,Time: 0.00\n",
      "mean_grad_res: 0.000079\n",
      "mean_grad_bcs: 0.046288\n",
      "It: 20300, Loss: 9.253e-05, Loss_bcs: 8.327e-05, Loss_res: 9.257e-06,Time: 0.01\n",
      "mean_grad_res: 0.000080\n",
      "mean_grad_bcs: 0.059551\n",
      "It: 20400, Loss: 6.316e-04, Loss_bcs: 6.226e-04, Loss_res: 8.997e-06,Time: 0.00\n",
      "mean_grad_res: 0.000068\n",
      "mean_grad_bcs: 0.143359\n",
      "It: 20500, Loss: 6.516e-05, Loss_bcs: 5.485e-05, Loss_res: 1.031e-05,Time: 0.01\n",
      "mean_grad_res: 0.000079\n",
      "mean_grad_bcs: 0.041892\n",
      "It: 20600, Loss: 1.407e-03, Loss_bcs: 1.395e-03, Loss_res: 1.174e-05,Time: 0.00\n",
      "mean_grad_res: 0.000088\n",
      "mean_grad_bcs: 0.243803\n",
      "It: 20700, Loss: 5.779e-05, Loss_bcs: 4.767e-05, Loss_res: 1.012e-05,Time: 0.01\n",
      "mean_grad_res: 0.000075\n",
      "mean_grad_bcs: 0.042526\n",
      "It: 20800, Loss: 1.382e-04, Loss_bcs: 1.284e-04, Loss_res: 9.807e-06,Time: 0.01\n",
      "mean_grad_res: 0.000075\n",
      "mean_grad_bcs: 0.072993\n",
      "It: 20900, Loss: 1.687e-04, Loss_bcs: 1.582e-04, Loss_res: 1.049e-05,Time: 0.00\n",
      "mean_grad_res: 0.000080\n",
      "mean_grad_bcs: 0.085222\n",
      "It: 21000, Loss: 2.877e-05, Loss_bcs: 1.857e-05, Loss_res: 1.020e-05,Time: 0.00\n",
      "mean_grad_res: 0.000076\n",
      "mean_grad_bcs: 0.021700\n",
      "It: 21100, Loss: 1.990e-04, Loss_bcs: 1.887e-04, Loss_res: 1.031e-05,Time: 0.01\n",
      "mean_grad_res: 0.000074\n",
      "mean_grad_bcs: 0.080171\n",
      "It: 21200, Loss: 1.355e-05, Loss_bcs: 2.518e-06, Loss_res: 1.103e-05,Time: 0.00\n",
      "mean_grad_res: 0.000102\n",
      "mean_grad_bcs: 0.000537\n",
      "It: 21300, Loss: 5.134e-04, Loss_bcs: 5.029e-04, Loss_res: 1.049e-05,Time: 0.00\n",
      "mean_grad_res: 0.000080\n",
      "mean_grad_bcs: 0.151124\n",
      "It: 21400, Loss: 1.305e-04, Loss_bcs: 1.202e-04, Loss_res: 1.030e-05,Time: 0.00\n",
      "mean_grad_res: 0.000082\n",
      "mean_grad_bcs: 0.048359\n",
      "It: 21500, Loss: 1.701e-05, Loss_bcs: 5.835e-06, Loss_res: 1.118e-05,Time: 0.00\n",
      "mean_grad_res: 0.000090\n",
      "mean_grad_bcs: 0.010291\n",
      "It: 21600, Loss: 8.549e-05, Loss_bcs: 7.584e-05, Loss_res: 9.653e-06,Time: 0.00\n",
      "mean_grad_res: 0.000071\n",
      "mean_grad_bcs: 0.058916\n",
      "It: 21700, Loss: 2.929e-05, Loss_bcs: 1.779e-05, Loss_res: 1.150e-05,Time: 0.01\n",
      "mean_grad_res: 0.000092\n",
      "mean_grad_bcs: 0.025142\n",
      "It: 21800, Loss: 6.546e-04, Loss_bcs: 6.465e-04, Loss_res: 8.172e-06,Time: 0.01\n",
      "mean_grad_res: 0.000075\n",
      "mean_grad_bcs: 0.147219\n",
      "It: 21900, Loss: 2.136e-05, Loss_bcs: 1.372e-05, Loss_res: 7.636e-06,Time: 0.00\n",
      "mean_grad_res: 0.000064\n",
      "mean_grad_bcs: 0.022750\n",
      "It: 22000, Loss: 1.378e-04, Loss_bcs: 1.288e-04, Loss_res: 9.020e-06,Time: 0.01\n",
      "mean_grad_res: 0.000073\n",
      "mean_grad_bcs: 0.073422\n",
      "It: 22100, Loss: 1.795e-04, Loss_bcs: 1.697e-04, Loss_res: 9.876e-06,Time: 0.00\n",
      "mean_grad_res: 0.000086\n",
      "mean_grad_bcs: 0.075388\n",
      "It: 22200, Loss: 4.450e-05, Loss_bcs: 3.256e-05, Loss_res: 1.194e-05,Time: 0.00\n",
      "mean_grad_res: 0.000106\n",
      "mean_grad_bcs: 0.037475\n",
      "It: 22300, Loss: 2.599e-04, Loss_bcs: 2.503e-04, Loss_res: 9.636e-06,Time: 0.00\n",
      "mean_grad_res: 0.000084\n",
      "mean_grad_bcs: 0.111580\n",
      "It: 22400, Loss: 2.601e-05, Loss_bcs: 1.606e-05, Loss_res: 9.943e-06,Time: 0.00\n",
      "mean_grad_res: 0.000080\n",
      "mean_grad_bcs: 0.022094\n",
      "It: 22500, Loss: 1.784e-05, Loss_bcs: 8.871e-06, Loss_res: 8.967e-06,Time: 0.00\n",
      "mean_grad_res: 0.000082\n",
      "mean_grad_bcs: 0.012390\n",
      "It: 22600, Loss: 7.322e-05, Loss_bcs: 6.483e-05, Loss_res: 8.388e-06,Time: 0.01\n",
      "mean_grad_res: 0.000064\n",
      "mean_grad_bcs: 0.044347\n",
      "It: 22700, Loss: 3.987e-05, Loss_bcs: 3.115e-05, Loss_res: 8.726e-06,Time: 0.00\n",
      "mean_grad_res: 0.000069\n",
      "mean_grad_bcs: 0.025025\n",
      "It: 22800, Loss: 3.240e-05, Loss_bcs: 2.405e-05, Loss_res: 8.356e-06,Time: 0.00\n",
      "mean_grad_res: 0.000077\n",
      "mean_grad_bcs: 0.031074\n",
      "It: 22900, Loss: 3.497e-05, Loss_bcs: 2.716e-05, Loss_res: 7.815e-06,Time: 0.00\n",
      "mean_grad_res: 0.000065\n",
      "mean_grad_bcs: 0.032209\n",
      "It: 23000, Loss: 1.926e-05, Loss_bcs: 1.160e-05, Loss_res: 7.654e-06,Time: 0.01\n",
      "mean_grad_res: 0.000062\n",
      "mean_grad_bcs: 0.021237\n",
      "It: 23100, Loss: 1.399e-05, Loss_bcs: 4.633e-06, Loss_res: 9.359e-06,Time: 0.00\n",
      "mean_grad_res: 0.000074\n",
      "mean_grad_bcs: 0.008819\n",
      "It: 23200, Loss: 9.072e-05, Loss_bcs: 8.158e-05, Loss_res: 9.140e-06,Time: 0.00\n",
      "mean_grad_res: 0.000073\n",
      "mean_grad_bcs: 0.061621\n",
      "It: 23300, Loss: 2.081e-05, Loss_bcs: 1.245e-05, Loss_res: 8.355e-06,Time: 0.00\n",
      "mean_grad_res: 0.000073\n",
      "mean_grad_bcs: 0.018216\n",
      "It: 23400, Loss: 1.827e-05, Loss_bcs: 1.092e-05, Loss_res: 7.349e-06,Time: 0.00\n",
      "mean_grad_res: 0.000060\n",
      "mean_grad_bcs: 0.017186\n",
      "It: 23500, Loss: 1.325e-05, Loss_bcs: 3.976e-06, Loss_res: 9.278e-06,Time: 0.01\n",
      "mean_grad_res: 0.000083\n",
      "mean_grad_bcs: 0.008366\n",
      "It: 23600, Loss: 2.562e-05, Loss_bcs: 1.744e-05, Loss_res: 8.183e-06,Time: 0.00\n",
      "mean_grad_res: 0.000066\n",
      "mean_grad_bcs: 0.024583\n",
      "It: 23700, Loss: 5.941e-04, Loss_bcs: 5.855e-04, Loss_res: 8.622e-06,Time: 0.00\n",
      "mean_grad_res: 0.000071\n",
      "mean_grad_bcs: 0.169959\n",
      "It: 23800, Loss: 1.368e-05, Loss_bcs: 5.813e-06, Loss_res: 7.870e-06,Time: 0.01\n",
      "mean_grad_res: 0.000076\n",
      "mean_grad_bcs: 0.011367\n",
      "It: 23900, Loss: 1.053e-03, Loss_bcs: 1.042e-03, Loss_res: 1.154e-05,Time: 0.01\n",
      "mean_grad_res: 0.000098\n",
      "mean_grad_bcs: 0.232557\n",
      "It: 24000, Loss: 1.373e-05, Loss_bcs: 4.813e-06, Loss_res: 8.921e-06,Time: 0.00\n",
      "mean_grad_res: 0.000084\n",
      "mean_grad_bcs: 0.010399\n",
      "It: 24100, Loss: 1.476e-05, Loss_bcs: 5.895e-06, Loss_res: 8.867e-06,Time: 0.00\n",
      "mean_grad_res: 0.000073\n",
      "mean_grad_bcs: 0.013635\n",
      "It: 24200, Loss: 1.044e-05, Loss_bcs: 3.451e-06, Loss_res: 6.986e-06,Time: 0.00\n",
      "mean_grad_res: 0.000056\n",
      "mean_grad_bcs: 0.005534\n",
      "It: 24300, Loss: 1.712e-04, Loss_bcs: 1.636e-04, Loss_res: 7.675e-06,Time: 0.00\n",
      "mean_grad_res: 0.000064\n",
      "mean_grad_bcs: 0.071629\n",
      "It: 24400, Loss: 4.456e-05, Loss_bcs: 3.633e-05, Loss_res: 8.232e-06,Time: 0.00\n",
      "mean_grad_res: 0.000077\n",
      "mean_grad_bcs: 0.040462\n",
      "It: 24500, Loss: 1.073e-05, Loss_bcs: 3.976e-06, Loss_res: 6.750e-06,Time: 0.01\n",
      "mean_grad_res: 0.000064\n",
      "mean_grad_bcs: 0.003778\n",
      "It: 24600, Loss: 1.563e-05, Loss_bcs: 8.461e-06, Loss_res: 7.171e-06,Time: 0.00\n",
      "mean_grad_res: 0.000064\n",
      "mean_grad_bcs: 0.014406\n",
      "It: 24700, Loss: 6.731e-05, Loss_bcs: 5.978e-05, Loss_res: 7.533e-06,Time: 0.01\n",
      "mean_grad_res: 0.000072\n",
      "mean_grad_bcs: 0.045683\n",
      "It: 24800, Loss: 1.794e-05, Loss_bcs: 9.859e-06, Loss_res: 8.084e-06,Time: 0.00\n",
      "mean_grad_res: 0.000065\n",
      "mean_grad_bcs: 0.017227\n",
      "It: 24900, Loss: 1.828e-05, Loss_bcs: 1.034e-05, Loss_res: 7.932e-06,Time: 0.00\n",
      "mean_grad_res: 0.000068\n",
      "mean_grad_bcs: 0.019574\n",
      "It: 25000, Loss: 1.342e-05, Loss_bcs: 3.734e-06, Loss_res: 9.688e-06,Time: 0.00\n",
      "mean_grad_res: 0.000084\n",
      "mean_grad_bcs: 0.008869\n",
      "It: 25100, Loss: 1.478e-05, Loss_bcs: 7.032e-06, Loss_res: 7.750e-06,Time: 0.00\n",
      "mean_grad_res: 0.000071\n",
      "mean_grad_bcs: 0.015832\n",
      "It: 25200, Loss: 2.317e-04, Loss_bcs: 2.258e-04, Loss_res: 5.942e-06,Time: 0.00\n",
      "mean_grad_res: 0.000062\n",
      "mean_grad_bcs: 0.091886\n",
      "It: 25300, Loss: 2.418e-05, Loss_bcs: 1.452e-05, Loss_res: 9.661e-06,Time: 0.01\n",
      "mean_grad_res: 0.000083\n",
      "mean_grad_bcs: 0.021608\n",
      "It: 25400, Loss: 3.549e-05, Loss_bcs: 2.800e-05, Loss_res: 7.493e-06,Time: 0.00\n",
      "mean_grad_res: 0.000066\n",
      "mean_grad_bcs: 0.033336\n",
      "It: 25500, Loss: 4.242e-04, Loss_bcs: 4.166e-04, Loss_res: 7.607e-06,Time: 0.00\n",
      "mean_grad_res: 0.000074\n",
      "mean_grad_bcs: 0.136397\n",
      "It: 25600, Loss: 2.978e-05, Loss_bcs: 2.069e-05, Loss_res: 9.095e-06,Time: 0.00\n",
      "mean_grad_res: 0.000071\n",
      "mean_grad_bcs: 0.024764\n",
      "It: 25700, Loss: 2.097e-05, Loss_bcs: 1.289e-05, Loss_res: 8.080e-06,Time: 0.03\n",
      "mean_grad_res: 0.000074\n",
      "mean_grad_bcs: 0.021992\n",
      "It: 25800, Loss: 2.606e-05, Loss_bcs: 1.847e-05, Loss_res: 7.586e-06,Time: 0.01\n",
      "mean_grad_res: 0.000060\n",
      "mean_grad_bcs: 0.019635\n",
      "It: 25900, Loss: 1.512e-05, Loss_bcs: 8.774e-06, Loss_res: 6.350e-06,Time: 0.00\n",
      "mean_grad_res: 0.000063\n",
      "mean_grad_bcs: 0.018720\n",
      "It: 26000, Loss: 3.369e-05, Loss_bcs: 2.550e-05, Loss_res: 8.190e-06,Time: 0.01\n",
      "mean_grad_res: 0.000070\n",
      "mean_grad_bcs: 0.030009\n",
      "It: 26100, Loss: 2.354e-05, Loss_bcs: 1.584e-05, Loss_res: 7.707e-06,Time: 0.00\n",
      "mean_grad_res: 0.000067\n",
      "mean_grad_bcs: 0.021375\n",
      "It: 26200, Loss: 1.624e-05, Loss_bcs: 8.266e-06, Loss_res: 7.976e-06,Time: 0.00\n",
      "mean_grad_res: 0.000072\n",
      "mean_grad_bcs: 0.014393\n",
      "It: 26300, Loss: 1.514e-04, Loss_bcs: 1.434e-04, Loss_res: 8.036e-06,Time: 0.00\n",
      "mean_grad_res: 0.000062\n",
      "mean_grad_bcs: 0.068636\n",
      "It: 26400, Loss: 1.894e-05, Loss_bcs: 1.168e-05, Loss_res: 7.265e-06,Time: 0.00\n",
      "mean_grad_res: 0.000076\n",
      "mean_grad_bcs: 0.019183\n",
      "It: 26500, Loss: 1.271e-05, Loss_bcs: 4.136e-06, Loss_res: 8.569e-06,Time: 0.00\n",
      "mean_grad_res: 0.000083\n",
      "mean_grad_bcs: 0.009074\n",
      "It: 26600, Loss: 2.998e-05, Loss_bcs: 2.265e-05, Loss_res: 7.330e-06,Time: 0.00\n",
      "mean_grad_res: 0.000064\n",
      "mean_grad_bcs: 0.025744\n",
      "It: 26700, Loss: 1.971e-05, Loss_bcs: 1.244e-05, Loss_res: 7.269e-06,Time: 0.01\n",
      "mean_grad_res: 0.000065\n",
      "mean_grad_bcs: 0.021974\n",
      "It: 26800, Loss: 4.615e-05, Loss_bcs: 3.878e-05, Loss_res: 7.369e-06,Time: 0.01\n",
      "mean_grad_res: 0.000063\n",
      "mean_grad_bcs: 0.020982\n",
      "It: 26900, Loss: 1.048e-05, Loss_bcs: 2.742e-06, Loss_res: 7.737e-06,Time: 0.00\n",
      "mean_grad_res: 0.000069\n",
      "mean_grad_bcs: 0.006689\n",
      "It: 27000, Loss: 9.017e-06, Loss_bcs: 2.508e-06, Loss_res: 6.509e-06,Time: 0.00\n",
      "mean_grad_res: 0.000065\n",
      "mean_grad_bcs: 0.005466\n",
      "It: 27100, Loss: 1.741e-05, Loss_bcs: 9.567e-06, Loss_res: 7.844e-06,Time: 0.01\n",
      "mean_grad_res: 0.000064\n",
      "mean_grad_bcs: 0.015765\n",
      "It: 27200, Loss: 4.333e-05, Loss_bcs: 3.649e-05, Loss_res: 6.836e-06,Time: 0.00\n",
      "mean_grad_res: 0.000058\n",
      "mean_grad_bcs: 0.034572\n",
      "It: 27300, Loss: 1.485e-05, Loss_bcs: 8.526e-06, Loss_res: 6.320e-06,Time: 0.00\n",
      "mean_grad_res: 0.000062\n",
      "mean_grad_bcs: 0.016581\n",
      "It: 27400, Loss: 1.383e-04, Loss_bcs: 1.323e-04, Loss_res: 5.960e-06,Time: 0.00\n",
      "mean_grad_res: 0.000050\n",
      "mean_grad_bcs: 0.074398\n",
      "It: 27500, Loss: 9.106e-06, Loss_bcs: 2.568e-06, Loss_res: 6.538e-06,Time: 0.01\n",
      "mean_grad_res: 0.000050\n",
      "mean_grad_bcs: 0.005708\n",
      "It: 27600, Loss: 2.386e-05, Loss_bcs: 1.812e-05, Loss_res: 5.741e-06,Time: 0.00\n",
      "mean_grad_res: 0.000066\n",
      "mean_grad_bcs: 0.026896\n",
      "It: 27700, Loss: 4.134e-05, Loss_bcs: 3.471e-05, Loss_res: 6.627e-06,Time: 0.01\n",
      "mean_grad_res: 0.000056\n",
      "mean_grad_bcs: 0.038324\n",
      "It: 27800, Loss: 9.101e-06, Loss_bcs: 2.938e-06, Loss_res: 6.162e-06,Time: 0.01\n",
      "mean_grad_res: 0.000061\n",
      "mean_grad_bcs: 0.006730\n",
      "It: 27900, Loss: 1.010e-05, Loss_bcs: 3.951e-06, Loss_res: 6.152e-06,Time: 0.01\n",
      "mean_grad_res: 0.000059\n",
      "mean_grad_bcs: 0.010015\n",
      "It: 28000, Loss: 4.981e-05, Loss_bcs: 4.354e-05, Loss_res: 6.266e-06,Time: 0.01\n",
      "mean_grad_res: 0.000053\n",
      "mean_grad_bcs: 0.033107\n",
      "It: 28100, Loss: 4.744e-05, Loss_bcs: 4.101e-05, Loss_res: 6.426e-06,Time: 0.01\n",
      "mean_grad_res: 0.000058\n",
      "mean_grad_bcs: 0.040843\n",
      "It: 28200, Loss: 8.994e-05, Loss_bcs: 8.240e-05, Loss_res: 7.532e-06,Time: 0.01\n",
      "mean_grad_res: 0.000065\n",
      "mean_grad_bcs: 0.061961\n",
      "It: 28300, Loss: 1.025e-04, Loss_bcs: 9.487e-05, Loss_res: 7.639e-06,Time: 0.00\n",
      "mean_grad_res: 0.000075\n",
      "mean_grad_bcs: 0.059094\n",
      "It: 28400, Loss: 1.319e-05, Loss_bcs: 6.982e-06, Loss_res: 6.212e-06,Time: 0.00\n",
      "mean_grad_res: 0.000058\n",
      "mean_grad_bcs: 0.015496\n",
      "It: 28500, Loss: 1.085e-05, Loss_bcs: 4.328e-06, Loss_res: 6.519e-06,Time: 0.00\n",
      "mean_grad_res: 0.000058\n",
      "mean_grad_bcs: 0.011466\n",
      "It: 28600, Loss: 1.317e-04, Loss_bcs: 1.250e-04, Loss_res: 6.773e-06,Time: 0.01\n",
      "mean_grad_res: 0.000073\n",
      "mean_grad_bcs: 0.062746\n",
      "It: 28700, Loss: 9.442e-06, Loss_bcs: 3.993e-06, Loss_res: 5.449e-06,Time: 0.00\n",
      "mean_grad_res: 0.000055\n",
      "mean_grad_bcs: 0.011135\n",
      "It: 28800, Loss: 1.432e-05, Loss_bcs: 6.515e-06, Loss_res: 7.801e-06,Time: 0.01\n",
      "mean_grad_res: 0.000061\n",
      "mean_grad_bcs: 0.013954\n",
      "It: 28900, Loss: 3.599e-05, Loss_bcs: 2.971e-05, Loss_res: 6.287e-06,Time: 0.00\n",
      "mean_grad_res: 0.000060\n",
      "mean_grad_bcs: 0.033107\n",
      "It: 29000, Loss: 3.026e-04, Loss_bcs: 2.969e-04, Loss_res: 5.724e-06,Time: 0.00\n",
      "mean_grad_res: 0.000059\n",
      "mean_grad_bcs: 0.109473\n",
      "It: 29100, Loss: 1.060e-05, Loss_bcs: 3.827e-06, Loss_res: 6.773e-06,Time: 0.00\n",
      "mean_grad_res: 0.000077\n",
      "mean_grad_bcs: 0.008957\n",
      "It: 29200, Loss: 2.243e-05, Loss_bcs: 1.708e-05, Loss_res: 5.343e-06,Time: 0.00\n",
      "mean_grad_res: 0.000054\n",
      "mean_grad_bcs: 0.021973\n",
      "It: 29300, Loss: 3.124e-05, Loss_bcs: 2.447e-05, Loss_res: 6.772e-06,Time: 0.01\n",
      "mean_grad_res: 0.000058\n",
      "mean_grad_bcs: 0.027122\n",
      "It: 29400, Loss: 1.967e-04, Loss_bcs: 1.904e-04, Loss_res: 6.365e-06,Time: 0.00\n",
      "mean_grad_res: 0.000056\n",
      "mean_grad_bcs: 0.097252\n",
      "It: 29500, Loss: 8.864e-06, Loss_bcs: 2.767e-06, Loss_res: 6.097e-06,Time: 0.01\n",
      "mean_grad_res: 0.000062\n",
      "mean_grad_bcs: 0.003839\n",
      "It: 29600, Loss: 9.017e-05, Loss_bcs: 8.440e-05, Loss_res: 5.770e-06,Time: 0.00\n",
      "mean_grad_res: 0.000057\n",
      "mean_grad_bcs: 0.056659\n",
      "It: 29700, Loss: 7.753e-06, Loss_bcs: 1.913e-06, Loss_res: 5.839e-06,Time: 0.00\n",
      "mean_grad_res: 0.000061\n",
      "mean_grad_bcs: 0.003783\n",
      "It: 29800, Loss: 4.246e-05, Loss_bcs: 3.696e-05, Loss_res: 5.499e-06,Time: 0.01\n",
      "mean_grad_res: 0.000058\n",
      "mean_grad_bcs: 0.040794\n",
      "It: 29900, Loss: 2.684e-05, Loss_bcs: 1.881e-05, Loss_res: 8.030e-06,Time: 0.00\n",
      "mean_grad_res: 0.000081\n",
      "mean_grad_bcs: 0.025464\n",
      "It: 30000, Loss: 2.709e-05, Loss_bcs: 2.052e-05, Loss_res: 6.568e-06,Time: 0.00\n",
      "mean_grad_res: 0.000070\n",
      "mean_grad_bcs: 0.031205\n",
      "It: 30100, Loss: 2.637e-05, Loss_bcs: 2.013e-05, Loss_res: 6.242e-06,Time: 0.01\n",
      "mean_grad_res: 0.000054\n",
      "mean_grad_bcs: 0.029222\n",
      "It: 30200, Loss: 9.565e-06, Loss_bcs: 3.665e-06, Loss_res: 5.900e-06,Time: 0.00\n",
      "mean_grad_res: 0.000069\n",
      "mean_grad_bcs: 0.010200\n",
      "It: 30300, Loss: 1.609e-05, Loss_bcs: 9.417e-06, Loss_res: 6.672e-06,Time: 0.00\n",
      "mean_grad_res: 0.000065\n",
      "mean_grad_bcs: 0.016396\n",
      "It: 30400, Loss: 3.961e-05, Loss_bcs: 3.331e-05, Loss_res: 6.305e-06,Time: 0.00\n",
      "mean_grad_res: 0.000055\n",
      "mean_grad_bcs: 0.039583\n",
      "It: 30500, Loss: 4.972e-05, Loss_bcs: 4.420e-05, Loss_res: 5.522e-06,Time: 0.00\n",
      "mean_grad_res: 0.000064\n",
      "mean_grad_bcs: 0.036207\n",
      "It: 30600, Loss: 8.359e-06, Loss_bcs: 2.506e-06, Loss_res: 5.853e-06,Time: 0.00\n",
      "mean_grad_res: 0.000049\n",
      "mean_grad_bcs: 0.003850\n",
      "It: 30700, Loss: 3.284e-05, Loss_bcs: 2.747e-05, Loss_res: 5.363e-06,Time: 0.00\n",
      "mean_grad_res: 0.000060\n",
      "mean_grad_bcs: 0.035007\n",
      "It: 30800, Loss: 1.047e-05, Loss_bcs: 4.820e-06, Loss_res: 5.650e-06,Time: 0.00\n",
      "mean_grad_res: 0.000056\n",
      "mean_grad_bcs: 0.010505\n",
      "It: 30900, Loss: 6.916e-06, Loss_bcs: 1.499e-06, Loss_res: 5.417e-06,Time: 0.00\n",
      "mean_grad_res: 0.000052\n",
      "mean_grad_bcs: 0.003030\n",
      "It: 31000, Loss: 3.616e-05, Loss_bcs: 2.908e-05, Loss_res: 7.077e-06,Time: 0.01\n",
      "mean_grad_res: 0.000073\n",
      "mean_grad_bcs: 0.029140\n",
      "It: 31100, Loss: 1.024e-05, Loss_bcs: 3.731e-06, Loss_res: 6.514e-06,Time: 0.00\n",
      "mean_grad_res: 0.000053\n",
      "mean_grad_bcs: 0.008523\n",
      "It: 31200, Loss: 9.603e-05, Loss_bcs: 8.934e-05, Loss_res: 6.689e-06,Time: 0.01\n",
      "mean_grad_res: 0.000061\n",
      "mean_grad_bcs: 0.065036\n",
      "It: 31300, Loss: 8.832e-06, Loss_bcs: 3.880e-06, Loss_res: 4.953e-06,Time: 0.01\n",
      "mean_grad_res: 0.000049\n",
      "mean_grad_bcs: 0.008234\n",
      "It: 31400, Loss: 3.479e-05, Loss_bcs: 2.867e-05, Loss_res: 6.124e-06,Time: 0.00\n",
      "mean_grad_res: 0.000060\n",
      "mean_grad_bcs: 0.034897\n",
      "It: 31500, Loss: 8.172e-05, Loss_bcs: 7.515e-05, Loss_res: 6.578e-06,Time: 0.01\n",
      "mean_grad_res: 0.000061\n",
      "mean_grad_bcs: 0.055965\n",
      "It: 31600, Loss: 1.321e-05, Loss_bcs: 6.424e-06, Loss_res: 6.789e-06,Time: 0.01\n",
      "mean_grad_res: 0.000067\n",
      "mean_grad_bcs: 0.016001\n",
      "It: 31700, Loss: 4.705e-05, Loss_bcs: 4.259e-05, Loss_res: 4.460e-06,Time: 0.01\n",
      "mean_grad_res: 0.000048\n",
      "mean_grad_bcs: 0.045248\n",
      "It: 31800, Loss: 7.650e-05, Loss_bcs: 7.090e-05, Loss_res: 5.593e-06,Time: 0.00\n",
      "mean_grad_res: 0.000054\n",
      "mean_grad_bcs: 0.052904\n",
      "It: 31900, Loss: 2.418e-05, Loss_bcs: 1.845e-05, Loss_res: 5.727e-06,Time: 0.00\n",
      "mean_grad_res: 0.000056\n",
      "mean_grad_bcs: 0.028110\n",
      "It: 32000, Loss: 1.223e-05, Loss_bcs: 7.355e-06, Loss_res: 4.871e-06,Time: 0.01\n",
      "mean_grad_res: 0.000046\n",
      "mean_grad_bcs: 0.014469\n",
      "It: 32100, Loss: 8.508e-06, Loss_bcs: 2.671e-06, Loss_res: 5.837e-06,Time: 0.00\n",
      "mean_grad_res: 0.000055\n",
      "mean_grad_bcs: 0.008379\n",
      "It: 32200, Loss: 7.015e-06, Loss_bcs: 1.996e-06, Loss_res: 5.019e-06,Time: 0.00\n",
      "mean_grad_res: 0.000046\n",
      "mean_grad_bcs: 0.004489\n",
      "It: 32300, Loss: 1.128e-05, Loss_bcs: 5.212e-06, Loss_res: 6.070e-06,Time: 0.01\n",
      "mean_grad_res: 0.000058\n",
      "mean_grad_bcs: 0.014247\n",
      "It: 32400, Loss: 4.681e-05, Loss_bcs: 4.187e-05, Loss_res: 4.944e-06,Time: 0.00\n",
      "mean_grad_res: 0.000049\n",
      "mean_grad_bcs: 0.042428\n",
      "It: 32500, Loss: 8.248e-06, Loss_bcs: 3.644e-06, Loss_res: 4.604e-06,Time: 0.00\n",
      "mean_grad_res: 0.000054\n",
      "mean_grad_bcs: 0.010614\n",
      "It: 32600, Loss: 8.574e-06, Loss_bcs: 3.455e-06, Loss_res: 5.119e-06,Time: 0.00\n",
      "mean_grad_res: 0.000059\n",
      "mean_grad_bcs: 0.010436\n",
      "It: 32700, Loss: 1.859e-05, Loss_bcs: 1.375e-05, Loss_res: 4.846e-06,Time: 0.00\n",
      "mean_grad_res: 0.000059\n",
      "mean_grad_bcs: 0.023443\n",
      "It: 32800, Loss: 7.531e-06, Loss_bcs: 2.135e-06, Loss_res: 5.396e-06,Time: 0.00\n",
      "mean_grad_res: 0.000059\n",
      "mean_grad_bcs: 0.006309\n",
      "It: 32900, Loss: 6.411e-06, Loss_bcs: 1.634e-06, Loss_res: 4.777e-06,Time: 0.00\n",
      "mean_grad_res: 0.000058\n",
      "mean_grad_bcs: 0.000580\n",
      "It: 33000, Loss: 1.837e-05, Loss_bcs: 1.312e-05, Loss_res: 5.251e-06,Time: 0.00\n",
      "mean_grad_res: 0.000054\n",
      "mean_grad_bcs: 0.020391\n",
      "It: 33100, Loss: 2.194e-05, Loss_bcs: 1.681e-05, Loss_res: 5.129e-06,Time: 0.01\n",
      "mean_grad_res: 0.000047\n",
      "mean_grad_bcs: 0.023741\n",
      "It: 33200, Loss: 1.232e-05, Loss_bcs: 7.915e-06, Loss_res: 4.410e-06,Time: 0.00\n",
      "mean_grad_res: 0.000047\n",
      "mean_grad_bcs: 0.016797\n",
      "It: 33300, Loss: 1.025e-05, Loss_bcs: 5.173e-06, Loss_res: 5.074e-06,Time: 0.00\n",
      "mean_grad_res: 0.000043\n",
      "mean_grad_bcs: 0.013895\n",
      "It: 33400, Loss: 9.658e-06, Loss_bcs: 5.244e-06, Loss_res: 4.414e-06,Time: 0.00\n",
      "mean_grad_res: 0.000053\n",
      "mean_grad_bcs: 0.011075\n",
      "It: 33500, Loss: 1.722e-05, Loss_bcs: 1.160e-05, Loss_res: 5.619e-06,Time: 0.00\n",
      "mean_grad_res: 0.000062\n",
      "mean_grad_bcs: 0.018405\n",
      "It: 33600, Loss: 4.529e-05, Loss_bcs: 4.031e-05, Loss_res: 4.981e-06,Time: 0.00\n",
      "mean_grad_res: 0.000052\n",
      "mean_grad_bcs: 0.041258\n",
      "It: 33700, Loss: 1.396e-05, Loss_bcs: 9.110e-06, Loss_res: 4.846e-06,Time: 0.00\n",
      "mean_grad_res: 0.000057\n",
      "mean_grad_bcs: 0.017341\n",
      "It: 33800, Loss: 3.020e-05, Loss_bcs: 2.556e-05, Loss_res: 4.639e-06,Time: 0.00\n",
      "mean_grad_res: 0.000051\n",
      "mean_grad_bcs: 0.028933\n",
      "It: 33900, Loss: 6.350e-05, Loss_bcs: 5.907e-05, Loss_res: 4.428e-06,Time: 0.01\n",
      "mean_grad_res: 0.000048\n",
      "mean_grad_bcs: 0.052365\n",
      "It: 34000, Loss: 8.553e-06, Loss_bcs: 3.990e-06, Loss_res: 4.562e-06,Time: 0.00\n",
      "mean_grad_res: 0.000047\n",
      "mean_grad_bcs: 0.011205\n",
      "It: 34100, Loss: 8.579e-05, Loss_bcs: 8.074e-05, Loss_res: 5.052e-06,Time: 0.00\n",
      "mean_grad_res: 0.000046\n",
      "mean_grad_bcs: 0.060047\n",
      "It: 34200, Loss: 1.129e-05, Loss_bcs: 7.191e-06, Loss_res: 4.094e-06,Time: 0.00\n",
      "mean_grad_res: 0.000040\n",
      "mean_grad_bcs: 0.017817\n",
      "It: 34300, Loss: 1.833e-05, Loss_bcs: 1.373e-05, Loss_res: 4.600e-06,Time: 0.00\n",
      "mean_grad_res: 0.000049\n",
      "mean_grad_bcs: 0.024821\n",
      "It: 34400, Loss: 8.843e-06, Loss_bcs: 3.920e-06, Loss_res: 4.922e-06,Time: 0.00\n",
      "mean_grad_res: 0.000055\n",
      "mean_grad_bcs: 0.010649\n",
      "It: 34500, Loss: 1.484e-05, Loss_bcs: 1.024e-05, Loss_res: 4.608e-06,Time: 0.00\n",
      "mean_grad_res: 0.000041\n",
      "mean_grad_bcs: 0.020964\n",
      "It: 34600, Loss: 1.219e-05, Loss_bcs: 6.165e-06, Loss_res: 6.023e-06,Time: 0.00\n",
      "mean_grad_res: 0.000056\n",
      "mean_grad_bcs: 0.012015\n",
      "It: 34700, Loss: 6.430e-06, Loss_bcs: 1.214e-06, Loss_res: 5.216e-06,Time: 0.00\n",
      "mean_grad_res: 0.000058\n",
      "mean_grad_bcs: 0.002804\n",
      "It: 34800, Loss: 5.645e-05, Loss_bcs: 5.156e-05, Loss_res: 4.885e-06,Time: 0.00\n",
      "mean_grad_res: 0.000049\n",
      "mean_grad_bcs: 0.047053\n",
      "It: 34900, Loss: 1.674e-05, Loss_bcs: 1.236e-05, Loss_res: 4.379e-06,Time: 0.00\n",
      "mean_grad_res: 0.000044\n",
      "mean_grad_bcs: 0.019414\n",
      "It: 35000, Loss: 1.874e-05, Loss_bcs: 1.328e-05, Loss_res: 5.456e-06,Time: 0.00\n",
      "mean_grad_res: 0.000063\n",
      "mean_grad_bcs: 0.019848\n",
      "It: 35100, Loss: 1.138e-05, Loss_bcs: 5.737e-06, Loss_res: 5.640e-06,Time: 0.00\n",
      "mean_grad_res: 0.000056\n",
      "mean_grad_bcs: 0.013901\n",
      "It: 35200, Loss: 1.156e-05, Loss_bcs: 7.214e-06, Loss_res: 4.347e-06,Time: 0.00\n",
      "mean_grad_res: 0.000059\n",
      "mean_grad_bcs: 0.014870\n",
      "It: 35300, Loss: 6.837e-06, Loss_bcs: 1.436e-06, Loss_res: 5.401e-06,Time: 0.00\n",
      "mean_grad_res: 0.000048\n",
      "mean_grad_bcs: 0.003875\n",
      "It: 35400, Loss: 2.085e-05, Loss_bcs: 1.628e-05, Loss_res: 4.570e-06,Time: 0.00\n",
      "mean_grad_res: 0.000048\n",
      "mean_grad_bcs: 0.021094\n",
      "It: 35500, Loss: 1.282e-05, Loss_bcs: 8.229e-06, Loss_res: 4.591e-06,Time: 0.01\n",
      "mean_grad_res: 0.000055\n",
      "mean_grad_bcs: 0.018038\n",
      "It: 35600, Loss: 1.309e-05, Loss_bcs: 8.946e-06, Loss_res: 4.147e-06,Time: 0.00\n",
      "mean_grad_res: 0.000050\n",
      "mean_grad_bcs: 0.019969\n",
      "It: 35700, Loss: 9.162e-06, Loss_bcs: 4.461e-06, Loss_res: 4.701e-06,Time: 0.00\n",
      "mean_grad_res: 0.000046\n",
      "mean_grad_bcs: 0.011454\n",
      "It: 35800, Loss: 8.394e-06, Loss_bcs: 3.386e-06, Loss_res: 5.008e-06,Time: 0.00\n",
      "mean_grad_res: 0.000055\n",
      "mean_grad_bcs: 0.010876\n",
      "It: 35900, Loss: 8.266e-06, Loss_bcs: 3.099e-06, Loss_res: 5.167e-06,Time: 0.00\n",
      "mean_grad_res: 0.000045\n",
      "mean_grad_bcs: 0.009553\n",
      "It: 36000, Loss: 1.107e-05, Loss_bcs: 6.716e-06, Loss_res: 4.350e-06,Time: 0.01\n",
      "mean_grad_res: 0.000044\n",
      "mean_grad_bcs: 0.016346\n",
      "It: 36100, Loss: 3.339e-05, Loss_bcs: 2.881e-05, Loss_res: 4.576e-06,Time: 0.01\n",
      "mean_grad_res: 0.000045\n",
      "mean_grad_bcs: 0.033657\n",
      "It: 36200, Loss: 7.133e-06, Loss_bcs: 2.795e-06, Loss_res: 4.338e-06,Time: 0.01\n",
      "mean_grad_res: 0.000038\n",
      "mean_grad_bcs: 0.009355\n",
      "It: 36300, Loss: 7.315e-06, Loss_bcs: 2.541e-06, Loss_res: 4.774e-06,Time: 0.01\n",
      "mean_grad_res: 0.000056\n",
      "mean_grad_bcs: 0.006946\n",
      "It: 36400, Loss: 8.125e-06, Loss_bcs: 3.474e-06, Loss_res: 4.651e-06,Time: 0.01\n",
      "mean_grad_res: 0.000053\n",
      "mean_grad_bcs: 0.008951\n",
      "It: 36500, Loss: 1.174e-05, Loss_bcs: 7.435e-06, Loss_res: 4.307e-06,Time: 0.01\n",
      "mean_grad_res: 0.000055\n",
      "mean_grad_bcs: 0.014972\n",
      "It: 36600, Loss: 1.485e-04, Loss_bcs: 1.433e-04, Loss_res: 5.243e-06,Time: 0.01\n",
      "mean_grad_res: 0.000051\n",
      "mean_grad_bcs: 0.080958\n",
      "It: 36700, Loss: 6.818e-06, Loss_bcs: 2.090e-06, Loss_res: 4.728e-06,Time: 0.01\n",
      "mean_grad_res: 0.000041\n",
      "mean_grad_bcs: 0.006796\n",
      "It: 36800, Loss: 1.502e-05, Loss_bcs: 1.069e-05, Loss_res: 4.330e-06,Time: 0.00\n",
      "mean_grad_res: 0.000043\n",
      "mean_grad_bcs: 0.017972\n",
      "It: 36900, Loss: 1.043e-05, Loss_bcs: 5.661e-06, Loss_res: 4.768e-06,Time: 0.00\n",
      "mean_grad_res: 0.000046\n",
      "mean_grad_bcs: 0.012037\n",
      "It: 37000, Loss: 9.736e-06, Loss_bcs: 5.931e-06, Loss_res: 3.804e-06,Time: 0.00\n",
      "mean_grad_res: 0.000048\n",
      "mean_grad_bcs: 0.014497\n",
      "It: 37100, Loss: 1.027e-05, Loss_bcs: 6.005e-06, Loss_res: 4.270e-06,Time: 0.00\n",
      "mean_grad_res: 0.000041\n",
      "mean_grad_bcs: 0.015223\n",
      "It: 37200, Loss: 1.330e-05, Loss_bcs: 9.376e-06, Loss_res: 3.928e-06,Time: 0.00\n",
      "mean_grad_res: 0.000044\n",
      "mean_grad_bcs: 0.020325\n",
      "It: 37300, Loss: 2.383e-05, Loss_bcs: 1.996e-05, Loss_res: 3.869e-06,Time: 0.00\n",
      "mean_grad_res: 0.000043\n",
      "mean_grad_bcs: 0.022167\n",
      "It: 37400, Loss: 3.134e-05, Loss_bcs: 2.739e-05, Loss_res: 3.951e-06,Time: 0.00\n",
      "mean_grad_res: 0.000046\n",
      "mean_grad_bcs: 0.035633\n",
      "It: 37500, Loss: 4.966e-06, Loss_bcs: 1.102e-06, Loss_res: 3.864e-06,Time: 0.00\n",
      "mean_grad_res: 0.000040\n",
      "mean_grad_bcs: 0.003134\n",
      "It: 37600, Loss: 3.115e-05, Loss_bcs: 2.753e-05, Loss_res: 3.624e-06,Time: 0.00\n",
      "mean_grad_res: 0.000053\n",
      "mean_grad_bcs: 0.031907\n",
      "It: 37700, Loss: 5.757e-06, Loss_bcs: 1.446e-06, Loss_res: 4.311e-06,Time: 0.00\n",
      "mean_grad_res: 0.000044\n",
      "mean_grad_bcs: 0.003791\n",
      "It: 37800, Loss: 7.643e-06, Loss_bcs: 3.983e-06, Loss_res: 3.659e-06,Time: 0.01\n",
      "mean_grad_res: 0.000050\n",
      "mean_grad_bcs: 0.010153\n",
      "It: 37900, Loss: 1.197e-05, Loss_bcs: 8.052e-06, Loss_res: 3.913e-06,Time: 0.00\n",
      "mean_grad_res: 0.000039\n",
      "mean_grad_bcs: 0.018652\n",
      "It: 38000, Loss: 1.333e-05, Loss_bcs: 9.638e-06, Loss_res: 3.693e-06,Time: 0.00\n",
      "mean_grad_res: 0.000034\n",
      "mean_grad_bcs: 0.020634\n",
      "It: 38100, Loss: 3.862e-05, Loss_bcs: 3.454e-05, Loss_res: 4.074e-06,Time: 0.00\n",
      "mean_grad_res: 0.000044\n",
      "mean_grad_bcs: 0.038779\n",
      "It: 38200, Loss: 5.590e-06, Loss_bcs: 1.385e-06, Loss_res: 4.205e-06,Time: 0.00\n",
      "mean_grad_res: 0.000044\n",
      "mean_grad_bcs: 0.005165\n",
      "It: 38300, Loss: 6.989e-06, Loss_bcs: 3.231e-06, Loss_res: 3.759e-06,Time: 0.00\n",
      "mean_grad_res: 0.000049\n",
      "mean_grad_bcs: 0.010414\n",
      "It: 38400, Loss: 8.079e-06, Loss_bcs: 3.863e-06, Loss_res: 4.216e-06,Time: 0.00\n",
      "mean_grad_res: 0.000046\n",
      "mean_grad_bcs: 0.011984\n",
      "It: 38500, Loss: 1.305e-05, Loss_bcs: 9.492e-06, Loss_res: 3.562e-06,Time: 0.00\n",
      "mean_grad_res: 0.000047\n",
      "mean_grad_bcs: 0.017106\n",
      "It: 38600, Loss: 1.145e-05, Loss_bcs: 6.965e-06, Loss_res: 4.489e-06,Time: 0.00\n",
      "mean_grad_res: 0.000052\n",
      "mean_grad_bcs: 0.014233\n",
      "It: 38700, Loss: 8.097e-06, Loss_bcs: 3.667e-06, Loss_res: 4.430e-06,Time: 0.00\n",
      "mean_grad_res: 0.000046\n",
      "mean_grad_bcs: 0.011855\n",
      "It: 38800, Loss: 5.856e-06, Loss_bcs: 2.448e-06, Loss_res: 3.409e-06,Time: 0.01\n",
      "mean_grad_res: 0.000042\n",
      "mean_grad_bcs: 0.009322\n",
      "It: 38900, Loss: 3.111e-05, Loss_bcs: 2.732e-05, Loss_res: 3.785e-06,Time: 0.00\n",
      "mean_grad_res: 0.000047\n",
      "mean_grad_bcs: 0.029667\n",
      "It: 39000, Loss: 1.451e-05, Loss_bcs: 1.054e-05, Loss_res: 3.979e-06,Time: 0.00\n",
      "mean_grad_res: 0.000048\n",
      "mean_grad_bcs: 0.017530\n",
      "It: 39100, Loss: 7.329e-06, Loss_bcs: 3.289e-06, Loss_res: 4.040e-06,Time: 0.00\n",
      "mean_grad_res: 0.000036\n",
      "mean_grad_bcs: 0.008941\n",
      "It: 39200, Loss: 9.629e-06, Loss_bcs: 5.920e-06, Loss_res: 3.709e-06,Time: 0.00\n",
      "mean_grad_res: 0.000046\n",
      "mean_grad_bcs: 0.016483\n",
      "It: 39300, Loss: 2.167e-05, Loss_bcs: 1.744e-05, Loss_res: 4.232e-06,Time: 0.00\n",
      "mean_grad_res: 0.000038\n",
      "mean_grad_bcs: 0.028759\n",
      "It: 39400, Loss: 6.505e-06, Loss_bcs: 3.192e-06, Loss_res: 3.313e-06,Time: 0.00\n",
      "mean_grad_res: 0.000042\n",
      "mean_grad_bcs: 0.009781\n",
      "It: 39500, Loss: 2.269e-05, Loss_bcs: 1.883e-05, Loss_res: 3.859e-06,Time: 0.01\n",
      "mean_grad_res: 0.000043\n",
      "mean_grad_bcs: 0.028649\n",
      "It: 39600, Loss: 7.333e-06, Loss_bcs: 3.499e-06, Loss_res: 3.833e-06,Time: 0.00\n",
      "mean_grad_res: 0.000041\n",
      "mean_grad_bcs: 0.011608\n",
      "It: 39700, Loss: 1.906e-05, Loss_bcs: 1.500e-05, Loss_res: 4.059e-06,Time: 0.00\n",
      "mean_grad_res: 0.000044\n",
      "mean_grad_bcs: 0.027312\n",
      "It: 39800, Loss: 5.207e-06, Loss_bcs: 7.275e-07, Loss_res: 4.480e-06,Time: 0.00\n",
      "mean_grad_res: 0.000055\n",
      "mean_grad_bcs: 0.001581\n",
      "It: 39900, Loss: 5.574e-06, Loss_bcs: 2.281e-06, Loss_res: 3.293e-06,Time: 0.00\n",
      "mean_grad_res: 0.000033\n",
      "mean_grad_bcs: 0.007028\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000, Loss: 6.464e-06, Loss_bcs: 2.717e-06, Loss_res: 3.746e-06,Time: 0.83\n",
      "mean_grad_res: 0.000045\n",
      "mean_grad_bcs: 0.004636\n",
      "Relative L2 error_u: 1.00e+00\n",
      "Relative L2 error_f: 1.00e+00\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Jan-09-2024_23-34-46-965780_M2\n",
      "Final loss total loss: 5.810353e-06\n",
      "Final loss loss_res: 3.746997e-06\n",
      "Final loss loss_bc1: 3.109994e-10\n",
      "Final loss loss_bc2: 2.186026e-10\n",
      "Final loss loss_bc3: 7.440831e-10\n",
      "Final loss loss_bc4: 7.896714e-10\n",
      "average lambda_bc1.0000e+03\n",
      "average lambda_res1.0\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:2.3342e+02\n",
      "average of error_u_list:1.0000e+00\n",
      "average of error_v_list:1.0000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "a_1 = 1\n",
    "a_2 = 4\n",
    "\n",
    "# Parameter\n",
    "lam = 1.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[-1.0, -1.0], [1.0, -1.0]])\n",
    "bc2_coords = np.array([[1.0, -1.0], [1.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "bc4_coords = np.array([[-1.0, 1.0], [-1.0, -1.0]])\n",
    "\n",
    "dom_coords = np.array([[-1.0, -1.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "x1 = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x2 = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star, a_1, a_2)\n",
    "f_star = f(X_star, a_1, a_2, lam)\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = None\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'            # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False    # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, a_1, a_2, lam), name='Forcing')\n",
    "\n",
    "        # [elapsed, error_u , error_f ,  mode] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , \n",
    "        #                                                                stiff_ratio , X_star ,u_star , f_star , nIter ,bcbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    " #def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                model.trainmb(nIter, batch_size=mbbatch_size )\n",
    "            else:\n",
    "                model.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            f_pred = model.predict_r(X_star)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "            model.plot_grad()\n",
    "            model.plot_lambda()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( x1 , x2 , X_star , u_star , u_pred , f_star , f_pred)\n",
    "\n",
    "            model.print(\"average lambda_bc\" , np.average(model.adpative_constant_log))\n",
    "            model.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "            time_list.append(elapsed)\n",
    "            error_u_list.append(error_u)\n",
    "            error_f_list.append(error_f)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_v_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_f_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_Helmholtz_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Method: mini_batch\n",
    "\n",
    "average of time_list:1.7314e+02\n",
    "average of error_u_list:5.2605e-03\n",
    "average of error_v_list:9.5779e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21548/2533309064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predicted solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mU_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mF_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u_pred' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot ###\n",
    "\n",
    "# Exact solution & Predicted solution\n",
    "# Exact soluton\n",
    "U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "# Predicted solution\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Exact $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Predicted $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual loss & Boundary loss\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "gradients_res_list = []\n",
    "gradients_bcs_list = []\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "for j in range(num_hidden_layers):\n",
    "    gradient_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradient_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    gradients_res_list.append(gradient_res)\n",
    "    gradients_bcs_list.append(gradient_bcs)\n",
    "\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim([-3.0, 3.0])\n",
    "    ax.set_ylim([0,100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = mode.eigenvalue_log\n",
    "    eigenvalues_bcs_list = mode.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = mode.eigenvalue_res_log\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
