{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "######################################################################################################\n",
    "def u(x, a_1, a_2):\n",
    "    return np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_xx(x, a_1, a_2):\n",
    "    return - (a_1 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_yy(x, a_1, a_2):\n",
    "    return - (a_2 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "# Forcing\n",
    "def f(x, a_1, a_2, lam):\n",
    "    return u_xx(x, a_1, a_2) + u_yy(x, a_1, a_2) + lam * u(x, a_1, a_2)\n",
    "\n",
    "def operator(u, x1, x2, lam, sigma_x1=1.0, sigma_x2=1.0):\n",
    "    u_x1 = tf.gradients(u, x1)[0] / sigma_x1\n",
    "    u_x2 = tf.gradients(u, x2)[0] / sigma_x2\n",
    "    u_xx1 = tf.gradients(u_x1, x1)[0] / sigma_x1\n",
    "    u_xx2 = tf.gradients(u_x2, x2)[0] / sigma_x2\n",
    "    residual = u_xx1 + u_xx2 + lam * u\n",
    "    return residual\n",
    "#######################################################################################################\n",
    "\n",
    "class Helmholtz2D:\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess):\n",
    "        # Normalization constants\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x1, self.sigma_x1 = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x2, self.sigma_x2 = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Helmoholtz constant\n",
    "        self.lam = tf.constant(lam, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = mode\n",
    "\n",
    "        # Record stiff ratio\n",
    "        # self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # if mode in ['M3', 'M4']:\n",
    "        #     # Initialize encoder weights and biases\n",
    "        #     self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        #     self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x1_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # Define placeholder for adaptive constant\n",
    "        self.adaptive_constant_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred = self.net_u(self.x1_bc1_tf, self.x2_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.x1_bc2_tf, self.x2_bc2_tf)\n",
    "        self.u_bc3_pred = self.net_u(self.x1_bc3_tf, self.x2_bc3_tf)\n",
    "        self.u_bc4_pred = self.net_u(self.x1_bc4_tf, self.x2_bc4_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x1_u_tf, self.x2_u_tf)\n",
    "        self.r_pred = self.net_r(self.x1_r_tf, self.x2_r_tf)\n",
    "\n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_tf - self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_tf - self.u_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.u_bc3_tf - self.u_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.u_bc4_tf - self.u_bc4_pred))\n",
    "        self.loss_bcs = self.adaptive_constant_tf * (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        # Compute and store the adaptive constant\n",
    "        self.adpative_constant_log = []\n",
    "        \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.adaptive_constant = self.max_grad_res / self.mean_grad_bcs\n",
    "\n",
    "        # # Stiff Ratio\n",
    "        # if self.stiff_ratio:\n",
    "        #     self.Hessian, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "        #     self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "        #     self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "        #     self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "        #     self.eigenvalue_log = []\n",
    "        #     self.eigenvalue_bcs_log = []\n",
    "        #     self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "     # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,  self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P), dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),  dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),  dtype='float32')\n",
    "\n",
    "        return H, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self,size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, x1, x2):\n",
    "        u = self.forward_pass(tf.concat([x1, x2], 1))\n",
    "        return u\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x1, x2):\n",
    "        u = self.net_u(x1, x2)\n",
    "        residual = self.operator(u, x1, x2,\n",
    "                                 self.lam,\n",
    "                                 self.sigma_x1,\n",
    "                                 self.sigma_x2)\n",
    "        return residual\n",
    "\n",
    "    # Feed minibatch\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter=10000, batch_size=128):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            if self.stiff_ratio:\n",
    "                if it % 1000 == 0:\n",
    "                    print(\"Eigenvalues information stored ...\")\n",
    "                    eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "                                                                                   self.eigenvalues_bcs,\n",
    "                                                                                   self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # Log eigenvalues\n",
    "                    self.eigenvalue_log.append(eigenvalues)\n",
    "                    self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                if self.model in ['M2', 'M4']:\n",
    "                    adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                    self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta) \\\n",
    "                                                 + self.beta * self.adaptive_constant_val\n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' %\n",
    "                      (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # Store gradients\n",
    "            if it % 10000 == 0:\n",
    "                self.save_gradients(tf_dict)\n",
    "                self.print(\"Gradients information stored ...\")\n",
    "\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter , bcbatch_size , fbatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Fetch boundary mini-batches\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], bcbatch_size)\n",
    "        X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], bcbatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, fbatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.u_bc3_tf: u_bc3_batch,\n",
    "                    self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.u_bc4_tf: u_bc4_batch,\n",
    "                    self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                    self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                    }\n",
    "\n",
    "\n",
    "        for it in range(nIter):\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                # self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                #     self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "\n",
    "                # self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "  # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value ,  loss_bcs_value, loss_res_value = self.sess.run([self.loss, self.loss_bcs, self.loss_res] , tf_dict)\n",
    "\n",
    " \n",
    "                self.print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, elapsed))\n",
    "\n",
    "            if it % 1000 == 0:\n",
    "                adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "                self.print('adaptive_constant_val: %f' % (self.adaptive_constant_val))\n",
    "\n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_u_tf: X_star[:, 0:1], self.x2_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_r_tf: X_star[:, 0:1], self.x2_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n",
    "\n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $f(x)$' , 'Predicted $f(x)$' , 'Absolute error']\n",
    "        data = [U_star , U_pred ,  np.abs(U_star - U_pred) , F_star , F_pred ,  np.abs(F_star - F_pred) ]\n",
    "        \n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        kwargs_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60),\n",
    "                cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, kwargs, minmax, title in zip(grid, data, kwargs_list, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "                #  print( z[timeStp,:,:])\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, lam ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f ,  model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19099/414925196.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19099/414925196.py:83: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19099/414925196.py:84: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19099/414925196.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19099/3216932631.py:279: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19099/3216932631.py:119: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 03:26:28.658946: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 03:26:28.682655: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-12 03:26:28.683127: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557b0a4c9310 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-12 03:26:28.683142: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-12 03:26:28.683847: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_19099/3216932631.py:171: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19099/3216932631.py:173: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19099/3216932631.py:222: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 8.529e+03, Loss_bcs: 2.109e+00, Loss_res: 8.527e+03,Time: 1.92\n",
      "adaptive_constant_val: 3.521661\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, Loss: 6.370e+03, Loss_bcs: 2.996e+00, Loss_res: 6.367e+03,Time: 0.01\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, Loss: 8.373e+03, Loss_bcs: 1.365e+01, Loss_res: 8.359e+03,Time: 0.01\n",
      "It: 300, Loss: 6.974e+03, Loss_bcs: 1.494e+01, Loss_res: 6.959e+03,Time: 0.01\n",
      "It: 400, Loss: 6.571e+03, Loss_bcs: 3.781e+01, Loss_res: 6.533e+03,Time: 0.01\n",
      "It: 500, Loss: 3.670e+03, Loss_bcs: 2.364e+01, Loss_res: 3.646e+03,Time: 0.01\n",
      "It: 600, Loss: 3.393e+03, Loss_bcs: 3.995e+01, Loss_res: 3.353e+03,Time: 0.01\n",
      "It: 700, Loss: 1.254e+03, Loss_bcs: 4.763e+01, Loss_res: 1.207e+03,Time: 0.01\n",
      "It: 800, Loss: 5.165e+02, Loss_bcs: 4.226e+01, Loss_res: 4.742e+02,Time: 0.01\n",
      "It: 900, Loss: 3.262e+02, Loss_bcs: 3.727e+01, Loss_res: 2.890e+02,Time: 0.01\n",
      "It: 1000, Loss: 3.752e+02, Loss_bcs: 3.495e+01, Loss_res: 3.402e+02,Time: 0.01\n",
      "adaptive_constant_val: 12.414429\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, Loss: 2.322e+02, Loss_bcs: 5.249e+01, Loss_res: 1.797e+02,Time: 0.01\n",
      "It: 1200, Loss: 1.469e+02, Loss_bcs: 4.336e+01, Loss_res: 1.036e+02,Time: 0.01\n",
      "It: 1300, Loss: 1.162e+02, Loss_bcs: 3.054e+01, Loss_res: 8.563e+01,Time: 0.01\n",
      "It: 1400, Loss: 1.884e+02, Loss_bcs: 2.409e+01, Loss_res: 1.643e+02,Time: 0.02\n",
      "It: 1500, Loss: 8.749e+01, Loss_bcs: 2.017e+01, Loss_res: 6.733e+01,Time: 0.01\n",
      "It: 1600, Loss: 9.778e+01, Loss_bcs: 2.115e+01, Loss_res: 7.663e+01,Time: 0.01\n",
      "It: 1700, Loss: 7.322e+01, Loss_bcs: 1.368e+01, Loss_res: 5.954e+01,Time: 0.01\n",
      "It: 1800, Loss: 1.085e+02, Loss_bcs: 1.106e+01, Loss_res: 9.746e+01,Time: 0.01\n",
      "It: 1900, Loss: 5.032e+01, Loss_bcs: 9.659e+00, Loss_res: 4.066e+01,Time: 0.02\n",
      "It: 2000, Loss: 3.935e+01, Loss_bcs: 7.451e+00, Loss_res: 3.190e+01,Time: 0.01\n",
      "adaptive_constant_val: 24.734807\n",
      "It: 2100, Loss: 4.902e+01, Loss_bcs: 9.700e+00, Loss_res: 3.932e+01,Time: 0.02\n",
      "It: 2200, Loss: 3.669e+01, Loss_bcs: 6.775e+00, Loss_res: 2.992e+01,Time: 0.02\n",
      "It: 2300, Loss: 2.296e+01, Loss_bcs: 4.942e+00, Loss_res: 1.802e+01,Time: 0.04\n",
      "It: 2400, Loss: 3.151e+01, Loss_bcs: 5.570e+00, Loss_res: 2.594e+01,Time: 0.01\n",
      "It: 2500, Loss: 2.901e+01, Loss_bcs: 5.333e+00, Loss_res: 2.368e+01,Time: 0.01\n",
      "It: 2600, Loss: 3.310e+01, Loss_bcs: 4.859e+00, Loss_res: 2.824e+01,Time: 0.01\n",
      "It: 2700, Loss: 2.336e+01, Loss_bcs: 4.782e+00, Loss_res: 1.858e+01,Time: 0.01\n",
      "It: 2800, Loss: 1.856e+01, Loss_bcs: 3.908e+00, Loss_res: 1.466e+01,Time: 0.01\n",
      "It: 2900, Loss: 1.878e+01, Loss_bcs: 4.799e+00, Loss_res: 1.398e+01,Time: 0.02\n",
      "It: 3000, Loss: 1.937e+01, Loss_bcs: 3.516e+00, Loss_res: 1.585e+01,Time: 0.01\n",
      "adaptive_constant_val: 26.758486\n",
      "It: 3100, Loss: 1.735e+01, Loss_bcs: 3.598e+00, Loss_res: 1.375e+01,Time: 0.02\n",
      "It: 3200, Loss: 1.742e+01, Loss_bcs: 3.728e+00, Loss_res: 1.370e+01,Time: 0.02\n",
      "It: 3300, Loss: 1.600e+01, Loss_bcs: 3.232e+00, Loss_res: 1.277e+01,Time: 0.02\n",
      "It: 3400, Loss: 1.117e+01, Loss_bcs: 3.329e+00, Loss_res: 7.839e+00,Time: 0.02\n",
      "It: 3500, Loss: 1.325e+01, Loss_bcs: 3.355e+00, Loss_res: 9.895e+00,Time: 0.02\n",
      "It: 3600, Loss: 1.170e+01, Loss_bcs: 3.062e+00, Loss_res: 8.634e+00,Time: 0.02\n",
      "It: 3700, Loss: 1.246e+01, Loss_bcs: 2.612e+00, Loss_res: 9.853e+00,Time: 0.02\n",
      "It: 3800, Loss: 1.417e+01, Loss_bcs: 3.124e+00, Loss_res: 1.104e+01,Time: 0.01\n",
      "It: 3900, Loss: 9.883e+00, Loss_bcs: 3.056e+00, Loss_res: 6.827e+00,Time: 0.01\n",
      "It: 4000, Loss: 9.299e+00, Loss_bcs: 2.986e+00, Loss_res: 6.313e+00,Time: 0.01\n",
      "adaptive_constant_val: 31.225300\n",
      "It: 4100, Loss: 1.700e+01, Loss_bcs: 3.228e+00, Loss_res: 1.378e+01,Time: 0.02\n",
      "It: 4200, Loss: 8.594e+00, Loss_bcs: 3.025e+00, Loss_res: 5.569e+00,Time: 0.02\n",
      "It: 4300, Loss: 9.271e+00, Loss_bcs: 3.040e+00, Loss_res: 6.231e+00,Time: 0.02\n",
      "It: 4400, Loss: 7.869e+00, Loss_bcs: 3.015e+00, Loss_res: 4.854e+00,Time: 0.02\n",
      "It: 4500, Loss: 9.346e+00, Loss_bcs: 3.439e+00, Loss_res: 5.907e+00,Time: 0.02\n",
      "It: 4600, Loss: 9.176e+00, Loss_bcs: 2.996e+00, Loss_res: 6.180e+00,Time: 0.02\n",
      "It: 4700, Loss: 7.689e+00, Loss_bcs: 2.919e+00, Loss_res: 4.770e+00,Time: 0.02\n",
      "It: 4800, Loss: 1.060e+01, Loss_bcs: 2.205e+00, Loss_res: 8.399e+00,Time: 0.02\n",
      "It: 4900, Loss: 7.202e+00, Loss_bcs: 2.686e+00, Loss_res: 4.516e+00,Time: 0.02\n",
      "It: 5000, Loss: 9.392e+00, Loss_bcs: 2.673e+00, Loss_res: 6.719e+00,Time: 0.02\n",
      "adaptive_constant_val: 33.402987\n",
      "It: 5100, Loss: 6.603e+00, Loss_bcs: 2.269e+00, Loss_res: 4.334e+00,Time: 0.02\n",
      "It: 5200, Loss: 6.871e+00, Loss_bcs: 2.404e+00, Loss_res: 4.467e+00,Time: 0.02\n",
      "It: 5300, Loss: 9.643e+00, Loss_bcs: 2.435e+00, Loss_res: 7.208e+00,Time: 0.02\n",
      "It: 5400, Loss: 8.149e+00, Loss_bcs: 2.181e+00, Loss_res: 5.968e+00,Time: 0.02\n",
      "It: 5500, Loss: 5.903e+00, Loss_bcs: 2.043e+00, Loss_res: 3.859e+00,Time: 0.01\n",
      "It: 5600, Loss: 6.414e+00, Loss_bcs: 2.052e+00, Loss_res: 4.361e+00,Time: 0.02\n",
      "It: 5700, Loss: 7.798e+00, Loss_bcs: 2.065e+00, Loss_res: 5.732e+00,Time: 0.02\n",
      "It: 5800, Loss: 6.559e+00, Loss_bcs: 2.236e+00, Loss_res: 4.324e+00,Time: 0.02\n",
      "It: 5900, Loss: 7.925e+00, Loss_bcs: 2.237e+00, Loss_res: 5.689e+00,Time: 0.02\n",
      "It: 6000, Loss: 6.490e+00, Loss_bcs: 2.219e+00, Loss_res: 4.271e+00,Time: 0.02\n",
      "adaptive_constant_val: 38.939817\n",
      "It: 6100, Loss: 8.430e+00, Loss_bcs: 2.211e+00, Loss_res: 6.219e+00,Time: 0.02\n",
      "It: 6200, Loss: 5.638e+00, Loss_bcs: 2.115e+00, Loss_res: 3.523e+00,Time: 0.02\n",
      "It: 6300, Loss: 6.249e+00, Loss_bcs: 2.008e+00, Loss_res: 4.241e+00,Time: 0.02\n",
      "It: 6400, Loss: 6.945e+00, Loss_bcs: 1.946e+00, Loss_res: 4.999e+00,Time: 0.02\n",
      "It: 6500, Loss: 9.115e+00, Loss_bcs: 1.577e+00, Loss_res: 7.538e+00,Time: 0.01\n",
      "It: 6600, Loss: 6.180e+00, Loss_bcs: 1.782e+00, Loss_res: 4.397e+00,Time: 0.02\n",
      "It: 6700, Loss: 6.214e+00, Loss_bcs: 1.875e+00, Loss_res: 4.339e+00,Time: 0.02\n",
      "It: 6800, Loss: 5.665e+00, Loss_bcs: 1.716e+00, Loss_res: 3.949e+00,Time: 0.02\n",
      "It: 6900, Loss: 4.004e+00, Loss_bcs: 1.464e+00, Loss_res: 2.539e+00,Time: 0.02\n",
      "It: 7000, Loss: 6.759e+00, Loss_bcs: 1.604e+00, Loss_res: 5.155e+00,Time: 0.02\n",
      "adaptive_constant_val: 44.120566\n",
      "It: 7100, Loss: 7.271e+00, Loss_bcs: 1.829e+00, Loss_res: 5.442e+00,Time: 0.02\n",
      "It: 7200, Loss: 6.094e+00, Loss_bcs: 1.517e+00, Loss_res: 4.576e+00,Time: 0.02\n",
      "It: 7300, Loss: 5.130e+00, Loss_bcs: 1.487e+00, Loss_res: 3.642e+00,Time: 0.02\n",
      "It: 7400, Loss: 6.673e+00, Loss_bcs: 1.422e+00, Loss_res: 5.251e+00,Time: 0.02\n",
      "It: 7500, Loss: 5.473e+00, Loss_bcs: 1.486e+00, Loss_res: 3.987e+00,Time: 0.02\n",
      "It: 7600, Loss: 5.508e+00, Loss_bcs: 1.230e+00, Loss_res: 4.278e+00,Time: 0.02\n",
      "It: 7700, Loss: 4.794e+00, Loss_bcs: 1.530e+00, Loss_res: 3.264e+00,Time: 0.02\n",
      "It: 7800, Loss: 5.440e+00, Loss_bcs: 1.433e+00, Loss_res: 4.007e+00,Time: 0.01\n",
      "It: 7900, Loss: 4.983e+00, Loss_bcs: 1.195e+00, Loss_res: 3.788e+00,Time: 0.01\n",
      "It: 8000, Loss: 4.847e+00, Loss_bcs: 1.257e+00, Loss_res: 3.591e+00,Time: 0.02\n",
      "adaptive_constant_val: 45.689046\n",
      "It: 8100, Loss: 4.381e+00, Loss_bcs: 1.058e+00, Loss_res: 3.323e+00,Time: 0.01\n",
      "It: 8200, Loss: 3.432e+00, Loss_bcs: 1.017e+00, Loss_res: 2.416e+00,Time: 0.02\n",
      "It: 8300, Loss: 3.815e+00, Loss_bcs: 1.200e+00, Loss_res: 2.616e+00,Time: 0.02\n",
      "It: 8400, Loss: 5.868e+00, Loss_bcs: 1.123e+00, Loss_res: 4.745e+00,Time: 0.02\n",
      "It: 8500, Loss: 4.435e+00, Loss_bcs: 1.184e+00, Loss_res: 3.251e+00,Time: 0.02\n",
      "It: 8600, Loss: 4.601e+00, Loss_bcs: 1.076e+00, Loss_res: 3.525e+00,Time: 0.01\n",
      "It: 8700, Loss: 5.555e+00, Loss_bcs: 1.348e+00, Loss_res: 4.207e+00,Time: 0.02\n",
      "It: 8800, Loss: 3.451e+00, Loss_bcs: 1.150e+00, Loss_res: 2.300e+00,Time: 0.01\n",
      "It: 8900, Loss: 2.954e+00, Loss_bcs: 1.130e+00, Loss_res: 1.824e+00,Time: 0.01\n",
      "It: 9000, Loss: 3.008e+00, Loss_bcs: 1.114e+00, Loss_res: 1.894e+00,Time: 0.02\n",
      "adaptive_constant_val: 45.807528\n",
      "It: 9100, Loss: 3.727e+00, Loss_bcs: 9.298e-01, Loss_res: 2.797e+00,Time: 0.02\n",
      "It: 9200, Loss: 3.903e+00, Loss_bcs: 1.053e+00, Loss_res: 2.850e+00,Time: 0.01\n",
      "It: 9300, Loss: 4.290e+00, Loss_bcs: 9.928e-01, Loss_res: 3.297e+00,Time: 0.01\n",
      "It: 9400, Loss: 4.265e+00, Loss_bcs: 1.078e+00, Loss_res: 3.187e+00,Time: 0.02\n",
      "It: 9500, Loss: 3.734e+00, Loss_bcs: 9.521e-01, Loss_res: 2.782e+00,Time: 0.02\n",
      "It: 9600, Loss: 3.369e+00, Loss_bcs: 9.941e-01, Loss_res: 2.375e+00,Time: 0.02\n",
      "It: 9700, Loss: 3.544e+00, Loss_bcs: 7.836e-01, Loss_res: 2.760e+00,Time: 0.02\n",
      "It: 9800, Loss: 3.750e+00, Loss_bcs: 1.017e+00, Loss_res: 2.733e+00,Time: 0.01\n",
      "It: 9900, Loss: 5.070e+00, Loss_bcs: 1.023e+00, Loss_res: 4.046e+00,Time: 0.01\n",
      "It: 10000, Loss: 4.129e+00, Loss_bcs: 7.179e-01, Loss_res: 3.411e+00,Time: 0.02\n",
      "adaptive_constant_val: 51.601503\n",
      "It: 10100, Loss: 4.709e+00, Loss_bcs: 9.610e-01, Loss_res: 3.748e+00,Time: 0.02\n",
      "It: 10200, Loss: 3.505e+00, Loss_bcs: 1.048e+00, Loss_res: 2.457e+00,Time: 0.04\n",
      "It: 10300, Loss: 3.365e+00, Loss_bcs: 9.521e-01, Loss_res: 2.413e+00,Time: 0.02\n",
      "It: 10400, Loss: 2.581e+00, Loss_bcs: 7.614e-01, Loss_res: 1.820e+00,Time: 0.02\n",
      "It: 10500, Loss: 4.241e+00, Loss_bcs: 7.606e-01, Loss_res: 3.481e+00,Time: 0.02\n",
      "It: 10600, Loss: 2.967e+00, Loss_bcs: 8.986e-01, Loss_res: 2.069e+00,Time: 0.01\n",
      "It: 10700, Loss: 3.258e+00, Loss_bcs: 9.577e-01, Loss_res: 2.300e+00,Time: 0.02\n",
      "It: 10800, Loss: 3.341e+00, Loss_bcs: 7.583e-01, Loss_res: 2.583e+00,Time: 0.02\n",
      "It: 10900, Loss: 2.237e+00, Loss_bcs: 8.814e-01, Loss_res: 1.355e+00,Time: 0.02\n",
      "It: 11000, Loss: 2.354e+00, Loss_bcs: 6.395e-01, Loss_res: 1.715e+00,Time: 0.01\n",
      "adaptive_constant_val: 68.833377\n",
      "It: 11100, Loss: 3.675e+00, Loss_bcs: 9.468e-01, Loss_res: 2.728e+00,Time: 0.01\n",
      "It: 11200, Loss: 2.684e+00, Loss_bcs: 7.020e-01, Loss_res: 1.982e+00,Time: 0.02\n",
      "It: 11300, Loss: 4.147e+00, Loss_bcs: 7.502e-01, Loss_res: 3.397e+00,Time: 0.02\n",
      "It: 11400, Loss: 2.939e+00, Loss_bcs: 1.196e+00, Loss_res: 1.743e+00,Time: 0.02\n",
      "It: 11500, Loss: 3.446e+00, Loss_bcs: 8.777e-01, Loss_res: 2.568e+00,Time: 0.02\n",
      "It: 11600, Loss: 2.737e+00, Loss_bcs: 7.363e-01, Loss_res: 2.001e+00,Time: 0.01\n",
      "It: 11700, Loss: 2.891e+00, Loss_bcs: 6.925e-01, Loss_res: 2.199e+00,Time: 0.02\n",
      "It: 11800, Loss: 3.710e+00, Loss_bcs: 5.634e-01, Loss_res: 3.147e+00,Time: 0.02\n",
      "It: 11900, Loss: 3.091e+00, Loss_bcs: 9.184e-01, Loss_res: 2.173e+00,Time: 0.02\n",
      "It: 12000, Loss: 2.459e+00, Loss_bcs: 7.266e-01, Loss_res: 1.733e+00,Time: 0.02\n",
      "adaptive_constant_val: 64.100654\n",
      "It: 12100, Loss: 2.410e+00, Loss_bcs: 6.217e-01, Loss_res: 1.788e+00,Time: 0.02\n",
      "It: 12200, Loss: 2.698e+00, Loss_bcs: 8.291e-01, Loss_res: 1.869e+00,Time: 0.02\n",
      "It: 12300, Loss: 2.737e+00, Loss_bcs: 7.452e-01, Loss_res: 1.992e+00,Time: 0.03\n",
      "It: 12400, Loss: 4.324e+00, Loss_bcs: 6.475e-01, Loss_res: 3.676e+00,Time: 0.02\n",
      "It: 12500, Loss: 2.033e+00, Loss_bcs: 7.468e-01, Loss_res: 1.286e+00,Time: 0.02\n",
      "It: 12600, Loss: 4.184e+00, Loss_bcs: 6.668e-01, Loss_res: 3.518e+00,Time: 0.02\n",
      "It: 12700, Loss: 3.781e+00, Loss_bcs: 6.256e-01, Loss_res: 3.156e+00,Time: 0.01\n",
      "It: 12800, Loss: 2.812e+00, Loss_bcs: 5.908e-01, Loss_res: 2.222e+00,Time: 0.01\n",
      "It: 12900, Loss: 2.040e+00, Loss_bcs: 7.111e-01, Loss_res: 1.329e+00,Time: 0.02\n",
      "It: 13000, Loss: 2.335e+00, Loss_bcs: 7.632e-01, Loss_res: 1.572e+00,Time: 0.02\n",
      "adaptive_constant_val: 61.739073\n",
      "It: 13100, Loss: 2.059e+00, Loss_bcs: 5.479e-01, Loss_res: 1.511e+00,Time: 0.03\n",
      "It: 13200, Loss: 2.524e+00, Loss_bcs: 7.795e-01, Loss_res: 1.745e+00,Time: 0.02\n",
      "It: 13300, Loss: 3.027e+00, Loss_bcs: 6.227e-01, Loss_res: 2.405e+00,Time: 0.02\n",
      "It: 13400, Loss: 2.113e+00, Loss_bcs: 5.473e-01, Loss_res: 1.565e+00,Time: 0.02\n",
      "It: 13500, Loss: 1.932e+00, Loss_bcs: 5.891e-01, Loss_res: 1.342e+00,Time: 0.02\n",
      "It: 13600, Loss: 4.536e+00, Loss_bcs: 6.736e-01, Loss_res: 3.863e+00,Time: 0.02\n",
      "It: 13700, Loss: 2.642e+00, Loss_bcs: 5.951e-01, Loss_res: 2.047e+00,Time: 0.02\n",
      "It: 13800, Loss: 2.788e+00, Loss_bcs: 6.790e-01, Loss_res: 2.109e+00,Time: 0.02\n",
      "It: 13900, Loss: 2.600e+00, Loss_bcs: 6.327e-01, Loss_res: 1.968e+00,Time: 0.02\n",
      "It: 14000, Loss: 2.488e+00, Loss_bcs: 6.319e-01, Loss_res: 1.856e+00,Time: 0.02\n",
      "adaptive_constant_val: 65.083469\n",
      "It: 14100, Loss: 2.458e+00, Loss_bcs: 6.495e-01, Loss_res: 1.808e+00,Time: 0.02\n",
      "It: 14200, Loss: 2.258e+00, Loss_bcs: 6.035e-01, Loss_res: 1.655e+00,Time: 0.02\n",
      "It: 14300, Loss: 5.347e+00, Loss_bcs: 5.909e-01, Loss_res: 4.756e+00,Time: 0.02\n",
      "It: 14400, Loss: 1.862e+00, Loss_bcs: 6.573e-01, Loss_res: 1.205e+00,Time: 0.02\n",
      "It: 14500, Loss: 2.442e+00, Loss_bcs: 4.657e-01, Loss_res: 1.976e+00,Time: 0.01\n",
      "It: 14600, Loss: 2.115e+00, Loss_bcs: 3.885e-01, Loss_res: 1.726e+00,Time: 0.02\n",
      "It: 14700, Loss: 2.099e+00, Loss_bcs: 6.845e-01, Loss_res: 1.414e+00,Time: 0.02\n",
      "It: 14800, Loss: 2.328e+00, Loss_bcs: 5.631e-01, Loss_res: 1.765e+00,Time: 0.02\n",
      "It: 14900, Loss: 1.119e+00, Loss_bcs: 4.212e-01, Loss_res: 6.975e-01,Time: 0.02\n",
      "It: 15000, Loss: 1.859e+00, Loss_bcs: 6.304e-01, Loss_res: 1.228e+00,Time: 0.02\n",
      "adaptive_constant_val: 63.557800\n",
      "It: 15100, Loss: 2.156e+00, Loss_bcs: 5.350e-01, Loss_res: 1.621e+00,Time: 0.02\n",
      "It: 15200, Loss: 1.617e+00, Loss_bcs: 5.388e-01, Loss_res: 1.078e+00,Time: 0.02\n",
      "It: 15300, Loss: 1.433e+00, Loss_bcs: 4.920e-01, Loss_res: 9.409e-01,Time: 0.02\n",
      "It: 15400, Loss: 2.288e+00, Loss_bcs: 4.799e-01, Loss_res: 1.808e+00,Time: 0.02\n",
      "It: 15500, Loss: 2.688e+00, Loss_bcs: 5.428e-01, Loss_res: 2.145e+00,Time: 0.02\n",
      "It: 15600, Loss: 1.563e+00, Loss_bcs: 4.091e-01, Loss_res: 1.154e+00,Time: 0.02\n",
      "It: 15700, Loss: 2.102e+00, Loss_bcs: 4.921e-01, Loss_res: 1.610e+00,Time: 0.02\n",
      "It: 15800, Loss: 2.376e+00, Loss_bcs: 4.860e-01, Loss_res: 1.890e+00,Time: 0.02\n",
      "It: 15900, Loss: 1.509e+00, Loss_bcs: 5.607e-01, Loss_res: 9.485e-01,Time: 0.02\n",
      "It: 16000, Loss: 1.983e+00, Loss_bcs: 5.635e-01, Loss_res: 1.419e+00,Time: 0.01\n",
      "adaptive_constant_val: 67.926254\n",
      "It: 16100, Loss: 1.750e+00, Loss_bcs: 4.611e-01, Loss_res: 1.289e+00,Time: 0.02\n",
      "It: 16200, Loss: 1.610e+00, Loss_bcs: 4.751e-01, Loss_res: 1.135e+00,Time: 0.01\n",
      "It: 16300, Loss: 1.500e+00, Loss_bcs: 5.006e-01, Loss_res: 9.996e-01,Time: 0.02\n",
      "It: 16400, Loss: 1.756e+00, Loss_bcs: 5.174e-01, Loss_res: 1.239e+00,Time: 0.02\n",
      "It: 16500, Loss: 2.911e+00, Loss_bcs: 5.174e-01, Loss_res: 2.394e+00,Time: 0.02\n",
      "It: 16600, Loss: 1.809e+00, Loss_bcs: 4.053e-01, Loss_res: 1.404e+00,Time: 0.02\n",
      "It: 16700, Loss: 1.783e+00, Loss_bcs: 4.333e-01, Loss_res: 1.350e+00,Time: 0.02\n",
      "It: 16800, Loss: 1.673e+00, Loss_bcs: 6.330e-01, Loss_res: 1.040e+00,Time: 0.02\n",
      "It: 16900, Loss: 2.186e+00, Loss_bcs: 5.730e-01, Loss_res: 1.613e+00,Time: 0.02\n",
      "It: 17000, Loss: 1.290e+00, Loss_bcs: 4.768e-01, Loss_res: 8.135e-01,Time: 0.05\n",
      "adaptive_constant_val: 69.914996\n",
      "It: 17100, Loss: 1.915e+00, Loss_bcs: 5.420e-01, Loss_res: 1.373e+00,Time: 0.02\n",
      "It: 17200, Loss: 1.623e+00, Loss_bcs: 5.591e-01, Loss_res: 1.064e+00,Time: 0.02\n",
      "It: 17300, Loss: 2.295e+00, Loss_bcs: 4.383e-01, Loss_res: 1.857e+00,Time: 0.02\n",
      "It: 17400, Loss: 1.456e+00, Loss_bcs: 4.645e-01, Loss_res: 9.915e-01,Time: 0.01\n",
      "It: 17500, Loss: 1.839e+00, Loss_bcs: 4.820e-01, Loss_res: 1.357e+00,Time: 0.02\n",
      "It: 17600, Loss: 1.366e+00, Loss_bcs: 5.142e-01, Loss_res: 8.518e-01,Time: 0.02\n",
      "It: 17700, Loss: 2.027e+00, Loss_bcs: 4.975e-01, Loss_res: 1.530e+00,Time: 0.02\n",
      "It: 17800, Loss: 1.686e+00, Loss_bcs: 6.245e-01, Loss_res: 1.062e+00,Time: 0.02\n",
      "It: 17900, Loss: 2.183e+00, Loss_bcs: 5.618e-01, Loss_res: 1.621e+00,Time: 0.02\n",
      "It: 18000, Loss: 1.555e+00, Loss_bcs: 4.568e-01, Loss_res: 1.098e+00,Time: 0.02\n",
      "adaptive_constant_val: 70.521249\n",
      "It: 18100, Loss: 1.226e+00, Loss_bcs: 4.736e-01, Loss_res: 7.521e-01,Time: 0.02\n",
      "It: 18200, Loss: 1.805e+00, Loss_bcs: 5.123e-01, Loss_res: 1.292e+00,Time: 0.02\n",
      "It: 18300, Loss: 1.883e+00, Loss_bcs: 4.589e-01, Loss_res: 1.424e+00,Time: 0.02\n",
      "It: 18400, Loss: 1.314e+00, Loss_bcs: 4.105e-01, Loss_res: 9.037e-01,Time: 0.02\n",
      "It: 18500, Loss: 1.274e+00, Loss_bcs: 5.291e-01, Loss_res: 7.448e-01,Time: 0.02\n",
      "It: 18600, Loss: 1.537e+00, Loss_bcs: 4.038e-01, Loss_res: 1.133e+00,Time: 0.02\n",
      "It: 18700, Loss: 1.817e+00, Loss_bcs: 4.014e-01, Loss_res: 1.415e+00,Time: 0.02\n",
      "It: 18800, Loss: 2.051e+00, Loss_bcs: 5.204e-01, Loss_res: 1.530e+00,Time: 0.02\n",
      "It: 18900, Loss: 2.625e+00, Loss_bcs: 5.094e-01, Loss_res: 2.116e+00,Time: 0.02\n",
      "It: 19000, Loss: 1.479e+00, Loss_bcs: 4.247e-01, Loss_res: 1.054e+00,Time: 0.02\n",
      "adaptive_constant_val: 75.396098\n",
      "It: 19100, Loss: 1.907e+00, Loss_bcs: 5.963e-01, Loss_res: 1.311e+00,Time: 0.01\n",
      "It: 19200, Loss: 1.769e+00, Loss_bcs: 4.791e-01, Loss_res: 1.290e+00,Time: 0.02\n",
      "It: 19300, Loss: 1.456e+00, Loss_bcs: 5.502e-01, Loss_res: 9.056e-01,Time: 0.02\n",
      "It: 19400, Loss: 1.454e+00, Loss_bcs: 4.615e-01, Loss_res: 9.921e-01,Time: 0.02\n",
      "It: 19500, Loss: 1.515e+00, Loss_bcs: 4.948e-01, Loss_res: 1.020e+00,Time: 0.02\n",
      "It: 19600, Loss: 1.913e+00, Loss_bcs: 3.938e-01, Loss_res: 1.520e+00,Time: 0.02\n",
      "It: 19700, Loss: 1.079e+00, Loss_bcs: 4.877e-01, Loss_res: 5.911e-01,Time: 0.02\n",
      "It: 19800, Loss: 1.640e+00, Loss_bcs: 4.734e-01, Loss_res: 1.166e+00,Time: 0.02\n",
      "It: 19900, Loss: 2.610e+00, Loss_bcs: 4.443e-01, Loss_res: 2.166e+00,Time: 0.02\n",
      "It: 20000, Loss: 2.086e+00, Loss_bcs: 4.892e-01, Loss_res: 1.597e+00,Time: 0.01\n",
      "adaptive_constant_val: 76.509223\n",
      "It: 20100, Loss: 2.450e+00, Loss_bcs: 4.186e-01, Loss_res: 2.032e+00,Time: 0.02\n",
      "It: 20200, Loss: 5.558e+00, Loss_bcs: 4.536e-01, Loss_res: 5.104e+00,Time: 0.02\n",
      "It: 20300, Loss: 1.342e+00, Loss_bcs: 4.031e-01, Loss_res: 9.385e-01,Time: 0.02\n",
      "It: 20400, Loss: 1.126e+00, Loss_bcs: 4.061e-01, Loss_res: 7.201e-01,Time: 0.02\n",
      "It: 20500, Loss: 1.894e+00, Loss_bcs: 5.439e-01, Loss_res: 1.350e+00,Time: 0.03\n",
      "It: 20600, Loss: 1.098e+00, Loss_bcs: 3.631e-01, Loss_res: 7.350e-01,Time: 0.02\n",
      "It: 20700, Loss: 1.107e+00, Loss_bcs: 4.113e-01, Loss_res: 6.955e-01,Time: 0.02\n",
      "It: 20800, Loss: 1.340e+00, Loss_bcs: 4.652e-01, Loss_res: 8.750e-01,Time: 0.02\n",
      "It: 20900, Loss: 1.589e+00, Loss_bcs: 3.834e-01, Loss_res: 1.205e+00,Time: 0.02\n",
      "It: 21000, Loss: 1.487e+00, Loss_bcs: 5.391e-01, Loss_res: 9.477e-01,Time: 0.02\n",
      "adaptive_constant_val: 74.570981\n",
      "It: 21100, Loss: 2.235e+00, Loss_bcs: 5.013e-01, Loss_res: 1.733e+00,Time: 0.02\n",
      "It: 21200, Loss: 1.287e+00, Loss_bcs: 4.006e-01, Loss_res: 8.859e-01,Time: 0.01\n",
      "It: 21300, Loss: 1.094e+00, Loss_bcs: 4.184e-01, Loss_res: 6.759e-01,Time: 0.02\n",
      "It: 21400, Loss: 1.020e+00, Loss_bcs: 4.791e-01, Loss_res: 5.408e-01,Time: 0.02\n",
      "It: 21500, Loss: 1.139e+00, Loss_bcs: 4.925e-01, Loss_res: 6.465e-01,Time: 0.02\n",
      "It: 21600, Loss: 1.148e+00, Loss_bcs: 3.235e-01, Loss_res: 8.248e-01,Time: 0.02\n",
      "It: 21700, Loss: 1.384e+00, Loss_bcs: 3.828e-01, Loss_res: 1.001e+00,Time: 0.02\n",
      "It: 21800, Loss: 1.104e+00, Loss_bcs: 3.825e-01, Loss_res: 7.213e-01,Time: 0.02\n",
      "It: 21900, Loss: 1.848e+00, Loss_bcs: 5.002e-01, Loss_res: 1.348e+00,Time: 0.02\n",
      "It: 22000, Loss: 1.390e+00, Loss_bcs: 4.144e-01, Loss_res: 9.760e-01,Time: 0.02\n",
      "adaptive_constant_val: 71.194571\n",
      "It: 22100, Loss: 1.716e+00, Loss_bcs: 3.554e-01, Loss_res: 1.361e+00,Time: 0.02\n",
      "It: 22200, Loss: 9.972e-01, Loss_bcs: 3.813e-01, Loss_res: 6.158e-01,Time: 0.02\n",
      "It: 22300, Loss: 1.396e+00, Loss_bcs: 3.230e-01, Loss_res: 1.073e+00,Time: 0.02\n",
      "It: 22400, Loss: 1.057e+00, Loss_bcs: 3.331e-01, Loss_res: 7.239e-01,Time: 0.04\n",
      "It: 22500, Loss: 1.267e+00, Loss_bcs: 3.704e-01, Loss_res: 8.964e-01,Time: 0.02\n",
      "It: 22600, Loss: 1.737e+00, Loss_bcs: 4.235e-01, Loss_res: 1.313e+00,Time: 0.02\n",
      "It: 22700, Loss: 1.532e+00, Loss_bcs: 3.961e-01, Loss_res: 1.135e+00,Time: 0.02\n",
      "It: 22800, Loss: 1.260e+00, Loss_bcs: 3.211e-01, Loss_res: 9.384e-01,Time: 0.02\n",
      "It: 22900, Loss: 1.396e+00, Loss_bcs: 3.309e-01, Loss_res: 1.065e+00,Time: 0.02\n",
      "It: 23000, Loss: 1.195e+00, Loss_bcs: 4.157e-01, Loss_res: 7.793e-01,Time: 0.02\n",
      "adaptive_constant_val: 67.282660\n",
      "It: 23100, Loss: 1.233e+00, Loss_bcs: 3.263e-01, Loss_res: 9.062e-01,Time: 0.02\n",
      "It: 23200, Loss: 1.030e+00, Loss_bcs: 3.160e-01, Loss_res: 7.138e-01,Time: 0.02\n",
      "It: 23300, Loss: 1.359e+00, Loss_bcs: 2.832e-01, Loss_res: 1.076e+00,Time: 0.02\n",
      "It: 23400, Loss: 1.123e+00, Loss_bcs: 2.960e-01, Loss_res: 8.268e-01,Time: 0.02\n",
      "It: 23500, Loss: 1.150e+00, Loss_bcs: 4.325e-01, Loss_res: 7.172e-01,Time: 0.02\n",
      "It: 23600, Loss: 1.015e+00, Loss_bcs: 3.310e-01, Loss_res: 6.844e-01,Time: 0.01\n",
      "It: 23700, Loss: 1.291e+00, Loss_bcs: 3.615e-01, Loss_res: 9.298e-01,Time: 0.02\n",
      "It: 23800, Loss: 1.229e+00, Loss_bcs: 2.838e-01, Loss_res: 9.449e-01,Time: 0.02\n",
      "It: 23900, Loss: 1.176e+00, Loss_bcs: 3.459e-01, Loss_res: 8.303e-01,Time: 0.02\n",
      "It: 24000, Loss: 1.296e+00, Loss_bcs: 2.719e-01, Loss_res: 1.024e+00,Time: 0.02\n",
      "adaptive_constant_val: 70.852846\n",
      "It: 24100, Loss: 1.460e+00, Loss_bcs: 3.199e-01, Loss_res: 1.140e+00,Time: 0.02\n",
      "It: 24200, Loss: 1.304e+00, Loss_bcs: 3.504e-01, Loss_res: 9.537e-01,Time: 0.02\n",
      "It: 24300, Loss: 9.728e-01, Loss_bcs: 3.303e-01, Loss_res: 6.424e-01,Time: 0.02\n",
      "It: 24400, Loss: 1.086e+00, Loss_bcs: 3.041e-01, Loss_res: 7.819e-01,Time: 0.02\n",
      "It: 24500, Loss: 1.570e+00, Loss_bcs: 3.453e-01, Loss_res: 1.224e+00,Time: 0.03\n",
      "It: 24600, Loss: 1.006e+00, Loss_bcs: 3.742e-01, Loss_res: 6.321e-01,Time: 0.02\n",
      "It: 24700, Loss: 1.243e+00, Loss_bcs: 2.797e-01, Loss_res: 9.632e-01,Time: 0.02\n",
      "It: 24800, Loss: 7.601e-01, Loss_bcs: 3.322e-01, Loss_res: 4.278e-01,Time: 0.02\n",
      "It: 24900, Loss: 1.326e+00, Loss_bcs: 3.754e-01, Loss_res: 9.508e-01,Time: 0.02\n",
      "It: 25000, Loss: 1.305e+00, Loss_bcs: 3.850e-01, Loss_res: 9.195e-01,Time: 0.02\n",
      "adaptive_constant_val: 75.419975\n",
      "It: 25100, Loss: 8.766e-01, Loss_bcs: 3.572e-01, Loss_res: 5.194e-01,Time: 0.02\n",
      "It: 25200, Loss: 1.505e+00, Loss_bcs: 3.143e-01, Loss_res: 1.191e+00,Time: 0.02\n",
      "It: 25300, Loss: 1.688e+00, Loss_bcs: 3.634e-01, Loss_res: 1.325e+00,Time: 0.02\n",
      "It: 25400, Loss: 1.370e+00, Loss_bcs: 3.804e-01, Loss_res: 9.892e-01,Time: 0.02\n",
      "It: 25500, Loss: 8.961e-01, Loss_bcs: 3.815e-01, Loss_res: 5.146e-01,Time: 0.02\n",
      "It: 25600, Loss: 1.203e+00, Loss_bcs: 2.534e-01, Loss_res: 9.493e-01,Time: 0.02\n",
      "It: 25700, Loss: 1.328e+00, Loss_bcs: 3.284e-01, Loss_res: 9.994e-01,Time: 0.02\n",
      "It: 25800, Loss: 1.953e+00, Loss_bcs: 3.307e-01, Loss_res: 1.622e+00,Time: 0.02\n",
      "It: 25900, Loss: 3.094e+00, Loss_bcs: 2.578e-01, Loss_res: 2.837e+00,Time: 0.02\n",
      "It: 26000, Loss: 1.437e+00, Loss_bcs: 3.150e-01, Loss_res: 1.122e+00,Time: 0.02\n",
      "adaptive_constant_val: 78.638960\n",
      "It: 26100, Loss: 1.221e+00, Loss_bcs: 3.710e-01, Loss_res: 8.502e-01,Time: 0.02\n",
      "It: 26200, Loss: 1.063e+00, Loss_bcs: 3.748e-01, Loss_res: 6.877e-01,Time: 0.02\n",
      "It: 26300, Loss: 2.610e+00, Loss_bcs: 3.427e-01, Loss_res: 2.267e+00,Time: 0.01\n",
      "It: 26400, Loss: 9.878e-01, Loss_bcs: 4.269e-01, Loss_res: 5.609e-01,Time: 0.01\n",
      "It: 26500, Loss: 2.978e+00, Loss_bcs: 3.314e-01, Loss_res: 2.647e+00,Time: 0.01\n",
      "It: 26600, Loss: 9.271e-01, Loss_bcs: 2.700e-01, Loss_res: 6.571e-01,Time: 0.01\n",
      "It: 26700, Loss: 9.841e-01, Loss_bcs: 3.944e-01, Loss_res: 5.898e-01,Time: 0.01\n",
      "It: 26800, Loss: 1.987e+00, Loss_bcs: 3.932e-01, Loss_res: 1.594e+00,Time: 0.01\n",
      "It: 26900, Loss: 7.162e-01, Loss_bcs: 3.432e-01, Loss_res: 3.730e-01,Time: 0.01\n",
      "It: 27000, Loss: 1.204e+00, Loss_bcs: 3.888e-01, Loss_res: 8.149e-01,Time: 0.01\n",
      "adaptive_constant_val: 76.553622\n",
      "It: 27100, Loss: 1.066e+00, Loss_bcs: 2.985e-01, Loss_res: 7.676e-01,Time: 0.01\n",
      "It: 27200, Loss: 8.263e-01, Loss_bcs: 2.659e-01, Loss_res: 5.605e-01,Time: 0.01\n",
      "It: 27300, Loss: 1.844e+00, Loss_bcs: 3.399e-01, Loss_res: 1.504e+00,Time: 0.01\n",
      "It: 27400, Loss: 1.225e+00, Loss_bcs: 2.704e-01, Loss_res: 9.548e-01,Time: 0.01\n",
      "It: 27500, Loss: 9.110e-01, Loss_bcs: 3.436e-01, Loss_res: 5.674e-01,Time: 0.02\n",
      "It: 27600, Loss: 1.254e+00, Loss_bcs: 2.666e-01, Loss_res: 9.875e-01,Time: 0.01\n",
      "It: 27700, Loss: 9.675e-01, Loss_bcs: 3.183e-01, Loss_res: 6.492e-01,Time: 0.01\n",
      "It: 27800, Loss: 8.156e-01, Loss_bcs: 3.572e-01, Loss_res: 4.584e-01,Time: 0.01\n",
      "It: 27900, Loss: 1.297e+00, Loss_bcs: 3.517e-01, Loss_res: 9.454e-01,Time: 0.01\n",
      "It: 28000, Loss: 1.736e+00, Loss_bcs: 2.907e-01, Loss_res: 1.445e+00,Time: 0.01\n",
      "adaptive_constant_val: 82.586592\n",
      "It: 28100, Loss: 1.408e+00, Loss_bcs: 4.187e-01, Loss_res: 9.893e-01,Time: 0.01\n",
      "It: 28200, Loss: 1.635e+00, Loss_bcs: 3.969e-01, Loss_res: 1.238e+00,Time: 0.01\n",
      "It: 28300, Loss: 1.371e+00, Loss_bcs: 4.159e-01, Loss_res: 9.555e-01,Time: 0.01\n",
      "It: 28400, Loss: 1.051e+00, Loss_bcs: 4.236e-01, Loss_res: 6.274e-01,Time: 0.02\n",
      "It: 28500, Loss: 1.219e+00, Loss_bcs: 4.033e-01, Loss_res: 8.160e-01,Time: 0.01\n",
      "It: 28600, Loss: 2.964e+00, Loss_bcs: 3.941e-01, Loss_res: 2.570e+00,Time: 0.01\n",
      "It: 28700, Loss: 2.675e+00, Loss_bcs: 3.692e-01, Loss_res: 2.306e+00,Time: 0.01\n",
      "It: 28800, Loss: 1.051e+00, Loss_bcs: 3.615e-01, Loss_res: 6.892e-01,Time: 0.01\n",
      "It: 28900, Loss: 1.004e+00, Loss_bcs: 4.263e-01, Loss_res: 5.781e-01,Time: 0.01\n",
      "It: 29000, Loss: 1.103e+00, Loss_bcs: 3.989e-01, Loss_res: 7.041e-01,Time: 0.01\n",
      "adaptive_constant_val: 79.630804\n",
      "It: 29100, Loss: 1.549e+00, Loss_bcs: 2.950e-01, Loss_res: 1.254e+00,Time: 0.01\n",
      "It: 29200, Loss: 1.218e+00, Loss_bcs: 3.656e-01, Loss_res: 8.525e-01,Time: 0.01\n",
      "It: 29300, Loss: 1.237e+00, Loss_bcs: 3.758e-01, Loss_res: 8.615e-01,Time: 0.01\n",
      "It: 29400, Loss: 1.612e+00, Loss_bcs: 4.201e-01, Loss_res: 1.192e+00,Time: 0.01\n",
      "It: 29500, Loss: 1.348e+00, Loss_bcs: 3.688e-01, Loss_res: 9.789e-01,Time: 0.02\n",
      "It: 29600, Loss: 7.233e-01, Loss_bcs: 3.616e-01, Loss_res: 3.618e-01,Time: 0.06\n",
      "It: 29700, Loss: 9.314e-01, Loss_bcs: 3.431e-01, Loss_res: 5.883e-01,Time: 0.01\n",
      "It: 29800, Loss: 1.973e+00, Loss_bcs: 3.276e-01, Loss_res: 1.646e+00,Time: 0.02\n",
      "It: 29900, Loss: 1.216e+00, Loss_bcs: 3.842e-01, Loss_res: 8.319e-01,Time: 0.01\n",
      "It: 30000, Loss: 6.483e-01, Loss_bcs: 3.064e-01, Loss_res: 3.419e-01,Time: 0.01\n",
      "adaptive_constant_val: 77.269759\n",
      "It: 30100, Loss: 1.902e+00, Loss_bcs: 3.028e-01, Loss_res: 1.600e+00,Time: 0.03\n",
      "It: 30200, Loss: 1.817e+00, Loss_bcs: 3.403e-01, Loss_res: 1.477e+00,Time: 0.01\n",
      "It: 30300, Loss: 1.363e+00, Loss_bcs: 2.900e-01, Loss_res: 1.073e+00,Time: 0.01\n",
      "It: 30400, Loss: 1.347e+00, Loss_bcs: 3.289e-01, Loss_res: 1.018e+00,Time: 0.01\n",
      "It: 30500, Loss: 8.041e-01, Loss_bcs: 2.670e-01, Loss_res: 5.370e-01,Time: 0.01\n",
      "It: 30600, Loss: 2.579e+00, Loss_bcs: 3.148e-01, Loss_res: 2.265e+00,Time: 0.01\n",
      "It: 30700, Loss: 1.646e+00, Loss_bcs: 3.285e-01, Loss_res: 1.317e+00,Time: 0.01\n",
      "It: 30800, Loss: 8.973e-01, Loss_bcs: 3.136e-01, Loss_res: 5.837e-01,Time: 0.01\n",
      "It: 30900, Loss: 6.659e-01, Loss_bcs: 2.888e-01, Loss_res: 3.771e-01,Time: 0.01\n",
      "It: 31000, Loss: 1.576e+00, Loss_bcs: 2.936e-01, Loss_res: 1.282e+00,Time: 0.01\n",
      "adaptive_constant_val: 77.813667\n",
      "It: 31100, Loss: 1.088e+00, Loss_bcs: 3.414e-01, Loss_res: 7.462e-01,Time: 0.01\n",
      "It: 31200, Loss: 1.483e+00, Loss_bcs: 2.822e-01, Loss_res: 1.200e+00,Time: 0.01\n",
      "It: 31300, Loss: 9.545e-01, Loss_bcs: 3.290e-01, Loss_res: 6.256e-01,Time: 0.01\n",
      "It: 31400, Loss: 1.656e+00, Loss_bcs: 3.442e-01, Loss_res: 1.312e+00,Time: 0.01\n",
      "It: 31500, Loss: 9.667e-01, Loss_bcs: 3.696e-01, Loss_res: 5.971e-01,Time: 0.01\n",
      "It: 31600, Loss: 1.058e+00, Loss_bcs: 3.054e-01, Loss_res: 7.530e-01,Time: 0.01\n",
      "It: 31700, Loss: 1.506e+00, Loss_bcs: 2.519e-01, Loss_res: 1.254e+00,Time: 0.02\n",
      "It: 31800, Loss: 1.568e+00, Loss_bcs: 2.602e-01, Loss_res: 1.308e+00,Time: 0.01\n",
      "It: 31900, Loss: 9.346e-01, Loss_bcs: 3.071e-01, Loss_res: 6.274e-01,Time: 0.01\n",
      "It: 32000, Loss: 7.261e-01, Loss_bcs: 3.046e-01, Loss_res: 4.215e-01,Time: 0.01\n",
      "adaptive_constant_val: 74.636585\n",
      "It: 32100, Loss: 1.375e+00, Loss_bcs: 2.318e-01, Loss_res: 1.144e+00,Time: 0.01\n",
      "It: 32200, Loss: 1.097e+00, Loss_bcs: 3.420e-01, Loss_res: 7.547e-01,Time: 0.01\n",
      "It: 32300, Loss: 1.093e+00, Loss_bcs: 3.427e-01, Loss_res: 7.500e-01,Time: 0.01\n",
      "It: 32400, Loss: 1.138e+00, Loss_bcs: 2.787e-01, Loss_res: 8.590e-01,Time: 0.02\n",
      "It: 32500, Loss: 1.087e+00, Loss_bcs: 3.144e-01, Loss_res: 7.725e-01,Time: 0.02\n",
      "It: 32600, Loss: 7.256e-01, Loss_bcs: 2.741e-01, Loss_res: 4.515e-01,Time: 0.01\n",
      "It: 32700, Loss: 1.265e+00, Loss_bcs: 2.832e-01, Loss_res: 9.821e-01,Time: 0.01\n",
      "It: 32800, Loss: 1.321e+00, Loss_bcs: 3.508e-01, Loss_res: 9.701e-01,Time: 0.01\n",
      "It: 32900, Loss: 1.449e+00, Loss_bcs: 3.315e-01, Loss_res: 1.118e+00,Time: 0.01\n",
      "It: 33000, Loss: 1.508e+00, Loss_bcs: 2.947e-01, Loss_res: 1.213e+00,Time: 0.01\n",
      "adaptive_constant_val: 104.170327\n",
      "It: 33100, Loss: 1.418e+00, Loss_bcs: 3.704e-01, Loss_res: 1.048e+00,Time: 0.01\n",
      "It: 33200, Loss: 1.823e+00, Loss_bcs: 5.607e-01, Loss_res: 1.263e+00,Time: 0.01\n",
      "It: 33300, Loss: 1.652e+00, Loss_bcs: 3.487e-01, Loss_res: 1.303e+00,Time: 0.01\n",
      "It: 33400, Loss: 1.003e+00, Loss_bcs: 4.340e-01, Loss_res: 5.692e-01,Time: 0.01\n",
      "It: 33500, Loss: 9.621e-01, Loss_bcs: 3.624e-01, Loss_res: 5.997e-01,Time: 0.01\n",
      "It: 33600, Loss: 1.051e+00, Loss_bcs: 4.199e-01, Loss_res: 6.310e-01,Time: 0.01\n",
      "It: 33700, Loss: 9.955e-01, Loss_bcs: 3.352e-01, Loss_res: 6.603e-01,Time: 0.01\n",
      "It: 33800, Loss: 1.934e+00, Loss_bcs: 5.108e-01, Loss_res: 1.423e+00,Time: 0.02\n",
      "It: 33900, Loss: 1.336e+00, Loss_bcs: 3.285e-01, Loss_res: 1.007e+00,Time: 0.01\n",
      "It: 34000, Loss: 1.152e+00, Loss_bcs: 5.202e-01, Loss_res: 6.319e-01,Time: 0.05\n",
      "adaptive_constant_val: 101.627654\n",
      "It: 34100, Loss: 1.108e+00, Loss_bcs: 4.123e-01, Loss_res: 6.955e-01,Time: 0.01\n",
      "It: 34200, Loss: 9.199e-01, Loss_bcs: 3.615e-01, Loss_res: 5.584e-01,Time: 0.01\n",
      "It: 34300, Loss: 1.331e+00, Loss_bcs: 3.632e-01, Loss_res: 9.678e-01,Time: 0.01\n",
      "It: 34400, Loss: 1.631e+00, Loss_bcs: 3.246e-01, Loss_res: 1.306e+00,Time: 0.01\n",
      "It: 34500, Loss: 3.345e+00, Loss_bcs: 3.206e-01, Loss_res: 3.024e+00,Time: 0.01\n",
      "It: 34600, Loss: 8.063e-01, Loss_bcs: 4.068e-01, Loss_res: 3.995e-01,Time: 0.01\n",
      "It: 34700, Loss: 1.205e+00, Loss_bcs: 3.318e-01, Loss_res: 8.730e-01,Time: 0.01\n",
      "It: 34800, Loss: 1.176e+00, Loss_bcs: 4.553e-01, Loss_res: 7.204e-01,Time: 0.01\n",
      "It: 34900, Loss: 9.521e-01, Loss_bcs: 4.258e-01, Loss_res: 5.263e-01,Time: 0.01\n",
      "It: 35000, Loss: 8.819e-01, Loss_bcs: 4.207e-01, Loss_res: 4.611e-01,Time: 0.01\n",
      "adaptive_constant_val: 96.928052\n",
      "It: 35100, Loss: 1.404e+00, Loss_bcs: 4.101e-01, Loss_res: 9.940e-01,Time: 0.01\n",
      "It: 35200, Loss: 1.675e+00, Loss_bcs: 3.636e-01, Loss_res: 1.312e+00,Time: 0.01\n",
      "It: 35300, Loss: 2.390e+00, Loss_bcs: 3.270e-01, Loss_res: 2.063e+00,Time: 0.01\n",
      "It: 35400, Loss: 1.879e+00, Loss_bcs: 3.277e-01, Loss_res: 1.552e+00,Time: 0.01\n",
      "It: 35500, Loss: 2.294e+00, Loss_bcs: 3.621e-01, Loss_res: 1.932e+00,Time: 0.01\n",
      "It: 35600, Loss: 1.170e+00, Loss_bcs: 3.183e-01, Loss_res: 8.512e-01,Time: 0.01\n",
      "It: 35700, Loss: 2.600e+00, Loss_bcs: 4.043e-01, Loss_res: 2.195e+00,Time: 0.01\n",
      "It: 35800, Loss: 8.100e-01, Loss_bcs: 3.758e-01, Loss_res: 4.342e-01,Time: 0.01\n",
      "It: 35900, Loss: 1.032e+00, Loss_bcs: 3.328e-01, Loss_res: 6.991e-01,Time: 0.01\n",
      "It: 36000, Loss: 1.164e+00, Loss_bcs: 3.399e-01, Loss_res: 8.237e-01,Time: 0.01\n",
      "adaptive_constant_val: 95.078505\n",
      "It: 36100, Loss: 1.397e+00, Loss_bcs: 4.376e-01, Loss_res: 9.599e-01,Time: 0.01\n",
      "It: 36200, Loss: 2.952e+00, Loss_bcs: 3.583e-01, Loss_res: 2.594e+00,Time: 0.01\n",
      "It: 36300, Loss: 9.806e-01, Loss_bcs: 3.172e-01, Loss_res: 6.634e-01,Time: 0.01\n",
      "It: 36400, Loss: 1.977e+00, Loss_bcs: 2.776e-01, Loss_res: 1.700e+00,Time: 0.01\n",
      "It: 36500, Loss: 7.680e-01, Loss_bcs: 3.406e-01, Loss_res: 4.274e-01,Time: 0.01\n",
      "It: 36600, Loss: 1.650e+00, Loss_bcs: 3.488e-01, Loss_res: 1.301e+00,Time: 0.01\n",
      "It: 36700, Loss: 1.049e+00, Loss_bcs: 3.190e-01, Loss_res: 7.296e-01,Time: 0.01\n",
      "It: 36800, Loss: 9.756e-01, Loss_bcs: 3.071e-01, Loss_res: 6.685e-01,Time: 0.02\n",
      "It: 36900, Loss: 9.012e-01, Loss_bcs: 2.602e-01, Loss_res: 6.410e-01,Time: 0.01\n",
      "It: 37000, Loss: 1.551e+00, Loss_bcs: 3.040e-01, Loss_res: 1.247e+00,Time: 0.01\n",
      "adaptive_constant_val: 107.989611\n",
      "It: 37100, Loss: 1.232e+00, Loss_bcs: 4.141e-01, Loss_res: 8.175e-01,Time: 0.01\n",
      "It: 37200, Loss: 1.506e+00, Loss_bcs: 3.315e-01, Loss_res: 1.175e+00,Time: 0.01\n",
      "It: 37300, Loss: 9.563e-01, Loss_bcs: 4.090e-01, Loss_res: 5.473e-01,Time: 0.02\n",
      "It: 37400, Loss: 7.995e-01, Loss_bcs: 3.039e-01, Loss_res: 4.956e-01,Time: 0.01\n",
      "It: 37500, Loss: 9.739e-01, Loss_bcs: 3.401e-01, Loss_res: 6.338e-01,Time: 0.01\n",
      "It: 37600, Loss: 7.568e-01, Loss_bcs: 4.264e-01, Loss_res: 3.304e-01,Time: 0.04\n",
      "It: 37700, Loss: 1.175e+00, Loss_bcs: 3.809e-01, Loss_res: 7.944e-01,Time: 0.01\n",
      "It: 37800, Loss: 1.200e+00, Loss_bcs: 4.234e-01, Loss_res: 7.767e-01,Time: 0.02\n",
      "It: 37900, Loss: 1.410e+00, Loss_bcs: 4.074e-01, Loss_res: 1.003e+00,Time: 0.01\n",
      "It: 38000, Loss: 1.426e+00, Loss_bcs: 4.034e-01, Loss_res: 1.022e+00,Time: 0.01\n",
      "adaptive_constant_val: 111.402249\n",
      "It: 38100, Loss: 1.140e+00, Loss_bcs: 3.703e-01, Loss_res: 7.697e-01,Time: 0.01\n",
      "It: 38200, Loss: 1.353e+00, Loss_bcs: 2.926e-01, Loss_res: 1.060e+00,Time: 0.03\n",
      "It: 38300, Loss: 1.555e+00, Loss_bcs: 3.825e-01, Loss_res: 1.172e+00,Time: 0.01\n",
      "It: 38400, Loss: 1.227e+00, Loss_bcs: 4.373e-01, Loss_res: 7.900e-01,Time: 0.01\n",
      "It: 38500, Loss: 9.328e-01, Loss_bcs: 3.947e-01, Loss_res: 5.381e-01,Time: 0.01\n",
      "It: 38600, Loss: 1.203e+00, Loss_bcs: 3.863e-01, Loss_res: 8.169e-01,Time: 0.02\n",
      "It: 38700, Loss: 1.174e+00, Loss_bcs: 3.208e-01, Loss_res: 8.530e-01,Time: 0.01\n",
      "It: 38800, Loss: 1.601e+00, Loss_bcs: 3.657e-01, Loss_res: 1.236e+00,Time: 0.01\n",
      "It: 38900, Loss: 1.225e+00, Loss_bcs: 3.881e-01, Loss_res: 8.374e-01,Time: 0.01\n",
      "It: 39000, Loss: 1.046e+00, Loss_bcs: 3.344e-01, Loss_res: 7.114e-01,Time: 0.01\n",
      "adaptive_constant_val: 103.286979\n",
      "It: 39100, Loss: 6.940e-01, Loss_bcs: 3.020e-01, Loss_res: 3.920e-01,Time: 0.01\n",
      "It: 39200, Loss: 7.929e-01, Loss_bcs: 3.217e-01, Loss_res: 4.712e-01,Time: 0.01\n",
      "It: 39300, Loss: 1.032e+00, Loss_bcs: 4.059e-01, Loss_res: 6.264e-01,Time: 0.01\n",
      "It: 39400, Loss: 1.108e+00, Loss_bcs: 3.294e-01, Loss_res: 7.781e-01,Time: 0.01\n",
      "It: 39500, Loss: 1.559e+00, Loss_bcs: 3.328e-01, Loss_res: 1.227e+00,Time: 0.01\n",
      "It: 39600, Loss: 1.154e+00, Loss_bcs: 2.976e-01, Loss_res: 8.563e-01,Time: 0.01\n",
      "It: 39700, Loss: 1.291e+00, Loss_bcs: 2.733e-01, Loss_res: 1.018e+00,Time: 0.01\n",
      "It: 39800, Loss: 7.626e-01, Loss_bcs: 3.250e-01, Loss_res: 4.376e-01,Time: 0.01\n",
      "It: 39900, Loss: 9.042e-01, Loss_bcs: 3.474e-01, Loss_res: 5.568e-01,Time: 0.01\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000, Loss: 9.659e-01, Loss_bcs: 2.688e-01, Loss_res: 6.970e-01,Time: 0.95\n",
      "adaptive_constant_val: 98.568652\n",
      "Relative L2 error_u: 1.86e-02\n",
      "Relative L2 error_f: 1.44e-02\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-12-2023_03-26-28-684074_M2\n",
      "Final loss total loss: 9.785352e-01\n",
      "Final loss loss_res: 7.092327e-01\n",
      "Final loss loss_bc1: 1.359597e-03\n",
      "Final loss loss_bc2: 4.442787e-04\n",
      "Final loss loss_bc3: 7.190942e-05\n",
      "Final loss loss_bc4: 7.315369e-04\n",
      "average lambda_bc6.7495e+01\n",
      "average lambda_res1.0\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:6.5756e+02\n",
      "average of error_u_list:1.8577e-02\n",
      "average of error_v_list:1.4351e-02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "a_1 = 1\n",
    "a_2 = 4\n",
    "\n",
    "# Parameter\n",
    "lam = 1.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[-1.0, -1.0], [1.0, -1.0]])\n",
    "bc2_coords = np.array([[1.0, -1.0], [1.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "bc4_coords = np.array([[-1.0, 1.0], [-1.0, -1.0]])\n",
    "\n",
    "dom_coords = np.array([[-1.0, -1.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "x1 = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x2 = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star, a_1, a_2)\n",
    "f_star = f(X_star, a_1, a_2, lam)\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = None\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'            # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False    # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, a_1, a_2, lam), name='Forcing')\n",
    "\n",
    "        # [elapsed, error_u , error_f ,  mode] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , \n",
    "        #                                                                stiff_ratio , X_star ,u_star , f_star , nIter ,bcbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    " #def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                model.trainmb(nIter, batch_size=mbbatch_size )\n",
    "            else:\n",
    "                model.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            f_pred = model.predict_r(X_star)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "            model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( x1 , x2 , X_star , u_star , u_pred , f_star , f_pred)\n",
    "\n",
    "            model.print(\"average lambda_bc\" , np.average(model.adpative_constant_log))\n",
    "            model.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "            time_list.append(elapsed)\n",
    "            error_u_list.append(error_u)\n",
    "            error_f_list.append(error_f)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_v_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_f_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_Helmholtz_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### database is a vailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21548/2533309064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predicted solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mU_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mF_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u_pred' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot ###\n",
    "\n",
    "# Exact solution & Predicted solution\n",
    "# Exact soluton\n",
    "U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "# Predicted solution\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Exact $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Predicted $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual loss & Boundary loss\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "gradients_res_list = []\n",
    "gradients_bcs_list = []\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "for j in range(num_hidden_layers):\n",
    "    gradient_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradient_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    gradients_res_list.append(gradient_res)\n",
    "    gradients_bcs_list.append(gradient_bcs)\n",
    "\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim([-3.0, 3.0])\n",
    "    ax.set_ylim([0,100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = mode.eigenvalue_log\n",
    "    eigenvalues_bcs_list = mode.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = mode.eigenvalue_res_log\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
