{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "######################################################################################################\n",
    "def u(x, a_1, a_2):\n",
    "    return np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_xx(x, a_1, a_2):\n",
    "    return - (a_1 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_yy(x, a_1, a_2):\n",
    "    return - (a_2 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "# Forcing\n",
    "def f(x, a_1, a_2, lam):\n",
    "    return u_xx(x, a_1, a_2) + u_yy(x, a_1, a_2) + lam * u(x, a_1, a_2)\n",
    "\n",
    "def operator(u, x1, x2, lam, sigma_x1=1.0, sigma_x2=1.0):\n",
    "    u_x1 = tf.gradients(u, x1)[0] / sigma_x1\n",
    "    u_x2 = tf.gradients(u, x2)[0] / sigma_x2\n",
    "    u_xx1 = tf.gradients(u_x1, x1)[0] / sigma_x1\n",
    "    u_xx2 = tf.gradients(u_x2, x2)[0] / sigma_x2\n",
    "    residual = u_xx1 + u_xx2 + lam * u\n",
    "    return residual\n",
    "#######################################################################################################\n",
    "\n",
    "class Helmholtz2D:\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess):\n",
    "        # Normalization constants\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x1, self.sigma_x1 = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x2, self.sigma_x2 = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Helmoholtz constant\n",
    "        self.lam = tf.constant(lam, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = mode\n",
    "\n",
    "        # Record stiff ratio\n",
    "        # self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive constant\n",
    "        self.rate = 0.5\n",
    "        self.rho = 0.5\n",
    "        self.T = 0.9\n",
    "\n",
    "        self.adaptive_constant_bc_val = np.array(1.0)\n",
    "        self.adaptive_constant_res_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # if mode in ['M3', 'M4']:\n",
    "        #     # Initialize encoder weights and biases\n",
    "        #     self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        #     self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x1_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # Define placeholder for adaptive constant\n",
    "        self.adaptive_constant_bc_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bc_val.shape)\n",
    "        self.adaptive_constant_res_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred = self.net_u(self.x1_bc1_tf, self.x2_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.x1_bc2_tf, self.x2_bc2_tf)\n",
    "        self.u_bc3_pred = self.net_u(self.x1_bc3_tf, self.x2_bc3_tf)\n",
    "        self.u_bc4_pred = self.net_u(self.x1_bc4_tf, self.x2_bc4_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x1_u_tf, self.x2_u_tf)\n",
    "        self.r_pred = self.net_r(self.x1_r_tf, self.x2_r_tf)\n",
    "\n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_tf - self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_tf - self.u_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.u_bc3_tf - self.u_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.u_bc4_tf - self.u_bc4_pred))\n",
    "        self.loss_bcs =  (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.adaptive_constant_res_tf * self.loss_res + self.adaptive_constant_bc_tf * (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res ,  self.loss_bcs,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "\n",
    "\n",
    "        self.prev_lam_bc =1\n",
    "        self.prev_lam_res = 1\n",
    "        self.index =1\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        # Compute and store the adaptive constant\n",
    "        self.adpative_constant_bc_log = []\n",
    "        self.adpative_constant_res_log = []\n",
    "        \n",
    "        self.mean_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        \n",
    "        self.mean_grad_res_log = []\n",
    "        self.mean_grad_bcs_log = []\n",
    "    \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.mean_grad_res_list.append(tf.reduce_mean(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        \n",
    "        self.mean_grad_res = tf.reduce_mean(tf.stack(self.mean_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "\n",
    "        # # Stiff Ratio\n",
    "        # if self.stiff_ratio:\n",
    "        #     self.Hessian, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "        #     self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "        #     self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "        #     self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "        #     self.eigenvalue_log = []\n",
    "        #     self.eigenvalue_bcs_log = []\n",
    "        #     self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "     # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self,size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, x1, x2):\n",
    "        u = self.forward_pass(tf.concat([x1, x2], 1))\n",
    "        return u\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x1, x2):\n",
    "        u = self.net_u(x1, x2)\n",
    "        residual = self.operator(u, x1, x2,\n",
    "                                 self.lam,\n",
    "                                 self.sigma_x1,\n",
    "                                 self.sigma_x2)\n",
    "        return residual\n",
    "\n",
    "    # Feed minibatch\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "    def lambda_balance(self ,term , index):\n",
    "\n",
    "        loss_list = [\"loss_res\" , \"loss_bcs\" ] \n",
    "\n",
    "        m = len(loss_list)\n",
    "        num = np.exp(self.loss_history[term][-1] /(self.T * self.loss_history[term][index] +1e-12))\n",
    "        denum = 0 \n",
    "\n",
    "        for  key in loss_list:\n",
    "            denum +=  np.exp(self.loss_history[key][-1] /(self.T * self.loss_history[key][index] +1e-12))\n",
    "        return m * (num / denum)\n",
    "     \n",
    "    def relobralo( self, term ,prev_lamda ):\n",
    "\n",
    "        lambda_hist = self.rho  * prev_lamda + (1.0 - self.rho) *  self.lambda_balance( term , 1)\n",
    "        lambda_i = self.rate  * lambda_hist + (1.0 - self.rate) *  self.lambda_balance( term , self.index)\n",
    "\n",
    "        return lambda_i\n",
    "    \n",
    "  # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(1 , nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_res_tf:  self.adaptive_constant_res_val,\n",
    "                       self.adaptive_constant_bc_tf:  self.adaptive_constant_bc_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "\n",
    "            for loss_values, key in zip(batch_losses, self.loss_list):\n",
    "                self.loss_history[key].append(loss_values)\n",
    " \n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value ,  loss_bcs_value, loss_res_value = self.sess.run([self.loss, self.loss_bcs, self.loss_res] , tf_dict)\n",
    "\n",
    " \n",
    "                self.print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, elapsed))\n",
    "\n",
    "            if it % 100 == 0:\n",
    "\n",
    "                self.adaptive_constant_res_val =  self.relobralo( \"loss_res\"  ,self.prev_lam_res )  #fres *  np.exp(self.rate * fres)  /norm\n",
    "\n",
    "                self.adaptive_constant_bc_val =  self.relobralo( \"loss_bcs\"  ,self.prev_lam_bc )  # adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "\n",
    "                self.print('adaptive_constant_res_val: %f' % (self.adaptive_constant_res_val))\n",
    "                self.print('adaptive_constant_bc_val: %f' % (self.adaptive_constant_bc_val))\n",
    "\n",
    "                self.adpative_constant_bc_log.append(self.adaptive_constant_bc_val)\n",
    "                self.adpative_constant_res_log.append(self.adaptive_constant_res_val)\n",
    "\n",
    "                mean_grad_bcs ,  max_grad_res = self.sess.run([self.mean_grad_bcs, self.mean_grad_res] , tf_dict)\n",
    "\n",
    "                self.mean_grad_bcs_log.append(mean_grad_bcs)\n",
    "                self.mean_grad_res_log.append(max_grad_res)\n",
    "\n",
    "                self.prev_lam_bc = self.adaptive_constant_bc_val\n",
    "                self.prev_lam_res = self.adaptive_constant_res_val\n",
    "                self.index = it\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    " \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_u_tf: X_star[:, 0:1], self.x2_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_r_tf: X_star[:, 0:1], self.x2_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n",
    "\n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $f(x)$' , 'Predicted $f(x)$' , 'Absolute error']\n",
    "        data = [U_star , U_pred ,  np.abs(U_star - U_pred) , F_star , F_pred ,  np.abs(F_star - F_pred) ]\n",
    "        \n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        kwargs_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60),\n",
    "                cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, kwargs, minmax, title in zip(grid, data, kwargs_list, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "                #  print( z[timeStp,:,:])\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_bc_log, label=r'$\\lambda_{bc}$')\n",
    "        ax.semilogy(self.adpative_constant_res_log, label=r'$\\lambda_{res}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(r'$\\lambda$', fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'lambda_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.mean_grad_bcs_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.semilogy(self.mean_grad_res_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{res}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_{\\theta} \\mathcal{L}}$', fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, lam ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f ,  model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_11615/1385146676.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_11615/1385146676.py:83: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_11615/1385146676.py:84: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_11615/1385146676.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 02:04:06.509804: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 02:04:06.547370: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-18 02:04:06.548114: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556ff2adce80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-18 02:04:06.548141: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-18 02:04:06.548988: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_11615/3853476717.py:264: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_11615/3853476717.py:123: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_11615/3853476717.py:176: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_11615/3853476717.py:178: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_11615/3853476717.py:235: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, Loss: 6.719e+03, Loss_bcs: 6.880e-01, Loss_res: 6.719e+03,Time: 0.01\n",
      "adaptive_constant_res_val: 0.485739\n",
      "adaptive_constant_bc_val: 1.514261\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, Loss: 3.014e+03, Loss_bcs: 2.398e+00, Loss_res: 6.197e+03,Time: 0.04\n",
      "adaptive_constant_res_val: 0.185336\n",
      "adaptive_constant_bc_val: 1.814664\n",
      "It: 300, Loss: 1.383e+03, Loss_bcs: 2.003e+00, Loss_res: 7.441e+03,Time: 0.02\n",
      "adaptive_constant_res_val: 0.612794\n",
      "adaptive_constant_bc_val: 1.387206\n",
      "It: 400, Loss: 4.861e+03, Loss_bcs: 6.678e+00, Loss_res: 7.917e+03,Time: 0.01\n",
      "adaptive_constant_res_val: 0.246575\n",
      "adaptive_constant_bc_val: 1.753425\n",
      "It: 500, Loss: 1.097e+03, Loss_bcs: 3.663e+00, Loss_res: 4.423e+03,Time: 0.05\n",
      "adaptive_constant_res_val: 0.593025\n",
      "adaptive_constant_bc_val: 1.406975\n",
      "It: 600, Loss: 2.433e+03, Loss_bcs: 7.659e+00, Loss_res: 4.084e+03,Time: 0.03\n",
      "adaptive_constant_res_val: 0.348721\n",
      "adaptive_constant_bc_val: 1.651279\n",
      "It: 700, Loss: 1.696e+03, Loss_bcs: 9.575e+00, Loss_res: 4.820e+03,Time: 0.01\n",
      "adaptive_constant_res_val: 0.582234\n",
      "adaptive_constant_bc_val: 1.417766\n",
      "It: 800, Loss: 1.660e+03, Loss_bcs: 2.031e+01, Loss_res: 2.802e+03,Time: 0.04\n",
      "adaptive_constant_res_val: 0.336913\n",
      "adaptive_constant_bc_val: 1.663087\n",
      "It: 900, Loss: 5.405e+02, Loss_bcs: 1.115e+01, Loss_res: 1.549e+03,Time: 0.02\n",
      "adaptive_constant_res_val: 0.575367\n",
      "adaptive_constant_bc_val: 1.424633\n",
      "It: 1000, Loss: 4.811e+02, Loss_bcs: 8.202e+00, Loss_res: 8.159e+02,Time: 0.02\n",
      "adaptive_constant_res_val: 0.562081\n",
      "adaptive_constant_bc_val: 1.437919\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, Loss: 5.295e+02, Loss_bcs: 6.252e+00, Loss_res: 9.260e+02,Time: 0.01\n",
      "adaptive_constant_res_val: 0.625168\n",
      "adaptive_constant_bc_val: 1.374832\n",
      "It: 1200, Loss: 5.342e+02, Loss_bcs: 9.681e+00, Loss_res: 8.332e+02,Time: 0.01\n",
      "adaptive_constant_res_val: 0.578414\n",
      "adaptive_constant_bc_val: 1.421586\n",
      "It: 1300, Loss: 2.349e+02, Loss_bcs: 1.263e+01, Loss_res: 3.750e+02,Time: 0.01\n",
      "adaptive_constant_res_val: 0.523391\n",
      "adaptive_constant_bc_val: 1.476609\n",
      "It: 1400, Loss: 1.818e+02, Loss_bcs: 1.315e+01, Loss_res: 3.102e+02,Time: 0.02\n",
      "adaptive_constant_res_val: 0.522091\n",
      "adaptive_constant_bc_val: 1.477909\n",
      "It: 1500, Loss: 2.526e+02, Loss_bcs: 1.339e+01, Loss_res: 4.458e+02,Time: 0.02\n",
      "adaptive_constant_res_val: 0.599125\n",
      "adaptive_constant_bc_val: 1.400875\n",
      "It: 1600, Loss: 1.934e+02, Loss_bcs: 1.440e+01, Loss_res: 2.891e+02,Time: 0.02\n",
      "adaptive_constant_res_val: 0.584703\n",
      "adaptive_constant_bc_val: 1.415297\n",
      "It: 1700, Loss: 8.589e+01, Loss_bcs: 1.143e+01, Loss_res: 1.192e+02,Time: 0.02\n",
      "adaptive_constant_res_val: 0.544988\n",
      "adaptive_constant_bc_val: 1.455012\n",
      "It: 1800, Loss: 5.246e+01, Loss_bcs: 9.182e+00, Loss_res: 7.174e+01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.500498\n",
      "adaptive_constant_bc_val: 1.499502\n",
      "It: 1900, Loss: 6.357e+01, Loss_bcs: 7.057e+00, Loss_res: 1.059e+02,Time: 0.01\n",
      "adaptive_constant_res_val: 0.724361\n",
      "adaptive_constant_bc_val: 1.275639\n",
      "It: 2000, Loss: 4.589e+01, Loss_bcs: 7.227e+00, Loss_res: 5.063e+01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.528873\n",
      "adaptive_constant_bc_val: 1.471127\n",
      "It: 2100, Loss: 4.098e+01, Loss_bcs: 5.762e+00, Loss_res: 6.146e+01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.826421\n",
      "adaptive_constant_bc_val: 1.173579\n",
      "It: 2200, Loss: 5.106e+01, Loss_bcs: 5.647e+00, Loss_res: 5.377e+01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.784006\n",
      "adaptive_constant_bc_val: 1.215994\n",
      "It: 2300, Loss: 3.946e+01, Loss_bcs: 5.134e+00, Loss_res: 4.237e+01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.760967\n",
      "adaptive_constant_bc_val: 1.239033\n",
      "It: 2400, Loss: 3.157e+01, Loss_bcs: 4.152e+00, Loss_res: 3.472e+01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.679206\n",
      "adaptive_constant_bc_val: 1.320794\n",
      "It: 2500, Loss: 2.491e+01, Loss_bcs: 3.675e+00, Loss_res: 2.953e+01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.659358\n",
      "adaptive_constant_bc_val: 1.340642\n",
      "It: 2600, Loss: 2.110e+01, Loss_bcs: 3.207e+00, Loss_res: 2.548e+01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.658835\n",
      "adaptive_constant_bc_val: 1.341165\n",
      "It: 2700, Loss: 1.743e+01, Loss_bcs: 2.746e+00, Loss_res: 2.087e+01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.613102\n",
      "adaptive_constant_bc_val: 1.386898\n",
      "It: 2800, Loss: 1.593e+01, Loss_bcs: 2.595e+00, Loss_res: 2.011e+01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.705050\n",
      "adaptive_constant_bc_val: 1.294950\n",
      "It: 2900, Loss: 1.748e+01, Loss_bcs: 2.602e+00, Loss_res: 2.001e+01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.598962\n",
      "adaptive_constant_bc_val: 1.401038\n",
      "It: 3000, Loss: 1.387e+01, Loss_bcs: 2.258e+00, Loss_res: 1.787e+01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.604817\n",
      "adaptive_constant_bc_val: 1.395183\n",
      "It: 3100, Loss: 1.570e+01, Loss_bcs: 2.125e+00, Loss_res: 2.105e+01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.762842\n",
      "adaptive_constant_bc_val: 1.237158\n",
      "It: 3200, Loss: 1.254e+01, Loss_bcs: 1.950e+00, Loss_res: 1.327e+01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.806319\n",
      "adaptive_constant_bc_val: 1.193681\n",
      "It: 3300, Loss: 1.861e+01, Loss_bcs: 1.943e+00, Loss_res: 2.020e+01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.794164\n",
      "adaptive_constant_bc_val: 1.205836\n",
      "It: 3400, Loss: 1.752e+01, Loss_bcs: 1.839e+00, Loss_res: 1.927e+01,Time: 0.05\n",
      "adaptive_constant_res_val: 0.907793\n",
      "adaptive_constant_bc_val: 1.092207\n",
      "It: 3500, Loss: 1.458e+01, Loss_bcs: 1.899e+00, Loss_res: 1.378e+01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.803633\n",
      "adaptive_constant_bc_val: 1.196367\n",
      "It: 3600, Loss: 1.128e+01, Loss_bcs: 1.810e+00, Loss_res: 1.134e+01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.616630\n",
      "adaptive_constant_bc_val: 1.383370\n",
      "It: 3700, Loss: 1.044e+01, Loss_bcs: 1.611e+00, Loss_res: 1.332e+01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.784440\n",
      "adaptive_constant_bc_val: 1.215560\n",
      "It: 3800, Loss: 1.059e+01, Loss_bcs: 1.530e+00, Loss_res: 1.113e+01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.649498\n",
      "adaptive_constant_bc_val: 1.350502\n",
      "It: 3900, Loss: 7.661e+00, Loss_bcs: 1.556e+00, Loss_res: 8.559e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.574698\n",
      "adaptive_constant_bc_val: 1.425302\n",
      "It: 4000, Loss: 8.428e+00, Loss_bcs: 1.439e+00, Loss_res: 1.110e+01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.688912\n",
      "adaptive_constant_bc_val: 1.311088\n",
      "It: 4100, Loss: 8.001e+00, Loss_bcs: 1.323e+00, Loss_res: 9.097e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.596915\n",
      "adaptive_constant_bc_val: 1.403085\n",
      "It: 4200, Loss: 6.990e+00, Loss_bcs: 1.314e+00, Loss_res: 8.622e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.647112\n",
      "adaptive_constant_bc_val: 1.352888\n",
      "It: 4300, Loss: 7.517e+00, Loss_bcs: 1.128e+00, Loss_res: 9.259e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.730900\n",
      "adaptive_constant_bc_val: 1.269100\n",
      "It: 4400, Loss: 1.019e+01, Loss_bcs: 1.149e+00, Loss_res: 1.195e+01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.780174\n",
      "adaptive_constant_bc_val: 1.219826\n",
      "It: 4500, Loss: 7.514e+00, Loss_bcs: 1.232e+00, Loss_res: 7.705e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.589181\n",
      "adaptive_constant_bc_val: 1.410819\n",
      "It: 4600, Loss: 6.241e+00, Loss_bcs: 1.130e+00, Loss_res: 7.886e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.587520\n",
      "adaptive_constant_bc_val: 1.412480\n",
      "It: 4700, Loss: 7.146e+00, Loss_bcs: 1.049e+00, Loss_res: 9.642e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.621280\n",
      "adaptive_constant_bc_val: 1.378720\n",
      "It: 4800, Loss: 5.159e+00, Loss_bcs: 9.920e-01, Loss_res: 6.103e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.542676\n",
      "adaptive_constant_bc_val: 1.457324\n",
      "It: 4900, Loss: 6.265e+00, Loss_bcs: 9.423e-01, Loss_res: 9.014e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.878366\n",
      "adaptive_constant_bc_val: 1.121634\n",
      "It: 5000, Loss: 6.941e+00, Loss_bcs: 1.012e+00, Loss_res: 6.611e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.736351\n",
      "adaptive_constant_bc_val: 1.263649\n",
      "It: 5100, Loss: 7.103e+00, Loss_bcs: 9.429e-01, Loss_res: 8.028e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.850952\n",
      "adaptive_constant_bc_val: 1.149048\n",
      "It: 5200, Loss: 7.126e+00, Loss_bcs: 1.079e+00, Loss_res: 6.917e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.746723\n",
      "adaptive_constant_bc_val: 1.253277\n",
      "It: 5300, Loss: 5.985e+00, Loss_bcs: 9.437e-01, Loss_res: 6.432e+00,Time: 0.05\n",
      "adaptive_constant_res_val: 0.804292\n",
      "adaptive_constant_bc_val: 1.195708\n",
      "It: 5400, Loss: 7.290e+00, Loss_bcs: 9.546e-01, Loss_res: 7.644e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.759041\n",
      "adaptive_constant_bc_val: 1.240959\n",
      "It: 5500, Loss: 5.744e+00, Loss_bcs: 8.627e-01, Loss_res: 6.157e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.682099\n",
      "adaptive_constant_bc_val: 1.317901\n",
      "It: 5600, Loss: 4.847e+00, Loss_bcs: 8.953e-01, Loss_res: 5.376e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.625192\n",
      "adaptive_constant_bc_val: 1.374808\n",
      "It: 5700, Loss: 3.484e+00, Loss_bcs: 8.112e-01, Loss_res: 3.789e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.676211\n",
      "adaptive_constant_bc_val: 1.323789\n",
      "It: 5800, Loss: 5.679e+00, Loss_bcs: 7.795e-01, Loss_res: 6.872e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.804394\n",
      "adaptive_constant_bc_val: 1.195606\n",
      "It: 5900, Loss: 7.084e+00, Loss_bcs: 8.845e-01, Loss_res: 7.492e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.705829\n",
      "adaptive_constant_bc_val: 1.294171\n",
      "It: 6000, Loss: 4.410e+00, Loss_bcs: 8.162e-01, Loss_res: 4.751e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.664718\n",
      "adaptive_constant_bc_val: 1.335282\n",
      "It: 6100, Loss: 4.132e+00, Loss_bcs: 7.733e-01, Loss_res: 4.662e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.587212\n",
      "adaptive_constant_bc_val: 1.412788\n",
      "It: 6200, Loss: 3.676e+00, Loss_bcs: 7.719e-01, Loss_res: 4.403e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.633796\n",
      "adaptive_constant_bc_val: 1.366204\n",
      "It: 6300, Loss: 5.047e+00, Loss_bcs: 7.055e-01, Loss_res: 6.442e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.774695\n",
      "adaptive_constant_bc_val: 1.225305\n",
      "It: 6400, Loss: 4.733e+00, Loss_bcs: 7.595e-01, Loss_res: 4.908e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.620793\n",
      "adaptive_constant_bc_val: 1.379207\n",
      "It: 6500, Loss: 3.385e+00, Loss_bcs: 7.115e-01, Loss_res: 3.872e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.675523\n",
      "adaptive_constant_bc_val: 1.324477\n",
      "It: 6600, Loss: 4.974e+00, Loss_bcs: 7.640e-01, Loss_res: 5.865e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.781214\n",
      "adaptive_constant_bc_val: 1.218786\n",
      "It: 6700, Loss: 4.451e+00, Loss_bcs: 6.854e-01, Loss_res: 4.628e+00,Time: 0.06\n",
      "adaptive_constant_res_val: 0.641845\n",
      "adaptive_constant_bc_val: 1.358155\n",
      "It: 6800, Loss: 3.222e+00, Loss_bcs: 7.005e-01, Loss_res: 3.538e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.655388\n",
      "adaptive_constant_bc_val: 1.344612\n",
      "It: 6900, Loss: 4.102e+00, Loss_bcs: 6.385e-01, Loss_res: 4.948e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.812357\n",
      "adaptive_constant_bc_val: 1.187643\n",
      "It: 7000, Loss: 4.958e+00, Loss_bcs: 6.616e-01, Loss_res: 5.136e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.935986\n",
      "adaptive_constant_bc_val: 1.064014\n",
      "It: 7100, Loss: 6.080e+00, Loss_bcs: 7.272e-01, Loss_res: 5.669e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.874409\n",
      "adaptive_constant_bc_val: 1.125591\n",
      "It: 7200, Loss: 4.125e+00, Loss_bcs: 6.567e-01, Loss_res: 3.872e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.715308\n",
      "adaptive_constant_bc_val: 1.284692\n",
      "It: 7300, Loss: 3.340e+00, Loss_bcs: 6.629e-01, Loss_res: 3.479e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.665954\n",
      "adaptive_constant_bc_val: 1.334046\n",
      "It: 7400, Loss: 2.456e+00, Loss_bcs: 5.965e-01, Loss_res: 2.493e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.635818\n",
      "adaptive_constant_bc_val: 1.364182\n",
      "It: 7500, Loss: 4.186e+00, Loss_bcs: 5.659e-01, Loss_res: 5.370e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.817354\n",
      "adaptive_constant_bc_val: 1.182646\n",
      "It: 7600, Loss: 4.445e+00, Loss_bcs: 6.097e-01, Loss_res: 4.556e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.759146\n",
      "adaptive_constant_bc_val: 1.240854\n",
      "It: 7700, Loss: 3.104e+00, Loss_bcs: 6.185e-01, Loss_res: 3.078e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.611657\n",
      "adaptive_constant_bc_val: 1.388343\n",
      "It: 7800, Loss: 2.466e+00, Loss_bcs: 5.800e-01, Loss_res: 2.716e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.574559\n",
      "adaptive_constant_bc_val: 1.425441\n",
      "It: 7900, Loss: 3.307e+00, Loss_bcs: 5.710e-01, Loss_res: 4.339e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.723291\n",
      "adaptive_constant_bc_val: 1.276709\n",
      "It: 8000, Loss: 2.706e+00, Loss_bcs: 5.698e-01, Loss_res: 2.735e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.672337\n",
      "adaptive_constant_bc_val: 1.327663\n",
      "It: 8100, Loss: 3.616e+00, Loss_bcs: 5.313e-01, Loss_res: 4.329e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.743017\n",
      "adaptive_constant_bc_val: 1.256983\n",
      "It: 8200, Loss: 2.793e+00, Loss_bcs: 5.348e-01, Loss_res: 2.855e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.716314\n",
      "adaptive_constant_bc_val: 1.283686\n",
      "It: 8300, Loss: 2.728e+00, Loss_bcs: 6.025e-01, Loss_res: 2.729e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.591520\n",
      "adaptive_constant_bc_val: 1.408480\n",
      "It: 8400, Loss: 4.569e+00, Loss_bcs: 5.421e-01, Loss_res: 6.433e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.868095\n",
      "adaptive_constant_bc_val: 1.131905\n",
      "It: 8500, Loss: 2.803e+00, Loss_bcs: 5.051e-01, Loss_res: 2.570e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.765970\n",
      "adaptive_constant_bc_val: 1.234030\n",
      "It: 8600, Loss: 2.658e+00, Loss_bcs: 5.239e-01, Loss_res: 2.626e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.687655\n",
      "adaptive_constant_bc_val: 1.312345\n",
      "It: 8700, Loss: 2.366e+00, Loss_bcs: 4.992e-01, Loss_res: 2.488e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.630218\n",
      "adaptive_constant_bc_val: 1.369782\n",
      "It: 8800, Loss: 2.489e+00, Loss_bcs: 4.647e-01, Loss_res: 2.939e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.655250\n",
      "adaptive_constant_bc_val: 1.344750\n",
      "It: 8900, Loss: 2.397e+00, Loss_bcs: 4.512e-01, Loss_res: 2.733e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.675550\n",
      "adaptive_constant_bc_val: 1.324450\n",
      "It: 9000, Loss: 3.298e+00, Loss_bcs: 4.813e-01, Loss_res: 3.938e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.858981\n",
      "adaptive_constant_bc_val: 1.141019\n",
      "It: 9100, Loss: 3.370e+00, Loss_bcs: 4.859e-01, Loss_res: 3.277e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.696399\n",
      "adaptive_constant_bc_val: 1.303601\n",
      "It: 9200, Loss: 2.652e+00, Loss_bcs: 4.337e-01, Loss_res: 2.996e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.759206\n",
      "adaptive_constant_bc_val: 1.240794\n",
      "It: 9300, Loss: 3.678e+00, Loss_bcs: 4.871e-01, Loss_res: 4.049e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.776574\n",
      "adaptive_constant_bc_val: 1.223426\n",
      "It: 9400, Loss: 2.892e+00, Loss_bcs: 4.701e-01, Loss_res: 2.983e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.718386\n",
      "adaptive_constant_bc_val: 1.281614\n",
      "It: 9500, Loss: 2.321e+00, Loss_bcs: 4.612e-01, Loss_res: 2.408e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.742489\n",
      "adaptive_constant_bc_val: 1.257511\n",
      "It: 9600, Loss: 3.026e+00, Loss_bcs: 4.303e-01, Loss_res: 3.347e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.829893\n",
      "adaptive_constant_bc_val: 1.170107\n",
      "It: 9700, Loss: 3.242e+00, Loss_bcs: 4.588e-01, Loss_res: 3.260e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.765215\n",
      "adaptive_constant_bc_val: 1.234785\n",
      "It: 9800, Loss: 3.803e+00, Loss_bcs: 4.631e-01, Loss_res: 4.222e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.826827\n",
      "adaptive_constant_bc_val: 1.173173\n",
      "It: 9900, Loss: 2.915e+00, Loss_bcs: 4.247e-01, Loss_res: 2.923e+00,Time: 0.05\n",
      "adaptive_constant_res_val: 0.878297\n",
      "adaptive_constant_bc_val: 1.121703\n",
      "It: 10000, Loss: 3.100e+00, Loss_bcs: 4.521e-01, Loss_res: 2.952e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.847652\n",
      "adaptive_constant_bc_val: 1.152348\n",
      "It: 10100, Loss: 3.195e+00, Loss_bcs: 4.217e-01, Loss_res: 3.196e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.815000\n",
      "adaptive_constant_bc_val: 1.185000\n",
      "It: 10200, Loss: 2.330e+00, Loss_bcs: 4.550e-01, Loss_res: 2.197e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.640010\n",
      "adaptive_constant_bc_val: 1.359990\n",
      "It: 10300, Loss: 2.036e+00, Loss_bcs: 4.686e-01, Loss_res: 2.185e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.813821\n",
      "adaptive_constant_bc_val: 1.186179\n",
      "It: 10400, Loss: 3.140e+00, Loss_bcs: 4.420e-01, Loss_res: 3.215e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.885167\n",
      "adaptive_constant_bc_val: 1.114833\n",
      "It: 10500, Loss: 2.753e+00, Loss_bcs: 4.773e-01, Loss_res: 2.508e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.709833\n",
      "adaptive_constant_bc_val: 1.290167\n",
      "It: 10600, Loss: 2.427e+00, Loss_bcs: 4.392e-01, Loss_res: 2.621e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.711814\n",
      "adaptive_constant_bc_val: 1.288186\n",
      "It: 10700, Loss: 2.118e+00, Loss_bcs: 4.143e-01, Loss_res: 2.225e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.767061\n",
      "adaptive_constant_bc_val: 1.232939\n",
      "It: 10800, Loss: 2.895e+00, Loss_bcs: 4.088e-01, Loss_res: 3.117e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.859234\n",
      "adaptive_constant_bc_val: 1.140766\n",
      "It: 10900, Loss: 2.593e+00, Loss_bcs: 4.533e-01, Loss_res: 2.416e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.756846\n",
      "adaptive_constant_bc_val: 1.243154\n",
      "It: 11000, Loss: 1.830e+00, Loss_bcs: 4.102e-01, Loss_res: 1.744e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.703256\n",
      "adaptive_constant_bc_val: 1.296744\n",
      "It: 11100, Loss: 2.088e+00, Loss_bcs: 3.997e-01, Loss_res: 2.233e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.764860\n",
      "adaptive_constant_bc_val: 1.235140\n",
      "It: 11200, Loss: 2.166e+00, Loss_bcs: 4.190e-01, Loss_res: 2.156e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.799978\n",
      "adaptive_constant_bc_val: 1.200022\n",
      "It: 11300, Loss: 2.330e+00, Loss_bcs: 3.932e-01, Loss_res: 2.322e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.747888\n",
      "adaptive_constant_bc_val: 1.252112\n",
      "It: 11400, Loss: 2.444e+00, Loss_bcs: 3.910e-01, Loss_res: 2.614e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.642511\n",
      "adaptive_constant_bc_val: 1.357489\n",
      "It: 11500, Loss: 1.563e+00, Loss_bcs: 4.100e-01, Loss_res: 1.566e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.784729\n",
      "adaptive_constant_bc_val: 1.215271\n",
      "It: 11600, Loss: 2.305e+00, Loss_bcs: 3.826e-01, Loss_res: 2.344e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 1.034476\n",
      "adaptive_constant_bc_val: 0.965524\n",
      "It: 11700, Loss: 2.459e+00, Loss_bcs: 3.801e-01, Loss_res: 2.022e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.806899\n",
      "adaptive_constant_bc_val: 1.193101\n",
      "It: 11800, Loss: 3.158e+00, Loss_bcs: 3.549e-01, Loss_res: 3.390e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.861379\n",
      "adaptive_constant_bc_val: 1.138621\n",
      "It: 11900, Loss: 2.771e+00, Loss_bcs: 3.762e-01, Loss_res: 2.720e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.745317\n",
      "adaptive_constant_bc_val: 1.254683\n",
      "It: 12000, Loss: 2.351e+00, Loss_bcs: 3.790e-01, Loss_res: 2.516e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.810435\n",
      "adaptive_constant_bc_val: 1.189565\n",
      "It: 12100, Loss: 2.017e+00, Loss_bcs: 3.853e-01, Loss_res: 1.924e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.850540\n",
      "adaptive_constant_bc_val: 1.149460\n",
      "It: 12200, Loss: 1.618e+00, Loss_bcs: 3.839e-01, Loss_res: 1.384e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.742994\n",
      "adaptive_constant_bc_val: 1.257006\n",
      "It: 12300, Loss: 1.923e+00, Loss_bcs: 4.040e-01, Loss_res: 1.904e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.848167\n",
      "adaptive_constant_bc_val: 1.151833\n",
      "It: 12400, Loss: 1.520e+00, Loss_bcs: 3.839e-01, Loss_res: 1.271e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.717072\n",
      "adaptive_constant_bc_val: 1.282928\n",
      "It: 12500, Loss: 2.588e+00, Loss_bcs: 3.308e-01, Loss_res: 3.017e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 1.044522\n",
      "adaptive_constant_bc_val: 0.955478\n",
      "It: 12600, Loss: 2.751e+00, Loss_bcs: 3.668e-01, Loss_res: 2.298e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.904987\n",
      "adaptive_constant_bc_val: 1.095013\n",
      "It: 12700, Loss: 2.920e+00, Loss_bcs: 3.945e-01, Loss_res: 2.749e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.990525\n",
      "adaptive_constant_bc_val: 1.009475\n",
      "It: 12800, Loss: 2.560e+00, Loss_bcs: 3.899e-01, Loss_res: 2.187e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.934807\n",
      "adaptive_constant_bc_val: 1.065193\n",
      "It: 12900, Loss: 2.088e+00, Loss_bcs: 3.927e-01, Loss_res: 1.786e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.751156\n",
      "adaptive_constant_bc_val: 1.248844\n",
      "It: 13000, Loss: 1.846e+00, Loss_bcs: 3.525e-01, Loss_res: 1.872e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.769832\n",
      "adaptive_constant_bc_val: 1.230168\n",
      "It: 13100, Loss: 1.584e+00, Loss_bcs: 3.726e-01, Loss_res: 1.463e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.759188\n",
      "adaptive_constant_bc_val: 1.240812\n",
      "It: 13200, Loss: 1.446e+00, Loss_bcs: 3.992e-01, Loss_res: 1.252e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.571192\n",
      "adaptive_constant_bc_val: 1.428808\n",
      "It: 13300, Loss: 1.403e+00, Loss_bcs: 3.456e-01, Loss_res: 1.591e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.679078\n",
      "adaptive_constant_bc_val: 1.320922\n",
      "It: 13400, Loss: 1.659e+00, Loss_bcs: 3.671e-01, Loss_res: 1.729e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.741549\n",
      "adaptive_constant_bc_val: 1.258451\n",
      "It: 13500, Loss: 1.831e+00, Loss_bcs: 3.375e-01, Loss_res: 1.896e+00,Time: 0.05\n",
      "adaptive_constant_res_val: 0.778400\n",
      "adaptive_constant_bc_val: 1.221600\n",
      "It: 13600, Loss: 1.638e+00, Loss_bcs: 3.194e-01, Loss_res: 1.603e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.736958\n",
      "adaptive_constant_bc_val: 1.263042\n",
      "It: 13700, Loss: 1.552e+00, Loss_bcs: 3.530e-01, Loss_res: 1.501e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.730410\n",
      "adaptive_constant_bc_val: 1.269590\n",
      "It: 13800, Loss: 1.546e+00, Loss_bcs: 3.696e-01, Loss_res: 1.474e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.827716\n",
      "adaptive_constant_bc_val: 1.172284\n",
      "It: 13900, Loss: 2.372e+00, Loss_bcs: 3.506e-01, Loss_res: 2.369e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.876342\n",
      "adaptive_constant_bc_val: 1.123658\n",
      "It: 14000, Loss: 1.661e+00, Loss_bcs: 3.250e-01, Loss_res: 1.479e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.784658\n",
      "adaptive_constant_bc_val: 1.215342\n",
      "It: 14100, Loss: 1.517e+00, Loss_bcs: 3.311e-01, Loss_res: 1.420e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.785079\n",
      "adaptive_constant_bc_val: 1.214921\n",
      "It: 14200, Loss: 1.804e+00, Loss_bcs: 3.364e-01, Loss_res: 1.777e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.942593\n",
      "adaptive_constant_bc_val: 1.057407\n",
      "It: 14300, Loss: 1.888e+00, Loss_bcs: 3.309e-01, Loss_res: 1.632e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.814643\n",
      "adaptive_constant_bc_val: 1.185357\n",
      "It: 14400, Loss: 1.678e+00, Loss_bcs: 3.414e-01, Loss_res: 1.563e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.817824\n",
      "adaptive_constant_bc_val: 1.182176\n",
      "It: 14500, Loss: 1.708e+00, Loss_bcs: 3.316e-01, Loss_res: 1.609e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.843596\n",
      "adaptive_constant_bc_val: 1.156404\n",
      "It: 14600, Loss: 2.056e+00, Loss_bcs: 3.895e-01, Loss_res: 1.904e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.709945\n",
      "adaptive_constant_bc_val: 1.290055\n",
      "It: 14700, Loss: 1.390e+00, Loss_bcs: 3.537e-01, Loss_res: 1.315e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.680589\n",
      "adaptive_constant_bc_val: 1.319411\n",
      "It: 14800, Loss: 1.616e+00, Loss_bcs: 3.343e-01, Loss_res: 1.726e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.773213\n",
      "adaptive_constant_bc_val: 1.226787\n",
      "It: 14900, Loss: 1.567e+00, Loss_bcs: 3.451e-01, Loss_res: 1.479e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.836080\n",
      "adaptive_constant_bc_val: 1.163920\n",
      "It: 15000, Loss: 2.503e+00, Loss_bcs: 3.329e-01, Loss_res: 2.531e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.935605\n",
      "adaptive_constant_bc_val: 1.064395\n",
      "It: 15100, Loss: 1.335e+00, Loss_bcs: 3.287e-01, Loss_res: 1.053e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.720121\n",
      "adaptive_constant_bc_val: 1.279879\n",
      "It: 15200, Loss: 1.229e+00, Loss_bcs: 3.085e-01, Loss_res: 1.159e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.684172\n",
      "adaptive_constant_bc_val: 1.315828\n",
      "It: 15300, Loss: 1.505e+00, Loss_bcs: 3.611e-01, Loss_res: 1.506e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.735709\n",
      "adaptive_constant_bc_val: 1.264291\n",
      "It: 15400, Loss: 1.497e+00, Loss_bcs: 3.300e-01, Loss_res: 1.467e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.994891\n",
      "adaptive_constant_bc_val: 1.005109\n",
      "It: 15500, Loss: 2.132e+00, Loss_bcs: 3.475e-01, Loss_res: 1.792e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.870898\n",
      "adaptive_constant_bc_val: 1.129102\n",
      "It: 15600, Loss: 1.593e+00, Loss_bcs: 3.307e-01, Loss_res: 1.400e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.851635\n",
      "adaptive_constant_bc_val: 1.148365\n",
      "It: 15700, Loss: 1.498e+00, Loss_bcs: 2.962e-01, Loss_res: 1.360e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.809965\n",
      "adaptive_constant_bc_val: 1.190035\n",
      "It: 15800, Loss: 1.098e+00, Loss_bcs: 3.290e-01, Loss_res: 8.717e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.737414\n",
      "adaptive_constant_bc_val: 1.262586\n",
      "It: 15900, Loss: 1.690e+00, Loss_bcs: 3.433e-01, Loss_res: 1.705e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.727416\n",
      "adaptive_constant_bc_val: 1.272584\n",
      "It: 16000, Loss: 1.115e+00, Loss_bcs: 3.439e-01, Loss_res: 9.314e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.650410\n",
      "adaptive_constant_bc_val: 1.349590\n",
      "It: 16100, Loss: 1.126e+00, Loss_bcs: 3.324e-01, Loss_res: 1.041e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.640635\n",
      "adaptive_constant_bc_val: 1.359365\n",
      "It: 16200, Loss: 1.202e+00, Loss_bcs: 3.033e-01, Loss_res: 1.232e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.763705\n",
      "adaptive_constant_bc_val: 1.236295\n",
      "It: 16300, Loss: 1.311e+00, Loss_bcs: 3.299e-01, Loss_res: 1.183e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.742870\n",
      "adaptive_constant_bc_val: 1.257130\n",
      "It: 16400, Loss: 1.157e+00, Loss_bcs: 2.999e-01, Loss_res: 1.050e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.842447\n",
      "adaptive_constant_bc_val: 1.157553\n",
      "It: 16500, Loss: 1.702e+00, Loss_bcs: 2.883e-01, Loss_res: 1.625e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.820080\n",
      "adaptive_constant_bc_val: 1.179920\n",
      "It: 16600, Loss: 1.481e+00, Loss_bcs: 3.161e-01, Loss_res: 1.351e+00,Time: 0.07\n",
      "adaptive_constant_res_val: 0.872499\n",
      "adaptive_constant_bc_val: 1.127501\n",
      "It: 16700, Loss: 1.417e+00, Loss_bcs: 3.203e-01, Loss_res: 1.210e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.809213\n",
      "adaptive_constant_bc_val: 1.190787\n",
      "It: 16800, Loss: 1.662e+00, Loss_bcs: 3.204e-01, Loss_res: 1.583e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.919229\n",
      "adaptive_constant_bc_val: 1.080771\n",
      "It: 16900, Loss: 1.587e+00, Loss_bcs: 2.883e-01, Loss_res: 1.387e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.965609\n",
      "adaptive_constant_bc_val: 1.034391\n",
      "It: 17000, Loss: 1.502e+00, Loss_bcs: 2.897e-01, Loss_res: 1.246e+00,Time: 0.05\n",
      "adaptive_constant_res_val: 0.752873\n",
      "adaptive_constant_bc_val: 1.247127\n",
      "It: 17100, Loss: 1.397e+00, Loss_bcs: 3.228e-01, Loss_res: 1.321e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.870440\n",
      "adaptive_constant_bc_val: 1.129560\n",
      "It: 17200, Loss: 9.746e-01, Loss_bcs: 3.232e-01, Loss_res: 7.003e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.713534\n",
      "adaptive_constant_bc_val: 1.286466\n",
      "It: 17300, Loss: 1.129e+00, Loss_bcs: 3.277e-01, Loss_res: 9.912e-01,Time: 0.05\n",
      "adaptive_constant_res_val: 0.716241\n",
      "adaptive_constant_bc_val: 1.283759\n",
      "It: 17400, Loss: 1.044e+00, Loss_bcs: 2.906e-01, Loss_res: 9.366e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.800764\n",
      "adaptive_constant_bc_val: 1.199236\n",
      "It: 17500, Loss: 1.219e+00, Loss_bcs: 2.968e-01, Loss_res: 1.078e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.833852\n",
      "adaptive_constant_bc_val: 1.166148\n",
      "It: 17600, Loss: 1.329e+00, Loss_bcs: 2.901e-01, Loss_res: 1.189e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.850235\n",
      "adaptive_constant_bc_val: 1.149765\n",
      "It: 17700, Loss: 1.464e+00, Loss_bcs: 3.115e-01, Loss_res: 1.301e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.879727\n",
      "adaptive_constant_bc_val: 1.120273\n",
      "It: 17800, Loss: 1.526e+00, Loss_bcs: 2.989e-01, Loss_res: 1.353e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.933031\n",
      "adaptive_constant_bc_val: 1.066969\n",
      "It: 17900, Loss: 1.367e+00, Loss_bcs: 3.197e-01, Loss_res: 1.100e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.780842\n",
      "adaptive_constant_bc_val: 1.219158\n",
      "It: 18000, Loss: 1.284e+00, Loss_bcs: 2.713e-01, Loss_res: 1.220e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.863544\n",
      "adaptive_constant_bc_val: 1.136456\n",
      "It: 18100, Loss: 1.244e+00, Loss_bcs: 2.841e-01, Loss_res: 1.067e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.860859\n",
      "adaptive_constant_bc_val: 1.139141\n",
      "It: 18200, Loss: 1.359e+00, Loss_bcs: 2.781e-01, Loss_res: 1.211e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.888517\n",
      "adaptive_constant_bc_val: 1.111483\n",
      "It: 18300, Loss: 1.939e+00, Loss_bcs: 2.985e-01, Loss_res: 1.809e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.950700\n",
      "adaptive_constant_bc_val: 1.049300\n",
      "It: 18400, Loss: 1.168e+00, Loss_bcs: 2.886e-01, Loss_res: 9.098e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.840901\n",
      "adaptive_constant_bc_val: 1.159099\n",
      "It: 18500, Loss: 1.465e+00, Loss_bcs: 2.655e-01, Loss_res: 1.376e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.961632\n",
      "adaptive_constant_bc_val: 1.038368\n",
      "It: 18600, Loss: 1.689e+00, Loss_bcs: 2.938e-01, Loss_res: 1.440e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 1.021956\n",
      "adaptive_constant_bc_val: 0.978044\n",
      "It: 18700, Loss: 1.615e+00, Loss_bcs: 3.095e-01, Loss_res: 1.284e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.825658\n",
      "adaptive_constant_bc_val: 1.174342\n",
      "It: 18800, Loss: 1.320e+00, Loss_bcs: 2.765e-01, Loss_res: 1.205e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.935924\n",
      "adaptive_constant_bc_val: 1.064076\n",
      "It: 18900, Loss: 1.374e+00, Loss_bcs: 3.333e-01, Loss_res: 1.089e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.772752\n",
      "adaptive_constant_bc_val: 1.227248\n",
      "It: 19000, Loss: 1.095e+00, Loss_bcs: 2.965e-01, Loss_res: 9.455e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.771472\n",
      "adaptive_constant_bc_val: 1.228528\n",
      "It: 19100, Loss: 1.089e+00, Loss_bcs: 2.940e-01, Loss_res: 9.441e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.724408\n",
      "adaptive_constant_bc_val: 1.275592\n",
      "It: 19200, Loss: 1.526e+00, Loss_bcs: 3.170e-01, Loss_res: 1.548e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.983472\n",
      "adaptive_constant_bc_val: 1.016528\n",
      "It: 19300, Loss: 1.105e+00, Loss_bcs: 3.003e-01, Loss_res: 8.132e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.790668\n",
      "adaptive_constant_bc_val: 1.209332\n",
      "It: 19400, Loss: 1.175e+00, Loss_bcs: 2.866e-01, Loss_res: 1.048e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.866540\n",
      "adaptive_constant_bc_val: 1.133460\n",
      "It: 19500, Loss: 1.494e+00, Loss_bcs: 3.001e-01, Loss_res: 1.331e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.812490\n",
      "adaptive_constant_bc_val: 1.187510\n",
      "It: 19600, Loss: 1.363e+00, Loss_bcs: 2.921e-01, Loss_res: 1.251e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.905873\n",
      "adaptive_constant_bc_val: 1.094127\n",
      "It: 19700, Loss: 1.472e+00, Loss_bcs: 3.055e-01, Loss_res: 1.255e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.980233\n",
      "adaptive_constant_bc_val: 1.019767\n",
      "It: 19800, Loss: 1.117e+00, Loss_bcs: 2.985e-01, Loss_res: 8.290e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.721622\n",
      "adaptive_constant_bc_val: 1.278378\n",
      "It: 19900, Loss: 1.304e+00, Loss_bcs: 2.896e-01, Loss_res: 1.294e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.988940\n",
      "adaptive_constant_bc_val: 1.011060\n",
      "It: 20000, Loss: 9.862e-01, Loss_bcs: 2.772e-01, Loss_res: 7.138e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.912066\n",
      "adaptive_constant_bc_val: 1.087934\n",
      "It: 20100, Loss: 1.190e+00, Loss_bcs: 2.726e-01, Loss_res: 9.798e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.808067\n",
      "adaptive_constant_bc_val: 1.191933\n",
      "It: 20200, Loss: 9.824e-01, Loss_bcs: 2.959e-01, Loss_res: 7.793e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.815181\n",
      "adaptive_constant_bc_val: 1.184819\n",
      "It: 20300, Loss: 1.270e+00, Loss_bcs: 2.782e-01, Loss_res: 1.154e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 1.034813\n",
      "adaptive_constant_bc_val: 0.965187\n",
      "It: 20400, Loss: 1.484e+00, Loss_bcs: 2.980e-01, Loss_res: 1.156e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.814665\n",
      "adaptive_constant_bc_val: 1.185335\n",
      "It: 20500, Loss: 1.157e+00, Loss_bcs: 2.811e-01, Loss_res: 1.011e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 0.842691\n",
      "adaptive_constant_bc_val: 1.157309\n",
      "It: 20600, Loss: 1.025e+00, Loss_bcs: 2.942e-01, Loss_res: 8.129e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.752078\n",
      "adaptive_constant_bc_val: 1.247922\n",
      "It: 20700, Loss: 1.166e+00, Loss_bcs: 2.859e-01, Loss_res: 1.077e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.963620\n",
      "adaptive_constant_bc_val: 1.036380\n",
      "It: 20800, Loss: 1.152e+00, Loss_bcs: 3.006e-01, Loss_res: 8.726e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.772922\n",
      "adaptive_constant_bc_val: 1.227078\n",
      "It: 20900, Loss: 1.117e+00, Loss_bcs: 2.806e-01, Loss_res: 1.000e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.785166\n",
      "adaptive_constant_bc_val: 1.214834\n",
      "It: 21000, Loss: 1.379e+00, Loss_bcs: 3.000e-01, Loss_res: 1.292e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.942409\n",
      "adaptive_constant_bc_val: 1.057591\n",
      "It: 21100, Loss: 1.409e+00, Loss_bcs: 2.568e-01, Loss_res: 1.207e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 1.072920\n",
      "adaptive_constant_bc_val: 0.927080\n",
      "It: 21200, Loss: 1.268e+00, Loss_bcs: 2.976e-01, Loss_res: 9.249e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.999476\n",
      "adaptive_constant_bc_val: 1.000524\n",
      "It: 21300, Loss: 8.879e-01, Loss_bcs: 2.926e-01, Loss_res: 5.955e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.748765\n",
      "adaptive_constant_bc_val: 1.251235\n",
      "It: 21400, Loss: 8.351e-01, Loss_bcs: 3.058e-01, Loss_res: 6.043e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.705839\n",
      "adaptive_constant_bc_val: 1.294161\n",
      "It: 21500, Loss: 7.931e-01, Loss_bcs: 2.689e-01, Loss_res: 6.307e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.848552\n",
      "adaptive_constant_bc_val: 1.151448\n",
      "It: 21600, Loss: 1.092e+00, Loss_bcs: 2.760e-01, Loss_res: 9.119e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.956321\n",
      "adaptive_constant_bc_val: 1.043679\n",
      "It: 21700, Loss: 1.267e+00, Loss_bcs: 2.922e-01, Loss_res: 1.006e+00,Time: 0.03\n",
      "adaptive_constant_res_val: 0.860786\n",
      "adaptive_constant_bc_val: 1.139214\n",
      "It: 21800, Loss: 1.045e+00, Loss_bcs: 2.882e-01, Loss_res: 8.328e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.920039\n",
      "adaptive_constant_bc_val: 1.079961\n",
      "It: 21900, Loss: 1.059e+00, Loss_bcs: 2.627e-01, Loss_res: 8.423e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.914291\n",
      "adaptive_constant_bc_val: 1.085709\n",
      "It: 22000, Loss: 1.350e+00, Loss_bcs: 3.007e-01, Loss_res: 1.119e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.989804\n",
      "adaptive_constant_bc_val: 1.010196\n",
      "It: 22100, Loss: 1.807e+00, Loss_bcs: 2.935e-01, Loss_res: 1.526e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 1.016759\n",
      "adaptive_constant_bc_val: 0.983241\n",
      "It: 22200, Loss: 1.590e+00, Loss_bcs: 2.846e-01, Loss_res: 1.289e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.971370\n",
      "adaptive_constant_bc_val: 1.028630\n",
      "It: 22300, Loss: 1.071e+00, Loss_bcs: 2.949e-01, Loss_res: 7.899e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.759296\n",
      "adaptive_constant_bc_val: 1.240704\n",
      "It: 22400, Loss: 1.107e+00, Loss_bcs: 2.891e-01, Loss_res: 9.854e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 1.068520\n",
      "adaptive_constant_bc_val: 0.931480\n",
      "It: 22500, Loss: 1.446e+00, Loss_bcs: 2.752e-01, Loss_res: 1.114e+00,Time: 0.06\n",
      "adaptive_constant_res_val: 1.038189\n",
      "adaptive_constant_bc_val: 0.961811\n",
      "It: 22600, Loss: 1.448e+00, Loss_bcs: 2.767e-01, Loss_res: 1.138e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 1.071935\n",
      "adaptive_constant_bc_val: 0.928065\n",
      "It: 22700, Loss: 1.528e+00, Loss_bcs: 2.858e-01, Loss_res: 1.178e+00,Time: 0.02\n",
      "adaptive_constant_res_val: 1.001522\n",
      "adaptive_constant_bc_val: 0.998478\n",
      "It: 22800, Loss: 1.277e+00, Loss_bcs: 2.861e-01, Loss_res: 9.898e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.852465\n",
      "adaptive_constant_bc_val: 1.147535\n",
      "It: 22900, Loss: 9.225e-01, Loss_bcs: 3.004e-01, Loss_res: 6.778e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.739037\n",
      "adaptive_constant_bc_val: 1.260963\n",
      "It: 23000, Loss: 9.533e-01, Loss_bcs: 2.771e-01, Loss_res: 8.170e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.806518\n",
      "adaptive_constant_bc_val: 1.193482\n",
      "It: 23100, Loss: 9.261e-01, Loss_bcs: 2.443e-01, Loss_res: 7.868e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.923201\n",
      "adaptive_constant_bc_val: 1.076799\n",
      "It: 23200, Loss: 9.082e-01, Loss_bcs: 2.629e-01, Loss_res: 6.772e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.782960\n",
      "adaptive_constant_bc_val: 1.217040\n",
      "It: 23300, Loss: 8.792e-01, Loss_bcs: 2.683e-01, Loss_res: 7.059e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.883341\n",
      "adaptive_constant_bc_val: 1.116659\n",
      "It: 23400, Loss: 1.004e+00, Loss_bcs: 2.733e-01, Loss_res: 7.913e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.744512\n",
      "adaptive_constant_bc_val: 1.255488\n",
      "It: 23500, Loss: 8.998e-01, Loss_bcs: 2.473e-01, Loss_res: 7.915e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.812848\n",
      "adaptive_constant_bc_val: 1.187152\n",
      "It: 23600, Loss: 8.654e-01, Loss_bcs: 2.787e-01, Loss_res: 6.576e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.689292\n",
      "adaptive_constant_bc_val: 1.310708\n",
      "It: 23700, Loss: 7.776e-01, Loss_bcs: 2.701e-01, Loss_res: 6.145e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.695129\n",
      "adaptive_constant_bc_val: 1.304871\n",
      "It: 23800, Loss: 7.052e-01, Loss_bcs: 2.859e-01, Loss_res: 4.779e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.756766\n",
      "adaptive_constant_bc_val: 1.243234\n",
      "It: 23900, Loss: 1.056e+00, Loss_bcs: 2.801e-01, Loss_res: 9.357e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.751376\n",
      "adaptive_constant_bc_val: 1.248624\n",
      "It: 24000, Loss: 1.243e+00, Loss_bcs: 2.907e-01, Loss_res: 1.171e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.880708\n",
      "adaptive_constant_bc_val: 1.119292\n",
      "It: 24100, Loss: 9.206e-01, Loss_bcs: 2.854e-01, Loss_res: 6.826e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.791904\n",
      "adaptive_constant_bc_val: 1.208096\n",
      "It: 24200, Loss: 1.023e+00, Loss_bcs: 2.636e-01, Loss_res: 8.898e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.924666\n",
      "adaptive_constant_bc_val: 1.075334\n",
      "It: 24300, Loss: 1.194e+00, Loss_bcs: 2.894e-01, Loss_res: 9.548e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.850595\n",
      "adaptive_constant_bc_val: 1.149405\n",
      "It: 24400, Loss: 8.532e-01, Loss_bcs: 2.591e-01, Loss_res: 6.529e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.756955\n",
      "adaptive_constant_bc_val: 1.243045\n",
      "It: 24500, Loss: 1.009e+00, Loss_bcs: 2.594e-01, Loss_res: 9.066e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.925170\n",
      "adaptive_constant_bc_val: 1.074830\n",
      "It: 24600, Loss: 1.056e+00, Loss_bcs: 2.653e-01, Loss_res: 8.328e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.975701\n",
      "adaptive_constant_bc_val: 1.024299\n",
      "It: 24700, Loss: 1.086e+00, Loss_bcs: 2.675e-01, Loss_res: 8.324e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 1.001038\n",
      "adaptive_constant_bc_val: 0.998962\n",
      "It: 24800, Loss: 1.440e+00, Loss_bcs: 2.836e-01, Loss_res: 1.155e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 1.095385\n",
      "adaptive_constant_bc_val: 0.904615\n",
      "It: 24900, Loss: 9.293e-01, Loss_bcs: 2.787e-01, Loss_res: 6.182e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.974024\n",
      "adaptive_constant_bc_val: 1.025976\n",
      "It: 25000, Loss: 1.084e+00, Loss_bcs: 2.603e-01, Loss_res: 8.386e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.950553\n",
      "adaptive_constant_bc_val: 1.049447\n",
      "It: 25100, Loss: 9.195e-01, Loss_bcs: 2.851e-01, Loss_res: 6.526e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.762148\n",
      "adaptive_constant_bc_val: 1.237852\n",
      "It: 25200, Loss: 9.607e-01, Loss_bcs: 2.718e-01, Loss_res: 8.190e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.715722\n",
      "adaptive_constant_bc_val: 1.284278\n",
      "It: 25300, Loss: 7.139e-01, Loss_bcs: 2.551e-01, Loss_res: 5.397e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.778117\n",
      "adaptive_constant_bc_val: 1.221883\n",
      "It: 25400, Loss: 8.459e-01, Loss_bcs: 2.602e-01, Loss_res: 6.786e-01,Time: 0.10\n",
      "adaptive_constant_res_val: 0.903319\n",
      "adaptive_constant_bc_val: 1.096681\n",
      "It: 25500, Loss: 8.389e-01, Loss_bcs: 2.739e-01, Loss_res: 5.961e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.767669\n",
      "adaptive_constant_bc_val: 1.232331\n",
      "It: 25600, Loss: 1.069e+00, Loss_bcs: 2.895e-01, Loss_res: 9.276e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.844512\n",
      "adaptive_constant_bc_val: 1.155488\n",
      "It: 25700, Loss: 1.041e+00, Loss_bcs: 2.593e-01, Loss_res: 8.776e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.910790\n",
      "adaptive_constant_bc_val: 1.089210\n",
      "It: 25800, Loss: 9.183e-01, Loss_bcs: 2.851e-01, Loss_res: 6.673e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.749526\n",
      "adaptive_constant_bc_val: 1.250474\n",
      "It: 25900, Loss: 8.543e-01, Loss_bcs: 2.659e-01, Loss_res: 6.962e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.886365\n",
      "adaptive_constant_bc_val: 1.113635\n",
      "It: 26000, Loss: 8.945e-01, Loss_bcs: 2.779e-01, Loss_res: 6.600e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.758655\n",
      "adaptive_constant_bc_val: 1.241345\n",
      "It: 26100, Loss: 9.517e-01, Loss_bcs: 3.026e-01, Loss_res: 7.593e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.800923\n",
      "adaptive_constant_bc_val: 1.199077\n",
      "It: 26200, Loss: 9.768e-01, Loss_bcs: 2.697e-01, Loss_res: 8.159e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.839428\n",
      "adaptive_constant_bc_val: 1.160572\n",
      "It: 26300, Loss: 1.018e+00, Loss_bcs: 2.612e-01, Loss_res: 8.515e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.941000\n",
      "adaptive_constant_bc_val: 1.059000\n",
      "It: 26400, Loss: 8.810e-01, Loss_bcs: 2.823e-01, Loss_res: 6.186e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.831131\n",
      "adaptive_constant_bc_val: 1.168869\n",
      "It: 26500, Loss: 8.279e-01, Loss_bcs: 2.601e-01, Loss_res: 6.303e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.756855\n",
      "adaptive_constant_bc_val: 1.243145\n",
      "It: 26600, Loss: 7.548e-01, Loss_bcs: 2.429e-01, Loss_res: 5.983e-01,Time: 0.09\n",
      "adaptive_constant_res_val: 0.842458\n",
      "adaptive_constant_bc_val: 1.157542\n",
      "It: 26700, Loss: 1.094e+00, Loss_bcs: 2.628e-01, Loss_res: 9.375e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.840826\n",
      "adaptive_constant_bc_val: 1.159174\n",
      "It: 26800, Loss: 8.048e-01, Loss_bcs: 2.453e-01, Loss_res: 6.191e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.852050\n",
      "adaptive_constant_bc_val: 1.147950\n",
      "It: 26900, Loss: 7.474e-01, Loss_bcs: 2.703e-01, Loss_res: 5.129e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.738844\n",
      "adaptive_constant_bc_val: 1.261156\n",
      "It: 27000, Loss: 9.981e-01, Loss_bcs: 2.635e-01, Loss_res: 9.011e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.867038\n",
      "adaptive_constant_bc_val: 1.132962\n",
      "It: 27100, Loss: 8.549e-01, Loss_bcs: 2.687e-01, Loss_res: 6.348e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.789057\n",
      "adaptive_constant_bc_val: 1.210943\n",
      "It: 27200, Loss: 1.011e+00, Loss_bcs: 2.645e-01, Loss_res: 8.752e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 1.009288\n",
      "adaptive_constant_bc_val: 0.990712\n",
      "It: 27300, Loss: 1.028e+00, Loss_bcs: 2.426e-01, Loss_res: 7.800e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.939176\n",
      "adaptive_constant_bc_val: 1.060824\n",
      "It: 27400, Loss: 9.473e-01, Loss_bcs: 2.522e-01, Loss_res: 7.237e-01,Time: 0.08\n",
      "adaptive_constant_res_val: 0.843750\n",
      "adaptive_constant_bc_val: 1.156250\n",
      "It: 27500, Loss: 9.768e-01, Loss_bcs: 2.801e-01, Loss_res: 7.738e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.701670\n",
      "adaptive_constant_bc_val: 1.298330\n",
      "It: 27600, Loss: 8.148e-01, Loss_bcs: 2.679e-01, Loss_res: 6.655e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.866102\n",
      "adaptive_constant_bc_val: 1.133898\n",
      "It: 27700, Loss: 1.030e+00, Loss_bcs: 2.703e-01, Loss_res: 8.359e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.854146\n",
      "adaptive_constant_bc_val: 1.145854\n",
      "It: 27800, Loss: 7.459e-01, Loss_bcs: 2.628e-01, Loss_res: 5.208e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.743708\n",
      "adaptive_constant_bc_val: 1.256292\n",
      "It: 27900, Loss: 9.439e-01, Loss_bcs: 2.835e-01, Loss_res: 7.902e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.797951\n",
      "adaptive_constant_bc_val: 1.202049\n",
      "It: 28000, Loss: 9.984e-01, Loss_bcs: 2.511e-01, Loss_res: 8.729e-01,Time: 0.12\n",
      "adaptive_constant_res_val: 0.789457\n",
      "adaptive_constant_bc_val: 1.210543\n",
      "It: 28100, Loss: 1.080e+00, Loss_bcs: 2.845e-01, Loss_res: 9.314e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.841209\n",
      "adaptive_constant_bc_val: 1.158791\n",
      "It: 28200, Loss: 9.236e-01, Loss_bcs: 2.618e-01, Loss_res: 7.373e-01,Time: 0.05\n",
      "adaptive_constant_res_val: 0.767323\n",
      "adaptive_constant_bc_val: 1.232677\n",
      "It: 28300, Loss: 7.928e-01, Loss_bcs: 2.341e-01, Loss_res: 6.571e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.733492\n",
      "adaptive_constant_bc_val: 1.266508\n",
      "It: 28400, Loss: 7.230e-01, Loss_bcs: 2.727e-01, Loss_res: 5.150e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.710899\n",
      "adaptive_constant_bc_val: 1.289101\n",
      "It: 28500, Loss: 8.317e-01, Loss_bcs: 2.736e-01, Loss_res: 6.739e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.767910\n",
      "adaptive_constant_bc_val: 1.232090\n",
      "It: 28600, Loss: 9.438e-01, Loss_bcs: 2.642e-01, Loss_res: 8.051e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.868914\n",
      "adaptive_constant_bc_val: 1.131086\n",
      "It: 28700, Loss: 1.018e+00, Loss_bcs: 2.783e-01, Loss_res: 8.095e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 1.044454\n",
      "adaptive_constant_bc_val: 0.955546\n",
      "It: 28800, Loss: 9.677e-01, Loss_bcs: 2.828e-01, Loss_res: 6.678e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.876244\n",
      "adaptive_constant_bc_val: 1.123756\n",
      "It: 28900, Loss: 9.542e-01, Loss_bcs: 2.576e-01, Loss_res: 7.586e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.767657\n",
      "adaptive_constant_bc_val: 1.232343\n",
      "It: 29000, Loss: 6.937e-01, Loss_bcs: 2.546e-01, Loss_res: 4.950e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.755894\n",
      "adaptive_constant_bc_val: 1.244106\n",
      "It: 29100, Loss: 1.057e+00, Loss_bcs: 2.512e-01, Loss_res: 9.844e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.919501\n",
      "adaptive_constant_bc_val: 1.080499\n",
      "It: 29200, Loss: 9.671e-01, Loss_bcs: 2.710e-01, Loss_res: 7.333e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 1.018671\n",
      "adaptive_constant_bc_val: 0.981329\n",
      "It: 29300, Loss: 9.800e-01, Loss_bcs: 2.543e-01, Loss_res: 7.170e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.967451\n",
      "adaptive_constant_bc_val: 1.032549\n",
      "It: 29400, Loss: 7.966e-01, Loss_bcs: 2.657e-01, Loss_res: 5.398e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.760312\n",
      "adaptive_constant_bc_val: 1.239688\n",
      "It: 29500, Loss: 1.003e+00, Loss_bcs: 2.736e-01, Loss_res: 8.727e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.823779\n",
      "adaptive_constant_bc_val: 1.176221\n",
      "It: 29600, Loss: 9.455e-01, Loss_bcs: 2.484e-01, Loss_res: 7.930e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.964130\n",
      "adaptive_constant_bc_val: 1.035870\n",
      "It: 29700, Loss: 9.582e-01, Loss_bcs: 2.252e-01, Loss_res: 7.519e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.977588\n",
      "adaptive_constant_bc_val: 1.022412\n",
      "It: 29800, Loss: 1.050e+00, Loss_bcs: 2.510e-01, Loss_res: 8.120e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 1.035034\n",
      "adaptive_constant_bc_val: 0.964966\n",
      "It: 29900, Loss: 1.060e+00, Loss_bcs: 2.731e-01, Loss_res: 7.693e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.913389\n",
      "adaptive_constant_bc_val: 1.086611\n",
      "It: 30000, Loss: 7.084e-01, Loss_bcs: 2.511e-01, Loss_res: 4.768e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.818806\n",
      "adaptive_constant_bc_val: 1.181194\n",
      "It: 30100, Loss: 8.199e-01, Loss_bcs: 2.457e-01, Loss_res: 6.470e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.829011\n",
      "adaptive_constant_bc_val: 1.170989\n",
      "It: 30200, Loss: 7.826e-01, Loss_bcs: 2.455e-01, Loss_res: 5.973e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.741044\n",
      "adaptive_constant_bc_val: 1.258956\n",
      "It: 30300, Loss: 6.435e-01, Loss_bcs: 2.456e-01, Loss_res: 4.511e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.701019\n",
      "adaptive_constant_bc_val: 1.298981\n",
      "It: 30400, Loss: 7.419e-01, Loss_bcs: 2.548e-01, Loss_res: 5.861e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.883800\n",
      "adaptive_constant_bc_val: 1.116200\n",
      "It: 30500, Loss: 8.693e-01, Loss_bcs: 2.614e-01, Loss_res: 6.535e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.772061\n",
      "adaptive_constant_bc_val: 1.227939\n",
      "It: 30600, Loss: 7.936e-01, Loss_bcs: 2.614e-01, Loss_res: 6.120e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.892943\n",
      "adaptive_constant_bc_val: 1.107057\n",
      "It: 30700, Loss: 9.198e-01, Loss_bcs: 2.690e-01, Loss_res: 6.966e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.770096\n",
      "adaptive_constant_bc_val: 1.229904\n",
      "It: 30800, Loss: 1.026e+00, Loss_bcs: 2.584e-01, Loss_res: 9.200e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.836739\n",
      "adaptive_constant_bc_val: 1.163261\n",
      "It: 30900, Loss: 8.635e-01, Loss_bcs: 2.295e-01, Loss_res: 7.129e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.873505\n",
      "adaptive_constant_bc_val: 1.126495\n",
      "It: 31000, Loss: 7.613e-01, Loss_bcs: 2.643e-01, Loss_res: 5.307e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.684764\n",
      "adaptive_constant_bc_val: 1.315236\n",
      "It: 31100, Loss: 8.081e-01, Loss_bcs: 2.579e-01, Loss_res: 6.848e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.869446\n",
      "adaptive_constant_bc_val: 1.130554\n",
      "It: 31200, Loss: 1.096e+00, Loss_bcs: 2.610e-01, Loss_res: 9.213e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.985267\n",
      "adaptive_constant_bc_val: 1.014733\n",
      "It: 31300, Loss: 1.035e+00, Loss_bcs: 2.660e-01, Loss_res: 7.767e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.903192\n",
      "adaptive_constant_bc_val: 1.096808\n",
      "It: 31400, Loss: 8.269e-01, Loss_bcs: 2.658e-01, Loss_res: 5.928e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.892460\n",
      "adaptive_constant_bc_val: 1.107540\n",
      "It: 31500, Loss: 8.295e-01, Loss_bcs: 2.445e-01, Loss_res: 6.261e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.980530\n",
      "adaptive_constant_bc_val: 1.019470\n",
      "It: 31600, Loss: 1.071e+00, Loss_bcs: 2.755e-01, Loss_res: 8.055e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.905107\n",
      "adaptive_constant_bc_val: 1.094893\n",
      "It: 31700, Loss: 1.056e+00, Loss_bcs: 2.283e-01, Loss_res: 8.902e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 1.035490\n",
      "adaptive_constant_bc_val: 0.964510\n",
      "It: 31800, Loss: 1.070e+00, Loss_bcs: 2.502e-01, Loss_res: 8.002e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 1.069380\n",
      "adaptive_constant_bc_val: 0.930620\n",
      "It: 31900, Loss: 7.666e-01, Loss_bcs: 2.560e-01, Loss_res: 4.940e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.811414\n",
      "adaptive_constant_bc_val: 1.188586\n",
      "It: 32000, Loss: 1.012e+00, Loss_bcs: 2.682e-01, Loss_res: 8.541e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.922455\n",
      "adaptive_constant_bc_val: 1.077545\n",
      "It: 32100, Loss: 6.625e-01, Loss_bcs: 2.356e-01, Loss_res: 4.430e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.820213\n",
      "adaptive_constant_bc_val: 1.179787\n",
      "It: 32200, Loss: 9.394e-01, Loss_bcs: 2.508e-01, Loss_res: 7.846e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.876613\n",
      "adaptive_constant_bc_val: 1.123387\n",
      "It: 32300, Loss: 8.866e-01, Loss_bcs: 2.491e-01, Loss_res: 6.921e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.843457\n",
      "adaptive_constant_bc_val: 1.156543\n",
      "It: 32400, Loss: 1.097e+00, Loss_bcs: 2.546e-01, Loss_res: 9.513e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.848556\n",
      "adaptive_constant_bc_val: 1.151444\n",
      "It: 32500, Loss: 1.042e+00, Loss_bcs: 2.439e-01, Loss_res: 8.976e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.997375\n",
      "adaptive_constant_bc_val: 1.002625\n",
      "It: 32600, Loss: 9.924e-01, Loss_bcs: 2.475e-01, Loss_res: 7.462e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.979188\n",
      "adaptive_constant_bc_val: 1.020812\n",
      "It: 32700, Loss: 8.722e-01, Loss_bcs: 2.517e-01, Loss_res: 6.284e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.795573\n",
      "adaptive_constant_bc_val: 1.204427\n",
      "It: 32800, Loss: 1.298e+00, Loss_bcs: 2.808e-01, Loss_res: 1.207e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 0.793259\n",
      "adaptive_constant_bc_val: 1.206741\n",
      "It: 32900, Loss: 9.269e-01, Loss_bcs: 2.553e-01, Loss_res: 7.802e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 1.076320\n",
      "adaptive_constant_bc_val: 0.923680\n",
      "It: 33000, Loss: 1.423e+00, Loss_bcs: 2.436e-01, Loss_res: 1.113e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 1.101644\n",
      "adaptive_constant_bc_val: 0.898356\n",
      "It: 33100, Loss: 9.107e-01, Loss_bcs: 2.564e-01, Loss_res: 6.176e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.946802\n",
      "adaptive_constant_bc_val: 1.053198\n",
      "It: 33200, Loss: 7.254e-01, Loss_bcs: 2.463e-01, Loss_res: 4.922e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.760549\n",
      "adaptive_constant_bc_val: 1.239451\n",
      "It: 33300, Loss: 8.381e-01, Loss_bcs: 2.429e-01, Loss_res: 7.060e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 1.003543\n",
      "adaptive_constant_bc_val: 0.996457\n",
      "It: 33400, Loss: 8.591e-01, Loss_bcs: 2.515e-01, Loss_res: 6.064e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.854919\n",
      "adaptive_constant_bc_val: 1.145081\n",
      "It: 33500, Loss: 8.816e-01, Loss_bcs: 2.772e-01, Loss_res: 6.599e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.877761\n",
      "adaptive_constant_bc_val: 1.122239\n",
      "It: 33600, Loss: 7.644e-01, Loss_bcs: 2.301e-01, Loss_res: 5.767e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.753351\n",
      "adaptive_constant_bc_val: 1.246649\n",
      "It: 33700, Loss: 1.010e+00, Loss_bcs: 2.443e-01, Loss_res: 9.358e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.897906\n",
      "adaptive_constant_bc_val: 1.102094\n",
      "It: 33800, Loss: 7.161e-01, Loss_bcs: 2.429e-01, Loss_res: 4.994e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.765538\n",
      "adaptive_constant_bc_val: 1.234462\n",
      "It: 33900, Loss: 6.823e-01, Loss_bcs: 2.443e-01, Loss_res: 4.973e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.796621\n",
      "adaptive_constant_bc_val: 1.203379\n",
      "It: 34000, Loss: 1.015e+00, Loss_bcs: 2.453e-01, Loss_res: 9.039e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 1.016551\n",
      "adaptive_constant_bc_val: 0.983449\n",
      "It: 34100, Loss: 8.375e-01, Loss_bcs: 2.430e-01, Loss_res: 5.888e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.897117\n",
      "adaptive_constant_bc_val: 1.102883\n",
      "It: 34200, Loss: 9.849e-01, Loss_bcs: 2.650e-01, Loss_res: 7.721e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.855854\n",
      "adaptive_constant_bc_val: 1.144146\n",
      "It: 34300, Loss: 8.448e-01, Loss_bcs: 2.449e-01, Loss_res: 6.596e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.802528\n",
      "adaptive_constant_bc_val: 1.197472\n",
      "It: 34400, Loss: 7.055e-01, Loss_bcs: 2.730e-01, Loss_res: 4.717e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.704906\n",
      "adaptive_constant_bc_val: 1.295094\n",
      "It: 34500, Loss: 6.738e-01, Loss_bcs: 2.601e-01, Loss_res: 4.781e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.727823\n",
      "adaptive_constant_bc_val: 1.272177\n",
      "It: 34600, Loss: 8.156e-01, Loss_bcs: 2.609e-01, Loss_res: 6.646e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.868445\n",
      "adaptive_constant_bc_val: 1.131555\n",
      "It: 34700, Loss: 6.440e-01, Loss_bcs: 2.291e-01, Loss_res: 4.430e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.753931\n",
      "adaptive_constant_bc_val: 1.246069\n",
      "It: 34800, Loss: 7.397e-01, Loss_bcs: 2.690e-01, Loss_res: 5.366e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.679920\n",
      "adaptive_constant_bc_val: 1.320080\n",
      "It: 34900, Loss: 8.835e-01, Loss_bcs: 2.358e-01, Loss_res: 8.416e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.871692\n",
      "adaptive_constant_bc_val: 1.128308\n",
      "It: 35000, Loss: 8.796e-01, Loss_bcs: 2.427e-01, Loss_res: 6.948e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 1.024420\n",
      "adaptive_constant_bc_val: 0.975580\n",
      "It: 35100, Loss: 6.946e-01, Loss_bcs: 2.557e-01, Loss_res: 4.346e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.912126\n",
      "adaptive_constant_bc_val: 1.087874\n",
      "It: 35200, Loss: 6.837e-01, Loss_bcs: 2.394e-01, Loss_res: 4.640e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.827287\n",
      "adaptive_constant_bc_val: 1.172713\n",
      "It: 35300, Loss: 7.597e-01, Loss_bcs: 2.351e-01, Loss_res: 5.849e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.880443\n",
      "adaptive_constant_bc_val: 1.119557\n",
      "It: 35400, Loss: 7.806e-01, Loss_bcs: 2.443e-01, Loss_res: 5.760e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.773629\n",
      "adaptive_constant_bc_val: 1.226371\n",
      "It: 35500, Loss: 7.246e-01, Loss_bcs: 2.256e-01, Loss_res: 5.790e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.892837\n",
      "adaptive_constant_bc_val: 1.107163\n",
      "It: 35600, Loss: 8.163e-01, Loss_bcs: 2.631e-01, Loss_res: 5.880e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.902570\n",
      "adaptive_constant_bc_val: 1.097430\n",
      "It: 35700, Loss: 8.187e-01, Loss_bcs: 2.542e-01, Loss_res: 5.980e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.941156\n",
      "adaptive_constant_bc_val: 1.058844\n",
      "It: 35800, Loss: 1.185e+00, Loss_bcs: 2.572e-01, Loss_res: 9.702e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.895836\n",
      "adaptive_constant_bc_val: 1.104164\n",
      "It: 35900, Loss: 9.104e-01, Loss_bcs: 2.425e-01, Loss_res: 7.173e-01,Time: 0.06\n",
      "adaptive_constant_res_val: 1.120957\n",
      "adaptive_constant_bc_val: 0.879043\n",
      "It: 36000, Loss: 8.942e-01, Loss_bcs: 2.625e-01, Loss_res: 5.919e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.839541\n",
      "adaptive_constant_bc_val: 1.160459\n",
      "It: 36100, Loss: 8.540e-01, Loss_bcs: 2.535e-01, Loss_res: 6.668e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.933922\n",
      "adaptive_constant_bc_val: 1.066078\n",
      "It: 36200, Loss: 1.017e+00, Loss_bcs: 2.148e-01, Loss_res: 8.442e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.985744\n",
      "adaptive_constant_bc_val: 1.014256\n",
      "It: 36300, Loss: 8.559e-01, Loss_bcs: 2.615e-01, Loss_res: 5.993e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.814649\n",
      "adaptive_constant_bc_val: 1.185351\n",
      "It: 36400, Loss: 8.443e-01, Loss_bcs: 2.236e-01, Loss_res: 7.110e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.915086\n",
      "adaptive_constant_bc_val: 1.084914\n",
      "It: 36500, Loss: 8.698e-01, Loss_bcs: 2.160e-01, Loss_res: 6.945e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.860643\n",
      "adaptive_constant_bc_val: 1.139357\n",
      "It: 36600, Loss: 9.446e-01, Loss_bcs: 2.516e-01, Loss_res: 7.645e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.986068\n",
      "adaptive_constant_bc_val: 1.013932\n",
      "It: 36700, Loss: 8.692e-01, Loss_bcs: 2.369e-01, Loss_res: 6.379e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 1.008609\n",
      "adaptive_constant_bc_val: 0.991391\n",
      "It: 36800, Loss: 8.430e-01, Loss_bcs: 2.411e-01, Loss_res: 5.988e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.960068\n",
      "adaptive_constant_bc_val: 1.039932\n",
      "It: 36900, Loss: 9.879e-01, Loss_bcs: 2.662e-01, Loss_res: 7.406e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.879533\n",
      "adaptive_constant_bc_val: 1.120467\n",
      "It: 37000, Loss: 5.985e-01, Loss_bcs: 2.423e-01, Loss_res: 3.719e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.828792\n",
      "adaptive_constant_bc_val: 1.171208\n",
      "It: 37100, Loss: 7.456e-01, Loss_bcs: 2.367e-01, Loss_res: 5.652e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.796959\n",
      "adaptive_constant_bc_val: 1.203041\n",
      "It: 37200, Loss: 1.109e+00, Loss_bcs: 2.436e-01, Loss_res: 1.023e+00,Time: 0.04\n",
      "adaptive_constant_res_val: 0.945153\n",
      "adaptive_constant_bc_val: 1.054847\n",
      "It: 37300, Loss: 7.890e-01, Loss_bcs: 2.756e-01, Loss_res: 5.272e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.840561\n",
      "adaptive_constant_bc_val: 1.159439\n",
      "It: 37400, Loss: 8.050e-01, Loss_bcs: 2.596e-01, Loss_res: 5.996e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.848853\n",
      "adaptive_constant_bc_val: 1.151147\n",
      "It: 37500, Loss: 7.536e-01, Loss_bcs: 2.532e-01, Loss_res: 5.444e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.814210\n",
      "adaptive_constant_bc_val: 1.185790\n",
      "It: 37600, Loss: 6.662e-01, Loss_bcs: 2.276e-01, Loss_res: 4.867e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.878635\n",
      "adaptive_constant_bc_val: 1.121365\n",
      "It: 37700, Loss: 1.072e+00, Loss_bcs: 2.403e-01, Loss_res: 9.138e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 1.150961\n",
      "adaptive_constant_bc_val: 0.849039\n",
      "It: 37800, Loss: 1.396e+00, Loss_bcs: 2.611e-01, Loss_res: 1.021e+00,Time: 0.01\n",
      "adaptive_constant_res_val: 1.043456\n",
      "adaptive_constant_bc_val: 0.956544\n",
      "It: 37900, Loss: 1.158e+00, Loss_bcs: 2.562e-01, Loss_res: 8.745e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 1.068040\n",
      "adaptive_constant_bc_val: 0.931960\n",
      "It: 38000, Loss: 8.778e-01, Loss_bcs: 2.568e-01, Loss_res: 5.978e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.918537\n",
      "adaptive_constant_bc_val: 1.081463\n",
      "It: 38100, Loss: 6.741e-01, Loss_bcs: 2.435e-01, Loss_res: 4.471e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.731203\n",
      "adaptive_constant_bc_val: 1.268797\n",
      "It: 38200, Loss: 7.860e-01, Loss_bcs: 2.640e-01, Loss_res: 6.168e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.833798\n",
      "adaptive_constant_bc_val: 1.166202\n",
      "It: 38300, Loss: 7.641e-01, Loss_bcs: 2.526e-01, Loss_res: 5.631e-01,Time: 0.03\n",
      "adaptive_constant_res_val: 0.857590\n",
      "adaptive_constant_bc_val: 1.142410\n",
      "It: 38400, Loss: 8.137e-01, Loss_bcs: 2.448e-01, Loss_res: 6.227e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.875374\n",
      "adaptive_constant_bc_val: 1.124626\n",
      "It: 38500, Loss: 7.097e-01, Loss_bcs: 2.419e-01, Loss_res: 4.999e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.837493\n",
      "adaptive_constant_bc_val: 1.162507\n",
      "It: 38600, Loss: 7.758e-01, Loss_bcs: 2.497e-01, Loss_res: 5.797e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.768361\n",
      "adaptive_constant_bc_val: 1.231639\n",
      "It: 38700, Loss: 8.277e-01, Loss_bcs: 2.539e-01, Loss_res: 6.703e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.725202\n",
      "adaptive_constant_bc_val: 1.274798\n",
      "It: 38800, Loss: 9.389e-01, Loss_bcs: 2.763e-01, Loss_res: 8.089e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.903670\n",
      "adaptive_constant_bc_val: 1.096330\n",
      "It: 38900, Loss: 7.643e-01, Loss_bcs: 2.446e-01, Loss_res: 5.490e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.856696\n",
      "adaptive_constant_bc_val: 1.143304\n",
      "It: 39000, Loss: 6.993e-01, Loss_bcs: 2.387e-01, Loss_res: 4.978e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.829084\n",
      "adaptive_constant_bc_val: 1.170916\n",
      "It: 39100, Loss: 1.066e+00, Loss_bcs: 2.368e-01, Loss_res: 9.517e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.963335\n",
      "adaptive_constant_bc_val: 1.036665\n",
      "It: 39200, Loss: 7.760e-01, Loss_bcs: 2.569e-01, Loss_res: 5.291e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.874404\n",
      "adaptive_constant_bc_val: 1.125596\n",
      "It: 39300, Loss: 8.046e-01, Loss_bcs: 2.650e-01, Loss_res: 5.791e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.692269\n",
      "adaptive_constant_bc_val: 1.307731\n",
      "It: 39400, Loss: 7.910e-01, Loss_bcs: 2.554e-01, Loss_res: 6.602e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.771005\n",
      "adaptive_constant_bc_val: 1.228995\n",
      "It: 39500, Loss: 7.379e-01, Loss_bcs: 2.552e-01, Loss_res: 5.503e-01,Time: 0.02\n",
      "adaptive_constant_res_val: 0.745388\n",
      "adaptive_constant_bc_val: 1.254612\n",
      "It: 39600, Loss: 6.375e-01, Loss_bcs: 2.554e-01, Loss_res: 4.253e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.706900\n",
      "adaptive_constant_bc_val: 1.293100\n",
      "It: 39700, Loss: 1.031e+00, Loss_bcs: 2.702e-01, Loss_res: 9.642e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.820454\n",
      "adaptive_constant_bc_val: 1.179546\n",
      "It: 39800, Loss: 7.725e-01, Loss_bcs: 2.616e-01, Loss_res: 5.654e-01,Time: 0.01\n",
      "adaptive_constant_res_val: 0.751875\n",
      "adaptive_constant_bc_val: 1.248125\n",
      "It: 39900, Loss: 8.878e-01, Loss_bcs: 2.484e-01, Loss_res: 7.685e-01,Time: 0.04\n",
      "adaptive_constant_res_val: 0.912698\n",
      "adaptive_constant_bc_val: 1.087302\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000, Loss: 8.871e-01, Loss_bcs: 2.402e-01, Loss_res: 6.859e-01,Time: 1.21\n",
      "adaptive_constant_res_val: 0.835018\n",
      "adaptive_constant_bc_val: 1.164982\n",
      "Relative L2 error_u: 2.25e-01\n",
      "Relative L2 error_f: 1.11e-02\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-18-2023_02-04-06-549308_relobralo\n",
      "Final loss total loss: 8.923846e-01\n",
      "Final loss loss_res: 6.915486e-01\n",
      "Final loss loss_bcs: 2.402364e-01\n",
      "Final loss loss_bc1: 3.191269e-02\n",
      "Final loss loss_bc2: 3.695214e-02\n",
      "Final loss loss_bc3: 1.278739e-01\n",
      "Final loss loss_bc4: 4.349773e-02\n",
      "average lambda_bc : 1.1938e+00\n",
      "average lambda_res : 8.0621e-01\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:7.7194e+02\n",
      "average of error_u_list:2.2531e-01\n",
      "average of error_v_list:1.1133e-02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "a_1 = 1\n",
    "a_2 = 4\n",
    "\n",
    "# Parameter\n",
    "lam = 1.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[-1.0, -1.0], [1.0, -1.0]])\n",
    "bc2_coords = np.array([[1.0, -1.0], [1.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "bc4_coords = np.array([[-1.0, 1.0], [-1.0, -1.0]])\n",
    "\n",
    "dom_coords = np.array([[-1.0, -1.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "x1 = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x2 = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star, a_1, a_2)\n",
    "f_star = f(X_star, a_1, a_2, lam)\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = None\n",
    "\n",
    "# Define model\n",
    "mode = 'relobralo'            # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False    # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, a_1, a_2, lam), name='Forcing')\n",
    "\n",
    "        # [elapsed, error_u , error_f ,  mode] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , \n",
    "        #                                                                stiff_ratio , X_star ,u_star , f_star , nIter ,bcbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    " #def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                model.trainmb(nIter, batch_size=mbbatch_size )\n",
    "            else:\n",
    "                model.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            f_pred = model.predict_r(X_star)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "            model.plot_lambda()\n",
    "            model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( x1 , x2 , X_star , u_star , u_pred , f_star , f_pred)\n",
    "\n",
    "            model.print(\"average lambda_bc : \" , np.average(model.adpative_constant_bc_log))\n",
    "            model.print(\"average lambda_res : \" ,  np.average(model.adpative_constant_res_log))\n",
    "            # sess.close()  \n",
    "\n",
    "            time_list.append(elapsed)\n",
    "            error_u_list.append(error_u)\n",
    "            error_f_list.append(error_f)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_v_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_f_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_Helmholtz_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1497590327.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_11615/1497590327.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    average of time_list:2.7972e+02\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Method: mini_batch\n",
    "\n",
    "average of time_list:2.7972e+02\n",
    "average of error_u_list:1.4240e-01\n",
    "average of error_v_list:7.3166e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10733/3211256151.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average lambda_res : \"\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madpative_constant_res_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.print(\"average lambda_res : \" ,  np.average(model.adpative_constant_res_log))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21548/2533309064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predicted solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mU_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mF_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u_pred' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot ###\n",
    "\n",
    "# Exact solution & Predicted solution\n",
    "# Exact soluton\n",
    "U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "# Predicted solution\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Exact $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Predicted $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual loss & Boundary loss\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "gradients_res_list = []\n",
    "gradients_bcs_list = []\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "for j in range(num_hidden_layers):\n",
    "    gradient_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradient_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    gradients_res_list.append(gradient_res)\n",
    "    gradients_bcs_list.append(gradient_bcs)\n",
    "\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim([-3.0, 3.0])\n",
    "    ax.set_ylim([0,100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = mode.eigenvalue_log\n",
    "    eigenvalues_bcs_list = mode.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = mode.eigenvalue_res_log\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
