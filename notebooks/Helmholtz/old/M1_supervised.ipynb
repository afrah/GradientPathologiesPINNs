{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "######################################################################################################\n",
    "def u(x, a_1, a_2):\n",
    "    return np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_xx(x, a_1, a_2):\n",
    "    return - (a_1 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_yy(x, a_1, a_2):\n",
    "    return - (a_2 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "# Forcing\n",
    "def f(x, a_1, a_2, lam=1.0):\n",
    "    return u_xx(x, a_1, a_2) + u_yy(x, a_1, a_2) + lam * u(x, a_1, a_2)\n",
    "\n",
    "def operator(u, x1, x2, lam, sigma_x1=1.0, sigma_x2=1.0):\n",
    "    u_x1 = tf.gradients(u, x1)[0] / sigma_x1\n",
    "    u_x2 = tf.gradients(u, x2)[0] / sigma_x2\n",
    "    u_xx1 = tf.gradients(u_x1, x1)[0] / sigma_x1\n",
    "    u_xx2 = tf.gradients(u_x2, x2)[0] / sigma_x2\n",
    "    residual = u_xx1 + u_xx2 + lam * u\n",
    "    return residual\n",
    "#######################################################################################################\n",
    "\n",
    "class Helmholtz2D:\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess):\n",
    "        # Normalization constants\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x1, self.sigma_x1 = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x2, self.sigma_x2 = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Helmoholtz constant\n",
    "        self.lam = tf.constant(lam, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = mode\n",
    "\n",
    "        # Record stiff ratio\n",
    "        # self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_val = np.array(1.0)\n",
    "        self.adaptive_constant_r_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # if mode in ['M3', 'M4']:\n",
    "        #     # Initialize encoder weights and biases\n",
    "        #     self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        #     self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x1_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_u_tf = tf.placeholder(tf.float32, shape=(None, 1))   # Define placeholders and computational graph\n",
    "\n",
    "        self.x1_f_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_f_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # Define placeholder for adaptive constant\n",
    "        self.adaptive_constant_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_val.shape)\n",
    "        self.adaptive_constant_r_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred = self.net_u(self.x1_bc1_tf, self.x2_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.x1_bc2_tf, self.x2_bc2_tf)\n",
    "        self.u_bc3_pred = self.net_u(self.x1_bc3_tf, self.x2_bc3_tf)\n",
    "        self.u_bc4_pred = self.net_u(self.x1_bc4_tf, self.x2_bc4_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x1_u_tf, self.x2_u_tf)\n",
    "        self.r_pred = self.net_r(self.x1_r_tf, self.x2_r_tf)\n",
    "\n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_tf - self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_tf - self.u_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.u_bc3_tf - self.u_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.u_bc4_tf - self.u_bc4_pred))\n",
    "        self.loss_bcs = self.adaptive_constant_tf * (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
    "\n",
    "        self.f_pred = self.net_u(self.x1_f_tf, self.x2_f_tf)\n",
    "        self.loss_f= tf.reduce_mean(tf.square(self.u_tf - self.f_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss =  1.0 * self.loss_f +  1* self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_f,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\", \"loss_f\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_f, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        # Compute and store the adaptive constant\n",
    "        self.adpative_constant_log = []\n",
    "        \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        \n",
    "        self.max_grad_res_log = []\n",
    "        self.mean_grad_bcs_log = []\n",
    "    \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_mean(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        \n",
    "        self.mean_grad_res = tf.reduce_mean(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.adaptive_constant = self.mean_grad_res / self.mean_grad_bcs\n",
    "\n",
    "        # # Stiff Ratio\n",
    "        # if self.stiff_ratio:\n",
    "        #     self.Hessian, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "        #     self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "        #     self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "        #     self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "        #     self.eigenvalue_log = []\n",
    "        #     self.eigenvalue_bcs_log = []\n",
    "        #     self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "     # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,  self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P), dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),  dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),  dtype='float32')\n",
    "\n",
    "        return H, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self,size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, x1, x2):\n",
    "        u = self.forward_pass(tf.concat([x1, x2], 1))\n",
    "        return u\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x1, x2):\n",
    "        u = self.net_u(x1, x2)\n",
    "        residual = self.operator(u, x1, x2,  self.lam, self.sigma_x1,  self.sigma_x2)\n",
    "        return residual\n",
    "\n",
    "    # Feed minibatch\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter , bcbatch_size , fbatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Fetch boundary mini-batches\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], bcbatch_size)\n",
    "        X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], bcbatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, fbatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.u_bc3_tf: u_bc3_batch,\n",
    "                    self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.u_bc4_tf: u_bc4_batch,\n",
    "                    self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], self.r_tf: f_res_batch,\n",
    "                    self.adaptive_constant_tf:  self.adaptive_constant_val\n",
    "                    }\n",
    "\n",
    "\n",
    "        for it in range(nIter):\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                        self.eigenvalues_bcs,\n",
    "            #                                                                        self.eigenvalues_res], tf_dict)\n",
    "\n",
    "                    # # Log eigenvalues\n",
    "                    # self.eigenvalue_log.append(eigenvalues)\n",
    "                    # self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    # self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # self.loss_bcs_log.append(loss_bcs_value /  self.adaptive_constant_val)\n",
    "                # self.loss_res_log.append(loss_res_value)\n",
    "\n",
    "                # # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                #     self.adaptive_constant_val = adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "\n",
    "                # self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e, Adaptive_Constant: %.2f ,Time: %.2f' % (it, loss_value, loss_bcs_value, loss_res_value, self.adaptive_constant_val, elapsed))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "  # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, r_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "            f_res_batch = u(X_res_batch , 1.0, 4.0)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2],\n",
    "                        self.r_tf: r_res_batch,\n",
    "                       self.x1_f_tf: X_res_batch[:, 0:1], self.x2_f_tf: X_res_batch[:, 1:2],\n",
    "                        self.u_tf: r_res_batch,\n",
    "                       self.adaptive_constant_tf:  self.adaptive_constant_val,\n",
    "                       self.adaptive_constant_r_tf:  self.adaptive_constant_r_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                [loss ,  loss_res,  loss_f,  loss_bc1 , loss_bc2 , loss_bc3, loss_bc4] = batch_losses\n",
    "\n",
    " \n",
    "                self.print('It: %d, loss: %.3e| loss_res: %.3e| loss_f: %.3e| loss_bc1: %.3e| loss_bc2: %.3e| loss_bc3: %.3e| loss_bc4: %.3e|Time: %.2f' % (it, loss, loss_res ,  loss_f, loss_bc1, loss_bc2, loss_bc3 , loss_bc4, elapsed))\n",
    "\n",
    "                mean_grad_res , mean_grad_bcs = self.sess.run([self.mean_grad_res , self.mean_grad_bcs], tf_dict)\n",
    "                self.max_grad_res_log.append(mean_grad_res)\n",
    "                self.mean_grad_bcs_log.append(mean_grad_bcs)\n",
    "\n",
    "                self.print('mean_grad_res: %f' % (mean_grad_res))\n",
    "                self.print('mean_grad_bcs: %f' % (mean_grad_bcs))\n",
    "\n",
    "            if it  == 0:\n",
    "                # adaptive_constant_value = self.sess.run(self.adaptive_constant, tf_dict)\n",
    "                self.adaptive_constant_val = (loss_bc1 + loss_bc2 + loss_bc3 + loss_bc4) / mean_grad_bcs # * adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "                self.adaptive_constant_r_val = loss_res / mean_grad_res # * adaptive_constant_value * (1.0 - self.beta)+ self.beta * self.adaptive_constant_val\n",
    "                # self.print('adaptive_constant_val: %f' % (self.adaptive_constant_val))\n",
    "\n",
    "                self.adpative_constant_log.append(self.adaptive_constant_val)\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    " \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_u_tf: X_star[:, 0:1], self.x2_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_r_tf: X_star[:, 0:1], self.x2_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n",
    "\n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        # shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $f(x)$' , 'Predicted $f(x)$' , 'Absolute error']\n",
    "        data = [U_star , U_pred ,  np.abs(U_star - U_pred) , F_star , F_pred ,  np.abs(F_star - F_pred) ]\n",
    "        \n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        kwargs_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60),\n",
    "                cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, kwargs, minmax, title in zip(grid, data, kwargs_list, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "                #  print( z[timeStp,:,:])\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fontsize = 17\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([16,8])\n",
    "        ax.semilogy(self.max_grad_res_log, label=r'$\\bar{\\nabla_\\theta {u_{res}}}$' , color = 'tab:green')\n",
    "        ax.semilogy(self.mean_grad_bcs_log, label=r'$\\bar{\\nabla_\\theta {u_{bc}}}$' , color = 'tab:blue')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
    "        ax.tick_params(labelsize=fontsize)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
    "\n",
    "        ax2 = ax.twinx() \n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_size_inches([15,8])\n",
    "    \n",
    "        ax2.semilogy(self.adpative_constant_log, label=r'$\\bar{\\lambda_{bc}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
    "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
    "        ax2.tick_params(labelsize=fontsize)\n",
    "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = os.path.join(self.dirname,'lambda_history.png')\n",
    "        plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, lam ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f ,  model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_60704/3308077638.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_60704/3308077638.py:83: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_60704/3308077638.py:84: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_60704/3308077638.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_60704/3156273567.py:291: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_60704/3156273567.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 21:33:54.353703: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 21:33:54.376507: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2024-01-11 21:33:54.377106: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f0eb3954a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-11 21:33:54.377122: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-01-11 21:33:54.377781: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_60704/3156273567.py:180: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_60704/3156273567.py:182: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_60704/3156273567.py:234: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It: 0, loss: 7.182e-01| loss_res: 2.460e-01| loss_f: 3.609e-01| loss_bc1: 3.924e-02| loss_bc2: 1.410e-01| loss_bc3: 3.854e-02| loss_bc4: 1.384e-01|Time: 0.90\n",
      "mean_grad_res: 0.017892\n",
      "mean_grad_bcs: 0.083979\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, loss: 2.585e-01| loss_res: 2.649e-01| loss_f: 2.580e-01| loss_bc1: 2.139e-05| loss_bc2: 3.734e-05| loss_bc3: 2.872e-05| loss_bc4: 2.888e-05|Time: 0.00\n",
      "mean_grad_res: 0.004707\n",
      "mean_grad_bcs: 0.021061\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, loss: 2.745e-01| loss_res: 2.180e-01| loss_f: 2.728e-01| loss_bc1: 6.790e-05| loss_bc2: 1.043e-04| loss_bc3: 9.529e-05| loss_bc4: 1.202e-04|Time: 0.00\n",
      "mean_grad_res: 0.004559\n",
      "mean_grad_bcs: 0.010598\n",
      "It: 300, loss: 3.198e-01| loss_res: 1.895e-01| loss_f: 3.181e-01| loss_bc1: 7.991e-05| loss_bc2: 6.346e-05| loss_bc3: 2.150e-04| loss_bc4: 5.113e-05|Time: 0.00\n",
      "mean_grad_res: 0.004362\n",
      "mean_grad_bcs: 0.011770\n",
      "It: 400, loss: 2.905e-01| loss_res: 2.319e-01| loss_f: 2.887e-01| loss_bc1: 3.620e-05| loss_bc2: 3.565e-05| loss_bc3: 1.969e-04| loss_bc4: 1.486e-04|Time: 0.00\n",
      "mean_grad_res: 0.004120\n",
      "mean_grad_bcs: 0.018893\n",
      "It: 500, loss: 3.324e-01| loss_res: 2.624e-01| loss_f: 3.311e-01| loss_bc1: 8.882e-05| loss_bc2: 5.868e-05| loss_bc3: 9.467e-05| loss_bc4: 7.716e-05|Time: 0.00\n",
      "mean_grad_res: 0.005129\n",
      "mean_grad_bcs: 0.008045\n",
      "It: 600, loss: 2.819e-01| loss_res: 2.925e-01| loss_f: 2.810e-01| loss_bc1: 8.582e-06| loss_bc2: 7.226e-06| loss_bc3: 9.503e-05| loss_bc4: 9.943e-05|Time: 0.01\n",
      "mean_grad_res: 0.002275\n",
      "mean_grad_bcs: 0.013652\n",
      "It: 700, loss: 2.972e-01| loss_res: 2.410e-01| loss_f: 2.968e-01| loss_bc1: 2.840e-05| loss_bc2: 1.929e-06| loss_bc3: 3.893e-06| loss_bc4: 4.987e-05|Time: 0.00\n",
      "mean_grad_res: 0.003479\n",
      "mean_grad_bcs: 0.001513\n",
      "It: 800, loss: 3.440e-01| loss_res: 2.302e-01| loss_f: 3.419e-01| loss_bc1: 1.637e-04| loss_bc2: 9.688e-06| loss_bc3: 4.491e-05| loss_bc4: 2.979e-04|Time: 0.00\n",
      "mean_grad_res: 0.011459\n",
      "mean_grad_bcs: 0.030483\n",
      "It: 900, loss: 2.574e-01| loss_res: 2.539e-01| loss_f: 2.561e-01| loss_bc1: 8.923e-05| loss_bc2: 4.756e-05| loss_bc3: 9.912e-05| loss_bc4: 7.174e-05|Time: 0.00\n",
      "mean_grad_res: 0.008079\n",
      "mean_grad_bcs: 0.008058\n",
      "It: 1000, loss: 2.866e-01| loss_res: 1.922e-01| loss_f: 2.866e-01| loss_bc1: 7.486e-06| loss_bc2: 6.108e-06| loss_bc3: 1.309e-06| loss_bc4: 1.636e-06|Time: 0.00\n",
      "mean_grad_res: 0.004451\n",
      "mean_grad_bcs: 0.023606\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, loss: 2.772e-01| loss_res: 2.867e-01| loss_f: 2.760e-01| loss_bc1: 1.738e-05| loss_bc2: 6.202e-05| loss_bc3: 1.269e-04| loss_bc4: 8.547e-05|Time: 0.00\n",
      "mean_grad_res: 0.008600\n",
      "mean_grad_bcs: 0.015044\n",
      "It: 1200, loss: 2.481e-01| loss_res: 2.532e-01| loss_f: 2.467e-01| loss_bc1: 1.012e-04| loss_bc2: 5.749e-05| loss_bc3: 1.050e-04| loss_bc4: 5.274e-05|Time: 0.00\n",
      "mean_grad_res: 0.009000\n",
      "mean_grad_bcs: 0.017434\n",
      "It: 1300, loss: 2.378e-01| loss_res: 2.893e-01| loss_f: 2.376e-01| loss_bc1: 5.471e-06| loss_bc2: 4.881e-06| loss_bc3: 3.109e-05| loss_bc4: 2.186e-05|Time: 0.00\n",
      "mean_grad_res: 0.000951\n",
      "mean_grad_bcs: 0.006387\n",
      "It: 1400, loss: 2.684e-01| loss_res: 2.682e-01| loss_f: 2.638e-01| loss_bc1: 5.798e-04| loss_bc2: 6.454e-05| loss_bc3: 7.164e-05| loss_bc4: 3.700e-04|Time: 0.00\n",
      "mean_grad_res: 0.004203\n",
      "mean_grad_bcs: 0.010939\n",
      "It: 1500, loss: 2.853e-01| loss_res: 2.387e-01| loss_f: 2.850e-01| loss_bc1: 1.703e-05| loss_bc2: 4.520e-06| loss_bc3: 3.075e-06| loss_bc4: 2.486e-05|Time: 0.01\n",
      "mean_grad_res: 0.009834\n",
      "mean_grad_bcs: 0.028197\n",
      "It: 1600, loss: 2.421e-01| loss_res: 2.427e-01| loss_f: 2.408e-01| loss_bc1: 1.029e-04| loss_bc2: 4.426e-05| loss_bc3: 4.150e-05| loss_bc4: 1.262e-04|Time: 0.00\n",
      "mean_grad_res: 0.001620\n",
      "mean_grad_bcs: 0.010699\n",
      "It: 1700, loss: 3.224e-01| loss_res: 2.248e-01| loss_f: 3.221e-01| loss_bc1: 8.772e-06| loss_bc2: 3.956e-05| loss_bc3: 9.735e-06| loss_bc4: 1.747e-05|Time: 0.00\n",
      "mean_grad_res: 0.003145\n",
      "mean_grad_bcs: 0.015120\n",
      "It: 1800, loss: 2.726e-01| loss_res: 2.288e-01| loss_f: 2.709e-01| loss_bc1: 6.271e-05| loss_bc2: 1.490e-04| loss_bc3: 4.562e-05| loss_bc4: 1.296e-04|Time: 0.00\n",
      "mean_grad_res: 0.002578\n",
      "mean_grad_bcs: 0.013849\n",
      "It: 1900, loss: 2.746e-01| loss_res: 2.655e-01| loss_f: 2.741e-01| loss_bc1: 8.979e-06| loss_bc2: 6.590e-05| loss_bc3: 2.229e-05| loss_bc4: 5.037e-06|Time: 0.00\n",
      "mean_grad_res: 0.015325\n",
      "mean_grad_bcs: 0.014890\n",
      "It: 2000, loss: 3.071e-01| loss_res: 2.264e-01| loss_f: 3.064e-01| loss_bc1: 7.256e-05| loss_bc2: 4.668e-06| loss_bc3: 6.594e-06| loss_bc4: 7.467e-05|Time: 0.01\n",
      "mean_grad_res: 0.003221\n",
      "mean_grad_bcs: 0.009867\n",
      "It: 2100, loss: 2.677e-01| loss_res: 2.826e-01| loss_f: 2.675e-01| loss_bc1: 8.688e-06| loss_bc2: 2.267e-05| loss_bc3: 1.466e-05| loss_bc4: 6.646e-06|Time: 0.00\n",
      "mean_grad_res: 0.000535\n",
      "mean_grad_bcs: 0.006867\n",
      "It: 2200, loss: 2.840e-01| loss_res: 2.987e-01| loss_f: 2.833e-01| loss_bc1: 4.172e-05| loss_bc2: 2.906e-05| loss_bc3: 8.075e-05| loss_bc4: 1.907e-05|Time: 0.00\n",
      "mean_grad_res: 0.002454\n",
      "mean_grad_bcs: 0.009876\n",
      "It: 2300, loss: 2.925e-01| loss_res: 2.578e-01| loss_f: 2.906e-01| loss_bc1: 1.744e-04| loss_bc2: 8.830e-05| loss_bc3: 1.190e-04| loss_bc4: 5.260e-05|Time: 0.00\n",
      "mean_grad_res: 0.005307\n",
      "mean_grad_bcs: 0.008280\n",
      "It: 2400, loss: 2.501e-01| loss_res: 2.476e-01| loss_f: 2.497e-01| loss_bc1: 2.404e-05| loss_bc2: 5.985e-05| loss_bc3: 5.752e-06| loss_bc4: 1.851e-05|Time: 0.01\n",
      "mean_grad_res: 0.006005\n",
      "mean_grad_bcs: 0.002271\n",
      "It: 2500, loss: 2.600e-01| loss_res: 2.461e-01| loss_f: 2.589e-01| loss_bc1: 4.803e-05| loss_bc2: 1.588e-04| loss_bc3: 3.716e-05| loss_bc4: 1.315e-05|Time: 0.00\n",
      "mean_grad_res: 0.003170\n",
      "mean_grad_bcs: 0.013336\n",
      "It: 2600, loss: 3.124e-01| loss_res: 2.586e-01| loss_f: 3.106e-01| loss_bc1: 8.762e-05| loss_bc2: 1.008e-04| loss_bc3: 9.117e-05| loss_bc4: 1.266e-04|Time: 0.00\n",
      "mean_grad_res: 0.009278\n",
      "mean_grad_bcs: 0.019274\n",
      "It: 2700, loss: 2.981e-01| loss_res: 2.626e-01| loss_f: 2.980e-01| loss_bc1: 4.118e-06| loss_bc2: 1.782e-06| loss_bc3: 1.158e-06| loss_bc4: 3.418e-06|Time: 0.00\n",
      "mean_grad_res: 0.002585\n",
      "mean_grad_bcs: 0.007077\n",
      "It: 2800, loss: 2.664e-01| loss_res: 2.964e-01| loss_f: 2.656e-01| loss_bc1: 1.039e-04| loss_bc2: 1.173e-05| loss_bc3: 6.101e-06| loss_bc4: 5.513e-05|Time: 0.00\n",
      "mean_grad_res: 0.001714\n",
      "mean_grad_bcs: 0.009092\n",
      "It: 2900, loss: 2.545e-01| loss_res: 2.761e-01| loss_f: 2.541e-01| loss_bc1: 3.399e-05| loss_bc2: 2.873e-05| loss_bc3: 1.361e-05| loss_bc4: 9.185e-06|Time: 0.00\n",
      "mean_grad_res: 0.002920\n",
      "mean_grad_bcs: 0.007784\n",
      "It: 3000, loss: 3.046e-01| loss_res: 2.476e-01| loss_f: 3.039e-01| loss_bc1: 7.190e-05| loss_bc2: 2.222e-05| loss_bc3: 4.347e-05| loss_bc4: 1.901e-05|Time: 0.00\n",
      "mean_grad_res: 0.004981\n",
      "mean_grad_bcs: 0.003008\n",
      "It: 3100, loss: 2.627e-01| loss_res: 2.224e-01| loss_f: 2.624e-01| loss_bc1: 2.933e-06| loss_bc2: 2.755e-05| loss_bc3: 4.789e-05| loss_bc4: 1.526e-06|Time: 0.00\n",
      "mean_grad_res: 0.001626\n",
      "mean_grad_bcs: 0.006578\n",
      "It: 3200, loss: 2.946e-01| loss_res: 2.262e-01| loss_f: 2.941e-01| loss_bc1: 3.164e-05| loss_bc2: 5.311e-05| loss_bc3: 1.443e-05| loss_bc4: 3.221e-05|Time: 0.00\n",
      "mean_grad_res: 0.001828\n",
      "mean_grad_bcs: 0.003412\n",
      "It: 3300, loss: 2.747e-01| loss_res: 2.269e-01| loss_f: 2.741e-01| loss_bc1: 1.368e-06| loss_bc2: 6.774e-05| loss_bc3: 6.906e-05| loss_bc4: 3.548e-06|Time: 0.01\n",
      "mean_grad_res: 0.005218\n",
      "mean_grad_bcs: 0.010422\n",
      "It: 3400, loss: 2.874e-01| loss_res: 3.033e-01| loss_f: 2.870e-01| loss_bc1: 4.375e-05| loss_bc2: 9.425e-06| loss_bc3: 3.057e-05| loss_bc4: 1.944e-05|Time: 0.00\n",
      "mean_grad_res: 0.004935\n",
      "mean_grad_bcs: 0.013499\n",
      "It: 3500, loss: 2.749e-01| loss_res: 2.719e-01| loss_f: 2.749e-01| loss_bc1: 6.302e-06| loss_bc2: 2.106e-06| loss_bc3: 1.614e-06| loss_bc4: 4.396e-06|Time: 0.00\n",
      "mean_grad_res: 0.004806\n",
      "mean_grad_bcs: 0.000958\n",
      "It: 3600, loss: 3.115e-01| loss_res: 2.280e-01| loss_f: 3.112e-01| loss_bc1: 1.484e-05| loss_bc2: 8.802e-06| loss_bc3: 2.731e-05| loss_bc4: 9.783e-06|Time: 0.00\n",
      "mean_grad_res: 0.001055\n",
      "mean_grad_bcs: 0.003842\n",
      "It: 3700, loss: 2.655e-01| loss_res: 2.153e-01| loss_f: 2.653e-01| loss_bc1: 1.744e-05| loss_bc2: 2.353e-05| loss_bc3: 5.725e-06| loss_bc4: 3.624e-06|Time: 0.00\n",
      "mean_grad_res: 0.006172\n",
      "mean_grad_bcs: 0.001982\n",
      "It: 3800, loss: 2.707e-01| loss_res: 2.326e-01| loss_f: 2.700e-01| loss_bc1: 1.048e-04| loss_bc2: 1.687e-05| loss_bc3: 8.940e-06| loss_bc4: 4.018e-05|Time: 0.00\n",
      "mean_grad_res: 0.002556\n",
      "mean_grad_bcs: 0.013250\n",
      "It: 3900, loss: 2.753e-01| loss_res: 2.688e-01| loss_f: 2.751e-01| loss_bc1: 9.221e-06| loss_bc2: 6.424e-06| loss_bc3: 1.495e-05| loss_bc4: 1.607e-05|Time: 0.00\n",
      "mean_grad_res: 0.007974\n",
      "mean_grad_bcs: 0.008803\n",
      "It: 4000, loss: 2.873e-01| loss_res: 2.337e-01| loss_f: 2.867e-01| loss_bc1: 3.090e-05| loss_bc2: 2.256e-05| loss_bc3: 4.857e-05| loss_bc4: 4.173e-05|Time: 0.00\n",
      "mean_grad_res: 0.002160\n",
      "mean_grad_bcs: 0.011343\n",
      "It: 4100, loss: 2.893e-01| loss_res: 2.497e-01| loss_f: 2.888e-01| loss_bc1: 5.071e-05| loss_bc2: 4.455e-05| loss_bc3: 1.191e-05| loss_bc4: 9.484e-06|Time: 0.00\n",
      "mean_grad_res: 0.004319\n",
      "mean_grad_bcs: 0.010062\n",
      "It: 4200, loss: 2.733e-01| loss_res: 2.278e-01| loss_f: 2.721e-01| loss_bc1: 2.429e-05| loss_bc2: 2.000e-05| loss_bc3: 1.656e-04| loss_bc4: 6.853e-05|Time: 0.01\n",
      "mean_grad_res: 0.002004\n",
      "mean_grad_bcs: 0.007093\n",
      "It: 4300, loss: 2.794e-01| loss_res: 2.339e-01| loss_f: 2.784e-01| loss_bc1: 4.019e-05| loss_bc2: 1.417e-04| loss_bc3: 2.230e-05| loss_bc4: 2.951e-05|Time: 0.00\n",
      "mean_grad_res: 0.002265\n",
      "mean_grad_bcs: 0.013005\n",
      "It: 4400, loss: 3.123e-01| loss_res: 2.610e-01| loss_f: 3.118e-01| loss_bc1: 6.082e-05| loss_bc2: 2.638e-05| loss_bc3: 1.872e-05| loss_bc4: 1.852e-05|Time: 0.01\n",
      "mean_grad_res: 0.000752\n",
      "mean_grad_bcs: 0.005635\n",
      "It: 4500, loss: 2.694e-01| loss_res: 2.401e-01| loss_f: 2.690e-01| loss_bc1: 4.327e-05| loss_bc2: 2.169e-05| loss_bc3: 2.517e-05| loss_bc4: 9.548e-06|Time: 0.00\n",
      "mean_grad_res: 0.001763\n",
      "mean_grad_bcs: 0.010038\n",
      "It: 4600, loss: 2.682e-01| loss_res: 2.482e-01| loss_f: 2.678e-01| loss_bc1: 1.476e-05| loss_bc2: 1.397e-05| loss_bc3: 5.329e-05| loss_bc4: 1.214e-05|Time: 0.00\n",
      "mean_grad_res: 0.004967\n",
      "mean_grad_bcs: 0.001268\n",
      "It: 4700, loss: 2.694e-01| loss_res: 2.813e-01| loss_f: 2.688e-01| loss_bc1: 2.922e-05| loss_bc2: 1.899e-05| loss_bc3: 6.137e-05| loss_bc4: 2.175e-05|Time: 0.01\n",
      "mean_grad_res: 0.002284\n",
      "mean_grad_bcs: 0.007864\n",
      "It: 4800, loss: 2.740e-01| loss_res: 2.745e-01| loss_f: 2.738e-01| loss_bc1: 1.475e-05| loss_bc2: 1.186e-06| loss_bc3: 1.257e-06| loss_bc4: 1.487e-05|Time: 0.00\n",
      "mean_grad_res: 0.003172\n",
      "mean_grad_bcs: 0.004709\n",
      "It: 4900, loss: 2.756e-01| loss_res: 2.502e-01| loss_f: 2.754e-01| loss_bc1: 9.052e-06| loss_bc2: 6.410e-06| loss_bc3: 1.233e-05| loss_bc4: 1.097e-05|Time: 0.00\n",
      "mean_grad_res: 0.002728\n",
      "mean_grad_bcs: 0.003014\n",
      "It: 5000, loss: 3.065e-01| loss_res: 2.448e-01| loss_f: 3.065e-01| loss_bc1: 9.474e-07| loss_bc2: 1.730e-06| loss_bc3: 2.006e-06| loss_bc4: 2.167e-07|Time: 0.00\n",
      "mean_grad_res: 0.006002\n",
      "mean_grad_bcs: 0.002997\n",
      "It: 5100, loss: 3.035e-01| loss_res: 2.402e-01| loss_f: 3.034e-01| loss_bc1: 6.824e-06| loss_bc2: 5.605e-06| loss_bc3: 4.450e-06| loss_bc4: 8.289e-06|Time: 0.01\n",
      "mean_grad_res: 0.002406\n",
      "mean_grad_bcs: 0.003349\n",
      "It: 5200, loss: 2.764e-01| loss_res: 2.537e-01| loss_f: 2.763e-01| loss_bc1: 5.539e-06| loss_bc2: 1.376e-05| loss_bc3: 1.400e-05| loss_bc4: 7.910e-06|Time: 0.00\n",
      "mean_grad_res: 0.001939\n",
      "mean_grad_bcs: 0.001665\n",
      "It: 5300, loss: 2.552e-01| loss_res: 2.315e-01| loss_f: 2.547e-01| loss_bc1: 7.114e-06| loss_bc2: 8.983e-06| loss_bc3: 5.042e-05| loss_bc4: 4.790e-05|Time: 0.00\n",
      "mean_grad_res: 0.003000\n",
      "mean_grad_bcs: 0.004126\n",
      "It: 5400, loss: 2.835e-01| loss_res: 2.564e-01| loss_f: 2.827e-01| loss_bc1: 5.807e-05| loss_bc2: 8.375e-05| loss_bc3: 1.917e-05| loss_bc4: 3.504e-05|Time: 0.00\n",
      "mean_grad_res: 0.003126\n",
      "mean_grad_bcs: 0.004326\n",
      "It: 5500, loss: 2.674e-01| loss_res: 2.479e-01| loss_f: 2.673e-01| loss_bc1: 2.993e-06| loss_bc2: 1.453e-07| loss_bc3: 4.031e-07| loss_bc4: 2.298e-06|Time: 0.01\n",
      "mean_grad_res: 0.002895\n",
      "mean_grad_bcs: 0.000885\n",
      "It: 5600, loss: 2.680e-01| loss_res: 3.114e-01| loss_f: 2.677e-01| loss_bc1: 2.587e-05| loss_bc2: 3.152e-06| loss_bc3: 2.357e-05| loss_bc4: 8.117e-06|Time: 0.00\n",
      "mean_grad_res: 0.004249\n",
      "mean_grad_bcs: 0.000540\n",
      "It: 5700, loss: 2.787e-01| loss_res: 2.643e-01| loss_f: 2.782e-01| loss_bc1: 5.745e-05| loss_bc2: 1.389e-05| loss_bc3: 3.530e-05| loss_bc4: 2.002e-05|Time: 0.00\n",
      "mean_grad_res: 0.001862\n",
      "mean_grad_bcs: 0.005169\n",
      "It: 5800, loss: 2.534e-01| loss_res: 2.563e-01| loss_f: 2.532e-01| loss_bc1: 3.177e-06| loss_bc2: 1.079e-05| loss_bc3: 6.736e-06| loss_bc4: 1.273e-05|Time: 0.01\n",
      "mean_grad_res: 0.001168\n",
      "mean_grad_bcs: 0.003496\n",
      "It: 5900, loss: 2.726e-01| loss_res: 2.564e-01| loss_f: 2.724e-01| loss_bc1: 2.201e-05| loss_bc2: 1.443e-05| loss_bc3: 1.194e-05| loss_bc4: 5.393e-06|Time: 0.00\n",
      "mean_grad_res: 0.001810\n",
      "mean_grad_bcs: 0.005075\n",
      "It: 6000, loss: 2.986e-01| loss_res: 2.330e-01| loss_f: 2.986e-01| loss_bc1: 6.656e-06| loss_bc2: 3.793e-06| loss_bc3: 2.008e-06| loss_bc4: 4.752e-06|Time: 0.00\n",
      "mean_grad_res: 0.001448\n",
      "mean_grad_bcs: 0.002696\n",
      "It: 6100, loss: 2.864e-01| loss_res: 2.307e-01| loss_f: 2.861e-01| loss_bc1: 4.084e-05| loss_bc2: 1.969e-05| loss_bc3: 3.684e-06| loss_bc4: 7.605e-06|Time: 0.00\n",
      "mean_grad_res: 0.001598\n",
      "mean_grad_bcs: 0.003060\n",
      "It: 6200, loss: 2.688e-01| loss_res: 2.439e-01| loss_f: 2.687e-01| loss_bc1: 6.623e-06| loss_bc2: 7.485e-06| loss_bc3: 1.120e-06| loss_bc4: 1.657e-06|Time: 0.00\n",
      "mean_grad_res: 0.003324\n",
      "mean_grad_bcs: 0.000977\n",
      "It: 6300, loss: 2.392e-01| loss_res: 2.223e-01| loss_f: 2.391e-01| loss_bc1: 5.316e-07| loss_bc2: 7.023e-07| loss_bc3: 6.676e-06| loss_bc4: 8.318e-06|Time: 0.00\n",
      "mean_grad_res: 0.000979\n",
      "mean_grad_bcs: 0.002097\n",
      "It: 6400, loss: 3.032e-01| loss_res: 2.660e-01| loss_f: 3.026e-01| loss_bc1: 3.889e-05| loss_bc2: 8.178e-05| loss_bc3: 2.326e-05| loss_bc4: 1.175e-06|Time: 0.01\n",
      "mean_grad_res: 0.001327\n",
      "mean_grad_bcs: 0.004116\n",
      "It: 6500, loss: 2.069e-01| loss_res: 2.506e-01| loss_f: 2.068e-01| loss_bc1: 1.071e-05| loss_bc2: 4.000e-06| loss_bc3: 8.703e-07| loss_bc4: 6.038e-06|Time: 0.00\n",
      "mean_grad_res: 0.001361\n",
      "mean_grad_bcs: 0.002580\n",
      "It: 6600, loss: 2.638e-01| loss_res: 2.404e-01| loss_f: 2.637e-01| loss_bc1: 1.031e-05| loss_bc2: 1.105e-05| loss_bc3: 1.373e-06| loss_bc4: 1.350e-06|Time: 0.01\n",
      "mean_grad_res: 0.002880\n",
      "mean_grad_bcs: 0.001651\n",
      "It: 6700, loss: 2.109e-01| loss_res: 2.250e-01| loss_f: 2.102e-01| loss_bc1: 2.155e-05| loss_bc2: 8.487e-05| loss_bc3: 5.818e-05| loss_bc4: 7.353e-06|Time: 0.00\n",
      "mean_grad_res: 0.002374\n",
      "mean_grad_bcs: 0.002176\n",
      "It: 6800, loss: 2.846e-01| loss_res: 2.300e-01| loss_f: 2.844e-01| loss_bc1: 1.971e-05| loss_bc2: 4.851e-06| loss_bc3: 7.747e-06| loss_bc4: 2.416e-05|Time: 0.01\n",
      "mean_grad_res: 0.001935\n",
      "mean_grad_bcs: 0.001180\n",
      "It: 6900, loss: 2.692e-01| loss_res: 2.585e-01| loss_f: 2.691e-01| loss_bc1: 1.411e-06| loss_bc2: 1.202e-05| loss_bc3: 9.385e-06| loss_bc4: 2.474e-06|Time: 0.00\n",
      "mean_grad_res: 0.002743\n",
      "mean_grad_bcs: 0.002474\n",
      "It: 7000, loss: 3.173e-01| loss_res: 2.742e-01| loss_f: 3.164e-01| loss_bc1: 8.073e-05| loss_bc2: 1.058e-04| loss_bc3: 8.826e-06| loss_bc4: 7.069e-06|Time: 0.00\n",
      "mean_grad_res: 0.003044\n",
      "mean_grad_bcs: 0.003920\n",
      "It: 7100, loss: 2.613e-01| loss_res: 3.227e-01| loss_f: 2.612e-01| loss_bc1: 6.243e-06| loss_bc2: 1.671e-05| loss_bc3: 8.110e-06| loss_bc4: 1.646e-06|Time: 0.00\n",
      "mean_grad_res: 0.001395\n",
      "mean_grad_bcs: 0.001424\n",
      "It: 7200, loss: 2.569e-01| loss_res: 3.279e-01| loss_f: 2.567e-01| loss_bc1: 1.362e-05| loss_bc2: 2.519e-05| loss_bc3: 5.795e-06| loss_bc4: 8.954e-07|Time: 0.00\n",
      "mean_grad_res: 0.001314\n",
      "mean_grad_bcs: 0.002608\n",
      "It: 7300, loss: 2.431e-01| loss_res: 2.741e-01| loss_f: 2.430e-01| loss_bc1: 2.203e-06| loss_bc2: 3.236e-06| loss_bc3: 7.303e-06| loss_bc4: 1.917e-05|Time: 0.00\n",
      "mean_grad_res: 0.002871\n",
      "mean_grad_bcs: 0.001165\n",
      "It: 7400, loss: 2.734e-01| loss_res: 2.701e-01| loss_f: 2.730e-01| loss_bc1: 2.750e-05| loss_bc2: 1.248e-05| loss_bc3: 4.304e-05| loss_bc4: 1.733e-05|Time: 0.00\n",
      "mean_grad_res: 0.002266\n",
      "mean_grad_bcs: 0.003813\n",
      "It: 7500, loss: 2.447e-01| loss_res: 2.652e-01| loss_f: 2.441e-01| loss_bc1: 6.283e-05| loss_bc2: 1.938e-05| loss_bc3: 3.249e-06| loss_bc4: 4.991e-05|Time: 0.00\n",
      "mean_grad_res: 0.000389\n",
      "mean_grad_bcs: 0.003104\n",
      "It: 7600, loss: 2.912e-01| loss_res: 2.884e-01| loss_f: 2.911e-01| loss_bc1: 6.594e-07| loss_bc2: 6.381e-06| loss_bc3: 9.405e-06| loss_bc4: 2.038e-06|Time: 0.00\n",
      "mean_grad_res: 0.002662\n",
      "mean_grad_bcs: 0.001416\n",
      "It: 7700, loss: 2.846e-01| loss_res: 2.018e-01| loss_f: 2.846e-01| loss_bc1: 2.411e-06| loss_bc2: 4.544e-06| loss_bc3: 2.534e-06| loss_bc4: 1.242e-06|Time: 0.02\n",
      "mean_grad_res: 0.002153\n",
      "mean_grad_bcs: 0.000492\n",
      "It: 7800, loss: 2.994e-01| loss_res: 2.234e-01| loss_f: 2.991e-01| loss_bc1: 1.007e-05| loss_bc2: 3.899e-05| loss_bc3: 6.195e-06| loss_bc4: 1.240e-05|Time: 0.00\n",
      "mean_grad_res: 0.000981\n",
      "mean_grad_bcs: 0.002134\n",
      "It: 7900, loss: 2.619e-01| loss_res: 2.314e-01| loss_f: 2.615e-01| loss_bc1: 1.238e-05| loss_bc2: 1.100e-05| loss_bc3: 3.320e-05| loss_bc4: 3.609e-05|Time: 0.00\n",
      "mean_grad_res: 0.002136\n",
      "mean_grad_bcs: 0.003011\n",
      "It: 8000, loss: 3.443e-01| loss_res: 2.138e-01| loss_f: 3.441e-01| loss_bc1: 3.213e-06| loss_bc2: 3.020e-06| loss_bc3: 3.109e-05| loss_bc4: 2.471e-05|Time: 0.00\n",
      "mean_grad_res: 0.002535\n",
      "mean_grad_bcs: 0.001540\n",
      "It: 8100, loss: 2.861e-01| loss_res: 2.329e-01| loss_f: 2.859e-01| loss_bc1: 6.230e-06| loss_bc2: 2.511e-05| loss_bc3: 7.725e-06| loss_bc4: 5.806e-06|Time: 0.00\n",
      "mean_grad_res: 0.001373\n",
      "mean_grad_bcs: 0.001995\n",
      "It: 8200, loss: 2.752e-01| loss_res: 2.827e-01| loss_f: 2.750e-01| loss_bc1: 2.467e-05| loss_bc2: 9.141e-06| loss_bc3: 2.929e-06| loss_bc4: 1.840e-05|Time: 0.01\n",
      "mean_grad_res: 0.000585\n",
      "mean_grad_bcs: 0.001589\n",
      "It: 8300, loss: 2.689e-01| loss_res: 2.655e-01| loss_f: 2.685e-01| loss_bc1: 7.080e-06| loss_bc2: 3.487e-05| loss_bc3: 3.312e-05| loss_bc4: 5.332e-06|Time: 0.00\n",
      "mean_grad_res: 0.002115\n",
      "mean_grad_bcs: 0.002134\n",
      "It: 8400, loss: 2.774e-01| loss_res: 2.432e-01| loss_f: 2.773e-01| loss_bc1: 8.991e-07| loss_bc2: 4.456e-06| loss_bc3: 4.468e-06| loss_bc4: 5.691e-07|Time: 0.00\n",
      "mean_grad_res: 0.001075\n",
      "mean_grad_bcs: 0.000464\n",
      "It: 8500, loss: 3.220e-01| loss_res: 2.479e-01| loss_f: 3.218e-01| loss_bc1: 1.518e-05| loss_bc2: 5.298e-06| loss_bc3: 7.507e-06| loss_bc4: 2.080e-05|Time: 0.00\n",
      "mean_grad_res: 0.001464\n",
      "mean_grad_bcs: 0.000319\n",
      "It: 8600, loss: 2.881e-01| loss_res: 2.654e-01| loss_f: 2.879e-01| loss_bc1: 7.139e-06| loss_bc2: 1.240e-05| loss_bc3: 1.300e-05| loss_bc4: 7.599e-06|Time: 0.01\n",
      "mean_grad_res: 0.001293\n",
      "mean_grad_bcs: 0.001914\n",
      "It: 8700, loss: 2.719e-01| loss_res: 2.825e-01| loss_f: 2.718e-01| loss_bc1: 3.119e-06| loss_bc2: 4.167e-06| loss_bc3: 4.918e-06| loss_bc4: 3.565e-06|Time: 0.00\n",
      "mean_grad_res: 0.001190\n",
      "mean_grad_bcs: 0.000891\n",
      "It: 8800, loss: 3.085e-01| loss_res: 2.444e-01| loss_f: 3.081e-01| loss_bc1: 3.157e-05| loss_bc2: 2.846e-05| loss_bc3: 2.224e-05| loss_bc4: 2.764e-05|Time: 0.00\n",
      "mean_grad_res: 0.002106\n",
      "mean_grad_bcs: 0.000427\n",
      "It: 8900, loss: 2.754e-01| loss_res: 2.584e-01| loss_f: 2.753e-01| loss_bc1: 2.389e-06| loss_bc2: 1.421e-05| loss_bc3: 4.949e-06| loss_bc4: 2.303e-06|Time: 0.01\n",
      "mean_grad_res: 0.000482\n",
      "mean_grad_bcs: 0.001255\n",
      "It: 9000, loss: 3.020e-01| loss_res: 2.535e-01| loss_f: 3.017e-01| loss_bc1: 5.201e-05| loss_bc2: 1.870e-05| loss_bc3: 1.485e-06| loss_bc4: 1.100e-05|Time: 0.00\n",
      "mean_grad_res: 0.002371\n",
      "mean_grad_bcs: 0.001131\n",
      "It: 9100, loss: 2.530e-01| loss_res: 2.442e-01| loss_f: 2.528e-01| loss_bc1: 1.547e-05| loss_bc2: 9.864e-06| loss_bc3: 6.208e-06| loss_bc4: 1.257e-05|Time: 0.00\n",
      "mean_grad_res: 0.001510\n",
      "mean_grad_bcs: 0.000366\n",
      "It: 9200, loss: 2.751e-01| loss_res: 2.225e-01| loss_f: 2.750e-01| loss_bc1: 6.433e-06| loss_bc2: 1.882e-06| loss_bc3: 4.938e-06| loss_bc4: 7.187e-07|Time: 0.00\n",
      "mean_grad_res: 0.001340\n",
      "mean_grad_bcs: 0.000985\n",
      "It: 9300, loss: 2.926e-01| loss_res: 2.361e-01| loss_f: 2.925e-01| loss_bc1: 2.407e-05| loss_bc2: 1.931e-06| loss_bc3: 4.263e-06| loss_bc4: 1.125e-05|Time: 0.00\n",
      "mean_grad_res: 0.002070\n",
      "mean_grad_bcs: 0.001506\n",
      "It: 9400, loss: 2.730e-01| loss_res: 2.841e-01| loss_f: 2.729e-01| loss_bc1: 1.970e-06| loss_bc2: 4.769e-06| loss_bc3: 1.522e-06| loss_bc4: 6.592e-06|Time: 0.00\n",
      "mean_grad_res: 0.000688\n",
      "mean_grad_bcs: 0.000779\n",
      "It: 9500, loss: 2.624e-01| loss_res: 2.774e-01| loss_f: 2.623e-01| loss_bc1: 6.623e-06| loss_bc2: 1.004e-05| loss_bc3: 1.926e-06| loss_bc4: 8.664e-07|Time: 0.00\n",
      "mean_grad_res: 0.002092\n",
      "mean_grad_bcs: 0.000247\n",
      "It: 9600, loss: 2.988e-01| loss_res: 2.304e-01| loss_f: 2.987e-01| loss_bc1: 3.370e-06| loss_bc2: 1.092e-05| loss_bc3: 1.206e-05| loss_bc4: 2.696e-06|Time: 0.00\n",
      "mean_grad_res: 0.001025\n",
      "mean_grad_bcs: 0.000853\n",
      "It: 9700, loss: 3.286e-01| loss_res: 2.606e-01| loss_f: 3.282e-01| loss_bc1: 5.810e-05| loss_bc2: 2.047e-05| loss_bc3: 3.694e-06| loss_bc4: 2.543e-05|Time: 0.00\n",
      "mean_grad_res: 0.001994\n",
      "mean_grad_bcs: 0.000948\n",
      "It: 9800, loss: 2.567e-01| loss_res: 2.988e-01| loss_f: 2.565e-01| loss_bc1: 7.990e-06| loss_bc2: 1.987e-05| loss_bc3: 5.630e-06| loss_bc4: 1.232e-07|Time: 0.00\n",
      "mean_grad_res: 0.000484\n",
      "mean_grad_bcs: 0.000533\n",
      "It: 9900, loss: 2.921e-01| loss_res: 2.746e-01| loss_f: 2.919e-01| loss_bc1: 8.165e-06| loss_bc2: 1.674e-05| loss_bc3: 2.014e-05| loss_bc4: 6.455e-06|Time: 0.00\n",
      "mean_grad_res: 0.001050\n",
      "mean_grad_bcs: 0.000921\n",
      "It: 10000, loss: 2.971e-01| loss_res: 1.985e-01| loss_f: 2.970e-01| loss_bc1: 2.007e-07| loss_bc2: 5.789e-07| loss_bc3: 7.358e-06| loss_bc4: 7.940e-06|Time: 0.00\n",
      "mean_grad_res: 0.001162\n",
      "mean_grad_bcs: 0.000381\n",
      "It: 10100, loss: 3.006e-01| loss_res: 2.431e-01| loss_f: 3.004e-01| loss_bc1: 3.121e-05| loss_bc2: 3.034e-06| loss_bc3: 8.619e-07| loss_bc4: 1.641e-05|Time: 0.00\n",
      "mean_grad_res: 0.001155\n",
      "mean_grad_bcs: 0.000967\n",
      "It: 10200, loss: 2.827e-01| loss_res: 2.593e-01| loss_f: 2.827e-01| loss_bc1: 2.546e-07| loss_bc2: 3.924e-06| loss_bc3: 6.920e-06| loss_bc4: 7.763e-07|Time: 0.00\n",
      "mean_grad_res: 0.001328\n",
      "mean_grad_bcs: 0.000307\n",
      "It: 10300, loss: 2.941e-01| loss_res: 2.281e-01| loss_f: 2.940e-01| loss_bc1: 1.405e-06| loss_bc2: 3.973e-06| loss_bc3: 6.759e-06| loss_bc4: 1.178e-05|Time: 0.01\n",
      "mean_grad_res: 0.000768\n",
      "mean_grad_bcs: 0.000739\n",
      "It: 10400, loss: 3.048e-01| loss_res: 1.986e-01| loss_f: 3.048e-01| loss_bc1: 1.214e-06| loss_bc2: 1.688e-07| loss_bc3: 6.686e-07| loss_bc4: 2.270e-06|Time: 0.00\n",
      "mean_grad_res: 0.000448\n",
      "mean_grad_bcs: 0.000087\n",
      "It: 10500, loss: 2.389e-01| loss_res: 2.758e-01| loss_f: 2.387e-01| loss_bc1: 1.870e-05| loss_bc2: 1.487e-05| loss_bc3: 3.049e-06| loss_bc4: 5.199e-06|Time: 0.01\n",
      "mean_grad_res: 0.000191\n",
      "mean_grad_bcs: 0.000831\n",
      "It: 10600, loss: 2.661e-01| loss_res: 1.992e-01| loss_f: 2.661e-01| loss_bc1: 8.997e-06| loss_bc2: 7.990e-06| loss_bc3: 2.013e-06| loss_bc4: 1.677e-06|Time: 0.00\n",
      "mean_grad_res: 0.000638\n",
      "mean_grad_bcs: 0.000605\n",
      "It: 10700, loss: 2.461e-01| loss_res: 2.722e-01| loss_f: 2.460e-01| loss_bc1: 1.778e-06| loss_bc2: 9.975e-07| loss_bc3: 9.133e-06| loss_bc4: 1.771e-05|Time: 0.00\n",
      "mean_grad_res: 0.000838\n",
      "mean_grad_bcs: 0.000498\n",
      "It: 10800, loss: 2.483e-01| loss_res: 2.596e-01| loss_f: 2.483e-01| loss_bc1: 2.201e-07| loss_bc2: 1.092e-06| loss_bc3: 7.133e-06| loss_bc4: 3.604e-06|Time: 0.00\n",
      "mean_grad_res: 0.000967\n",
      "mean_grad_bcs: 0.000301\n",
      "It: 10900, loss: 2.843e-01| loss_res: 2.697e-01| loss_f: 2.842e-01| loss_bc1: 6.172e-06| loss_bc2: 5.597e-06| loss_bc3: 1.520e-06| loss_bc4: 8.197e-07|Time: 0.00\n",
      "mean_grad_res: 0.001049\n",
      "mean_grad_bcs: 0.000233\n",
      "It: 11000, loss: 2.997e-01| loss_res: 2.378e-01| loss_f: 2.997e-01| loss_bc1: 5.846e-07| loss_bc2: 2.093e-06| loss_bc3: 6.657e-07| loss_bc4: 5.046e-08|Time: 0.01\n",
      "mean_grad_res: 0.000657\n",
      "mean_grad_bcs: 0.000199\n",
      "It: 11100, loss: 2.516e-01| loss_res: 2.372e-01| loss_f: 2.514e-01| loss_bc1: 1.411e-06| loss_bc2: 1.011e-05| loss_bc3: 1.742e-05| loss_bc4: 6.240e-06|Time: 0.00\n",
      "mean_grad_res: 0.000720\n",
      "mean_grad_bcs: 0.000319\n",
      "It: 11200, loss: 2.851e-01| loss_res: 2.902e-01| loss_f: 2.851e-01| loss_bc1: 4.283e-07| loss_bc2: 4.153e-07| loss_bc3: 4.511e-07| loss_bc4: 4.057e-07|Time: 0.00\n",
      "mean_grad_res: 0.000424\n",
      "mean_grad_bcs: 0.000182\n",
      "It: 11300, loss: 3.000e-01| loss_res: 2.531e-01| loss_f: 3.000e-01| loss_bc1: 1.935e-06| loss_bc2: 3.120e-06| loss_bc3: 4.704e-06| loss_bc4: 3.350e-06|Time: 0.00\n",
      "mean_grad_res: 0.000676\n",
      "mean_grad_bcs: 0.000313\n",
      "It: 11400, loss: 2.338e-01| loss_res: 2.187e-01| loss_f: 2.336e-01| loss_bc1: 1.084e-05| loss_bc2: 5.827e-06| loss_bc3: 4.426e-06| loss_bc4: 1.942e-05|Time: 0.00\n",
      "mean_grad_res: 0.000555\n",
      "mean_grad_bcs: 0.000555\n",
      "It: 11500, loss: 2.101e-01| loss_res: 2.787e-01| loss_f: 2.100e-01| loss_bc1: 1.313e-06| loss_bc2: 6.108e-06| loss_bc3: 6.935e-06| loss_bc4: 7.976e-07|Time: 0.01\n",
      "mean_grad_res: 0.000676\n",
      "mean_grad_bcs: 0.000220\n",
      "It: 11600, loss: 2.880e-01| loss_res: 2.392e-01| loss_f: 2.879e-01| loss_bc1: 3.171e-06| loss_bc2: 3.096e-06| loss_bc3: 2.462e-06| loss_bc4: 2.576e-06|Time: 0.00\n",
      "mean_grad_res: 0.000880\n",
      "mean_grad_bcs: 0.000056\n",
      "It: 11700, loss: 2.589e-01| loss_res: 2.319e-01| loss_f: 2.589e-01| loss_bc1: 3.030e-06| loss_bc2: 3.696e-06| loss_bc3: 1.589e-06| loss_bc4: 1.046e-06|Time: 0.00\n",
      "mean_grad_res: 0.000526\n",
      "mean_grad_bcs: 0.000081\n",
      "It: 11800, loss: 2.482e-01| loss_res: 2.526e-01| loss_f: 2.482e-01| loss_bc1: 5.728e-06| loss_bc2: 1.027e-05| loss_bc3: 3.044e-06| loss_bc4: 6.980e-07|Time: 0.00\n",
      "mean_grad_res: 0.000998\n",
      "mean_grad_bcs: 0.000189\n",
      "It: 11900, loss: 2.704e-01| loss_res: 2.339e-01| loss_f: 2.704e-01| loss_bc1: 5.990e-06| loss_bc2: 4.819e-06| loss_bc3: 1.428e-06| loss_bc4: 2.462e-06|Time: 0.01\n",
      "mean_grad_res: 0.000142\n",
      "mean_grad_bcs: 0.000148\n",
      "It: 12000, loss: 2.852e-01| loss_res: 2.614e-01| loss_f: 2.852e-01| loss_bc1: 5.780e-06| loss_bc2: 5.853e-07| loss_bc3: 3.435e-06| loss_bc4: 1.770e-06|Time: 0.00\n",
      "mean_grad_res: 0.000276\n",
      "mean_grad_bcs: 0.000276\n",
      "It: 12100, loss: 2.795e-01| loss_res: 2.545e-01| loss_f: 2.795e-01| loss_bc1: 1.148e-06| loss_bc2: 4.847e-07| loss_bc3: 2.024e-06| loss_bc4: 3.626e-06|Time: 0.00\n",
      "mean_grad_res: 0.000448\n",
      "mean_grad_bcs: 0.000181\n",
      "It: 12200, loss: 2.830e-01| loss_res: 2.395e-01| loss_f: 2.829e-01| loss_bc1: 1.676e-06| loss_bc2: 2.139e-06| loss_bc3: 1.760e-06| loss_bc4: 9.064e-07|Time: 0.00\n",
      "mean_grad_res: 0.000420\n",
      "mean_grad_bcs: 0.000114\n",
      "It: 12300, loss: 2.831e-01| loss_res: 2.600e-01| loss_f: 2.831e-01| loss_bc1: 1.420e-06| loss_bc2: 8.536e-07| loss_bc3: 3.587e-06| loss_bc4: 3.911e-07|Time: 0.00\n",
      "mean_grad_res: 0.000278\n",
      "mean_grad_bcs: 0.000231\n",
      "It: 12400, loss: 2.333e-01| loss_res: 2.092e-01| loss_f: 2.332e-01| loss_bc1: 1.137e-06| loss_bc2: 8.006e-06| loss_bc3: 1.104e-05| loss_bc4: 3.431e-06|Time: 0.00\n",
      "mean_grad_res: 0.000318\n",
      "mean_grad_bcs: 0.000170\n",
      "It: 12500, loss: 2.999e-01| loss_res: 2.582e-01| loss_f: 2.999e-01| loss_bc1: 1.256e-06| loss_bc2: 2.908e-07| loss_bc3: 1.244e-06| loss_bc4: 2.314e-06|Time: 0.00\n",
      "mean_grad_res: 0.000172\n",
      "mean_grad_bcs: 0.000087\n",
      "It: 12600, loss: 2.899e-01| loss_res: 2.236e-01| loss_f: 2.899e-01| loss_bc1: 1.230e-06| loss_bc2: 3.723e-07| loss_bc3: 6.783e-08| loss_bc4: 5.389e-07|Time: 0.01\n",
      "mean_grad_res: 0.000305\n",
      "mean_grad_bcs: 0.000039\n",
      "It: 12700, loss: 2.112e-01| loss_res: 2.866e-01| loss_f: 2.111e-01| loss_bc1: 1.539e-05| loss_bc2: 2.151e-06| loss_bc3: 4.960e-07| loss_bc4: 6.300e-06|Time: 0.00\n",
      "mean_grad_res: 0.000306\n",
      "mean_grad_bcs: 0.000247\n",
      "It: 12800, loss: 2.665e-01| loss_res: 2.863e-01| loss_f: 2.665e-01| loss_bc1: 3.235e-07| loss_bc2: 1.675e-06| loss_bc3: 1.543e-06| loss_bc4: 2.596e-07|Time: 0.00\n",
      "mean_grad_res: 0.000238\n",
      "mean_grad_bcs: 0.000038\n",
      "It: 12900, loss: 2.483e-01| loss_res: 2.462e-01| loss_f: 2.482e-01| loss_bc1: 3.081e-06| loss_bc2: 1.737e-06| loss_bc3: 4.393e-06| loss_bc4: 6.678e-06|Time: 0.00\n",
      "mean_grad_res: 0.000184\n",
      "mean_grad_bcs: 0.000054\n",
      "It: 13000, loss: 3.023e-01| loss_res: 2.547e-01| loss_f: 3.022e-01| loss_bc1: 6.361e-06| loss_bc2: 6.222e-06| loss_bc3: 6.247e-06| loss_bc4: 6.661e-06|Time: 0.00\n",
      "mean_grad_res: 0.000217\n",
      "mean_grad_bcs: 0.000065\n",
      "It: 13100, loss: 3.134e-01| loss_res: 2.487e-01| loss_f: 3.133e-01| loss_bc1: 3.069e-06| loss_bc2: 1.251e-06| loss_bc3: 2.674e-06| loss_bc4: 4.591e-06|Time: 0.00\n",
      "mean_grad_res: 0.000132\n",
      "mean_grad_bcs: 0.000096\n",
      "It: 13200, loss: 2.861e-01| loss_res: 2.281e-01| loss_f: 2.860e-01| loss_bc1: 2.227e-06| loss_bc2: 9.680e-07| loss_bc3: 2.536e-06| loss_bc4: 1.159e-06|Time: 0.00\n",
      "mean_grad_res: 0.000293\n",
      "mean_grad_bcs: 0.000173\n",
      "It: 13300, loss: 2.737e-01| loss_res: 2.434e-01| loss_f: 2.736e-01| loss_bc1: 1.003e-06| loss_bc2: 1.117e-06| loss_bc3: 3.732e-07| loss_bc4: 4.160e-07|Time: 0.00\n",
      "mean_grad_res: 0.000249\n",
      "mean_grad_bcs: 0.000043\n",
      "It: 13400, loss: 2.740e-01| loss_res: 2.266e-01| loss_f: 2.740e-01| loss_bc1: 1.499e-06| loss_bc2: 2.117e-07| loss_bc3: 5.728e-06| loss_bc4: 6.019e-06|Time: 0.01\n",
      "mean_grad_res: 0.000152\n",
      "mean_grad_bcs: 0.000120\n",
      "It: 13500, loss: 2.543e-01| loss_res: 2.459e-01| loss_f: 2.542e-01| loss_bc1: 6.209e-06| loss_bc2: 4.375e-06| loss_bc3: 3.723e-06| loss_bc4: 5.215e-06|Time: 0.00\n",
      "mean_grad_res: 0.000254\n",
      "mean_grad_bcs: 0.000046\n",
      "It: 13600, loss: 2.794e-01| loss_res: 2.322e-01| loss_f: 2.794e-01| loss_bc1: 5.891e-07| loss_bc2: 8.252e-07| loss_bc3: 7.393e-07| loss_bc4: 4.109e-07|Time: 0.00\n",
      "mean_grad_res: 0.000152\n",
      "mean_grad_bcs: 0.000043\n",
      "It: 13700, loss: 2.816e-01| loss_res: 2.261e-01| loss_f: 2.816e-01| loss_bc1: 4.020e-07| loss_bc2: 6.902e-07| loss_bc3: 1.948e-07| loss_bc4: 1.429e-07|Time: 0.03\n",
      "mean_grad_res: 0.000181\n",
      "mean_grad_bcs: 0.000021\n",
      "It: 13800, loss: 2.743e-01| loss_res: 2.419e-01| loss_f: 2.743e-01| loss_bc1: 1.237e-06| loss_bc2: 9.826e-07| loss_bc3: 1.881e-06| loss_bc4: 3.140e-06|Time: 0.00\n",
      "mean_grad_res: 0.000406\n",
      "mean_grad_bcs: 0.000042\n",
      "It: 13900, loss: 2.708e-01| loss_res: 2.228e-01| loss_f: 2.708e-01| loss_bc1: 1.169e-07| loss_bc2: 2.063e-07| loss_bc3: 2.626e-07| loss_bc4: 3.608e-07|Time: 0.00\n",
      "mean_grad_res: 0.000100\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 14000, loss: 3.167e-01| loss_res: 2.414e-01| loss_f: 3.166e-01| loss_bc1: 5.848e-06| loss_bc2: 4.525e-06| loss_bc3: 4.377e-06| loss_bc4: 5.295e-06|Time: 0.00\n",
      "mean_grad_res: 0.000557\n",
      "mean_grad_bcs: 0.000061\n",
      "It: 14100, loss: 2.656e-01| loss_res: 2.179e-01| loss_f: 2.656e-01| loss_bc1: 1.801e-07| loss_bc2: 2.859e-07| loss_bc3: 2.319e-07| loss_bc4: 1.502e-07|Time: 0.00\n",
      "mean_grad_res: 0.000151\n",
      "mean_grad_bcs: 0.000034\n",
      "It: 14200, loss: 2.597e-01| loss_res: 2.277e-01| loss_f: 2.597e-01| loss_bc1: 6.635e-07| loss_bc2: 3.140e-07| loss_bc3: 1.397e-07| loss_bc4: 4.648e-07|Time: 0.00\n",
      "mean_grad_res: 0.000184\n",
      "mean_grad_bcs: 0.000025\n",
      "It: 14300, loss: 2.482e-01| loss_res: 2.916e-01| loss_f: 2.482e-01| loss_bc1: 8.364e-07| loss_bc2: 1.359e-06| loss_bc3: 7.395e-07| loss_bc4: 1.186e-06|Time: 0.00\n",
      "mean_grad_res: 0.000194\n",
      "mean_grad_bcs: 0.000096\n",
      "It: 14400, loss: 2.562e-01| loss_res: 2.389e-01| loss_f: 2.562e-01| loss_bc1: 8.699e-07| loss_bc2: 4.230e-07| loss_bc3: 1.231e-06| loss_bc4: 1.535e-06|Time: 0.00\n",
      "mean_grad_res: 0.000103\n",
      "mean_grad_bcs: 0.000026\n",
      "It: 14500, loss: 3.161e-01| loss_res: 2.828e-01| loss_f: 3.161e-01| loss_bc1: 4.398e-07| loss_bc2: 2.808e-07| loss_bc3: 3.405e-07| loss_bc4: 3.985e-07|Time: 0.00\n",
      "mean_grad_res: 0.000123\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 14600, loss: 2.607e-01| loss_res: 2.875e-01| loss_f: 2.606e-01| loss_bc1: 5.618e-08| loss_bc2: 1.592e-06| loss_bc3: 5.379e-06| loss_bc4: 1.351e-06|Time: 0.00\n",
      "mean_grad_res: 0.000180\n",
      "mean_grad_bcs: 0.000069\n",
      "It: 14700, loss: 2.776e-01| loss_res: 2.588e-01| loss_f: 2.775e-01| loss_bc1: 2.800e-06| loss_bc2: 4.421e-06| loss_bc3: 1.688e-06| loss_bc4: 4.792e-07|Time: 0.00\n",
      "mean_grad_res: 0.000089\n",
      "mean_grad_bcs: 0.000059\n",
      "It: 14800, loss: 2.610e-01| loss_res: 2.551e-01| loss_f: 2.610e-01| loss_bc1: 3.829e-07| loss_bc2: 4.365e-07| loss_bc3: 8.857e-07| loss_bc4: 1.334e-07|Time: 0.00\n",
      "mean_grad_res: 0.000115\n",
      "mean_grad_bcs: 0.000044\n",
      "It: 14900, loss: 2.952e-01| loss_res: 2.757e-01| loss_f: 2.952e-01| loss_bc1: 7.033e-07| loss_bc2: 1.039e-06| loss_bc3: 1.847e-06| loss_bc4: 1.171e-06|Time: 0.00\n",
      "mean_grad_res: 0.000130\n",
      "mean_grad_bcs: 0.000026\n",
      "It: 15000, loss: 2.105e-01| loss_res: 2.234e-01| loss_f: 2.105e-01| loss_bc1: 1.178e-07| loss_bc2: 1.562e-07| loss_bc3: 2.815e-07| loss_bc4: 2.674e-07|Time: 0.00\n",
      "mean_grad_res: 0.000096\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 15100, loss: 3.143e-01| loss_res: 2.477e-01| loss_f: 3.143e-01| loss_bc1: 2.156e-07| loss_bc2: 8.403e-07| loss_bc3: 1.269e-06| loss_bc4: 2.649e-07|Time: 0.00\n",
      "mean_grad_res: 0.000137\n",
      "mean_grad_bcs: 0.000028\n",
      "It: 15200, loss: 2.277e-01| loss_res: 2.492e-01| loss_f: 2.277e-01| loss_bc1: 5.059e-07| loss_bc2: 9.671e-08| loss_bc3: 1.363e-07| loss_bc4: 1.660e-07|Time: 0.00\n",
      "mean_grad_res: 0.000091\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 15300, loss: 2.834e-01| loss_res: 2.445e-01| loss_f: 2.834e-01| loss_bc1: 4.383e-07| loss_bc2: 4.015e-07| loss_bc3: 1.792e-08| loss_bc4: 8.891e-08|Time: 0.00\n",
      "mean_grad_res: 0.000124\n",
      "mean_grad_bcs: 0.000026\n",
      "It: 15400, loss: 2.656e-01| loss_res: 2.202e-01| loss_f: 2.656e-01| loss_bc1: 1.247e-06| loss_bc2: 1.972e-06| loss_bc3: 2.989e-06| loss_bc4: 1.818e-06|Time: 0.00\n",
      "mean_grad_res: 0.000275\n",
      "mean_grad_bcs: 0.000037\n",
      "It: 15500, loss: 2.329e-01| loss_res: 2.728e-01| loss_f: 2.328e-01| loss_bc1: 2.368e-06| loss_bc2: 4.075e-06| loss_bc3: 5.697e-06| loss_bc4: 3.690e-06|Time: 0.00\n",
      "mean_grad_res: 0.000280\n",
      "mean_grad_bcs: 0.000067\n",
      "It: 15600, loss: 3.121e-01| loss_res: 2.284e-01| loss_f: 3.121e-01| loss_bc1: 1.083e-07| loss_bc2: 6.724e-08| loss_bc3: 6.980e-08| loss_bc4: 5.826e-08|Time: 0.00\n",
      "mean_grad_res: 0.000156\n",
      "mean_grad_bcs: 0.000012\n",
      "It: 15700, loss: 2.620e-01| loss_res: 2.702e-01| loss_f: 2.620e-01| loss_bc1: 2.303e-07| loss_bc2: 2.842e-08| loss_bc3: 1.138e-07| loss_bc4: 3.164e-07|Time: 0.00\n",
      "mean_grad_res: 0.000109\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 15800, loss: 2.549e-01| loss_res: 2.819e-01| loss_f: 2.549e-01| loss_bc1: 8.740e-07| loss_bc2: 3.650e-08| loss_bc3: 3.757e-07| loss_bc4: 2.382e-07|Time: 0.00\n",
      "mean_grad_res: 0.000161\n",
      "mean_grad_bcs: 0.000029\n",
      "It: 15900, loss: 2.458e-01| loss_res: 2.519e-01| loss_f: 2.457e-01| loss_bc1: 6.354e-06| loss_bc2: 9.781e-06| loss_bc3: 4.592e-06| loss_bc4: 2.577e-06|Time: 0.00\n",
      "mean_grad_res: 0.000117\n",
      "mean_grad_bcs: 0.000078\n",
      "It: 16000, loss: 2.987e-01| loss_res: 2.483e-01| loss_f: 2.987e-01| loss_bc1: 8.193e-07| loss_bc2: 6.246e-07| loss_bc3: 2.331e-06| loss_bc4: 2.283e-07|Time: 0.00\n",
      "mean_grad_res: 0.000323\n",
      "mean_grad_bcs: 0.000064\n",
      "It: 16100, loss: 2.715e-01| loss_res: 2.817e-01| loss_f: 2.715e-01| loss_bc1: 1.574e-07| loss_bc2: 1.311e-07| loss_bc3: 1.170e-07| loss_bc4: 3.494e-07|Time: 0.00\n",
      "mean_grad_res: 0.000193\n",
      "mean_grad_bcs: 0.000034\n",
      "It: 16200, loss: 2.413e-01| loss_res: 2.591e-01| loss_f: 2.412e-01| loss_bc1: 5.056e-06| loss_bc2: 2.556e-06| loss_bc3: 2.944e-06| loss_bc4: 7.005e-06|Time: 0.00\n",
      "mean_grad_res: 0.000361\n",
      "mean_grad_bcs: 0.000100\n",
      "It: 16300, loss: 2.629e-01| loss_res: 2.847e-01| loss_f: 2.629e-01| loss_bc1: 1.833e-06| loss_bc2: 1.030e-06| loss_bc3: 1.711e-07| loss_bc4: 6.816e-08|Time: 0.00\n",
      "mean_grad_res: 0.000130\n",
      "mean_grad_bcs: 0.000059\n",
      "It: 16400, loss: 2.986e-01| loss_res: 2.130e-01| loss_f: 2.986e-01| loss_bc1: 5.111e-07| loss_bc2: 2.213e-07| loss_bc3: 5.933e-07| loss_bc4: 2.846e-07|Time: 0.01\n",
      "mean_grad_res: 0.000237\n",
      "mean_grad_bcs: 0.000040\n",
      "It: 16500, loss: 2.771e-01| loss_res: 2.544e-01| loss_f: 2.771e-01| loss_bc1: 3.222e-07| loss_bc2: 1.860e-07| loss_bc3: 3.161e-07| loss_bc4: 3.931e-07|Time: 0.00\n",
      "mean_grad_res: 0.000098\n",
      "mean_grad_bcs: 0.000027\n",
      "It: 16600, loss: 2.640e-01| loss_res: 2.840e-01| loss_f: 2.639e-01| loss_bc1: 4.914e-06| loss_bc2: 2.600e-06| loss_bc3: 3.023e-06| loss_bc4: 4.931e-06|Time: 0.00\n",
      "mean_grad_res: 0.000238\n",
      "mean_grad_bcs: 0.000051\n",
      "It: 16700, loss: 2.993e-01| loss_res: 2.220e-01| loss_f: 2.993e-01| loss_bc1: 6.233e-07| loss_bc2: 4.497e-07| loss_bc3: 6.599e-07| loss_bc4: 1.641e-06|Time: 0.00\n",
      "mean_grad_res: 0.000173\n",
      "mean_grad_bcs: 0.000055\n",
      "It: 16800, loss: 2.489e-01| loss_res: 2.695e-01| loss_f: 2.489e-01| loss_bc1: 6.576e-07| loss_bc2: 4.554e-07| loss_bc3: 4.905e-07| loss_bc4: 6.523e-07|Time: 0.00\n",
      "mean_grad_res: 0.000126\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 16900, loss: 2.533e-01| loss_res: 2.354e-01| loss_f: 2.533e-01| loss_bc1: 1.275e-06| loss_bc2: 8.822e-07| loss_bc3: 7.061e-07| loss_bc4: 1.073e-06|Time: 0.01\n",
      "mean_grad_res: 0.000100\n",
      "mean_grad_bcs: 0.000023\n",
      "It: 17000, loss: 2.739e-01| loss_res: 2.209e-01| loss_f: 2.739e-01| loss_bc1: 7.372e-08| loss_bc2: 7.642e-08| loss_bc3: 1.473e-08| loss_bc4: 1.638e-07|Time: 0.00\n",
      "mean_grad_res: 0.000197\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 17100, loss: 2.603e-01| loss_res: 2.509e-01| loss_f: 2.603e-01| loss_bc1: 1.491e-07| loss_bc2: 1.029e-07| loss_bc3: 6.521e-08| loss_bc4: 1.006e-07|Time: 0.00\n",
      "mean_grad_res: 0.000153\n",
      "mean_grad_bcs: 0.000002\n",
      "It: 17200, loss: 2.706e-01| loss_res: 2.359e-01| loss_f: 2.706e-01| loss_bc1: 1.448e-07| loss_bc2: 1.703e-07| loss_bc3: 1.135e-06| loss_bc4: 9.260e-07|Time: 0.00\n",
      "mean_grad_res: 0.000176\n",
      "mean_grad_bcs: 0.000030\n",
      "It: 17300, loss: 2.385e-01| loss_res: 2.483e-01| loss_f: 2.385e-01| loss_bc1: 3.540e-07| loss_bc2: 5.383e-08| loss_bc3: 9.479e-07| loss_bc4: 4.082e-07|Time: 0.00\n",
      "mean_grad_res: 0.000196\n",
      "mean_grad_bcs: 0.000063\n",
      "It: 17400, loss: 2.903e-01| loss_res: 2.689e-01| loss_f: 2.903e-01| loss_bc1: 7.499e-07| loss_bc2: 3.183e-07| loss_bc3: 1.623e-06| loss_bc4: 2.769e-06|Time: 0.00\n",
      "mean_grad_res: 0.000169\n",
      "mean_grad_bcs: 0.000041\n",
      "It: 17500, loss: 2.641e-01| loss_res: 2.588e-01| loss_f: 2.640e-01| loss_bc1: 2.941e-06| loss_bc2: 1.057e-06| loss_bc3: 1.700e-06| loss_bc4: 3.729e-06|Time: 0.00\n",
      "mean_grad_res: 0.000102\n",
      "mean_grad_bcs: 0.000031\n",
      "It: 17600, loss: 2.463e-01| loss_res: 2.796e-01| loss_f: 2.463e-01| loss_bc1: 8.797e-07| loss_bc2: 2.690e-06| loss_bc3: 3.808e-06| loss_bc4: 2.080e-06|Time: 0.00\n",
      "mean_grad_res: 0.000100\n",
      "mean_grad_bcs: 0.000036\n",
      "It: 17700, loss: 2.952e-01| loss_res: 2.616e-01| loss_f: 2.952e-01| loss_bc1: 2.696e-07| loss_bc2: 2.166e-07| loss_bc3: 1.118e-07| loss_bc4: 1.589e-07|Time: 0.01\n",
      "mean_grad_res: 0.000125\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 17800, loss: 2.665e-01| loss_res: 2.466e-01| loss_f: 2.664e-01| loss_bc1: 3.200e-06| loss_bc2: 3.154e-06| loss_bc3: 2.663e-06| loss_bc4: 2.605e-06|Time: 0.01\n",
      "mean_grad_res: 0.000090\n",
      "mean_grad_bcs: 0.000021\n",
      "It: 17900, loss: 3.093e-01| loss_res: 2.637e-01| loss_f: 3.092e-01| loss_bc1: 9.642e-06| loss_bc2: 1.976e-06| loss_bc3: 8.096e-07| loss_bc4: 4.799e-06|Time: 0.00\n",
      "mean_grad_res: 0.000288\n",
      "mean_grad_bcs: 0.000091\n",
      "It: 18000, loss: 3.191e-01| loss_res: 2.650e-01| loss_f: 3.191e-01| loss_bc1: 3.900e-07| loss_bc2: 2.118e-07| loss_bc3: 1.877e-07| loss_bc4: 3.830e-07|Time: 0.01\n",
      "mean_grad_res: 0.000117\n",
      "mean_grad_bcs: 0.000031\n",
      "It: 18100, loss: 3.380e-01| loss_res: 2.501e-01| loss_f: 3.380e-01| loss_bc1: 6.312e-06| loss_bc2: 6.268e-06| loss_bc3: 1.507e-06| loss_bc4: 5.524e-07|Time: 0.01\n",
      "mean_grad_res: 0.000428\n",
      "mean_grad_bcs: 0.000096\n",
      "It: 18200, loss: 2.266e-01| loss_res: 2.211e-01| loss_f: 2.266e-01| loss_bc1: 1.388e-06| loss_bc2: 1.529e-07| loss_bc3: 1.881e-06| loss_bc4: 4.550e-07|Time: 0.00\n",
      "mean_grad_res: 0.000186\n",
      "mean_grad_bcs: 0.000095\n",
      "It: 18300, loss: 2.953e-01| loss_res: 2.279e-01| loss_f: 2.953e-01| loss_bc1: 2.974e-06| loss_bc2: 2.526e-06| loss_bc3: 2.144e-06| loss_bc4: 2.651e-06|Time: 0.00\n",
      "mean_grad_res: 0.000115\n",
      "mean_grad_bcs: 0.000031\n",
      "It: 18400, loss: 2.614e-01| loss_res: 3.182e-01| loss_f: 2.614e-01| loss_bc1: 2.249e-07| loss_bc2: 1.645e-06| loss_bc3: 4.432e-07| loss_bc4: 1.064e-07|Time: 0.00\n",
      "mean_grad_res: 0.000193\n",
      "mean_grad_bcs: 0.000026\n",
      "It: 18500, loss: 2.825e-01| loss_res: 2.778e-01| loss_f: 2.825e-01| loss_bc1: 5.752e-07| loss_bc2: 4.500e-07| loss_bc3: 2.593e-07| loss_bc4: 3.241e-07|Time: 0.00\n",
      "mean_grad_res: 0.000139\n",
      "mean_grad_bcs: 0.000007\n",
      "It: 18600, loss: 2.598e-01| loss_res: 2.595e-01| loss_f: 2.597e-01| loss_bc1: 2.139e-06| loss_bc2: 2.388e-06| loss_bc3: 9.346e-07| loss_bc4: 5.083e-07|Time: 0.00\n",
      "mean_grad_res: 0.000163\n",
      "mean_grad_bcs: 0.000038\n",
      "It: 18700, loss: 3.036e-01| loss_res: 2.316e-01| loss_f: 3.036e-01| loss_bc1: 8.006e-07| loss_bc2: 2.876e-08| loss_bc3: 5.124e-07| loss_bc4: 1.446e-06|Time: 0.00\n",
      "mean_grad_res: 0.000237\n",
      "mean_grad_bcs: 0.000039\n",
      "It: 18800, loss: 3.020e-01| loss_res: 2.576e-01| loss_f: 3.020e-01| loss_bc1: 5.641e-08| loss_bc2: 1.484e-07| loss_bc3: 1.055e-07| loss_bc4: 8.662e-08|Time: 0.00\n",
      "mean_grad_res: 0.000087\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 18900, loss: 2.840e-01| loss_res: 2.617e-01| loss_f: 2.840e-01| loss_bc1: 8.022e-07| loss_bc2: 1.201e-06| loss_bc3: 1.415e-06| loss_bc4: 1.028e-06|Time: 0.00\n",
      "mean_grad_res: 0.000148\n",
      "mean_grad_bcs: 0.000025\n",
      "It: 19000, loss: 2.633e-01| loss_res: 2.449e-01| loss_f: 2.633e-01| loss_bc1: 3.435e-07| loss_bc2: 5.370e-07| loss_bc3: 4.858e-07| loss_bc4: 5.676e-07|Time: 0.00\n",
      "mean_grad_res: 0.000123\n",
      "mean_grad_bcs: 0.000031\n",
      "It: 19100, loss: 2.689e-01| loss_res: 2.359e-01| loss_f: 2.689e-01| loss_bc1: 2.198e-06| loss_bc2: 1.121e-06| loss_bc3: 6.668e-07| loss_bc4: 1.385e-06|Time: 0.00\n",
      "mean_grad_res: 0.000086\n",
      "mean_grad_bcs: 0.000030\n",
      "It: 19200, loss: 2.840e-01| loss_res: 2.204e-01| loss_f: 2.840e-01| loss_bc1: 6.761e-07| loss_bc2: 2.224e-07| loss_bc3: 2.443e-07| loss_bc4: 6.809e-07|Time: 0.00\n",
      "mean_grad_res: 0.000075\n",
      "mean_grad_bcs: 0.000033\n",
      "It: 19300, loss: 2.854e-01| loss_res: 2.538e-01| loss_f: 2.854e-01| loss_bc1: 5.777e-08| loss_bc2: 1.932e-09| loss_bc3: 3.122e-08| loss_bc4: 1.275e-08|Time: 0.00\n",
      "mean_grad_res: 0.000179\n",
      "mean_grad_bcs: 0.000006\n",
      "It: 19400, loss: 2.833e-01| loss_res: 2.578e-01| loss_f: 2.833e-01| loss_bc1: 2.032e-06| loss_bc2: 4.315e-06| loss_bc3: 2.782e-06| loss_bc4: 9.538e-07|Time: 0.00\n",
      "mean_grad_res: 0.000168\n",
      "mean_grad_bcs: 0.000051\n",
      "It: 19500, loss: 2.401e-01| loss_res: 2.522e-01| loss_f: 2.401e-01| loss_bc1: 1.228e-07| loss_bc2: 3.039e-07| loss_bc3: 5.430e-07| loss_bc4: 2.376e-07|Time: 0.00\n",
      "mean_grad_res: 0.000126\n",
      "mean_grad_bcs: 0.000026\n",
      "It: 19600, loss: 2.351e-01| loss_res: 2.515e-01| loss_f: 2.351e-01| loss_bc1: 6.849e-07| loss_bc2: 1.401e-06| loss_bc3: 1.554e-06| loss_bc4: 5.089e-07|Time: 0.00\n",
      "mean_grad_res: 0.000191\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 19700, loss: 2.669e-01| loss_res: 2.502e-01| loss_f: 2.668e-01| loss_bc1: 1.840e-06| loss_bc2: 1.831e-06| loss_bc3: 8.397e-07| loss_bc4: 1.049e-06|Time: 0.00\n",
      "mean_grad_res: 0.000281\n",
      "mean_grad_bcs: 0.000023\n",
      "It: 19800, loss: 2.713e-01| loss_res: 2.316e-01| loss_f: 2.713e-01| loss_bc1: 8.042e-07| loss_bc2: 3.102e-07| loss_bc3: 1.275e-07| loss_bc4: 5.523e-08|Time: 0.00\n",
      "mean_grad_res: 0.000245\n",
      "mean_grad_bcs: 0.000024\n",
      "It: 19900, loss: 2.708e-01| loss_res: 2.327e-01| loss_f: 2.708e-01| loss_bc1: 5.371e-07| loss_bc2: 4.155e-07| loss_bc3: 6.767e-07| loss_bc4: 6.622e-07|Time: 0.01\n",
      "mean_grad_res: 0.000141\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 20000, loss: 2.602e-01| loss_res: 2.420e-01| loss_f: 2.602e-01| loss_bc1: 1.446e-06| loss_bc2: 1.223e-06| loss_bc3: 5.411e-06| loss_bc4: 5.968e-06|Time: 0.00\n",
      "mean_grad_res: 0.000078\n",
      "mean_grad_bcs: 0.000079\n",
      "It: 20100, loss: 2.974e-01| loss_res: 2.495e-01| loss_f: 2.974e-01| loss_bc1: 1.548e-06| loss_bc2: 2.814e-06| loss_bc3: 5.940e-07| loss_bc4: 1.602e-07|Time: 0.00\n",
      "mean_grad_res: 0.000079\n",
      "mean_grad_bcs: 0.000071\n",
      "It: 20200, loss: 2.699e-01| loss_res: 2.812e-01| loss_f: 2.699e-01| loss_bc1: 2.978e-06| loss_bc2: 3.200e-06| loss_bc3: 7.788e-07| loss_bc4: 7.115e-07|Time: 0.00\n",
      "mean_grad_res: 0.000206\n",
      "mean_grad_bcs: 0.000042\n",
      "It: 20300, loss: 3.223e-01| loss_res: 2.447e-01| loss_f: 3.223e-01| loss_bc1: 5.308e-07| loss_bc2: 1.231e-06| loss_bc3: 4.727e-07| loss_bc4: 2.364e-07|Time: 0.00\n",
      "mean_grad_res: 0.000400\n",
      "mean_grad_bcs: 0.000026\n",
      "It: 20400, loss: 2.606e-01| loss_res: 3.008e-01| loss_f: 2.606e-01| loss_bc1: 1.820e-07| loss_bc2: 4.057e-09| loss_bc3: 1.403e-07| loss_bc4: 4.405e-07|Time: 0.00\n",
      "mean_grad_res: 0.000224\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 20500, loss: 2.675e-01| loss_res: 2.700e-01| loss_f: 2.675e-01| loss_bc1: 2.177e-07| loss_bc2: 2.190e-07| loss_bc3: 1.251e-06| loss_bc4: 2.043e-07|Time: 0.00\n",
      "mean_grad_res: 0.000342\n",
      "mean_grad_bcs: 0.000030\n",
      "It: 20600, loss: 3.143e-01| loss_res: 2.427e-01| loss_f: 3.143e-01| loss_bc1: 2.147e-06| loss_bc2: 2.148e-06| loss_bc3: 1.733e-06| loss_bc4: 1.710e-06|Time: 0.00\n",
      "mean_grad_res: 0.000127\n",
      "mean_grad_bcs: 0.000018\n",
      "It: 20700, loss: 3.150e-01| loss_res: 2.347e-01| loss_f: 3.150e-01| loss_bc1: 1.332e-07| loss_bc2: 2.087e-07| loss_bc3: 1.509e-07| loss_bc4: 4.333e-07|Time: 0.01\n",
      "mean_grad_res: 0.000081\n",
      "mean_grad_bcs: 0.000021\n",
      "It: 20800, loss: 2.558e-01| loss_res: 2.774e-01| loss_f: 2.557e-01| loss_bc1: 8.564e-08| loss_bc2: 4.506e-08| loss_bc3: 2.962e-07| loss_bc4: 5.203e-07|Time: 0.00\n",
      "mean_grad_res: 0.000087\n",
      "mean_grad_bcs: 0.000012\n",
      "It: 20900, loss: 2.944e-01| loss_res: 2.852e-01| loss_f: 2.944e-01| loss_bc1: 2.172e-06| loss_bc2: 5.726e-06| loss_bc3: 3.247e-06| loss_bc4: 6.291e-07|Time: 0.00\n",
      "mean_grad_res: 0.000174\n",
      "mean_grad_bcs: 0.000066\n",
      "It: 21000, loss: 2.661e-01| loss_res: 2.551e-01| loss_f: 2.661e-01| loss_bc1: 5.159e-07| loss_bc2: 3.784e-07| loss_bc3: 3.157e-07| loss_bc4: 4.464e-07|Time: 0.00\n",
      "mean_grad_res: 0.000131\n",
      "mean_grad_bcs: 0.000007\n",
      "It: 21100, loss: 2.492e-01| loss_res: 3.078e-01| loss_f: 2.491e-01| loss_bc1: 2.164e-06| loss_bc2: 1.551e-06| loss_bc3: 2.116e-06| loss_bc4: 3.053e-06|Time: 0.00\n",
      "mean_grad_res: 0.000210\n",
      "mean_grad_bcs: 0.000033\n",
      "It: 21200, loss: 2.767e-01| loss_res: 2.646e-01| loss_f: 2.767e-01| loss_bc1: 8.121e-07| loss_bc2: 8.405e-07| loss_bc3: 5.664e-07| loss_bc4: 3.710e-07|Time: 0.00\n",
      "mean_grad_res: 0.000114\n",
      "mean_grad_bcs: 0.000024\n",
      "It: 21300, loss: 2.294e-01| loss_res: 2.581e-01| loss_f: 2.294e-01| loss_bc1: 4.168e-07| loss_bc2: 5.263e-07| loss_bc3: 2.477e-07| loss_bc4: 4.224e-07|Time: 0.00\n",
      "mean_grad_res: 0.000095\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 21400, loss: 2.461e-01| loss_res: 2.669e-01| loss_f: 2.461e-01| loss_bc1: 7.834e-08| loss_bc2: 4.046e-08| loss_bc3: 7.829e-08| loss_bc4: 8.819e-08|Time: 0.00\n",
      "mean_grad_res: 0.000126\n",
      "mean_grad_bcs: 0.000005\n",
      "It: 21500, loss: 2.986e-01| loss_res: 2.580e-01| loss_f: 2.986e-01| loss_bc1: 1.746e-06| loss_bc2: 2.096e-06| loss_bc3: 1.640e-06| loss_bc4: 1.479e-06|Time: 0.00\n",
      "mean_grad_res: 0.000053\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 21600, loss: 3.084e-01| loss_res: 2.178e-01| loss_f: 3.084e-01| loss_bc1: 1.931e-08| loss_bc2: 1.436e-07| loss_bc3: 1.603e-07| loss_bc4: 2.753e-08|Time: 0.00\n",
      "mean_grad_res: 0.000173\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 21700, loss: 2.503e-01| loss_res: 2.089e-01| loss_f: 2.503e-01| loss_bc1: 1.711e-06| loss_bc2: 1.922e-06| loss_bc3: 1.852e-06| loss_bc4: 1.627e-06|Time: 0.00\n",
      "mean_grad_res: 0.000176\n",
      "mean_grad_bcs: 0.000015\n",
      "It: 21800, loss: 2.810e-01| loss_res: 2.228e-01| loss_f: 2.810e-01| loss_bc1: 5.147e-07| loss_bc2: 4.038e-07| loss_bc3: 2.294e-07| loss_bc4: 3.125e-07|Time: 0.00\n",
      "mean_grad_res: 0.000074\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 21900, loss: 2.642e-01| loss_res: 2.505e-01| loss_f: 2.641e-01| loss_bc1: 1.876e-06| loss_bc2: 1.881e-06| loss_bc3: 1.872e-06| loss_bc4: 1.854e-06|Time: 0.00\n",
      "mean_grad_res: 0.000165\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 22000, loss: 2.374e-01| loss_res: 2.789e-01| loss_f: 2.374e-01| loss_bc1: 7.215e-08| loss_bc2: 1.687e-07| loss_bc3: 5.608e-08| loss_bc4: 2.207e-07|Time: 0.00\n",
      "mean_grad_res: 0.000137\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 22100, loss: 2.918e-01| loss_res: 2.570e-01| loss_f: 2.918e-01| loss_bc1: 3.553e-08| loss_bc2: 6.416e-08| loss_bc3: 3.575e-08| loss_bc4: 4.695e-08|Time: 0.00\n",
      "mean_grad_res: 0.000119\n",
      "mean_grad_bcs: 0.000008\n",
      "It: 22200, loss: 2.522e-01| loss_res: 2.439e-01| loss_f: 2.522e-01| loss_bc1: 5.789e-07| loss_bc2: 2.148e-07| loss_bc3: 4.597e-07| loss_bc4: 1.081e-06|Time: 0.00\n",
      "mean_grad_res: 0.000212\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 22300, loss: 2.804e-01| loss_res: 2.449e-01| loss_f: 2.804e-01| loss_bc1: 3.411e-07| loss_bc2: 2.392e-07| loss_bc3: 1.328e-06| loss_bc4: 1.787e-07|Time: 0.00\n",
      "mean_grad_res: 0.000261\n",
      "mean_grad_bcs: 0.000040\n",
      "It: 22400, loss: 3.360e-01| loss_res: 2.477e-01| loss_f: 3.360e-01| loss_bc1: 2.968e-07| loss_bc2: 4.926e-08| loss_bc3: 3.996e-08| loss_bc4: 4.098e-07|Time: 0.01\n",
      "mean_grad_res: 0.000089\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 22500, loss: 2.861e-01| loss_res: 2.147e-01| loss_f: 2.861e-01| loss_bc1: 2.558e-07| loss_bc2: 5.167e-07| loss_bc3: 4.063e-07| loss_bc4: 1.194e-07|Time: 0.00\n",
      "mean_grad_res: 0.000077\n",
      "mean_grad_bcs: 0.000013\n",
      "It: 22600, loss: 3.032e-01| loss_res: 2.227e-01| loss_f: 3.032e-01| loss_bc1: 3.530e-08| loss_bc2: 2.943e-08| loss_bc3: 4.781e-08| loss_bc4: 5.195e-08|Time: 0.00\n",
      "mean_grad_res: 0.000092\n",
      "mean_grad_bcs: 0.000006\n",
      "It: 22700, loss: 2.545e-01| loss_res: 2.295e-01| loss_f: 2.545e-01| loss_bc1: 1.257e-07| loss_bc2: 6.621e-07| loss_bc3: 8.084e-07| loss_bc4: 1.236e-07|Time: 0.00\n",
      "mean_grad_res: 0.000199\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 22800, loss: 2.799e-01| loss_res: 2.691e-01| loss_f: 2.798e-01| loss_bc1: 1.618e-06| loss_bc2: 2.211e-06| loss_bc3: 3.209e-06| loss_bc4: 2.564e-06|Time: 0.00\n",
      "mean_grad_res: 0.000070\n",
      "mean_grad_bcs: 0.000033\n",
      "It: 22900, loss: 2.803e-01| loss_res: 2.846e-01| loss_f: 2.802e-01| loss_bc1: 2.239e-06| loss_bc2: 1.193e-06| loss_bc3: 1.333e-07| loss_bc4: 8.168e-07|Time: 0.00\n",
      "mean_grad_res: 0.000071\n",
      "mean_grad_bcs: 0.000044\n",
      "It: 23000, loss: 2.602e-01| loss_res: 2.432e-01| loss_f: 2.602e-01| loss_bc1: 4.014e-08| loss_bc2: 1.284e-07| loss_bc3: 1.400e-07| loss_bc4: 3.470e-07|Time: 0.00\n",
      "mean_grad_res: 0.000071\n",
      "mean_grad_bcs: 0.000018\n",
      "It: 23100, loss: 2.796e-01| loss_res: 2.409e-01| loss_f: 2.796e-01| loss_bc1: 3.903e-07| loss_bc2: 4.048e-07| loss_bc3: 1.387e-06| loss_bc4: 1.300e-06|Time: 0.00\n",
      "mean_grad_res: 0.000182\n",
      "mean_grad_bcs: 0.000030\n",
      "It: 23200, loss: 2.750e-01| loss_res: 2.598e-01| loss_f: 2.750e-01| loss_bc1: 1.213e-07| loss_bc2: 1.980e-07| loss_bc3: 1.718e-06| loss_bc4: 3.918e-07|Time: 0.00\n",
      "mean_grad_res: 0.000102\n",
      "mean_grad_bcs: 0.000040\n",
      "It: 23300, loss: 2.578e-01| loss_res: 2.629e-01| loss_f: 2.578e-01| loss_bc1: 1.247e-07| loss_bc2: 8.266e-08| loss_bc3: 1.650e-09| loss_bc4: 1.099e-08|Time: 0.00\n",
      "mean_grad_res: 0.000105\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 23400, loss: 2.299e-01| loss_res: 2.160e-01| loss_f: 2.299e-01| loss_bc1: 3.093e-08| loss_bc2: 2.373e-07| loss_bc3: 3.237e-07| loss_bc4: 6.364e-08|Time: 0.00\n",
      "mean_grad_res: 0.000059\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 23500, loss: 2.479e-01| loss_res: 2.698e-01| loss_f: 2.479e-01| loss_bc1: 2.156e-07| loss_bc2: 4.683e-07| loss_bc3: 1.290e-06| loss_bc4: 7.187e-07|Time: 0.00\n",
      "mean_grad_res: 0.000116\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 23600, loss: 3.155e-01| loss_res: 2.725e-01| loss_f: 3.155e-01| loss_bc1: 3.244e-06| loss_bc2: 2.763e-06| loss_bc3: 3.068e-06| loss_bc4: 3.309e-06|Time: 0.00\n",
      "mean_grad_res: 0.000161\n",
      "mean_grad_bcs: 0.000036\n",
      "It: 23700, loss: 2.805e-01| loss_res: 2.567e-01| loss_f: 2.805e-01| loss_bc1: 6.852e-07| loss_bc2: 6.077e-07| loss_bc3: 4.653e-07| loss_bc4: 6.037e-07|Time: 0.00\n",
      "mean_grad_res: 0.000138\n",
      "mean_grad_bcs: 0.000013\n",
      "It: 23800, loss: 2.513e-01| loss_res: 2.922e-01| loss_f: 2.513e-01| loss_bc1: 3.316e-08| loss_bc2: 2.307e-08| loss_bc3: 4.234e-08| loss_bc4: 1.835e-08|Time: 0.00\n",
      "mean_grad_res: 0.000069\n",
      "mean_grad_bcs: 0.000006\n",
      "It: 23900, loss: 2.696e-01| loss_res: 2.357e-01| loss_f: 2.696e-01| loss_bc1: 4.503e-07| loss_bc2: 2.218e-07| loss_bc3: 8.026e-08| loss_bc4: 4.307e-07|Time: 0.00\n",
      "mean_grad_res: 0.000221\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 24000, loss: 3.007e-01| loss_res: 2.621e-01| loss_f: 3.007e-01| loss_bc1: 4.018e-08| loss_bc2: 2.202e-09| loss_bc3: 1.874e-07| loss_bc4: 4.620e-08|Time: 0.00\n",
      "mean_grad_res: 0.000123\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 24100, loss: 2.728e-01| loss_res: 2.986e-01| loss_f: 2.727e-01| loss_bc1: 1.461e-06| loss_bc2: 8.819e-07| loss_bc3: 3.669e-07| loss_bc4: 9.489e-07|Time: 0.00\n",
      "mean_grad_res: 0.000103\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 24200, loss: 2.977e-01| loss_res: 2.513e-01| loss_f: 2.977e-01| loss_bc1: 5.523e-07| loss_bc2: 2.524e-07| loss_bc3: 2.746e-07| loss_bc4: 5.707e-07|Time: 0.00\n",
      "mean_grad_res: 0.000164\n",
      "mean_grad_bcs: 0.000015\n",
      "It: 24300, loss: 2.602e-01| loss_res: 2.595e-01| loss_f: 2.602e-01| loss_bc1: 7.773e-07| loss_bc2: 6.634e-07| loss_bc3: 4.592e-07| loss_bc4: 6.341e-07|Time: 0.00\n",
      "mean_grad_res: 0.000091\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 24400, loss: 2.705e-01| loss_res: 2.657e-01| loss_f: 2.705e-01| loss_bc1: 3.068e-08| loss_bc2: 4.966e-08| loss_bc3: 3.114e-08| loss_bc4: 2.372e-08|Time: 0.00\n",
      "mean_grad_res: 0.000030\n",
      "mean_grad_bcs: 0.000002\n",
      "It: 24500, loss: 2.524e-01| loss_res: 2.519e-01| loss_f: 2.524e-01| loss_bc1: 1.526e-07| loss_bc2: 9.730e-07| loss_bc3: 1.009e-07| loss_bc4: 6.260e-07|Time: 0.01\n",
      "mean_grad_res: 0.000137\n",
      "mean_grad_bcs: 0.000040\n",
      "It: 24600, loss: 2.797e-01| loss_res: 2.984e-01| loss_f: 2.797e-01| loss_bc1: 5.614e-08| loss_bc2: 3.431e-08| loss_bc3: 5.791e-08| loss_bc4: 5.073e-08|Time: 0.00\n",
      "mean_grad_res: 0.000113\n",
      "mean_grad_bcs: 0.000007\n",
      "It: 24700, loss: 2.591e-01| loss_res: 2.485e-01| loss_f: 2.591e-01| loss_bc1: 3.870e-07| loss_bc2: 3.388e-07| loss_bc3: 4.522e-07| loss_bc4: 5.184e-07|Time: 0.00\n",
      "mean_grad_res: 0.000168\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 24800, loss: 2.733e-01| loss_res: 2.590e-01| loss_f: 2.733e-01| loss_bc1: 3.089e-08| loss_bc2: 3.052e-07| loss_bc3: 4.702e-07| loss_bc4: 8.143e-08|Time: 0.00\n",
      "mean_grad_res: 0.000218\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 24900, loss: 2.947e-01| loss_res: 2.755e-01| loss_f: 2.947e-01| loss_bc1: 3.644e-07| loss_bc2: 3.348e-07| loss_bc3: 5.620e-07| loss_bc4: 2.075e-07|Time: 0.00\n",
      "mean_grad_res: 0.000213\n",
      "mean_grad_bcs: 0.000035\n",
      "It: 25000, loss: 2.558e-01| loss_res: 2.926e-01| loss_f: 2.558e-01| loss_bc1: 5.450e-07| loss_bc2: 1.529e-07| loss_bc3: 1.377e-07| loss_bc4: 2.575e-07|Time: 0.00\n",
      "mean_grad_res: 0.000092\n",
      "mean_grad_bcs: 0.000021\n",
      "It: 25100, loss: 3.012e-01| loss_res: 2.714e-01| loss_f: 3.012e-01| loss_bc1: 3.743e-06| loss_bc2: 2.581e-06| loss_bc3: 2.697e-06| loss_bc4: 3.890e-06|Time: 0.00\n",
      "mean_grad_res: 0.000140\n",
      "mean_grad_bcs: 0.000032\n",
      "It: 25200, loss: 2.956e-01| loss_res: 2.355e-01| loss_f: 2.956e-01| loss_bc1: 3.011e-07| loss_bc2: 8.801e-07| loss_bc3: 1.497e-06| loss_bc4: 8.815e-07|Time: 0.00\n",
      "mean_grad_res: 0.000102\n",
      "mean_grad_bcs: 0.000036\n",
      "It: 25300, loss: 2.708e-01| loss_res: 2.808e-01| loss_f: 2.708e-01| loss_bc1: 1.947e-06| loss_bc2: 1.661e-06| loss_bc3: 3.201e-06| loss_bc4: 4.051e-06|Time: 0.00\n",
      "mean_grad_res: 0.000172\n",
      "mean_grad_bcs: 0.000049\n",
      "It: 25400, loss: 2.642e-01| loss_res: 2.413e-01| loss_f: 2.642e-01| loss_bc1: 5.496e-08| loss_bc2: 1.418e-07| loss_bc3: 1.779e-07| loss_bc4: 6.161e-08|Time: 0.01\n",
      "mean_grad_res: 0.000113\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 25500, loss: 2.915e-01| loss_res: 2.377e-01| loss_f: 2.915e-01| loss_bc1: 4.556e-08| loss_bc2: 1.058e-07| loss_bc3: 6.645e-09| loss_bc4: 1.440e-08|Time: 0.00\n",
      "mean_grad_res: 0.000136\n",
      "mean_grad_bcs: 0.000008\n",
      "It: 25600, loss: 2.697e-01| loss_res: 2.191e-01| loss_f: 2.697e-01| loss_bc1: 1.868e-08| loss_bc2: 1.737e-08| loss_bc3: 3.014e-08| loss_bc4: 1.994e-08|Time: 0.00\n",
      "mean_grad_res: 0.000075\n",
      "mean_grad_bcs: 0.000004\n",
      "It: 25700, loss: 2.738e-01| loss_res: 2.320e-01| loss_f: 2.738e-01| loss_bc1: 1.434e-07| loss_bc2: 2.598e-07| loss_bc3: 5.999e-07| loss_bc4: 3.151e-07|Time: 0.00\n",
      "mean_grad_res: 0.000063\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 25800, loss: 2.371e-01| loss_res: 2.616e-01| loss_f: 2.371e-01| loss_bc1: 1.692e-06| loss_bc2: 4.561e-07| loss_bc3: 1.507e-06| loss_bc4: 3.134e-06|Time: 0.00\n",
      "mean_grad_res: 0.000117\n",
      "mean_grad_bcs: 0.000042\n",
      "It: 25900, loss: 2.758e-01| loss_res: 2.102e-01| loss_f: 2.758e-01| loss_bc1: 1.787e-06| loss_bc2: 1.227e-06| loss_bc3: 6.641e-07| loss_bc4: 1.134e-06|Time: 0.00\n",
      "mean_grad_res: 0.000127\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 26000, loss: 2.457e-01| loss_res: 2.693e-01| loss_f: 2.457e-01| loss_bc1: 4.671e-07| loss_bc2: 4.510e-07| loss_bc3: 1.501e-06| loss_bc4: 7.421e-07|Time: 0.00\n",
      "mean_grad_res: 0.000205\n",
      "mean_grad_bcs: 0.000063\n",
      "It: 26100, loss: 2.609e-01| loss_res: 2.250e-01| loss_f: 2.609e-01| loss_bc1: 1.189e-07| loss_bc2: 6.215e-10| loss_bc3: 5.048e-07| loss_bc4: 1.988e-07|Time: 0.00\n",
      "mean_grad_res: 0.000137\n",
      "mean_grad_bcs: 0.000021\n",
      "It: 26200, loss: 2.235e-01| loss_res: 2.632e-01| loss_f: 2.235e-01| loss_bc1: 2.674e-07| loss_bc2: 1.150e-07| loss_bc3: 3.185e-07| loss_bc4: 5.399e-07|Time: 0.00\n",
      "mean_grad_res: 0.000167\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 26300, loss: 2.919e-01| loss_res: 2.740e-01| loss_f: 2.919e-01| loss_bc1: 2.998e-07| loss_bc2: 5.654e-07| loss_bc3: 9.206e-07| loss_bc4: 2.263e-07|Time: 0.00\n",
      "mean_grad_res: 0.000100\n",
      "mean_grad_bcs: 0.000050\n",
      "It: 26400, loss: 2.655e-01| loss_res: 2.106e-01| loss_f: 2.655e-01| loss_bc1: 6.486e-08| loss_bc2: 3.246e-08| loss_bc3: 2.195e-08| loss_bc4: 5.812e-08|Time: 0.00\n",
      "mean_grad_res: 0.000099\n",
      "mean_grad_bcs: 0.000005\n",
      "It: 26500, loss: 2.581e-01| loss_res: 2.728e-01| loss_f: 2.581e-01| loss_bc1: 3.005e-07| loss_bc2: 1.181e-07| loss_bc3: 2.434e-07| loss_bc4: 5.747e-07|Time: 0.00\n",
      "mean_grad_res: 0.000131\n",
      "mean_grad_bcs: 0.000021\n",
      "It: 26600, loss: 3.079e-01| loss_res: 2.678e-01| loss_f: 3.078e-01| loss_bc1: 3.266e-08| loss_bc2: 3.411e-07| loss_bc3: 3.959e-07| loss_bc4: 3.313e-08|Time: 0.00\n",
      "mean_grad_res: 0.000286\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 26700, loss: 2.656e-01| loss_res: 2.662e-01| loss_f: 2.656e-01| loss_bc1: 3.938e-07| loss_bc2: 3.737e-07| loss_bc3: 5.003e-07| loss_bc4: 5.593e-07|Time: 0.00\n",
      "mean_grad_res: 0.000076\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 26800, loss: 2.671e-01| loss_res: 2.637e-01| loss_f: 2.671e-01| loss_bc1: 7.223e-08| loss_bc2: 9.198e-08| loss_bc3: 2.069e-07| loss_bc4: 1.571e-07|Time: 0.00\n",
      "mean_grad_res: 0.000125\n",
      "mean_grad_bcs: 0.000005\n",
      "It: 26900, loss: 2.558e-01| loss_res: 2.405e-01| loss_f: 2.558e-01| loss_bc1: 6.268e-08| loss_bc2: 4.794e-08| loss_bc3: 2.066e-07| loss_bc4: 1.738e-07|Time: 0.00\n",
      "mean_grad_res: 0.000143\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 27000, loss: 3.072e-01| loss_res: 2.369e-01| loss_f: 3.072e-01| loss_bc1: 4.226e-09| loss_bc2: 3.388e-09| loss_bc3: 5.049e-08| loss_bc4: 5.141e-08|Time: 0.00\n",
      "mean_grad_res: 0.000092\n",
      "mean_grad_bcs: 0.000004\n",
      "It: 27100, loss: 2.574e-01| loss_res: 2.879e-01| loss_f: 2.574e-01| loss_bc1: 3.926e-07| loss_bc2: 1.550e-07| loss_bc3: 1.709e-07| loss_bc4: 6.639e-07|Time: 0.00\n",
      "mean_grad_res: 0.000173\n",
      "mean_grad_bcs: 0.000027\n",
      "It: 27200, loss: 2.503e-01| loss_res: 2.485e-01| loss_f: 2.503e-01| loss_bc1: 3.577e-07| loss_bc2: 7.239e-08| loss_bc3: 1.845e-07| loss_bc4: 8.404e-07|Time: 0.00\n",
      "mean_grad_res: 0.000160\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 27300, loss: 3.054e-01| loss_res: 2.406e-01| loss_f: 3.054e-01| loss_bc1: 8.692e-07| loss_bc2: 7.159e-07| loss_bc3: 2.036e-07| loss_bc4: 6.669e-07|Time: 0.00\n",
      "mean_grad_res: 0.000294\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 27400, loss: 2.221e-01| loss_res: 2.468e-01| loss_f: 2.221e-01| loss_bc1: 8.033e-07| loss_bc2: 2.441e-07| loss_bc3: 4.265e-07| loss_bc4: 1.021e-06|Time: 0.00\n",
      "mean_grad_res: 0.000092\n",
      "mean_grad_bcs: 0.000018\n",
      "It: 27500, loss: 3.037e-01| loss_res: 2.221e-01| loss_f: 3.036e-01| loss_bc1: 7.805e-07| loss_bc2: 9.095e-08| loss_bc3: 1.615e-07| loss_bc4: 2.153e-07|Time: 0.00\n",
      "mean_grad_res: 0.000241\n",
      "mean_grad_bcs: 0.000029\n",
      "It: 27600, loss: 2.593e-01| loss_res: 2.900e-01| loss_f: 2.593e-01| loss_bc1: 5.703e-07| loss_bc2: 1.730e-06| loss_bc3: 6.183e-07| loss_bc4: 1.389e-07|Time: 0.00\n",
      "mean_grad_res: 0.000113\n",
      "mean_grad_bcs: 0.000028\n",
      "It: 27700, loss: 2.647e-01| loss_res: 2.694e-01| loss_f: 2.647e-01| loss_bc1: 2.309e-07| loss_bc2: 2.227e-07| loss_bc3: 2.947e-07| loss_bc4: 6.485e-07|Time: 0.01\n",
      "mean_grad_res: 0.000078\n",
      "mean_grad_bcs: 0.000022\n",
      "It: 27800, loss: 2.588e-01| loss_res: 2.735e-01| loss_f: 2.588e-01| loss_bc1: 2.285e-08| loss_bc2: 3.546e-07| loss_bc3: 6.451e-07| loss_bc4: 6.997e-08|Time: 0.00\n",
      "mean_grad_res: 0.000095\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 27900, loss: 2.354e-01| loss_res: 2.716e-01| loss_f: 2.354e-01| loss_bc1: 4.032e-07| loss_bc2: 2.069e-07| loss_bc3: 1.461e-07| loss_bc4: 3.975e-07|Time: 0.00\n",
      "mean_grad_res: 0.000172\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 28000, loss: 2.773e-01| loss_res: 2.457e-01| loss_f: 2.773e-01| loss_bc1: 1.176e-07| loss_bc2: 1.480e-08| loss_bc3: 5.459e-08| loss_bc4: 1.531e-07|Time: 0.00\n",
      "mean_grad_res: 0.000088\n",
      "mean_grad_bcs: 0.000008\n",
      "It: 28100, loss: 2.501e-01| loss_res: 2.472e-01| loss_f: 2.501e-01| loss_bc1: 9.262e-07| loss_bc2: 1.158e-06| loss_bc3: 1.041e-06| loss_bc4: 7.667e-07|Time: 0.00\n",
      "mean_grad_res: 0.000141\n",
      "mean_grad_bcs: 0.000015\n",
      "It: 28200, loss: 2.882e-01| loss_res: 2.397e-01| loss_f: 2.882e-01| loss_bc1: 9.127e-08| loss_bc2: 2.361e-08| loss_bc3: 1.417e-07| loss_bc4: 1.985e-07|Time: 0.01\n",
      "mean_grad_res: 0.000062\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 28300, loss: 2.833e-01| loss_res: 1.869e-01| loss_f: 2.833e-01| loss_bc1: 4.698e-08| loss_bc2: 1.126e-07| loss_bc3: 1.388e-07| loss_bc4: 2.112e-08|Time: 0.00\n",
      "mean_grad_res: 0.000139\n",
      "mean_grad_bcs: 0.000006\n",
      "It: 28400, loss: 2.902e-01| loss_res: 2.547e-01| loss_f: 2.902e-01| loss_bc1: 1.135e-07| loss_bc2: 2.626e-07| loss_bc3: 2.875e-07| loss_bc4: 2.624e-07|Time: 0.00\n",
      "mean_grad_res: 0.000145\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 28500, loss: 2.903e-01| loss_res: 2.794e-01| loss_f: 2.903e-01| loss_bc1: 1.860e-07| loss_bc2: 1.323e-07| loss_bc3: 1.734e-07| loss_bc4: 2.555e-07|Time: 0.00\n",
      "mean_grad_res: 0.000065\n",
      "mean_grad_bcs: 0.000007\n",
      "It: 28600, loss: 2.274e-01| loss_res: 2.537e-01| loss_f: 2.273e-01| loss_bc1: 3.562e-06| loss_bc2: 2.984e-06| loss_bc3: 2.377e-06| loss_bc4: 2.948e-06|Time: 0.00\n",
      "mean_grad_res: 0.000170\n",
      "mean_grad_bcs: 0.000024\n",
      "It: 28700, loss: 2.480e-01| loss_res: 2.646e-01| loss_f: 2.479e-01| loss_bc1: 1.753e-06| loss_bc2: 1.839e-06| loss_bc3: 2.589e-06| loss_bc4: 2.652e-06|Time: 0.00\n",
      "mean_grad_res: 0.000170\n",
      "mean_grad_bcs: 0.000033\n",
      "It: 28800, loss: 2.886e-01| loss_res: 2.368e-01| loss_f: 2.885e-01| loss_bc1: 4.982e-07| loss_bc2: 2.179e-07| loss_bc3: 1.522e-07| loss_bc4: 4.133e-07|Time: 0.00\n",
      "mean_grad_res: 0.000186\n",
      "mean_grad_bcs: 0.000013\n",
      "It: 28900, loss: 2.545e-01| loss_res: 2.689e-01| loss_f: 2.545e-01| loss_bc1: 5.674e-07| loss_bc2: 1.935e-06| loss_bc3: 1.709e-06| loss_bc4: 6.749e-07|Time: 0.00\n",
      "mean_grad_res: 0.000117\n",
      "mean_grad_bcs: 0.000046\n",
      "It: 29000, loss: 2.661e-01| loss_res: 2.536e-01| loss_f: 2.661e-01| loss_bc1: 2.042e-07| loss_bc2: 2.464e-07| loss_bc3: 2.407e-07| loss_bc4: 2.327e-07|Time: 0.00\n",
      "mean_grad_res: 0.000087\n",
      "mean_grad_bcs: 0.000007\n",
      "It: 29100, loss: 3.012e-01| loss_res: 2.682e-01| loss_f: 3.011e-01| loss_bc1: 2.316e-06| loss_bc2: 1.982e-06| loss_bc3: 2.097e-06| loss_bc4: 2.430e-06|Time: 0.00\n",
      "mean_grad_res: 0.000077\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 29200, loss: 2.927e-01| loss_res: 2.776e-01| loss_f: 2.927e-01| loss_bc1: 4.766e-08| loss_bc2: 8.932e-08| loss_bc3: 2.485e-07| loss_bc4: 3.324e-07|Time: 0.00\n",
      "mean_grad_res: 0.000099\n",
      "mean_grad_bcs: 0.000021\n",
      "It: 29300, loss: 2.242e-01| loss_res: 2.215e-01| loss_f: 2.242e-01| loss_bc1: 7.821e-08| loss_bc2: 5.248e-08| loss_bc3: 1.901e-08| loss_bc4: 1.594e-07|Time: 0.00\n",
      "mean_grad_res: 0.000099\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 29400, loss: 2.495e-01| loss_res: 2.418e-01| loss_f: 2.494e-01| loss_bc1: 1.782e-07| loss_bc2: 2.184e-07| loss_bc3: 1.832e-07| loss_bc4: 2.134e-07|Time: 0.00\n",
      "mean_grad_res: 0.000073\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 29500, loss: 3.086e-01| loss_res: 2.637e-01| loss_f: 3.086e-01| loss_bc1: 6.474e-08| loss_bc2: 5.880e-08| loss_bc3: 3.368e-08| loss_bc4: 1.017e-07|Time: 0.01\n",
      "mean_grad_res: 0.000095\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 29600, loss: 2.954e-01| loss_res: 2.242e-01| loss_f: 2.954e-01| loss_bc1: 9.660e-07| loss_bc2: 4.978e-07| loss_bc3: 3.401e-07| loss_bc4: 7.157e-07|Time: 0.00\n",
      "mean_grad_res: 0.000095\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 29700, loss: 3.061e-01| loss_res: 2.131e-01| loss_f: 3.061e-01| loss_bc1: 5.811e-07| loss_bc2: 9.071e-07| loss_bc3: 8.606e-07| loss_bc4: 4.663e-07|Time: 0.00\n",
      "mean_grad_res: 0.000085\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 29800, loss: 2.754e-01| loss_res: 2.710e-01| loss_f: 2.754e-01| loss_bc1: 5.679e-08| loss_bc2: 2.497e-07| loss_bc3: 1.173e-07| loss_bc4: 5.257e-08|Time: 0.00\n",
      "mean_grad_res: 0.000164\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 29900, loss: 2.495e-01| loss_res: 2.066e-01| loss_f: 2.495e-01| loss_bc1: 2.141e-08| loss_bc2: 8.530e-08| loss_bc3: 3.429e-08| loss_bc4: 2.764e-08|Time: 0.00\n",
      "mean_grad_res: 0.000120\n",
      "mean_grad_bcs: 0.000006\n",
      "It: 30000, loss: 2.782e-01| loss_res: 2.493e-01| loss_f: 2.782e-01| loss_bc1: 2.770e-07| loss_bc2: 2.216e-07| loss_bc3: 7.417e-07| loss_bc4: 2.973e-08|Time: 0.00\n",
      "mean_grad_res: 0.000077\n",
      "mean_grad_bcs: 0.000027\n",
      "It: 30100, loss: 2.580e-01| loss_res: 2.312e-01| loss_f: 2.579e-01| loss_bc1: 1.341e-06| loss_bc2: 2.379e-06| loss_bc3: 3.251e-06| loss_bc4: 1.766e-06|Time: 0.00\n",
      "mean_grad_res: 0.000118\n",
      "mean_grad_bcs: 0.000022\n",
      "It: 30200, loss: 2.686e-01| loss_res: 2.350e-01| loss_f: 2.686e-01| loss_bc1: 1.170e-07| loss_bc2: 5.217e-08| loss_bc3: 1.537e-07| loss_bc4: 4.461e-07|Time: 0.00\n",
      "mean_grad_res: 0.000084\n",
      "mean_grad_bcs: 0.000018\n",
      "It: 30300, loss: 2.488e-01| loss_res: 2.254e-01| loss_f: 2.488e-01| loss_bc1: 4.892e-07| loss_bc2: 3.358e-07| loss_bc3: 1.773e-07| loss_bc4: 3.251e-07|Time: 0.00\n",
      "mean_grad_res: 0.000121\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 30400, loss: 2.925e-01| loss_res: 2.192e-01| loss_f: 2.925e-01| loss_bc1: 1.240e-07| loss_bc2: 1.242e-07| loss_bc3: 9.154e-08| loss_bc4: 1.571e-07|Time: 0.00\n",
      "mean_grad_res: 0.000098\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 30500, loss: 2.945e-01| loss_res: 2.311e-01| loss_f: 2.945e-01| loss_bc1: 8.748e-08| loss_bc2: 6.452e-08| loss_bc3: 9.828e-08| loss_bc4: 4.087e-08|Time: 0.00\n",
      "mean_grad_res: 0.000111\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 30600, loss: 2.452e-01| loss_res: 2.643e-01| loss_f: 2.452e-01| loss_bc1: 3.769e-07| loss_bc2: 1.263e-06| loss_bc3: 3.567e-07| loss_bc4: 1.508e-07|Time: 0.01\n",
      "mean_grad_res: 0.000120\n",
      "mean_grad_bcs: 0.000035\n",
      "It: 30700, loss: 2.782e-01| loss_res: 2.348e-01| loss_f: 2.782e-01| loss_bc1: 1.771e-07| loss_bc2: 2.795e-07| loss_bc3: 3.867e-07| loss_bc4: 1.743e-07|Time: 0.00\n",
      "mean_grad_res: 0.000117\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 30800, loss: 2.255e-01| loss_res: 2.510e-01| loss_f: 2.255e-01| loss_bc1: 2.031e-07| loss_bc2: 1.463e-07| loss_bc3: 5.364e-07| loss_bc4: 5.270e-07|Time: 0.01\n",
      "mean_grad_res: 0.000080\n",
      "mean_grad_bcs: 0.000012\n",
      "It: 30900, loss: 2.325e-01| loss_res: 2.738e-01| loss_f: 2.325e-01| loss_bc1: 1.343e-08| loss_bc2: 3.415e-08| loss_bc3: 1.532e-08| loss_bc4: 7.626e-09|Time: 0.00\n",
      "mean_grad_res: 0.000049\n",
      "mean_grad_bcs: 0.000003\n",
      "It: 31000, loss: 2.454e-01| loss_res: 2.681e-01| loss_f: 2.454e-01| loss_bc1: 1.508e-07| loss_bc2: 1.428e-07| loss_bc3: 1.215e-07| loss_bc4: 2.072e-07|Time: 0.00\n",
      "mean_grad_res: 0.000093\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 31100, loss: 2.405e-01| loss_res: 2.753e-01| loss_f: 2.405e-01| loss_bc1: 2.513e-07| loss_bc2: 5.856e-07| loss_bc3: 6.319e-07| loss_bc4: 2.816e-07|Time: 0.00\n",
      "mean_grad_res: 0.000132\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 31200, loss: 3.102e-01| loss_res: 2.435e-01| loss_f: 3.102e-01| loss_bc1: 5.125e-08| loss_bc2: 4.756e-08| loss_bc3: 3.027e-07| loss_bc4: 4.759e-07|Time: 0.00\n",
      "mean_grad_res: 0.000204\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 31300, loss: 3.026e-01| loss_res: 2.373e-01| loss_f: 3.026e-01| loss_bc1: 3.656e-06| loss_bc2: 1.029e-06| loss_bc3: 6.965e-07| loss_bc4: 2.614e-06|Time: 0.00\n",
      "mean_grad_res: 0.000207\n",
      "mean_grad_bcs: 0.000040\n",
      "It: 31400, loss: 2.758e-01| loss_res: 2.553e-01| loss_f: 2.758e-01| loss_bc1: 6.460e-07| loss_bc2: 2.334e-07| loss_bc3: 5.487e-07| loss_bc4: 1.214e-06|Time: 0.00\n",
      "mean_grad_res: 0.000060\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 31500, loss: 2.799e-01| loss_res: 2.792e-01| loss_f: 2.799e-01| loss_bc1: 3.920e-08| loss_bc2: 2.538e-08| loss_bc3: 6.089e-08| loss_bc4: 1.246e-07|Time: 0.00\n",
      "mean_grad_res: 0.000124\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 31600, loss: 2.897e-01| loss_res: 2.356e-01| loss_f: 2.897e-01| loss_bc1: 2.524e-08| loss_bc2: 1.984e-07| loss_bc3: 7.444e-08| loss_bc4: 3.364e-08|Time: 0.00\n",
      "mean_grad_res: 0.000086\n",
      "mean_grad_bcs: 0.000008\n",
      "It: 31700, loss: 2.482e-01| loss_res: 2.821e-01| loss_f: 2.482e-01| loss_bc1: 4.216e-08| loss_bc2: 7.930e-08| loss_bc3: 5.212e-08| loss_bc4: 6.093e-08|Time: 0.00\n",
      "mean_grad_res: 0.000113\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 31800, loss: 2.619e-01| loss_res: 2.655e-01| loss_f: 2.619e-01| loss_bc1: 2.622e-07| loss_bc2: 2.084e-07| loss_bc3: 1.316e-06| loss_bc4: 1.602e-06|Time: 0.00\n",
      "mean_grad_res: 0.000050\n",
      "mean_grad_bcs: 0.000036\n",
      "It: 31900, loss: 2.486e-01| loss_res: 1.988e-01| loss_f: 2.486e-01| loss_bc1: 3.494e-08| loss_bc2: 7.927e-08| loss_bc3: 1.611e-07| loss_bc4: 3.028e-07|Time: 0.00\n",
      "mean_grad_res: 0.000081\n",
      "mean_grad_bcs: 0.000022\n",
      "It: 32000, loss: 2.858e-01| loss_res: 2.791e-01| loss_f: 2.858e-01| loss_bc1: 3.317e-07| loss_bc2: 2.159e-07| loss_bc3: 8.049e-07| loss_bc4: 9.192e-07|Time: 0.01\n",
      "mean_grad_res: 0.000131\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 32100, loss: 2.646e-01| loss_res: 2.298e-01| loss_f: 2.646e-01| loss_bc1: 2.046e-07| loss_bc2: 4.724e-08| loss_bc3: 2.858e-07| loss_bc4: 1.562e-07|Time: 0.00\n",
      "mean_grad_res: 0.000140\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 32200, loss: 2.528e-01| loss_res: 2.595e-01| loss_f: 2.528e-01| loss_bc1: 1.176e-07| loss_bc2: 2.062e-08| loss_bc3: 1.124e-07| loss_bc4: 7.407e-08|Time: 0.00\n",
      "mean_grad_res: 0.000117\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 32300, loss: 2.998e-01| loss_res: 2.420e-01| loss_f: 2.998e-01| loss_bc1: 1.810e-06| loss_bc2: 2.806e-07| loss_bc3: 2.340e-07| loss_bc4: 1.460e-06|Time: 0.02\n",
      "mean_grad_res: 0.000061\n",
      "mean_grad_bcs: 0.000029\n",
      "It: 32400, loss: 2.606e-01| loss_res: 2.269e-01| loss_f: 2.606e-01| loss_bc1: 9.717e-07| loss_bc2: 1.624e-06| loss_bc3: 1.560e-06| loss_bc4: 9.911e-07|Time: 0.00\n",
      "mean_grad_res: 0.000300\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 32500, loss: 2.339e-01| loss_res: 2.943e-01| loss_f: 2.339e-01| loss_bc1: 3.006e-08| loss_bc2: 7.993e-08| loss_bc3: 6.182e-08| loss_bc4: 9.861e-08|Time: 0.01\n",
      "mean_grad_res: 0.000123\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 32600, loss: 3.206e-01| loss_res: 2.650e-01| loss_f: 3.206e-01| loss_bc1: 5.293e-07| loss_bc2: 2.362e-07| loss_bc3: 1.384e-07| loss_bc4: 2.277e-07|Time: 0.00\n",
      "mean_grad_res: 0.000157\n",
      "mean_grad_bcs: 0.000013\n",
      "It: 32700, loss: 2.955e-01| loss_res: 2.551e-01| loss_f: 2.954e-01| loss_bc1: 2.409e-06| loss_bc2: 8.918e-07| loss_bc3: 6.414e-07| loss_bc4: 1.841e-06|Time: 0.00\n",
      "mean_grad_res: 0.000119\n",
      "mean_grad_bcs: 0.000025\n",
      "It: 32800, loss: 3.090e-01| loss_res: 2.389e-01| loss_f: 3.090e-01| loss_bc1: 6.543e-08| loss_bc2: 1.802e-07| loss_bc3: 1.728e-07| loss_bc4: 8.005e-08|Time: 0.00\n",
      "mean_grad_res: 0.000165\n",
      "mean_grad_bcs: 0.000007\n",
      "It: 32900, loss: 3.152e-01| loss_res: 2.507e-01| loss_f: 3.152e-01| loss_bc1: 4.607e-08| loss_bc2: 7.305e-08| loss_bc3: 1.094e-07| loss_bc4: 2.415e-07|Time: 0.00\n",
      "mean_grad_res: 0.000148\n",
      "mean_grad_bcs: 0.000015\n",
      "It: 33000, loss: 2.719e-01| loss_res: 2.663e-01| loss_f: 2.719e-01| loss_bc1: 8.558e-08| loss_bc2: 2.180e-07| loss_bc3: 1.921e-07| loss_bc4: 1.765e-07|Time: 0.00\n",
      "mean_grad_res: 0.000137\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 33100, loss: 2.666e-01| loss_res: 2.618e-01| loss_f: 2.665e-01| loss_bc1: 2.415e-06| loss_bc2: 6.075e-07| loss_bc3: 4.794e-07| loss_bc4: 2.083e-06|Time: 0.00\n",
      "mean_grad_res: 0.000163\n",
      "mean_grad_bcs: 0.000031\n",
      "It: 33200, loss: 2.774e-01| loss_res: 2.886e-01| loss_f: 2.774e-01| loss_bc1: 1.812e-06| loss_bc2: 1.038e-06| loss_bc3: 1.709e-06| loss_bc4: 2.405e-06|Time: 0.00\n",
      "mean_grad_res: 0.000140\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 33300, loss: 2.718e-01| loss_res: 2.415e-01| loss_f: 2.718e-01| loss_bc1: 4.063e-07| loss_bc2: 7.656e-07| loss_bc3: 4.989e-07| loss_bc4: 2.460e-07|Time: 0.00\n",
      "mean_grad_res: 0.000143\n",
      "mean_grad_bcs: 0.000012\n",
      "It: 33400, loss: 2.746e-01| loss_res: 2.118e-01| loss_f: 2.746e-01| loss_bc1: 2.687e-07| loss_bc2: 2.230e-07| loss_bc3: 5.683e-07| loss_bc4: 5.232e-07|Time: 0.02\n",
      "mean_grad_res: 0.000138\n",
      "mean_grad_bcs: 0.000012\n",
      "It: 33500, loss: 2.703e-01| loss_res: 2.833e-01| loss_f: 2.703e-01| loss_bc1: 1.948e-08| loss_bc2: 1.236e-07| loss_bc3: 2.154e-08| loss_bc4: 3.873e-08|Time: 0.01\n",
      "mean_grad_res: 0.000145\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 33600, loss: 2.610e-01| loss_res: 2.390e-01| loss_f: 2.610e-01| loss_bc1: 3.146e-07| loss_bc2: 5.728e-08| loss_bc3: 3.514e-08| loss_bc4: 2.936e-07|Time: 0.00\n",
      "mean_grad_res: 0.000156\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 33700, loss: 2.700e-01| loss_res: 2.728e-01| loss_f: 2.700e-01| loss_bc1: 2.320e-07| loss_bc2: 1.272e-07| loss_bc3: 4.406e-08| loss_bc4: 2.220e-08|Time: 0.00\n",
      "mean_grad_res: 0.000293\n",
      "mean_grad_bcs: 0.000012\n",
      "It: 33800, loss: 2.385e-01| loss_res: 2.345e-01| loss_f: 2.385e-01| loss_bc1: 4.190e-07| loss_bc2: 9.815e-08| loss_bc3: 2.243e-07| loss_bc4: 3.567e-07|Time: 0.00\n",
      "mean_grad_res: 0.000125\n",
      "mean_grad_bcs: 0.000018\n",
      "It: 33900, loss: 2.363e-01| loss_res: 2.546e-01| loss_f: 2.363e-01| loss_bc1: 8.493e-07| loss_bc2: 1.179e-07| loss_bc3: 1.364e-07| loss_bc4: 4.749e-07|Time: 0.00\n",
      "mean_grad_res: 0.000170\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 34000, loss: 2.728e-01| loss_res: 2.460e-01| loss_f: 2.728e-01| loss_bc1: 6.577e-08| loss_bc2: 7.781e-08| loss_bc3: 6.909e-07| loss_bc4: 3.930e-07|Time: 0.00\n",
      "mean_grad_res: 0.000101\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 34100, loss: 2.390e-01| loss_res: 2.987e-01| loss_f: 2.390e-01| loss_bc1: 7.670e-07| loss_bc2: 2.920e-07| loss_bc3: 1.076e-06| loss_bc4: 1.558e-06|Time: 0.00\n",
      "mean_grad_res: 0.000158\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 34200, loss: 2.476e-01| loss_res: 2.731e-01| loss_f: 2.476e-01| loss_bc1: 2.074e-07| loss_bc2: 1.246e-07| loss_bc3: 7.554e-08| loss_bc4: 6.538e-08|Time: 0.00\n",
      "mean_grad_res: 0.000121\n",
      "mean_grad_bcs: 0.000013\n",
      "It: 34300, loss: 2.940e-01| loss_res: 2.624e-01| loss_f: 2.939e-01| loss_bc1: 4.887e-07| loss_bc2: 3.800e-07| loss_bc3: 4.505e-07| loss_bc4: 7.278e-07|Time: 0.00\n",
      "mean_grad_res: 0.000082\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 34400, loss: 2.881e-01| loss_res: 2.204e-01| loss_f: 2.880e-01| loss_bc1: 4.407e-08| loss_bc2: 1.392e-08| loss_bc3: 1.508e-07| loss_bc4: 2.019e-07|Time: 0.00\n",
      "mean_grad_res: 0.000187\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 34500, loss: 2.789e-01| loss_res: 2.443e-01| loss_f: 2.789e-01| loss_bc1: 3.039e-07| loss_bc2: 3.327e-07| loss_bc3: 2.224e-07| loss_bc4: 2.816e-07|Time: 0.00\n",
      "mean_grad_res: 0.000112\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 34600, loss: 2.742e-01| loss_res: 2.329e-01| loss_f: 2.742e-01| loss_bc1: 7.241e-08| loss_bc2: 3.508e-08| loss_bc3: 6.333e-08| loss_bc4: 1.270e-07|Time: 0.01\n",
      "mean_grad_res: 0.000115\n",
      "mean_grad_bcs: 0.000006\n",
      "It: 34700, loss: 2.584e-01| loss_res: 2.317e-01| loss_f: 2.584e-01| loss_bc1: 6.708e-08| loss_bc2: 6.580e-08| loss_bc3: 3.203e-07| loss_bc4: 1.790e-07|Time: 0.00\n",
      "mean_grad_res: 0.000167\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 34800, loss: 2.933e-01| loss_res: 2.524e-01| loss_f: 2.933e-01| loss_bc1: 1.880e-06| loss_bc2: 1.424e-06| loss_bc3: 5.656e-07| loss_bc4: 1.030e-06|Time: 0.01\n",
      "mean_grad_res: 0.000120\n",
      "mean_grad_bcs: 0.000021\n",
      "It: 34900, loss: 2.975e-01| loss_res: 2.179e-01| loss_f: 2.975e-01| loss_bc1: 4.003e-08| loss_bc2: 6.442e-08| loss_bc3: 3.762e-08| loss_bc4: 2.403e-07|Time: 0.00\n",
      "mean_grad_res: 0.000148\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 35000, loss: 3.012e-01| loss_res: 2.854e-01| loss_f: 3.012e-01| loss_bc1: 1.046e-06| loss_bc2: 8.147e-07| loss_bc3: 3.795e-07| loss_bc4: 5.251e-07|Time: 0.00\n",
      "mean_grad_res: 0.000154\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 35100, loss: 2.412e-01| loss_res: 2.611e-01| loss_f: 2.412e-01| loss_bc1: 1.830e-08| loss_bc2: 1.012e-08| loss_bc3: 1.633e-07| loss_bc4: 2.368e-07|Time: 0.00\n",
      "mean_grad_res: 0.000098\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 35200, loss: 2.688e-01| loss_res: 2.119e-01| loss_f: 2.688e-01| loss_bc1: 9.778e-08| loss_bc2: 1.225e-07| loss_bc3: 4.658e-08| loss_bc4: 3.269e-08|Time: 0.00\n",
      "mean_grad_res: 0.000152\n",
      "mean_grad_bcs: 0.000012\n",
      "It: 35300, loss: 3.033e-01| loss_res: 2.413e-01| loss_f: 3.033e-01| loss_bc1: 9.226e-08| loss_bc2: 2.049e-08| loss_bc3: 5.673e-08| loss_bc4: 2.063e-07|Time: 0.00\n",
      "mean_grad_res: 0.000126\n",
      "mean_grad_bcs: 0.000007\n",
      "It: 35400, loss: 2.931e-01| loss_res: 2.063e-01| loss_f: 2.931e-01| loss_bc1: 2.154e-07| loss_bc2: 6.450e-07| loss_bc3: 3.246e-07| loss_bc4: 1.856e-07|Time: 0.00\n",
      "mean_grad_res: 0.000152\n",
      "mean_grad_bcs: 0.000018\n",
      "It: 35500, loss: 2.898e-01| loss_res: 2.012e-01| loss_f: 2.898e-01| loss_bc1: 1.016e-07| loss_bc2: 3.737e-08| loss_bc3: 3.030e-08| loss_bc4: 9.776e-08|Time: 0.00\n",
      "mean_grad_res: 0.000207\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 35600, loss: 3.125e-01| loss_res: 2.378e-01| loss_f: 3.125e-01| loss_bc1: 9.113e-07| loss_bc2: 1.515e-06| loss_bc3: 2.255e-06| loss_bc4: 1.133e-06|Time: 0.00\n",
      "mean_grad_res: 0.000151\n",
      "mean_grad_bcs: 0.000035\n",
      "It: 35700, loss: 2.779e-01| loss_res: 3.049e-01| loss_f: 2.779e-01| loss_bc1: 1.086e-07| loss_bc2: 3.598e-07| loss_bc3: 4.875e-07| loss_bc4: 1.451e-07|Time: 0.00\n",
      "mean_grad_res: 0.000109\n",
      "mean_grad_bcs: 0.000027\n",
      "It: 35800, loss: 2.450e-01| loss_res: 2.753e-01| loss_f: 2.450e-01| loss_bc1: 3.971e-08| loss_bc2: 2.292e-07| loss_bc3: 2.682e-07| loss_bc4: 8.985e-08|Time: 0.00\n",
      "mean_grad_res: 0.000147\n",
      "mean_grad_bcs: 0.000013\n",
      "It: 35900, loss: 2.548e-01| loss_res: 2.910e-01| loss_f: 2.548e-01| loss_bc1: 3.661e-08| loss_bc2: 1.256e-07| loss_bc3: 1.770e-07| loss_bc4: 5.209e-08|Time: 0.00\n",
      "mean_grad_res: 0.000065\n",
      "mean_grad_bcs: 0.000011\n",
      "It: 36000, loss: 2.839e-01| loss_res: 2.455e-01| loss_f: 2.839e-01| loss_bc1: 1.476e-08| loss_bc2: 1.105e-07| loss_bc3: 1.025e-07| loss_bc4: 3.401e-08|Time: 0.00\n",
      "mean_grad_res: 0.000140\n",
      "mean_grad_bcs: 0.000006\n",
      "It: 36100, loss: 3.032e-01| loss_res: 2.466e-01| loss_f: 3.032e-01| loss_bc1: 1.958e-07| loss_bc2: 2.192e-07| loss_bc3: 2.086e-07| loss_bc4: 2.502e-07|Time: 0.00\n",
      "mean_grad_res: 0.000083\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 36200, loss: 2.498e-01| loss_res: 2.608e-01| loss_f: 2.498e-01| loss_bc1: 3.616e-08| loss_bc2: 1.671e-07| loss_bc3: 5.430e-07| loss_bc4: 2.784e-07|Time: 0.00\n",
      "mean_grad_res: 0.000112\n",
      "mean_grad_bcs: 0.000013\n",
      "It: 36300, loss: 2.654e-01| loss_res: 2.409e-01| loss_f: 2.654e-01| loss_bc1: 7.606e-08| loss_bc2: 2.960e-08| loss_bc3: 1.437e-07| loss_bc4: 3.065e-07|Time: 0.00\n",
      "mean_grad_res: 0.000100\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 36400, loss: 3.189e-01| loss_res: 2.511e-01| loss_f: 3.189e-01| loss_bc1: 1.240e-08| loss_bc2: 4.103e-08| loss_bc3: 1.963e-07| loss_bc4: 8.854e-08|Time: 0.00\n",
      "mean_grad_res: 0.000176\n",
      "mean_grad_bcs: 0.000007\n",
      "It: 36500, loss: 2.865e-01| loss_res: 2.848e-01| loss_f: 2.865e-01| loss_bc1: 2.678e-07| loss_bc2: 3.743e-07| loss_bc3: 3.377e-07| loss_bc4: 3.584e-07|Time: 0.00\n",
      "mean_grad_res: 0.000132\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 36600, loss: 2.731e-01| loss_res: 2.684e-01| loss_f: 2.731e-01| loss_bc1: 3.432e-07| loss_bc2: 6.079e-08| loss_bc3: 1.107e-07| loss_bc4: 3.169e-07|Time: 0.00\n",
      "mean_grad_res: 0.000131\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 36700, loss: 3.075e-01| loss_res: 2.670e-01| loss_f: 3.075e-01| loss_bc1: 6.979e-08| loss_bc2: 5.068e-08| loss_bc3: 2.868e-07| loss_bc4: 4.198e-07|Time: 0.00\n",
      "mean_grad_res: 0.000060\n",
      "mean_grad_bcs: 0.000025\n",
      "It: 36800, loss: 3.125e-01| loss_res: 2.536e-01| loss_f: 3.125e-01| loss_bc1: 2.391e-07| loss_bc2: 2.671e-07| loss_bc3: 7.716e-07| loss_bc4: 7.563e-07|Time: 0.00\n",
      "mean_grad_res: 0.000155\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 36900, loss: 2.813e-01| loss_res: 2.555e-01| loss_f: 2.813e-01| loss_bc1: 9.997e-08| loss_bc2: 1.882e-07| loss_bc3: 1.436e-07| loss_bc4: 2.202e-07|Time: 0.00\n",
      "mean_grad_res: 0.000183\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 37000, loss: 2.791e-01| loss_res: 2.521e-01| loss_f: 2.791e-01| loss_bc1: 1.615e-06| loss_bc2: 5.262e-07| loss_bc3: 5.719e-07| loss_bc4: 1.415e-06|Time: 0.00\n",
      "mean_grad_res: 0.000159\n",
      "mean_grad_bcs: 0.000023\n",
      "It: 37100, loss: 2.927e-01| loss_res: 2.468e-01| loss_f: 2.927e-01| loss_bc1: 7.992e-08| loss_bc2: 2.477e-08| loss_bc3: 1.131e-07| loss_bc4: 1.802e-07|Time: 0.00\n",
      "mean_grad_res: 0.000080\n",
      "mean_grad_bcs: 0.000010\n",
      "It: 37200, loss: 2.687e-01| loss_res: 2.432e-01| loss_f: 2.687e-01| loss_bc1: 8.113e-08| loss_bc2: 3.068e-08| loss_bc3: 2.152e-08| loss_bc4: 1.270e-07|Time: 0.00\n",
      "mean_grad_res: 0.000120\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 37300, loss: 2.745e-01| loss_res: 2.631e-01| loss_f: 2.745e-01| loss_bc1: 2.302e-08| loss_bc2: 2.051e-07| loss_bc3: 2.194e-07| loss_bc4: 1.108e-07|Time: 0.00\n",
      "mean_grad_res: 0.000107\n",
      "mean_grad_bcs: 0.000008\n",
      "It: 37400, loss: 2.387e-01| loss_res: 2.504e-01| loss_f: 2.387e-01| loss_bc1: 2.389e-07| loss_bc2: 3.406e-07| loss_bc3: 8.312e-07| loss_bc4: 6.123e-07|Time: 0.00\n",
      "mean_grad_res: 0.000155\n",
      "mean_grad_bcs: 0.000018\n",
      "It: 37500, loss: 2.430e-01| loss_res: 2.421e-01| loss_f: 2.430e-01| loss_bc1: 2.303e-07| loss_bc2: 2.058e-07| loss_bc3: 9.716e-08| loss_bc4: 7.925e-08|Time: 0.00\n",
      "mean_grad_res: 0.000265\n",
      "mean_grad_bcs: 0.000018\n",
      "It: 37600, loss: 2.629e-01| loss_res: 2.601e-01| loss_f: 2.629e-01| loss_bc1: 2.711e-08| loss_bc2: 7.945e-08| loss_bc3: 1.976e-07| loss_bc4: 4.899e-08|Time: 0.00\n",
      "mean_grad_res: 0.000187\n",
      "mean_grad_bcs: 0.000009\n",
      "It: 37700, loss: 2.892e-01| loss_res: 2.198e-01| loss_f: 2.892e-01| loss_bc1: 1.419e-07| loss_bc2: 5.727e-08| loss_bc3: 9.331e-08| loss_bc4: 1.204e-07|Time: 0.00\n",
      "mean_grad_res: 0.000100\n",
      "mean_grad_bcs: 0.000006\n",
      "It: 37800, loss: 2.719e-01| loss_res: 2.516e-01| loss_f: 2.719e-01| loss_bc1: 1.033e-06| loss_bc2: 4.408e-08| loss_bc3: 3.761e-08| loss_bc4: 9.270e-07|Time: 0.00\n",
      "mean_grad_res: 0.000186\n",
      "mean_grad_bcs: 0.000045\n",
      "It: 37900, loss: 2.601e-01| loss_res: 2.782e-01| loss_f: 2.601e-01| loss_bc1: 1.012e-07| loss_bc2: 1.373e-07| loss_bc3: 3.356e-07| loss_bc4: 2.066e-07|Time: 0.00\n",
      "mean_grad_res: 0.000124\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 38000, loss: 2.600e-01| loss_res: 2.135e-01| loss_f: 2.600e-01| loss_bc1: 1.387e-07| loss_bc2: 2.164e-07| loss_bc3: 2.595e-07| loss_bc4: 2.175e-07|Time: 0.00\n",
      "mean_grad_res: 0.000198\n",
      "mean_grad_bcs: 0.000018\n",
      "It: 38100, loss: 2.569e-01| loss_res: 2.570e-01| loss_f: 2.569e-01| loss_bc1: 1.336e-07| loss_bc2: 8.644e-08| loss_bc3: 1.852e-07| loss_bc4: 3.688e-07|Time: 0.00\n",
      "mean_grad_res: 0.000177\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 38200, loss: 2.441e-01| loss_res: 2.474e-01| loss_f: 2.441e-01| loss_bc1: 1.881e-07| loss_bc2: 2.562e-07| loss_bc3: 2.684e-07| loss_bc4: 5.446e-08|Time: 0.00\n",
      "mean_grad_res: 0.000167\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 38300, loss: 2.603e-01| loss_res: 2.793e-01| loss_f: 2.603e-01| loss_bc1: 2.139e-07| loss_bc2: 5.407e-07| loss_bc3: 6.138e-07| loss_bc4: 1.720e-07|Time: 0.00\n",
      "mean_grad_res: 0.000074\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 38400, loss: 3.028e-01| loss_res: 2.100e-01| loss_f: 3.028e-01| loss_bc1: 7.298e-07| loss_bc2: 9.805e-07| loss_bc3: 9.760e-07| loss_bc4: 4.256e-07|Time: 0.00\n",
      "mean_grad_res: 0.000096\n",
      "mean_grad_bcs: 0.000019\n",
      "It: 38500, loss: 2.631e-01| loss_res: 2.683e-01| loss_f: 2.631e-01| loss_bc1: 8.221e-08| loss_bc2: 1.634e-07| loss_bc3: 2.083e-07| loss_bc4: 5.022e-08|Time: 0.00\n",
      "mean_grad_res: 0.000167\n",
      "mean_grad_bcs: 0.000017\n",
      "It: 38600, loss: 2.881e-01| loss_res: 2.503e-01| loss_f: 2.881e-01| loss_bc1: 1.614e-07| loss_bc2: 2.604e-07| loss_bc3: 3.991e-07| loss_bc4: 4.171e-07|Time: 0.00\n",
      "mean_grad_res: 0.000135\n",
      "mean_grad_bcs: 0.000015\n",
      "It: 38700, loss: 2.845e-01| loss_res: 2.485e-01| loss_f: 2.845e-01| loss_bc1: 2.618e-07| loss_bc2: 2.008e-07| loss_bc3: 1.725e-07| loss_bc4: 1.491e-07|Time: 0.00\n",
      "mean_grad_res: 0.000134\n",
      "mean_grad_bcs: 0.000023\n",
      "It: 38800, loss: 2.374e-01| loss_res: 2.765e-01| loss_f: 2.374e-01| loss_bc1: 5.991e-07| loss_bc2: 7.467e-07| loss_bc3: 2.056e-07| loss_bc4: 3.104e-07|Time: 0.00\n",
      "mean_grad_res: 0.000151\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 38900, loss: 2.777e-01| loss_res: 2.814e-01| loss_f: 2.777e-01| loss_bc1: 2.661e-07| loss_bc2: 1.316e-07| loss_bc3: 8.128e-08| loss_bc4: 7.753e-08|Time: 0.00\n",
      "mean_grad_res: 0.000255\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 39000, loss: 2.492e-01| loss_res: 1.888e-01| loss_f: 2.492e-01| loss_bc1: 5.418e-07| loss_bc2: 1.946e-07| loss_bc3: 8.704e-08| loss_bc4: 7.937e-08|Time: 0.00\n",
      "mean_grad_res: 0.000175\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 39100, loss: 2.923e-01| loss_res: 2.386e-01| loss_f: 2.923e-01| loss_bc1: 2.864e-07| loss_bc2: 1.682e-07| loss_bc3: 5.130e-08| loss_bc4: 3.123e-07|Time: 0.00\n",
      "mean_grad_res: 0.000502\n",
      "mean_grad_bcs: 0.000024\n",
      "It: 39200, loss: 2.906e-01| loss_res: 2.263e-01| loss_f: 2.906e-01| loss_bc1: 2.142e-07| loss_bc2: 5.244e-08| loss_bc3: 1.983e-07| loss_bc4: 3.151e-07|Time: 0.01\n",
      "mean_grad_res: 0.000342\n",
      "mean_grad_bcs: 0.000012\n",
      "It: 39300, loss: 2.790e-01| loss_res: 2.246e-01| loss_f: 2.790e-01| loss_bc1: 1.019e-06| loss_bc2: 3.245e-07| loss_bc3: 1.388e-07| loss_bc4: 5.105e-07|Time: 0.01\n",
      "mean_grad_res: 0.000181\n",
      "mean_grad_bcs: 0.000027\n",
      "It: 39400, loss: 2.639e-01| loss_res: 2.199e-01| loss_f: 2.639e-01| loss_bc1: 4.147e-07| loss_bc2: 5.982e-09| loss_bc3: 7.764e-08| loss_bc4: 5.183e-07|Time: 0.01\n",
      "mean_grad_res: 0.000207\n",
      "mean_grad_bcs: 0.000030\n",
      "It: 39500, loss: 2.842e-01| loss_res: 2.535e-01| loss_f: 2.842e-01| loss_bc1: 3.138e-08| loss_bc2: 2.725e-08| loss_bc3: 2.084e-07| loss_bc4: 2.828e-07|Time: 0.00\n",
      "mean_grad_res: 0.000160\n",
      "mean_grad_bcs: 0.000014\n",
      "It: 39600, loss: 2.343e-01| loss_res: 2.864e-01| loss_f: 2.343e-01| loss_bc1: 2.727e-07| loss_bc2: 1.436e-07| loss_bc3: 1.222e-07| loss_bc4: 1.023e-07|Time: 0.01\n",
      "mean_grad_res: 0.000125\n",
      "mean_grad_bcs: 0.000020\n",
      "It: 39700, loss: 2.608e-01| loss_res: 2.531e-01| loss_f: 2.608e-01| loss_bc1: 2.869e-07| loss_bc2: 2.040e-07| loss_bc3: 2.616e-08| loss_bc4: 1.428e-08|Time: 0.00\n",
      "mean_grad_res: 0.000095\n",
      "mean_grad_bcs: 0.000016\n",
      "It: 39800, loss: 2.689e-01| loss_res: 2.206e-01| loss_f: 2.689e-01| loss_bc1: 1.075e-07| loss_bc2: 1.948e-07| loss_bc3: 8.952e-08| loss_bc4: 5.275e-08|Time: 0.00\n",
      "mean_grad_res: 0.000185\n",
      "mean_grad_bcs: 0.000013\n",
      "It: 39900, loss: 2.464e-01| loss_res: 2.659e-01| loss_f: 2.464e-01| loss_bc1: 2.027e-07| loss_bc2: 1.322e-07| loss_bc3: 1.473e-07| loss_bc4: 1.286e-07|Time: 0.00\n",
      "mean_grad_res: 0.000208\n",
      "mean_grad_bcs: 0.000008\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000, loss: 2.946e-01| loss_res: 2.309e-01| loss_f: 2.946e-01| loss_bc1: 2.136e-08| loss_bc2: 7.065e-08| loss_bc3: 1.841e-07| loss_bc4: 1.826e-07|Time: 0.56\n",
      "mean_grad_res: 0.000231\n",
      "mean_grad_bcs: 0.000013\n",
      "Relative L2 error_u: 1.00e+00\n",
      "Relative L2 error_f: 1.00e+00\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Jan-11-2024_21-33-54-378092_M2\n",
      "Final loss total loss: 2.945609e-01\n",
      "Final loss loss_res: 2.309276e-01\n",
      "Final loss loss_f: 2.945589e-01\n",
      "Final loss loss_bc1: 2.135981e-08\n",
      "Final loss loss_bc2: 7.064919e-08\n",
      "Final loss loss_bc3: 1.841172e-07\n",
      "Final loss loss_bc4: 1.826119e-07\n",
      "average lambda_bc4.2541e+00\n",
      "average lambda_res1.0\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:1.7605e+02\n",
      "average of error_u_list:9.9995e-01\n",
      "average of error_v_list:1.0000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "a_1 = 1\n",
    "a_2 = 4\n",
    "\n",
    "# Parameter\n",
    "lam = 1.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[-1.0, -1.0], [1.0, -1.0]])\n",
    "bc2_coords = np.array([[1.0, -1.0], [1.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "bc4_coords = np.array([[-1.0, 1.0], [-1.0, -1.0]])\n",
    "\n",
    "dom_coords = np.array([[-1.0, -1.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "x1 = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x2 = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star, a_1, a_2)\n",
    "f_star = f(X_star, a_1, a_2, lam)\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = None\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'            # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False    # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: u( x, a_1, a_2 ), name='Forcing')\n",
    "\n",
    "        # [elapsed, error_u , error_f ,  mode] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , \n",
    "        #                                                                stiff_ratio , X_star ,u_star , f_star , nIter ,bcbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    " #def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                model.trainmb(nIter, batch_size=mbbatch_size )\n",
    "            else:\n",
    "                model.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            f_pred = model.predict_r(X_star)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "            model.plot_grad()\n",
    "            model.plot_lambda()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( x1 , x2 , X_star , u_star , u_pred , f_star , f_pred)\n",
    "\n",
    "            model.print(\"average lambda_bc\" , np.average(model.adpative_constant_log))\n",
    "            model.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "            time_list.append(elapsed)\n",
    "            error_u_list.append(error_u)\n",
    "            error_f_list.append(error_f)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_v_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_f_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_Helmholtz_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (422007974.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_60704/422007974.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    average of time_list:1.8459e+02\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "average of time_list:1.8459e+02\n",
    "average of error_u_list:3.6929e-01\n",
    "average of error_v_list:7.7160e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21548/2533309064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predicted solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mU_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mF_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u_pred' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot ###\n",
    "\n",
    "# Exact solution & Predicted solution\n",
    "# Exact soluton\n",
    "U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "# Predicted solution\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Exact $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Predicted $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual loss & Boundary loss\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "gradients_res_list = []\n",
    "gradients_bcs_list = []\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "for j in range(num_hidden_layers):\n",
    "    gradient_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradient_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    gradients_res_list.append(gradient_res)\n",
    "    gradients_bcs_list.append(gradient_bcs)\n",
    "\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim([-3.0, 3.0])\n",
    "    ax.set_ylim([0,100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = mode.eigenvalue_log\n",
    "    eigenvalues_bcs_list = mode.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = mode.eigenvalue_res_log\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
