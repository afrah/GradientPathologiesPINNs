{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "######################################################################################################\n",
    "def u(x, a_1, a_2):\n",
    "    return np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_xx(x, a_1, a_2):\n",
    "    return - (a_1 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "def u_yy(x, a_1, a_2):\n",
    "    return - (a_2 * np.pi) ** 2 * np.sin(a_1 * np.pi * x[:, 0:1]) * np.sin(a_2 * np.pi * x[:, 1:2])\n",
    "\n",
    "# Forcing\n",
    "def f(x, a_1, a_2, lam):\n",
    "    return u_xx(x, a_1, a_2) + u_yy(x, a_1, a_2) + lam * u(x, a_1, a_2)\n",
    "\n",
    "def operator(u, x1, x2, lam, sigma_x1=1.0, sigma_x2=1.0):\n",
    "    u_x1 = tf.gradients(u, x1)[0] / sigma_x1\n",
    "    u_x2 = tf.gradients(u, x2)[0] / sigma_x2\n",
    "    u_xx1 = tf.gradients(u_x1, x1)[0] / sigma_x1\n",
    "    u_xx2 = tf.gradients(u_x2, x2)[0] / sigma_x2\n",
    "    residual = u_xx1 + u_xx2 + lam * u\n",
    "    return residual\n",
    "#######################################################################################################\n",
    "\n",
    "class Helmholtz2D:\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess):\n",
    "        # Normalization constants\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x1, self.sigma_x1 = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x2, self.sigma_x2 = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Helmoholtz constant\n",
    "        self.lam = tf.constant(lam, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = mode\n",
    "\n",
    "        # Record stiff ratio\n",
    "        # self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_bcs_val = np.array(1.0)\n",
    "        self.adaptive_res_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        # if mode in ['M3', 'M4']:\n",
    "        #     # Initialize encoder weights and biases\n",
    "        #     self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        #     self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "        #     self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x1_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x1_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x2_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf    = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # Define placeholder for adaptive constant\n",
    "        self.adaptive_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_bcs_val.shape)\n",
    "        self.adaptive_res_tf = tf.placeholder(tf.float32, shape=self.adaptive_res_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred = self.net_u(self.x1_bc1_tf, self.x2_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.x1_bc2_tf, self.x2_bc2_tf)\n",
    "        self.u_bc3_pred = self.net_u(self.x1_bc3_tf, self.x2_bc3_tf)\n",
    "        self.u_bc4_pred = self.net_u(self.x1_bc4_tf, self.x2_bc4_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.x1_u_tf, self.x2_u_tf)\n",
    "        self.r_pred = self.net_r(self.x1_r_tf, self.x2_r_tf)\n",
    "\n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_tf - self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_tf - self.u_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.u_bc3_tf - self.u_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.u_bc4_tf - self.u_bc4_pred))\n",
    "        self.loss_bcs =  (self.loss_bc1 + self.loss_bc2 + self.loss_bc3 + self.loss_bc4)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.adaptive_res_tf * self.loss_res + self.adaptive_bcs_tf * self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_bcs_log = []\n",
    "        self.loss_res_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "        self.first_grad_res = 0\n",
    "        self.first_grad_bcs = 0\n",
    "        \n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        # Compute and store the adaptive constant\n",
    "        self.adpative_bcs_log = []\n",
    "        self.adpative_res_log = []\n",
    "\n",
    "        self.std_grad_res_list = []\n",
    "        self.std_grad_bcs_list = []\n",
    "        self.mean_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        \n",
    "        self.max_grad_res_log = []\n",
    "        self.mean_grad_bcs_log = []\n",
    "    \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.std_grad_res_list.append(tf.math.reduce_std(tf.abs(self.grad_res[i]))) \n",
    "            self.std_grad_bcs_list.append(tf.math.reduce_std(tf.abs(self.grad_bcs[i]))) \n",
    "            self.mean_grad_res_list.append(tf.math.reduce_mean(tf.abs(self.grad_res[i])))\n",
    "            self.mean_grad_bcs_list.append(tf.math.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "        \n",
    "        self.std_grad_res = tf.math.reduce_std(tf.stack(self.std_grad_res_list))\n",
    "        self.std_grad_bcs = tf.math.reduce_std(tf.stack(self.std_grad_bcs_list))\n",
    "        self.mean_grad_res = tf.reduce_mean(tf.stack(self.mean_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "\n",
    "        self.kurtosis_grad_res_list = []\n",
    "        self.kurtosis_grad_bcs_list = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "\n",
    "            self.kurtosis_grad_res_list.append(tf.math.reduce_mean(   tf.math.pow ((self.grad_res[i] - self.mean_grad_res) / self.std_grad_res,4)))\n",
    "            self.kurtosis_grad_bcs_list.append(tf.math.reduce_mean(   tf.math.pow ((self.grad_bcs[i] - self.mean_grad_bcs) / self.std_grad_bcs,4)))\n",
    "\n",
    "        self.kurtosis_grad_res = tf.math.reduce_mean(tf.stack(self.kurtosis_grad_res_list))\n",
    "        self.kurtosis_grad_bcs = tf.math.reduce_mean(tf.stack(self.kurtosis_grad_bcs_list))\n",
    "    \n",
    "        # # Stiff Ratio\n",
    "        # if self.stiff_ratio:\n",
    "        #     self.Hessian, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "        #     self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "        #     self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "        #     self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "        #     self.eigenvalue_log = []\n",
    "        #     self.eigenvalue_bcs_log = []\n",
    "        #     self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "     # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,  self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients, tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P), dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),  dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),  dtype='float32')\n",
    "\n",
    "        return H, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self,size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, x1, x2):\n",
    "        u = self.forward_pass(tf.concat([x1, x2], 1))\n",
    "        return u\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x1, x2):\n",
    "        u = self.net_u(x1, x2)\n",
    "        residual = self.operator(u, x1, x2,  self.lam, self.sigma_x1,  self.sigma_x2)\n",
    "        return residual\n",
    "\n",
    "    # Feed minibatch\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "  # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, u_bc3_batch = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, u_bc4_batch = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x1_bc1_tf: X_bc1_batch[:, 0:1], self.x2_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.x1_bc2_tf: X_bc2_batch[:, 0:1], self.x2_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.x1_bc3_tf: X_bc3_batch[:, 0:1], self.x2_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.u_bc3_tf: u_bc3_batch,\n",
    "                       self.x1_bc4_tf: X_bc4_batch[:, 0:1], self.x2_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.u_bc4_tf: u_bc4_batch,\n",
    "                       self.x1_r_tf: X_res_batch[:, 0:1], self.x2_r_tf: X_res_batch[:, 1:2], \n",
    "                       self.r_tf: f_res_batch,\n",
    "                       self.adaptive_bcs_tf:  self.adaptive_bcs_val,\n",
    "                       self.adaptive_res_tf:  self.adaptive_res_val\n",
    "\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss ,  loss_bcs, loss_res = self.sess.run([self.loss, self.loss_bcs, self.loss_res] , tf_dict)\n",
    "\n",
    " \n",
    "                self.print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e,Time: %.2f' % (it, loss, loss_bcs, loss_res, elapsed))\n",
    "\n",
    "                mean_grad_res , mean_grad_bcs , kurtosis_grad_res , kurtosis_grad_bcs = self.sess.run( [self.mean_grad_res , self.mean_grad_bcs , self.kurtosis_grad_res , self.kurtosis_grad_bcs ], tf_dict)\n",
    "                if it  == 0:\n",
    "                   # Print adaptive weights during training\n",
    "\n",
    "                    self.adaptive_bcs_val =   10 * np.max([kurtosis_grad_res , kurtosis_grad_bcs]) / mean_grad_bcs \n",
    "                    self.adaptive_res_val =   10 * np.max([kurtosis_grad_res , kurtosis_grad_bcs]) /  mean_grad_res \n",
    "                \n",
    "                self.print(' kurtosis_grad_res: {:.3e}'.format(  kurtosis_grad_res))\n",
    "                self.print(' kurtosis_grad_bcs: {:.3e}'.format(  kurtosis_grad_bcs))\n",
    "\n",
    "                self.print('adaptive_bcs_val: {:.3e}'.format( self.adaptive_bcs_val))\n",
    "                self.print('adaptive_res_val: {:.3e}'.format( self.adaptive_res_val))\n",
    "                \n",
    "                self.print('mean_grad_res: %f' % (mean_grad_res))\n",
    "                self.print('mean_grad_bcs: %f' % (mean_grad_bcs))\n",
    "\n",
    "\n",
    "                self.adpative_bcs_log.append(self.adaptive_bcs_val)\n",
    "                self.adpative_res_log.append(self.adaptive_res_val)\n",
    "\n",
    "\n",
    "                self.max_grad_res_log.append(mean_grad_res)\n",
    "                self.mean_grad_bcs_log.append(mean_grad_bcs)\n",
    "\n",
    "  \n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    " \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_u_tf: X_star[:, 0:1], self.x2_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x1_r_tf: X_star[:, 0:1], self.x2_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n",
    "\n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        # shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $f(x)$' , 'Predicted $f(x)$' , 'Absolute error']\n",
    "        data = [U_star , U_pred ,  np.abs(U_star - U_pred) , F_star , F_pred ,  np.abs(F_star - F_pred) ]\n",
    "        \n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        kwargs_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60),\n",
    "                cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, kwargs, minmax, title in zip(grid, data, kwargs_list, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "                #  print( z[timeStp,:,:])\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_bcs_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fontsize = 17\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([16,8])\n",
    "        ax.semilogy(self.max_grad_res_log, label=r'$\\bar{\\nabla_\\theta {u_{res}}}$' , color = 'tab:green')\n",
    "        ax.semilogy(self.mean_grad_bcs_log, label=r'$\\bar{\\nabla_\\theta {u_{bc}}}$' , color = 'tab:blue')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
    "        ax.tick_params(labelsize=fontsize)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
    "\n",
    "        ax2 = ax.twinx() \n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_size_inches([15,8])\n",
    "    \n",
    "        ax2.semilogy(self.adpative_bcs_log, label=r'$\\bar{\\lambda_{bc}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
    "        ax2.semilogy(self.adpative_res_log, label=r'$\\bar{\\lambda_{res}}$'  ,  linestyle='dashed' , color = 'tab:blue') \n",
    "\n",
    "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
    "        ax2.tick_params(labelsize=fontsize)\n",
    "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = os.path.join(self.dirname,'lambda_history.png')\n",
    "        plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, lam ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f ,  model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_79215/3928250666.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_79215/3928250666.py:83: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_79215/3928250666.py:84: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_79215/3928250666.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_79215/1035729483.py:293: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_79215/1035729483.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 14:25:58.058195: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-16 14:25:58.080024: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2024-01-16 14:25:58.080465: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d566775a50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-16 14:25:58.080479: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-01-16 14:25:58.092197: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_79215/1035729483.py:174: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_79215/1035729483.py:176: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_79215/1035729483.py:236: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 6.908e+03, Loss_bcs: 4.316e-01, Loss_res: 6.907e+03,Time: 1.51\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.461e+00\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 0.995038\n",
      "mean_grad_bcs: 0.201211\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, Loss: 4.523e+06, Loss_bcs: 3.168e-01, Loss_res: 4.499e+03,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.481e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.071506\n",
      "mean_grad_bcs: 0.074380\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, Loss: 7.817e+06, Loss_bcs: 2.101e+00, Loss_res: 7.768e+03,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.481e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.772070\n",
      "mean_grad_bcs: 0.222038\n",
      "It: 300, Loss: 6.771e+06, Loss_bcs: 4.379e+00, Loss_res: 6.715e+03,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.649e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.024974\n",
      "mean_grad_bcs: 0.410677\n",
      "It: 400, Loss: 7.793e+06, Loss_bcs: 6.488e+00, Loss_res: 7.723e+03,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.612e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 18.200148\n",
      "mean_grad_bcs: 0.711805\n",
      "It: 500, Loss: 6.651e+06, Loss_bcs: 7.267e+00, Loss_res: 6.582e+03,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.418e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 13.045492\n",
      "mean_grad_bcs: 0.673477\n",
      "It: 600, Loss: 4.338e+06, Loss_bcs: 1.083e+01, Loss_res: 4.263e+03,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.743e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 84.974716\n",
      "mean_grad_bcs: 0.864551\n",
      "It: 700, Loss: 2.605e+06, Loss_bcs: 8.469e+00, Loss_res: 2.550e+03,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.229e+03\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 180.506989\n",
      "mean_grad_bcs: 0.590351\n",
      "It: 800, Loss: 1.471e+06, Loss_bcs: 7.021e+00, Loss_res: 1.429e+03,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.439e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 65.490250\n",
      "mean_grad_bcs: 0.601707\n",
      "It: 900, Loss: 8.109e+05, Loss_bcs: 7.056e+00, Loss_res: 7.720e+02,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.524e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 61.143135\n",
      "mean_grad_bcs: 0.730675\n",
      "It: 1000, Loss: 3.097e+05, Loss_bcs: 7.020e+00, Loss_res: 2.735e+02,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.603e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 32.435585\n",
      "mean_grad_bcs: 0.890655\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, Loss: 1.699e+05, Loss_bcs: 6.112e+00, Loss_res: 1.388e+02,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.014e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 62.827370\n",
      "mean_grad_bcs: 0.800563\n",
      "It: 1200, Loss: 1.371e+05, Loss_bcs: 4.812e+00, Loss_res: 1.126e+02,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.233e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 67.877213\n",
      "mean_grad_bcs: 0.720702\n",
      "It: 1300, Loss: 1.029e+05, Loss_bcs: 4.213e+00, Loss_res: 8.157e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.642e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 49.985382\n",
      "mean_grad_bcs: 0.650169\n",
      "It: 1400, Loss: 8.141e+04, Loss_bcs: 3.353e+00, Loss_res: 6.442e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.711e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 50.797321\n",
      "mean_grad_bcs: 0.576787\n",
      "It: 1500, Loss: 5.818e+04, Loss_bcs: 3.037e+00, Loss_res: 4.288e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.679e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 26.277575\n",
      "mean_grad_bcs: 0.569662\n",
      "It: 1600, Loss: 4.424e+04, Loss_bcs: 2.258e+00, Loss_res: 3.285e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.906e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 27.311337\n",
      "mean_grad_bcs: 0.465934\n",
      "It: 1700, Loss: 3.556e+04, Loss_bcs: 1.820e+00, Loss_res: 2.638e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.515e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 18.038795\n",
      "mean_grad_bcs: 0.438737\n",
      "It: 1800, Loss: 3.056e+04, Loss_bcs: 1.229e+00, Loss_res: 2.433e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.114e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 28.572342\n",
      "mean_grad_bcs: 0.358657\n",
      "It: 1900, Loss: 2.606e+04, Loss_bcs: 9.764e-01, Loss_res: 2.110e+01,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.308e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 35.704872\n",
      "mean_grad_bcs: 0.320302\n",
      "It: 2000, Loss: 2.199e+04, Loss_bcs: 8.388e-01, Loss_res: 1.773e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.473e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 15.785881\n",
      "mean_grad_bcs: 0.294890\n",
      "It: 2100, Loss: 1.732e+04, Loss_bcs: 6.391e-01, Loss_res: 1.408e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.291e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 24.993713\n",
      "mean_grad_bcs: 0.251641\n",
      "It: 2200, Loss: 1.651e+04, Loss_bcs: 5.459e-01, Loss_res: 1.373e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.181e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 23.028336\n",
      "mean_grad_bcs: 0.224457\n",
      "It: 2300, Loss: 1.495e+04, Loss_bcs: 4.428e-01, Loss_res: 1.269e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.102e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.039916\n",
      "mean_grad_bcs: 0.185181\n",
      "It: 2400, Loss: 1.461e+04, Loss_bcs: 4.206e-01, Loss_res: 1.246e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.996e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 25.607090\n",
      "mean_grad_bcs: 0.197125\n",
      "It: 2500, Loss: 1.318e+04, Loss_bcs: 3.687e-01, Loss_res: 1.129e+01,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.464e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 13.582491\n",
      "mean_grad_bcs: 0.177074\n",
      "It: 2600, Loss: 1.442e+04, Loss_bcs: 3.375e-01, Loss_res: 1.268e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.961e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 20.224781\n",
      "mean_grad_bcs: 0.188573\n",
      "It: 2700, Loss: 9.704e+03, Loss_bcs: 2.927e-01, Loss_res: 8.208e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.742e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 25.408035\n",
      "mean_grad_bcs: 0.111312\n",
      "It: 2800, Loss: 1.105e+04, Loss_bcs: 2.920e-01, Loss_res: 9.555e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.758e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 20.011992\n",
      "mean_grad_bcs: 0.138843\n",
      "It: 2900, Loss: 9.788e+03, Loss_bcs: 3.101e-01, Loss_res: 8.206e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.580e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 15.897651\n",
      "mean_grad_bcs: 0.154284\n",
      "It: 3000, Loss: 9.299e+03, Loss_bcs: 3.197e-01, Loss_res: 7.672e+00,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.633e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 16.521164\n",
      "mean_grad_bcs: 0.124750\n",
      "It: 3100, Loss: 8.058e+03, Loss_bcs: 2.638e-01, Loss_res: 6.713e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.962e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 16.132788\n",
      "mean_grad_bcs: 0.115890\n",
      "It: 3200, Loss: 1.150e+04, Loss_bcs: 2.591e-01, Loss_res: 1.016e+01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.398e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 28.028933\n",
      "mean_grad_bcs: 0.108205\n",
      "It: 3300, Loss: 8.317e+03, Loss_bcs: 2.867e-01, Loss_res: 6.858e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.313e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.809877\n",
      "mean_grad_bcs: 0.124326\n",
      "It: 3400, Loss: 6.446e+03, Loss_bcs: 2.579e-01, Loss_res: 5.139e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.407e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.409564\n",
      "mean_grad_bcs: 0.110002\n",
      "It: 3500, Loss: 6.505e+03, Loss_bcs: 2.551e-01, Loss_res: 5.211e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.942e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 21.010475\n",
      "mean_grad_bcs: 0.137339\n",
      "It: 3600, Loss: 6.381e+03, Loss_bcs: 2.298e-01, Loss_res: 5.213e+00,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.230e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.477911\n",
      "mean_grad_bcs: 0.110446\n",
      "It: 3700, Loss: 9.334e+03, Loss_bcs: 2.510e-01, Loss_res: 8.046e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.487e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 18.179428\n",
      "mean_grad_bcs: 0.108149\n",
      "It: 3800, Loss: 8.472e+03, Loss_bcs: 2.358e-01, Loss_res: 7.264e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.990e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 15.856243\n",
      "mean_grad_bcs: 0.101753\n",
      "It: 3900, Loss: 6.159e+03, Loss_bcs: 2.099e-01, Loss_res: 5.091e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.818e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.878914\n",
      "mean_grad_bcs: 0.087763\n",
      "It: 4000, Loss: 8.004e+03, Loss_bcs: 2.239e-01, Loss_res: 6.857e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.487e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.375048\n",
      "mean_grad_bcs: 0.095206\n",
      "It: 4100, Loss: 6.144e+03, Loss_bcs: 2.272e-01, Loss_res: 4.991e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.689e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 15.498476\n",
      "mean_grad_bcs: 0.083142\n",
      "It: 4200, Loss: 8.009e+03, Loss_bcs: 2.089e-01, Loss_res: 6.936e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.811e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 34.890430\n",
      "mean_grad_bcs: 0.107259\n",
      "It: 4300, Loss: 7.459e+03, Loss_bcs: 2.120e-01, Loss_res: 6.373e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.969e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 20.814224\n",
      "mean_grad_bcs: 0.105038\n",
      "It: 4400, Loss: 4.558e+03, Loss_bcs: 2.325e-01, Loss_res: 3.385e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.688e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.948610\n",
      "mean_grad_bcs: 0.095111\n",
      "It: 4500, Loss: 6.092e+03, Loss_bcs: 2.023e-01, Loss_res: 5.061e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.306e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.325214\n",
      "mean_grad_bcs: 0.080652\n",
      "It: 4600, Loss: 5.449e+03, Loss_bcs: 2.036e-01, Loss_res: 4.415e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.331e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 15.595831\n",
      "mean_grad_bcs: 0.080982\n",
      "It: 4700, Loss: 4.390e+03, Loss_bcs: 2.042e-01, Loss_res: 3.358e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.139e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.252271\n",
      "mean_grad_bcs: 0.093445\n",
      "It: 4800, Loss: 5.237e+03, Loss_bcs: 1.985e-01, Loss_res: 4.229e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.248e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 18.821016\n",
      "mean_grad_bcs: 0.084617\n",
      "It: 4900, Loss: 5.640e+03, Loss_bcs: 2.137e-01, Loss_res: 4.555e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.434e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 19.886934\n",
      "mean_grad_bcs: 0.099933\n",
      "It: 5000, Loss: 4.460e+03, Loss_bcs: 2.049e-01, Loss_res: 3.425e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.963e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.411632\n",
      "mean_grad_bcs: 0.079669\n",
      "It: 5100, Loss: 3.755e+03, Loss_bcs: 1.755e-01, Loss_res: 2.868e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.157e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.612020\n",
      "mean_grad_bcs: 0.078257\n",
      "It: 5200, Loss: 6.123e+03, Loss_bcs: 1.917e-01, Loss_res: 5.145e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.670e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 23.935970\n",
      "mean_grad_bcs: 0.104119\n",
      "It: 5300, Loss: 4.057e+03, Loss_bcs: 1.617e-01, Loss_res: 3.238e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.478e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.349811\n",
      "mean_grad_bcs: 0.088795\n",
      "It: 5400, Loss: 4.114e+03, Loss_bcs: 2.234e-01, Loss_res: 2.988e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.441e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.174818\n",
      "mean_grad_bcs: 0.085971\n",
      "It: 5500, Loss: 5.450e+03, Loss_bcs: 1.899e-01, Loss_res: 4.485e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.175e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 16.638397\n",
      "mean_grad_bcs: 0.077428\n",
      "It: 5600, Loss: 4.659e+03, Loss_bcs: 1.780e-01, Loss_res: 3.756e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.131e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.309634\n",
      "mean_grad_bcs: 0.080870\n",
      "It: 5700, Loss: 4.038e+03, Loss_bcs: 1.890e-01, Loss_res: 3.083e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.136e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.506087\n",
      "mean_grad_bcs: 0.091491\n",
      "It: 5800, Loss: 4.942e+03, Loss_bcs: 1.532e-01, Loss_res: 4.160e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.518e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 17.175739\n",
      "mean_grad_bcs: 0.078522\n",
      "It: 5900, Loss: 4.575e+03, Loss_bcs: 1.832e-01, Loss_res: 3.647e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.492e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.437531\n",
      "mean_grad_bcs: 0.096639\n",
      "It: 6000, Loss: 3.917e+03, Loss_bcs: 1.679e-01, Loss_res: 3.067e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.754e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.505706\n",
      "mean_grad_bcs: 0.079958\n",
      "It: 6100, Loss: 4.654e+03, Loss_bcs: 1.777e-01, Loss_res: 3.752e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.545e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 15.645733\n",
      "mean_grad_bcs: 0.089447\n",
      "It: 6200, Loss: 3.351e+03, Loss_bcs: 1.592e-01, Loss_res: 2.547e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.256e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.416417\n",
      "mean_grad_bcs: 0.106711\n",
      "It: 6300, Loss: 3.547e+03, Loss_bcs: 1.592e-01, Loss_res: 2.742e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.507e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.234222\n",
      "mean_grad_bcs: 0.077708\n",
      "It: 6400, Loss: 3.148e+03, Loss_bcs: 1.704e-01, Loss_res: 2.290e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.774e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.853207\n",
      "mean_grad_bcs: 0.084081\n",
      "It: 6500, Loss: 3.267e+03, Loss_bcs: 1.352e-01, Loss_res: 2.583e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.118e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.940631\n",
      "mean_grad_bcs: 0.066119\n",
      "It: 6600, Loss: 2.739e+03, Loss_bcs: 1.292e-01, Loss_res: 2.086e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.694e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.857193\n",
      "mean_grad_bcs: 0.066127\n",
      "It: 6700, Loss: 4.655e+03, Loss_bcs: 1.607e-01, Loss_res: 3.837e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.392e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 14.067811\n",
      "mean_grad_bcs: 0.080015\n",
      "It: 6800, Loss: 4.006e+03, Loss_bcs: 1.545e-01, Loss_res: 3.222e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.305e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.768661\n",
      "mean_grad_bcs: 0.087494\n",
      "It: 6900, Loss: 4.378e+03, Loss_bcs: 1.849e-01, Loss_res: 3.442e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.740e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 17.403759\n",
      "mean_grad_bcs: 0.075168\n",
      "It: 7000, Loss: 3.326e+03, Loss_bcs: 1.577e-01, Loss_res: 2.530e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.162e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.348602\n",
      "mean_grad_bcs: 0.077125\n",
      "It: 7100, Loss: 3.118e+03, Loss_bcs: 1.557e-01, Loss_res: 2.332e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.273e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.965451\n",
      "mean_grad_bcs: 0.058655\n",
      "It: 7200, Loss: 3.518e+03, Loss_bcs: 1.373e-01, Loss_res: 2.821e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.116e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.277282\n",
      "mean_grad_bcs: 0.087795\n",
      "It: 7300, Loss: 3.714e+03, Loss_bcs: 1.351e-01, Loss_res: 3.028e+00,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.146e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 22.076754\n",
      "mean_grad_bcs: 0.072324\n",
      "It: 7400, Loss: 2.684e+03, Loss_bcs: 1.122e-01, Loss_res: 2.116e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.656e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.310939\n",
      "mean_grad_bcs: 0.097309\n",
      "It: 7500, Loss: 2.737e+03, Loss_bcs: 1.477e-01, Loss_res: 1.993e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.585e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.732063\n",
      "mean_grad_bcs: 0.072157\n",
      "It: 7600, Loss: 3.780e+03, Loss_bcs: 1.238e-01, Loss_res: 3.150e+00,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.169e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 15.380770\n",
      "mean_grad_bcs: 0.058176\n",
      "It: 7700, Loss: 2.914e+03, Loss_bcs: 1.315e-01, Loss_res: 2.249e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.061e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.614163\n",
      "mean_grad_bcs: 0.064544\n",
      "It: 7800, Loss: 3.082e+03, Loss_bcs: 1.372e-01, Loss_res: 2.388e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.095e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.834090\n",
      "mean_grad_bcs: 0.074020\n",
      "It: 7900, Loss: 3.203e+03, Loss_bcs: 1.227e-01, Loss_res: 2.581e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.874e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 16.630985\n",
      "mean_grad_bcs: 0.073936\n",
      "It: 8000, Loss: 2.325e+03, Loss_bcs: 1.244e-01, Loss_res: 1.698e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.910e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.042813\n",
      "mean_grad_bcs: 0.099462\n",
      "It: 8100, Loss: 3.756e+03, Loss_bcs: 1.150e-01, Loss_res: 3.169e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.209e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.847322\n",
      "mean_grad_bcs: 0.058528\n",
      "It: 8200, Loss: 2.990e+03, Loss_bcs: 1.405e-01, Loss_res: 2.280e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.120e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.713150\n",
      "mean_grad_bcs: 0.059615\n",
      "It: 8300, Loss: 2.741e+03, Loss_bcs: 1.370e-01, Loss_res: 2.050e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.178e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.611134\n",
      "mean_grad_bcs: 0.066578\n",
      "It: 8400, Loss: 3.927e+03, Loss_bcs: 1.228e-01, Loss_res: 3.300e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.555e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 17.277802\n",
      "mean_grad_bcs: 0.081484\n",
      "It: 8500, Loss: 2.235e+03, Loss_bcs: 1.156e-01, Loss_res: 1.652e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.587e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.178150\n",
      "mean_grad_bcs: 0.066863\n",
      "It: 8600, Loss: 2.492e+03, Loss_bcs: 1.311e-01, Loss_res: 1.831e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.058e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.102647\n",
      "mean_grad_bcs: 0.068703\n",
      "It: 8700, Loss: 3.318e+03, Loss_bcs: 1.050e-01, Loss_res: 2.783e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.434e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 13.182948\n",
      "mean_grad_bcs: 0.087615\n",
      "It: 8800, Loss: 2.492e+03, Loss_bcs: 1.213e-01, Loss_res: 1.880e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.780e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.716124\n",
      "mean_grad_bcs: 0.069230\n",
      "It: 8900, Loss: 2.250e+03, Loss_bcs: 1.085e-01, Loss_res: 1.702e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.093e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.227593\n",
      "mean_grad_bcs: 0.055689\n",
      "It: 9000, Loss: 2.968e+03, Loss_bcs: 8.955e-02, Loss_res: 2.510e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.228e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.124437\n",
      "mean_grad_bcs: 0.060063\n",
      "It: 9100, Loss: 3.284e+03, Loss_bcs: 1.161e-01, Loss_res: 2.694e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.038e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 20.657990\n",
      "mean_grad_bcs: 0.084506\n",
      "It: 9200, Loss: 2.228e+03, Loss_bcs: 1.057e-01, Loss_res: 1.694e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.060e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.454073\n",
      "mean_grad_bcs: 0.053135\n",
      "It: 9300, Loss: 2.604e+03, Loss_bcs: 1.094e-01, Loss_res: 2.050e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.032e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.942863\n",
      "mean_grad_bcs: 0.068196\n",
      "It: 9400, Loss: 2.647e+03, Loss_bcs: 9.561e-02, Loss_res: 2.162e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.475e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 14.568475\n",
      "mean_grad_bcs: 0.051528\n",
      "It: 9500, Loss: 2.244e+03, Loss_bcs: 8.824e-02, Loss_res: 1.797e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.518e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.683496\n",
      "mean_grad_bcs: 0.061529\n",
      "It: 9600, Loss: 1.990e+03, Loss_bcs: 9.828e-02, Loss_res: 1.495e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.614e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.261567\n",
      "mean_grad_bcs: 0.059449\n",
      "It: 9700, Loss: 2.314e+03, Loss_bcs: 1.019e-01, Loss_res: 1.798e+00,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.388e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.788461\n",
      "mean_grad_bcs: 0.057443\n",
      "It: 9800, Loss: 2.140e+03, Loss_bcs: 9.823e-02, Loss_res: 1.643e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.868e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.955630\n",
      "mean_grad_bcs: 0.072396\n",
      "It: 9900, Loss: 3.196e+03, Loss_bcs: 1.054e-01, Loss_res: 2.659e+00,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.528e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 14.454651\n",
      "mean_grad_bcs: 0.055620\n",
      "It: 10000, Loss: 2.391e+03, Loss_bcs: 8.896e-02, Loss_res: 1.939e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.367e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 16.291220\n",
      "mean_grad_bcs: 0.053260\n",
      "It: 10100, Loss: 2.153e+03, Loss_bcs: 9.180e-02, Loss_res: 1.688e+00,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.718e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.651795\n",
      "mean_grad_bcs: 0.069208\n",
      "It: 10200, Loss: 2.078e+03, Loss_bcs: 1.254e-01, Loss_res: 1.448e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.492e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.390904\n",
      "mean_grad_bcs: 0.063356\n",
      "It: 10300, Loss: 2.602e+03, Loss_bcs: 9.372e-02, Loss_res: 2.126e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.279e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.091012\n",
      "mean_grad_bcs: 0.055722\n",
      "It: 10400, Loss: 3.133e+03, Loss_bcs: 1.010e-01, Loss_res: 2.619e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.964e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.471666\n",
      "mean_grad_bcs: 0.069021\n",
      "It: 10500, Loss: 2.122e+03, Loss_bcs: 7.848e-02, Loss_res: 1.723e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.311e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.270168\n",
      "mean_grad_bcs: 0.054661\n",
      "It: 10600, Loss: 2.190e+03, Loss_bcs: 9.452e-02, Loss_res: 1.712e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.595e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.232850\n",
      "mean_grad_bcs: 0.060749\n",
      "It: 10700, Loss: 2.149e+03, Loss_bcs: 9.238e-02, Loss_res: 1.681e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.215e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.874440\n",
      "mean_grad_bcs: 0.053182\n",
      "It: 10800, Loss: 1.930e+03, Loss_bcs: 1.156e-01, Loss_res: 1.349e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.129e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.649103\n",
      "mean_grad_bcs: 0.078280\n",
      "It: 10900, Loss: 2.449e+03, Loss_bcs: 9.192e-02, Loss_res: 1.983e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.634e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.807332\n",
      "mean_grad_bcs: 0.049698\n",
      "It: 11000, Loss: 2.356e+03, Loss_bcs: 9.597e-02, Loss_res: 1.870e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.907e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.755711\n",
      "mean_grad_bcs: 0.049886\n",
      "It: 11100, Loss: 1.869e+03, Loss_bcs: 8.923e-02, Loss_res: 1.419e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.051e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.885323\n",
      "mean_grad_bcs: 0.080786\n",
      "It: 11200, Loss: 2.218e+03, Loss_bcs: 9.598e-02, Loss_res: 1.732e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.166e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.474480\n",
      "mean_grad_bcs: 0.052678\n",
      "It: 11300, Loss: 2.088e+03, Loss_bcs: 9.572e-02, Loss_res: 1.604e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.702e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.215285\n",
      "mean_grad_bcs: 0.050253\n",
      "It: 11400, Loss: 1.667e+03, Loss_bcs: 8.979e-02, Loss_res: 1.214e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.474e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.290484\n",
      "mean_grad_bcs: 0.067898\n",
      "It: 11500, Loss: 1.977e+03, Loss_bcs: 9.423e-02, Loss_res: 1.501e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.260e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.406265\n",
      "mean_grad_bcs: 0.053388\n",
      "It: 11600, Loss: 2.361e+03, Loss_bcs: 8.935e-02, Loss_res: 1.907e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.324e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 12.285301\n",
      "mean_grad_bcs: 0.048300\n",
      "It: 11700, Loss: 1.777e+03, Loss_bcs: 9.057e-02, Loss_res: 1.320e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.115e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.100541\n",
      "mean_grad_bcs: 0.054653\n",
      "It: 11800, Loss: 1.772e+03, Loss_bcs: 8.012e-02, Loss_res: 1.367e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.632e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.253097\n",
      "mean_grad_bcs: 0.069856\n",
      "It: 11900, Loss: 2.443e+03, Loss_bcs: 8.791e-02, Loss_res: 1.996e+00,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.393e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.002929\n",
      "mean_grad_bcs: 0.064390\n",
      "It: 12000, Loss: 1.740e+03, Loss_bcs: 8.641e-02, Loss_res: 1.305e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.685e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.987176\n",
      "mean_grad_bcs: 0.063920\n",
      "It: 12100, Loss: 1.898e+03, Loss_bcs: 7.542e-02, Loss_res: 1.516e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.001e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.200969\n",
      "mean_grad_bcs: 0.064344\n",
      "It: 12200, Loss: 2.882e+03, Loss_bcs: 8.289e-02, Loss_res: 2.458e+00,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.180e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 20.653160\n",
      "mean_grad_bcs: 0.055661\n",
      "It: 12300, Loss: 1.838e+03, Loss_bcs: 6.952e-02, Loss_res: 1.485e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.695e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.851339\n",
      "mean_grad_bcs: 0.047984\n",
      "It: 12400, Loss: 1.671e+03, Loss_bcs: 7.268e-02, Loss_res: 1.304e+00,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.239e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.143449\n",
      "mean_grad_bcs: 0.053234\n",
      "It: 12500, Loss: 1.418e+03, Loss_bcs: 8.202e-02, Loss_res: 1.006e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.120e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.480673\n",
      "mean_grad_bcs: 0.045730\n",
      "It: 12600, Loss: 1.426e+03, Loss_bcs: 8.238e-02, Loss_res: 1.012e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.615e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.555601\n",
      "mean_grad_bcs: 0.044611\n",
      "It: 12700, Loss: 1.885e+03, Loss_bcs: 7.989e-02, Loss_res: 1.481e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.556e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.858146\n",
      "mean_grad_bcs: 0.046939\n",
      "It: 12800, Loss: 1.678e+03, Loss_bcs: 7.519e-02, Loss_res: 1.298e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.053e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.078395\n",
      "mean_grad_bcs: 0.055669\n",
      "It: 12900, Loss: 2.000e+03, Loss_bcs: 7.041e-02, Loss_res: 1.642e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.620e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.063332\n",
      "mean_grad_bcs: 0.057304\n",
      "It: 13000, Loss: 1.763e+03, Loss_bcs: 7.793e-02, Loss_res: 1.369e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.712e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.359771\n",
      "mean_grad_bcs: 0.045330\n",
      "It: 13100, Loss: 1.812e+03, Loss_bcs: 8.179e-02, Loss_res: 1.399e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.762e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.603060\n",
      "mean_grad_bcs: 0.059128\n",
      "It: 13200, Loss: 1.633e+03, Loss_bcs: 6.260e-02, Loss_res: 1.315e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.618e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.210573\n",
      "mean_grad_bcs: 0.042017\n",
      "It: 13300, Loss: 1.476e+03, Loss_bcs: 7.561e-02, Loss_res: 1.095e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.252e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.379414\n",
      "mean_grad_bcs: 0.063044\n",
      "It: 13400, Loss: 1.866e+03, Loss_bcs: 7.359e-02, Loss_res: 1.493e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.301e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.058884\n",
      "mean_grad_bcs: 0.054802\n",
      "It: 13500, Loss: 1.463e+03, Loss_bcs: 7.475e-02, Loss_res: 1.086e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.893e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.083499\n",
      "mean_grad_bcs: 0.051107\n",
      "It: 13600, Loss: 1.869e+03, Loss_bcs: 7.456e-02, Loss_res: 1.491e+00,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.320e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.852995\n",
      "mean_grad_bcs: 0.058330\n",
      "It: 13700, Loss: 1.302e+03, Loss_bcs: 6.993e-02, Loss_res: 9.495e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.059e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.897099\n",
      "mean_grad_bcs: 0.046097\n",
      "It: 13800, Loss: 1.153e+03, Loss_bcs: 5.960e-02, Loss_res: 8.530e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.203e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.254908\n",
      "mean_grad_bcs: 0.037182\n",
      "It: 13900, Loss: 1.720e+03, Loss_bcs: 6.967e-02, Loss_res: 1.367e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.202e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.702367\n",
      "mean_grad_bcs: 0.061391\n",
      "It: 14000, Loss: 1.422e+03, Loss_bcs: 7.168e-02, Loss_res: 1.061e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.849e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.311204\n",
      "mean_grad_bcs: 0.061267\n",
      "It: 14100, Loss: 1.719e+03, Loss_bcs: 6.687e-02, Loss_res: 1.380e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.034e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.922409\n",
      "mean_grad_bcs: 0.049708\n",
      "It: 14200, Loss: 1.235e+03, Loss_bcs: 7.113e-02, Loss_res: 8.767e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.471e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.932640\n",
      "mean_grad_bcs: 0.042179\n",
      "It: 14300, Loss: 2.022e+03, Loss_bcs: 7.263e-02, Loss_res: 1.653e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.500e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.224498\n",
      "mean_grad_bcs: 0.062185\n",
      "It: 14400, Loss: 1.794e+03, Loss_bcs: 6.903e-02, Loss_res: 1.443e+00,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.412e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.897009\n",
      "mean_grad_bcs: 0.050743\n",
      "It: 14500, Loss: 1.342e+03, Loss_bcs: 7.314e-02, Loss_res: 9.732e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.294e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.503041\n",
      "mean_grad_bcs: 0.047210\n",
      "It: 14600, Loss: 1.564e+03, Loss_bcs: 7.284e-02, Loss_res: 1.196e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.280e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.198495\n",
      "mean_grad_bcs: 0.041624\n",
      "It: 14700, Loss: 1.650e+03, Loss_bcs: 6.747e-02, Loss_res: 1.308e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.338e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.037949\n",
      "mean_grad_bcs: 0.042642\n",
      "It: 14800, Loss: 1.908e+03, Loss_bcs: 7.101e-02, Loss_res: 1.547e+00,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.016e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.835647\n",
      "mean_grad_bcs: 0.047040\n",
      "It: 14900, Loss: 1.446e+03, Loss_bcs: 7.781e-02, Loss_res: 1.054e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.229e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.859864\n",
      "mean_grad_bcs: 0.043367\n",
      "It: 15000, Loss: 1.392e+03, Loss_bcs: 6.112e-02, Loss_res: 1.083e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.353e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.451283\n",
      "mean_grad_bcs: 0.072322\n",
      "It: 15100, Loss: 1.721e+03, Loss_bcs: 7.255e-02, Loss_res: 1.353e+00,Time: 0.03\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.267e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.598996\n",
      "mean_grad_bcs: 0.065230\n",
      "It: 15200, Loss: 1.529e+03, Loss_bcs: 6.372e-02, Loss_res: 1.206e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.663e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.312128\n",
      "mean_grad_bcs: 0.041186\n",
      "It: 15300, Loss: 1.236e+03, Loss_bcs: 7.491e-02, Loss_res: 8.593e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.139e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.543461\n",
      "mean_grad_bcs: 0.069993\n",
      "It: 15400, Loss: 1.863e+03, Loss_bcs: 6.317e-02, Loss_res: 1.541e+00,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.975e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.905051\n",
      "mean_grad_bcs: 0.044856\n",
      "It: 15500, Loss: 1.374e+03, Loss_bcs: 6.169e-02, Loss_res: 1.063e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.719e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.541684\n",
      "mean_grad_bcs: 0.041816\n",
      "It: 15600, Loss: 1.253e+03, Loss_bcs: 5.431e-02, Loss_res: 9.785e-01,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.407e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.280169\n",
      "mean_grad_bcs: 0.032381\n",
      "It: 15700, Loss: 1.821e+03, Loss_bcs: 6.021e-02, Loss_res: 1.515e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.014e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.112537\n",
      "mean_grad_bcs: 0.040009\n",
      "It: 15800, Loss: 1.512e+03, Loss_bcs: 7.331e-02, Loss_res: 1.142e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.598e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.729544\n",
      "mean_grad_bcs: 0.043122\n",
      "It: 15900, Loss: 1.099e+03, Loss_bcs: 5.859e-02, Loss_res: 8.038e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.847e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.803408\n",
      "mean_grad_bcs: 0.043775\n",
      "It: 16000, Loss: 1.089e+03, Loss_bcs: 6.326e-02, Loss_res: 7.708e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.267e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.297266\n",
      "mean_grad_bcs: 0.050940\n",
      "It: 16100, Loss: 1.162e+03, Loss_bcs: 6.934e-02, Loss_res: 8.130e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.250e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.970593\n",
      "mean_grad_bcs: 0.045353\n",
      "It: 16200, Loss: 1.419e+03, Loss_bcs: 6.462e-02, Loss_res: 1.093e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.309e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.676442\n",
      "mean_grad_bcs: 0.040608\n",
      "It: 16300, Loss: 1.474e+03, Loss_bcs: 5.607e-02, Loss_res: 1.189e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.712e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.317608\n",
      "mean_grad_bcs: 0.046058\n",
      "It: 16400, Loss: 1.387e+03, Loss_bcs: 6.195e-02, Loss_res: 1.074e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.941e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.661162\n",
      "mean_grad_bcs: 0.036170\n",
      "It: 16500, Loss: 1.032e+03, Loss_bcs: 6.064e-02, Loss_res: 7.268e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.945e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.781312\n",
      "mean_grad_bcs: 0.040016\n",
      "It: 16600, Loss: 2.043e+03, Loss_bcs: 6.063e-02, Loss_res: 1.733e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.172e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 10.373163\n",
      "mean_grad_bcs: 0.044458\n",
      "It: 16700, Loss: 9.417e+02, Loss_bcs: 5.270e-02, Loss_res: 6.765e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.708e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.415552\n",
      "mean_grad_bcs: 0.036096\n",
      "It: 16800, Loss: 1.125e+03, Loss_bcs: 5.681e-02, Loss_res: 8.383e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.794e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.904469\n",
      "mean_grad_bcs: 0.040897\n",
      "It: 16900, Loss: 1.077e+03, Loss_bcs: 5.499e-02, Loss_res: 7.996e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.208e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.949534\n",
      "mean_grad_bcs: 0.040136\n",
      "It: 17000, Loss: 1.187e+03, Loss_bcs: 4.968e-02, Loss_res: 9.355e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.224e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.994406\n",
      "mean_grad_bcs: 0.040232\n",
      "It: 17100, Loss: 1.391e+03, Loss_bcs: 5.371e-02, Loss_res: 1.118e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.080e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.031083\n",
      "mean_grad_bcs: 0.042697\n",
      "It: 17200, Loss: 1.331e+03, Loss_bcs: 5.694e-02, Loss_res: 1.042e+00,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.625e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.132715\n",
      "mean_grad_bcs: 0.037772\n",
      "It: 17300, Loss: 1.304e+03, Loss_bcs: 5.016e-02, Loss_res: 1.049e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.149e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.356134\n",
      "mean_grad_bcs: 0.039341\n",
      "It: 17400, Loss: 1.324e+03, Loss_bcs: 5.026e-02, Loss_res: 1.069e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.261e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.449130\n",
      "mean_grad_bcs: 0.034648\n",
      "It: 17500, Loss: 1.243e+03, Loss_bcs: 5.097e-02, Loss_res: 9.844e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.253e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.316724\n",
      "mean_grad_bcs: 0.037260\n",
      "It: 17600, Loss: 1.168e+03, Loss_bcs: 5.304e-02, Loss_res: 9.003e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.730e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.978407\n",
      "mean_grad_bcs: 0.038408\n",
      "It: 17700, Loss: 1.061e+03, Loss_bcs: 4.683e-02, Loss_res: 8.239e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.124e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.972478\n",
      "mean_grad_bcs: 0.042272\n",
      "It: 17800, Loss: 9.790e+02, Loss_bcs: 5.057e-02, Loss_res: 7.241e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.943e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.611187\n",
      "mean_grad_bcs: 0.040771\n",
      "It: 17900, Loss: 1.216e+03, Loss_bcs: 6.097e-02, Loss_res: 9.082e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.254e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.196210\n",
      "mean_grad_bcs: 0.041490\n",
      "It: 18000, Loss: 1.168e+03, Loss_bcs: 5.079e-02, Loss_res: 9.105e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.041e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.104885\n",
      "mean_grad_bcs: 0.055493\n",
      "It: 18100, Loss: 1.343e+03, Loss_bcs: 5.894e-02, Loss_res: 1.045e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.360e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.622704\n",
      "mean_grad_bcs: 0.038903\n",
      "It: 18200, Loss: 1.056e+03, Loss_bcs: 5.475e-02, Loss_res: 7.801e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.481e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.463031\n",
      "mean_grad_bcs: 0.045575\n",
      "It: 18300, Loss: 9.150e+02, Loss_bcs: 5.608e-02, Loss_res: 6.331e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.555e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.915305\n",
      "mean_grad_bcs: 0.039700\n",
      "It: 18400, Loss: 1.132e+03, Loss_bcs: 6.471e-02, Loss_res: 8.064e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.476e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.766181\n",
      "mean_grad_bcs: 0.039949\n",
      "It: 18500, Loss: 1.124e+03, Loss_bcs: 5.400e-02, Loss_res: 8.515e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.489e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.296207\n",
      "mean_grad_bcs: 0.057328\n",
      "It: 18600, Loss: 9.371e+02, Loss_bcs: 5.750e-02, Loss_res: 6.481e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.275e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.187564\n",
      "mean_grad_bcs: 0.043004\n",
      "It: 18700, Loss: 1.087e+03, Loss_bcs: 5.268e-02, Loss_res: 8.216e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.209e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.530220\n",
      "mean_grad_bcs: 0.037765\n",
      "It: 18800, Loss: 1.442e+03, Loss_bcs: 5.630e-02, Loss_res: 1.157e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.235e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.634051\n",
      "mean_grad_bcs: 0.042893\n",
      "It: 18900, Loss: 1.079e+03, Loss_bcs: 5.904e-02, Loss_res: 7.820e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.683e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.869136\n",
      "mean_grad_bcs: 0.055532\n",
      "It: 19000, Loss: 1.481e+03, Loss_bcs: 5.510e-02, Loss_res: 1.201e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.692e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.292164\n",
      "mean_grad_bcs: 0.038448\n",
      "It: 19100, Loss: 1.209e+03, Loss_bcs: 5.131e-02, Loss_res: 9.490e-01,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.402e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.243805\n",
      "mean_grad_bcs: 0.039608\n",
      "It: 19200, Loss: 9.822e+02, Loss_bcs: 5.432e-02, Loss_res: 7.086e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.846e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.844130\n",
      "mean_grad_bcs: 0.040407\n",
      "It: 19300, Loss: 8.794e+02, Loss_bcs: 4.923e-02, Loss_res: 6.316e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.804e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.719323\n",
      "mean_grad_bcs: 0.037095\n",
      "It: 19400, Loss: 1.114e+03, Loss_bcs: 5.053e-02, Loss_res: 8.586e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.598e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.862084\n",
      "mean_grad_bcs: 0.037307\n",
      "It: 19500, Loss: 1.586e+03, Loss_bcs: 5.603e-02, Loss_res: 1.301e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.051e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.191849\n",
      "mean_grad_bcs: 0.036219\n",
      "It: 19600, Loss: 1.414e+03, Loss_bcs: 4.551e-02, Loss_res: 1.182e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.525e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.786507\n",
      "mean_grad_bcs: 0.033304\n",
      "It: 19700, Loss: 1.012e+03, Loss_bcs: 4.585e-02, Loss_res: 7.798e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.428e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.691532\n",
      "mean_grad_bcs: 0.039278\n",
      "It: 19800, Loss: 1.174e+03, Loss_bcs: 5.917e-02, Loss_res: 8.758e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.836e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.430162\n",
      "mean_grad_bcs: 0.039158\n",
      "It: 19900, Loss: 1.086e+03, Loss_bcs: 5.108e-02, Loss_res: 8.275e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.412e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.535304\n",
      "mean_grad_bcs: 0.033805\n",
      "It: 20000, Loss: 1.120e+03, Loss_bcs: 6.062e-02, Loss_res: 8.145e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.851e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.076495\n",
      "mean_grad_bcs: 0.034961\n",
      "It: 20100, Loss: 1.001e+03, Loss_bcs: 4.806e-02, Loss_res: 7.582e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.844e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.203223\n",
      "mean_grad_bcs: 0.033669\n",
      "It: 20200, Loss: 1.168e+03, Loss_bcs: 4.915e-02, Loss_res: 9.192e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.771e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.750193\n",
      "mean_grad_bcs: 0.042013\n",
      "It: 20300, Loss: 1.311e+03, Loss_bcs: 5.102e-02, Loss_res: 1.053e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.275e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 11.002032\n",
      "mean_grad_bcs: 0.041137\n",
      "It: 20400, Loss: 8.923e+02, Loss_bcs: 5.020e-02, Loss_res: 6.396e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.128e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.743292\n",
      "mean_grad_bcs: 0.040181\n",
      "It: 20500, Loss: 1.029e+03, Loss_bcs: 4.839e-02, Loss_res: 7.842e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.784e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.901635\n",
      "mean_grad_bcs: 0.034921\n",
      "It: 20600, Loss: 1.071e+03, Loss_bcs: 4.673e-02, Loss_res: 8.347e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.613e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.987864\n",
      "mean_grad_bcs: 0.038750\n",
      "It: 20700, Loss: 1.255e+03, Loss_bcs: 4.910e-02, Loss_res: 1.006e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.968e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.974088\n",
      "mean_grad_bcs: 0.044837\n",
      "It: 20800, Loss: 9.041e+02, Loss_bcs: 5.397e-02, Loss_res: 6.327e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.392e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.386858\n",
      "mean_grad_bcs: 0.037567\n",
      "It: 20900, Loss: 9.120e+02, Loss_bcs: 5.146e-02, Loss_res: 6.530e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.858e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.430470\n",
      "mean_grad_bcs: 0.038249\n",
      "It: 21000, Loss: 9.660e+02, Loss_bcs: 5.168e-02, Loss_res: 7.056e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.020e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.776685\n",
      "mean_grad_bcs: 0.037340\n",
      "It: 21100, Loss: 1.022e+03, Loss_bcs: 4.549e-02, Loss_res: 7.924e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.062e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.541544\n",
      "mean_grad_bcs: 0.043659\n",
      "It: 21200, Loss: 6.813e+02, Loss_bcs: 4.266e-02, Loss_res: 4.670e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.282e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.721868\n",
      "mean_grad_bcs: 0.034837\n",
      "It: 21300, Loss: 1.448e+03, Loss_bcs: 4.818e-02, Loss_res: 1.203e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.979e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.383826\n",
      "mean_grad_bcs: 0.031001\n",
      "It: 21400, Loss: 1.306e+03, Loss_bcs: 4.557e-02, Loss_res: 1.075e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.368e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.131701\n",
      "mean_grad_bcs: 0.034415\n",
      "It: 21500, Loss: 8.720e+02, Loss_bcs: 4.638e-02, Loss_res: 6.384e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.343e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.466932\n",
      "mean_grad_bcs: 0.039322\n",
      "It: 21600, Loss: 7.490e+02, Loss_bcs: 4.586e-02, Loss_res: 5.185e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.886e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.561621\n",
      "mean_grad_bcs: 0.033125\n",
      "It: 21700, Loss: 1.043e+03, Loss_bcs: 4.118e-02, Loss_res: 8.337e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.450e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.989728\n",
      "mean_grad_bcs: 0.032581\n",
      "It: 21800, Loss: 9.472e+02, Loss_bcs: 4.950e-02, Loss_res: 6.977e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.808e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.871365\n",
      "mean_grad_bcs: 0.033022\n",
      "It: 21900, Loss: 1.021e+03, Loss_bcs: 4.483e-02, Loss_res: 7.945e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.249e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.041817\n",
      "mean_grad_bcs: 0.034064\n",
      "It: 22000, Loss: 8.454e+02, Loss_bcs: 4.450e-02, Loss_res: 6.212e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.740e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.829415\n",
      "mean_grad_bcs: 0.035212\n",
      "It: 22100, Loss: 1.074e+03, Loss_bcs: 4.507e-02, Loss_res: 8.454e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.950e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.900920\n",
      "mean_grad_bcs: 0.034919\n",
      "It: 22200, Loss: 1.184e+03, Loss_bcs: 4.126e-02, Loss_res: 9.737e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.170e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.373632\n",
      "mean_grad_bcs: 0.033485\n",
      "It: 22300, Loss: 8.288e+02, Loss_bcs: 4.674e-02, Loss_res: 5.936e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.961e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.944329\n",
      "mean_grad_bcs: 0.036986\n",
      "It: 22400, Loss: 9.338e+02, Loss_bcs: 4.754e-02, Loss_res: 6.941e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.128e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.153194\n",
      "mean_grad_bcs: 0.037949\n",
      "It: 22500, Loss: 6.791e+02, Loss_bcs: 4.737e-02, Loss_res: 4.414e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.542e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.834742\n",
      "mean_grad_bcs: 0.036533\n",
      "It: 22600, Loss: 8.309e+02, Loss_bcs: 4.245e-02, Loss_res: 6.169e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.645e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.975518\n",
      "mean_grad_bcs: 0.046267\n",
      "It: 22700, Loss: 8.241e+02, Loss_bcs: 4.525e-02, Loss_res: 5.962e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.851e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.847457\n",
      "mean_grad_bcs: 0.037247\n",
      "It: 22800, Loss: 1.017e+03, Loss_bcs: 4.653e-02, Loss_res: 7.814e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.930e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.082999\n",
      "mean_grad_bcs: 0.032405\n",
      "It: 22900, Loss: 8.315e+02, Loss_bcs: 4.618e-02, Loss_res: 5.990e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.513e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.123611\n",
      "mean_grad_bcs: 0.035231\n",
      "It: 23000, Loss: 1.013e+03, Loss_bcs: 5.080e-02, Loss_res: 7.569e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.119e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.504122\n",
      "mean_grad_bcs: 0.040532\n",
      "It: 23100, Loss: 8.620e+02, Loss_bcs: 4.736e-02, Loss_res: 6.235e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.394e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.669344\n",
      "mean_grad_bcs: 0.031977\n",
      "It: 23200, Loss: 8.376e+02, Loss_bcs: 3.908e-02, Loss_res: 6.402e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.631e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.964934\n",
      "mean_grad_bcs: 0.038479\n",
      "It: 23300, Loss: 1.402e+03, Loss_bcs: 4.956e-02, Loss_res: 1.150e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.470e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.885535\n",
      "mean_grad_bcs: 0.033787\n",
      "It: 23400, Loss: 9.570e+02, Loss_bcs: 4.354e-02, Loss_res: 7.369e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.146e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.807946\n",
      "mean_grad_bcs: 0.040146\n",
      "It: 23500, Loss: 9.696e+02, Loss_bcs: 4.455e-02, Loss_res: 7.445e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.438e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.639256\n",
      "mean_grad_bcs: 0.031905\n",
      "It: 23600, Loss: 9.767e+02, Loss_bcs: 4.158e-02, Loss_res: 7.662e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.250e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.359897\n",
      "mean_grad_bcs: 0.038351\n",
      "It: 23700, Loss: 8.783e+02, Loss_bcs: 4.382e-02, Loss_res: 6.573e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.671e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.759538\n",
      "mean_grad_bcs: 0.034229\n",
      "It: 23800, Loss: 8.942e+02, Loss_bcs: 4.068e-02, Loss_res: 6.886e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.431e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.767183\n",
      "mean_grad_bcs: 0.046103\n",
      "It: 23900, Loss: 7.244e+02, Loss_bcs: 4.407e-02, Loss_res: 5.029e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.488e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.891432\n",
      "mean_grad_bcs: 0.044935\n",
      "It: 24000, Loss: 1.154e+03, Loss_bcs: 4.637e-02, Loss_res: 9.188e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.716e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.132480\n",
      "mean_grad_bcs: 0.031580\n",
      "It: 24100, Loss: 1.283e+03, Loss_bcs: 5.055e-02, Loss_res: 1.027e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.171e+02\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 9.198545\n",
      "mean_grad_bcs: 0.036833\n",
      "It: 24200, Loss: 1.477e+03, Loss_bcs: 4.529e-02, Loss_res: 1.246e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.180e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.485042\n",
      "mean_grad_bcs: 0.032499\n",
      "It: 24300, Loss: 1.141e+03, Loss_bcs: 4.465e-02, Loss_res: 9.148e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.756e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 8.008003\n",
      "mean_grad_bcs: 0.031590\n",
      "It: 24400, Loss: 1.384e+03, Loss_bcs: 3.992e-02, Loss_res: 1.179e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.388e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.507455\n",
      "mean_grad_bcs: 0.037216\n",
      "It: 24500, Loss: 9.547e+02, Loss_bcs: 4.595e-02, Loss_res: 7.227e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.356e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.240425\n",
      "mean_grad_bcs: 0.043896\n",
      "It: 24600, Loss: 8.192e+02, Loss_bcs: 3.803e-02, Loss_res: 6.270e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.566e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.693894\n",
      "mean_grad_bcs: 0.028443\n",
      "It: 24700, Loss: 1.038e+03, Loss_bcs: 4.297e-02, Loss_res: 8.203e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.584e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.789779\n",
      "mean_grad_bcs: 0.030913\n",
      "It: 24800, Loss: 7.044e+02, Loss_bcs: 4.576e-02, Loss_res: 4.746e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.064e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.586958\n",
      "mean_grad_bcs: 0.030633\n",
      "It: 24900, Loss: 8.527e+02, Loss_bcs: 4.103e-02, Loss_res: 6.455e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.197e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.357217\n",
      "mean_grad_bcs: 0.041856\n",
      "It: 25000, Loss: 8.073e+02, Loss_bcs: 4.236e-02, Loss_res: 5.939e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.230e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.536164\n",
      "mean_grad_bcs: 0.038870\n",
      "It: 25100, Loss: 9.166e+02, Loss_bcs: 4.755e-02, Loss_res: 6.769e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.345e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.847647\n",
      "mean_grad_bcs: 0.036402\n",
      "It: 25200, Loss: 1.072e+03, Loss_bcs: 4.086e-02, Loss_res: 8.651e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.383e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.210070\n",
      "mean_grad_bcs: 0.033572\n",
      "It: 25300, Loss: 7.280e+02, Loss_bcs: 3.579e-02, Loss_res: 5.474e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.628e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.172328\n",
      "mean_grad_bcs: 0.029662\n",
      "It: 25400, Loss: 7.378e+02, Loss_bcs: 3.987e-02, Loss_res: 5.370e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.108e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.170798\n",
      "mean_grad_bcs: 0.044478\n",
      "It: 25500, Loss: 1.055e+03, Loss_bcs: 4.130e-02, Loss_res: 8.459e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.080e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.798206\n",
      "mean_grad_bcs: 0.027181\n",
      "It: 25600, Loss: 1.098e+03, Loss_bcs: 4.252e-02, Loss_res: 8.820e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.761e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.691077\n",
      "mean_grad_bcs: 0.037736\n",
      "It: 25700, Loss: 7.733e+02, Loss_bcs: 4.489e-02, Loss_res: 5.474e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.740e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.082645\n",
      "mean_grad_bcs: 0.033342\n",
      "It: 25800, Loss: 7.505e+02, Loss_bcs: 4.096e-02, Loss_res: 5.442e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.515e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.555368\n",
      "mean_grad_bcs: 0.033039\n",
      "It: 25900, Loss: 7.511e+02, Loss_bcs: 3.952e-02, Loss_res: 5.520e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.993e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.238236\n",
      "mean_grad_bcs: 0.032135\n",
      "It: 26000, Loss: 9.655e+02, Loss_bcs: 4.276e-02, Loss_res: 7.493e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.789e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.250688\n",
      "mean_grad_bcs: 0.033656\n",
      "It: 26100, Loss: 1.034e+03, Loss_bcs: 3.378e-02, Loss_res: 8.614e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 9.127e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.693537\n",
      "mean_grad_bcs: 0.030778\n",
      "It: 26200, Loss: 8.159e+02, Loss_bcs: 4.397e-02, Loss_res: 5.944e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.322e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.700425\n",
      "mean_grad_bcs: 0.029004\n",
      "It: 26300, Loss: 6.512e+02, Loss_bcs: 3.837e-02, Loss_res: 4.582e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.740e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.469688\n",
      "mean_grad_bcs: 0.030869\n",
      "It: 26400, Loss: 1.078e+03, Loss_bcs: 3.816e-02, Loss_res: 8.840e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.721e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.709820\n",
      "mean_grad_bcs: 0.032105\n",
      "It: 26500, Loss: 6.714e+02, Loss_bcs: 3.900e-02, Loss_res: 4.752e-01,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.433e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.739380\n",
      "mean_grad_bcs: 0.031939\n",
      "It: 26600, Loss: 7.309e+02, Loss_bcs: 3.914e-02, Loss_res: 5.337e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.444e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.400670\n",
      "mean_grad_bcs: 0.038421\n",
      "It: 26700, Loss: 6.554e+02, Loss_bcs: 3.411e-02, Loss_res: 4.834e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.672e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.155289\n",
      "mean_grad_bcs: 0.029232\n",
      "It: 26800, Loss: 6.658e+02, Loss_bcs: 4.129e-02, Loss_res: 4.583e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.021e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.625639\n",
      "mean_grad_bcs: 0.052498\n",
      "It: 26900, Loss: 8.199e+02, Loss_bcs: 3.839e-02, Loss_res: 6.260e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.712e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.151152\n",
      "mean_grad_bcs: 0.031609\n",
      "It: 27000, Loss: 9.530e+02, Loss_bcs: 4.068e-02, Loss_res: 7.472e-01,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.027e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.049959\n",
      "mean_grad_bcs: 0.030772\n",
      "It: 27100, Loss: 6.315e+02, Loss_bcs: 4.123e-02, Loss_res: 4.244e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.545e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.196569\n",
      "mean_grad_bcs: 0.032142\n",
      "It: 27200, Loss: 6.757e+02, Loss_bcs: 3.758e-02, Loss_res: 4.865e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.240e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.638602\n",
      "mean_grad_bcs: 0.032413\n",
      "It: 27300, Loss: 1.623e+03, Loss_bcs: 3.817e-02, Loss_res: 1.427e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.771e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.696989\n",
      "mean_grad_bcs: 0.034233\n",
      "It: 27400, Loss: 9.048e+02, Loss_bcs: 3.654e-02, Loss_res: 7.197e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.952e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.236962\n",
      "mean_grad_bcs: 0.030026\n",
      "It: 27500, Loss: 8.271e+02, Loss_bcs: 3.618e-02, Loss_res: 6.441e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.015e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.670872\n",
      "mean_grad_bcs: 0.039332\n",
      "It: 27600, Loss: 6.603e+02, Loss_bcs: 4.255e-02, Loss_res: 4.466e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.992e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.860505\n",
      "mean_grad_bcs: 0.029950\n",
      "It: 27700, Loss: 7.613e+02, Loss_bcs: 3.618e-02, Loss_res: 5.786e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.597e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.429042\n",
      "mean_grad_bcs: 0.033891\n",
      "It: 27800, Loss: 7.941e+02, Loss_bcs: 4.349e-02, Loss_res: 5.751e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.185e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.087902\n",
      "mean_grad_bcs: 0.046120\n",
      "It: 27900, Loss: 1.097e+03, Loss_bcs: 3.980e-02, Loss_res: 8.943e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.728e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.632134\n",
      "mean_grad_bcs: 0.032269\n",
      "It: 28000, Loss: 7.482e+02, Loss_bcs: 4.122e-02, Loss_res: 5.406e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.207e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.775781\n",
      "mean_grad_bcs: 0.033291\n",
      "It: 28100, Loss: 6.171e+02, Loss_bcs: 3.776e-02, Loss_res: 4.273e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.461e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.022667\n",
      "mean_grad_bcs: 0.029586\n",
      "It: 28200, Loss: 9.165e+02, Loss_bcs: 3.916e-02, Loss_res: 7.183e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.879e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 7.060755\n",
      "mean_grad_bcs: 0.029484\n",
      "It: 28300, Loss: 7.376e+02, Loss_bcs: 3.672e-02, Loss_res: 5.523e-01,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.745e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.400280\n",
      "mean_grad_bcs: 0.032929\n",
      "It: 28400, Loss: 8.608e+02, Loss_bcs: 3.323e-02, Loss_res: 6.922e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.893e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.645380\n",
      "mean_grad_bcs: 0.036288\n",
      "It: 28500, Loss: 6.695e+02, Loss_bcs: 3.557e-02, Loss_res: 4.903e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.510e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.357269\n",
      "mean_grad_bcs: 0.034913\n",
      "It: 28600, Loss: 6.007e+02, Loss_bcs: 3.485e-02, Loss_res: 4.254e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.317e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.631117\n",
      "mean_grad_bcs: 0.031250\n",
      "It: 28700, Loss: 7.197e+02, Loss_bcs: 3.514e-02, Loss_res: 5.423e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.731e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.478270\n",
      "mean_grad_bcs: 0.032661\n",
      "It: 28800, Loss: 8.273e+02, Loss_bcs: 4.537e-02, Loss_res: 5.988e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.545e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.294946\n",
      "mean_grad_bcs: 0.033712\n",
      "It: 28900, Loss: 9.091e+02, Loss_bcs: 3.754e-02, Loss_res: 7.190e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.285e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.554522\n",
      "mean_grad_bcs: 0.031029\n",
      "It: 29000, Loss: 5.378e+02, Loss_bcs: 3.335e-02, Loss_res: 3.702e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.878e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.342940\n",
      "mean_grad_bcs: 0.028002\n",
      "It: 29100, Loss: 7.399e+02, Loss_bcs: 3.389e-02, Loss_res: 5.687e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.533e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.350984\n",
      "mean_grad_bcs: 0.028310\n",
      "It: 29200, Loss: 1.254e+03, Loss_bcs: 3.489e-02, Loss_res: 1.075e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.860e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.133029\n",
      "mean_grad_bcs: 0.023682\n",
      "It: 29300, Loss: 7.453e+02, Loss_bcs: 3.460e-02, Loss_res: 5.705e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.845e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.125631\n",
      "mean_grad_bcs: 0.032563\n",
      "It: 29400, Loss: 7.081e+02, Loss_bcs: 3.300e-02, Loss_res: 5.413e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.770e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.724554\n",
      "mean_grad_bcs: 0.034091\n",
      "It: 29500, Loss: 8.394e+02, Loss_bcs: 4.006e-02, Loss_res: 6.371e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.005e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.862521\n",
      "mean_grad_bcs: 0.041350\n",
      "It: 29600, Loss: 7.186e+02, Loss_bcs: 3.404e-02, Loss_res: 5.467e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.054e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.363870\n",
      "mean_grad_bcs: 0.031836\n",
      "It: 29700, Loss: 6.681e+02, Loss_bcs: 3.938e-02, Loss_res: 4.700e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.193e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.046972\n",
      "mean_grad_bcs: 0.033161\n",
      "It: 29800, Loss: 8.454e+02, Loss_bcs: 3.740e-02, Loss_res: 6.562e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.136e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.283894\n",
      "mean_grad_bcs: 0.025374\n",
      "It: 29900, Loss: 9.811e+02, Loss_bcs: 3.531e-02, Loss_res: 8.016e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.722e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.374865\n",
      "mean_grad_bcs: 0.030242\n",
      "It: 30000, Loss: 6.482e+02, Loss_bcs: 3.654e-02, Loss_res: 4.643e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.309e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.018894\n",
      "mean_grad_bcs: 0.032995\n",
      "It: 30100, Loss: 1.024e+03, Loss_bcs: 3.783e-02, Loss_res: 8.321e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.176e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.531639\n",
      "mean_grad_bcs: 0.034694\n",
      "It: 30200, Loss: 8.722e+02, Loss_bcs: 4.101e-02, Loss_res: 6.651e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.124e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.763967\n",
      "mean_grad_bcs: 0.029678\n",
      "It: 30300, Loss: 7.537e+02, Loss_bcs: 3.646e-02, Loss_res: 5.696e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.889e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.491762\n",
      "mean_grad_bcs: 0.039264\n",
      "It: 30400, Loss: 7.123e+02, Loss_bcs: 3.400e-02, Loss_res: 5.406e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.931e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.792397\n",
      "mean_grad_bcs: 0.029743\n",
      "It: 30500, Loss: 6.092e+02, Loss_bcs: 3.690e-02, Loss_res: 4.237e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.514e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.527619\n",
      "mean_grad_bcs: 0.032806\n",
      "It: 30600, Loss: 7.516e+02, Loss_bcs: 3.406e-02, Loss_res: 5.794e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.060e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.616598\n",
      "mean_grad_bcs: 0.031741\n",
      "It: 30700, Loss: 7.775e+02, Loss_bcs: 3.143e-02, Loss_res: 6.182e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.224e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.329874\n",
      "mean_grad_bcs: 0.038851\n",
      "It: 30800, Loss: 7.945e+02, Loss_bcs: 2.897e-02, Loss_res: 6.473e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.722e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.067657\n",
      "mean_grad_bcs: 0.034078\n",
      "It: 30900, Loss: 8.961e+02, Loss_bcs: 3.491e-02, Loss_res: 7.190e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.091e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.069786\n",
      "mean_grad_bcs: 0.025451\n",
      "It: 31000, Loss: 6.129e+02, Loss_bcs: 3.500e-02, Loss_res: 4.368e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.531e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.381637\n",
      "mean_grad_bcs: 0.035373\n",
      "It: 31100, Loss: 8.332e+02, Loss_bcs: 4.377e-02, Loss_res: 6.127e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.195e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.096956\n",
      "mean_grad_bcs: 0.030806\n",
      "It: 31200, Loss: 5.661e+02, Loss_bcs: 3.571e-02, Loss_res: 3.866e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.289e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.077989\n",
      "mean_grad_bcs: 0.031954\n",
      "It: 31300, Loss: 6.801e+02, Loss_bcs: 3.573e-02, Loss_res: 5.001e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.067e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.274267\n",
      "mean_grad_bcs: 0.040773\n",
      "It: 31400, Loss: 9.096e+02, Loss_bcs: 3.494e-02, Loss_res: 7.323e-01,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.432e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.653175\n",
      "mean_grad_bcs: 0.028722\n",
      "It: 31500, Loss: 6.335e+02, Loss_bcs: 3.592e-02, Loss_res: 4.527e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.375e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.177117\n",
      "mean_grad_bcs: 0.027721\n",
      "It: 31600, Loss: 8.361e+02, Loss_bcs: 3.736e-02, Loss_res: 6.472e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.616e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.433787\n",
      "mean_grad_bcs: 0.031527\n",
      "It: 31700, Loss: 7.563e+02, Loss_bcs: 3.514e-02, Loss_res: 5.787e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.131e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.548786\n",
      "mean_grad_bcs: 0.031779\n",
      "It: 31800, Loss: 8.852e+02, Loss_bcs: 3.492e-02, Loss_res: 7.082e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.676e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.782552\n",
      "mean_grad_bcs: 0.030086\n",
      "It: 31900, Loss: 5.972e+02, Loss_bcs: 2.987e-02, Loss_res: 4.466e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.260e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.505718\n",
      "mean_grad_bcs: 0.032865\n",
      "It: 32000, Loss: 7.444e+02, Loss_bcs: 3.011e-02, Loss_res: 5.918e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.741e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.219288\n",
      "mean_grad_bcs: 0.035080\n",
      "It: 32100, Loss: 6.659e+02, Loss_bcs: 4.002e-02, Loss_res: 4.647e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.733e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.528307\n",
      "mean_grad_bcs: 0.032039\n",
      "It: 32200, Loss: 8.897e+02, Loss_bcs: 3.962e-02, Loss_res: 6.893e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.036e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.380501\n",
      "mean_grad_bcs: 0.030651\n",
      "It: 32300, Loss: 6.383e+02, Loss_bcs: 3.701e-02, Loss_res: 4.521e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.401e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.785769\n",
      "mean_grad_bcs: 0.028264\n",
      "It: 32400, Loss: 7.370e+02, Loss_bcs: 3.805e-02, Loss_res: 5.452e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.993e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.417452\n",
      "mean_grad_bcs: 0.030302\n",
      "It: 32500, Loss: 8.431e+02, Loss_bcs: 3.475e-02, Loss_res: 6.671e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.152e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.535793\n",
      "mean_grad_bcs: 0.028965\n",
      "It: 32600, Loss: 6.986e+02, Loss_bcs: 3.345e-02, Loss_res: 5.297e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.865e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.515911\n",
      "mean_grad_bcs: 0.029848\n",
      "It: 32700, Loss: 7.837e+02, Loss_bcs: 3.688e-02, Loss_res: 5.974e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.102e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.998623\n",
      "mean_grad_bcs: 0.037739\n",
      "It: 32800, Loss: 6.341e+02, Loss_bcs: 3.286e-02, Loss_res: 4.684e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.992e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.403229\n",
      "mean_grad_bcs: 0.029368\n",
      "It: 32900, Loss: 6.311e+02, Loss_bcs: 3.402e-02, Loss_res: 4.597e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.458e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.979442\n",
      "mean_grad_bcs: 0.029850\n",
      "It: 33000, Loss: 6.485e+02, Loss_bcs: 3.666e-02, Loss_res: 4.640e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.706e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.284967\n",
      "mean_grad_bcs: 0.032473\n",
      "It: 33100, Loss: 9.022e+02, Loss_bcs: 3.482e-02, Loss_res: 7.255e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.151e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.994671\n",
      "mean_grad_bcs: 0.028823\n",
      "It: 33200, Loss: 5.278e+02, Loss_bcs: 3.263e-02, Loss_res: 3.638e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.569e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.563931\n",
      "mean_grad_bcs: 0.029260\n",
      "It: 33300, Loss: 9.683e+02, Loss_bcs: 3.505e-02, Loss_res: 7.901e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.326e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.257671\n",
      "mean_grad_bcs: 0.030621\n",
      "It: 33400, Loss: 6.362e+02, Loss_bcs: 3.224e-02, Loss_res: 4.736e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.277e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.252762\n",
      "mean_grad_bcs: 0.032598\n",
      "It: 33500, Loss: 5.607e+02, Loss_bcs: 3.263e-02, Loss_res: 3.965e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.709e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.549718\n",
      "mean_grad_bcs: 0.028310\n",
      "It: 33600, Loss: 6.633e+02, Loss_bcs: 3.268e-02, Loss_res: 4.984e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.371e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.861140\n",
      "mean_grad_bcs: 0.025558\n",
      "It: 33700, Loss: 6.904e+02, Loss_bcs: 2.984e-02, Loss_res: 5.394e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.636e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.263657\n",
      "mean_grad_bcs: 0.034123\n",
      "It: 33800, Loss: 6.508e+02, Loss_bcs: 3.469e-02, Loss_res: 4.760e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.973e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.312181\n",
      "mean_grad_bcs: 0.031108\n",
      "It: 33900, Loss: 8.073e+02, Loss_bcs: 3.922e-02, Loss_res: 6.093e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.280e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.778265\n",
      "mean_grad_bcs: 0.033426\n",
      "It: 34000, Loss: 9.496e+02, Loss_bcs: 3.917e-02, Loss_res: 7.512e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.766e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.485971\n",
      "mean_grad_bcs: 0.033046\n",
      "It: 34100, Loss: 6.282e+02, Loss_bcs: 3.668e-02, Loss_res: 4.436e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.330e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.181688\n",
      "mean_grad_bcs: 0.028823\n",
      "It: 34200, Loss: 6.314e+02, Loss_bcs: 3.134e-02, Loss_res: 4.733e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.934e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.462943\n",
      "mean_grad_bcs: 0.025686\n",
      "It: 34300, Loss: 7.827e+02, Loss_bcs: 3.591e-02, Loss_res: 6.012e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.695e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.780214\n",
      "mean_grad_bcs: 0.028682\n",
      "It: 34400, Loss: 6.104e+02, Loss_bcs: 3.162e-02, Loss_res: 4.510e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.420e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.078311\n",
      "mean_grad_bcs: 0.039873\n",
      "It: 34500, Loss: 1.319e+03, Loss_bcs: 3.731e-02, Loss_res: 1.128e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.232e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.933187\n",
      "mean_grad_bcs: 0.026027\n",
      "It: 34600, Loss: 6.591e+02, Loss_bcs: 3.318e-02, Loss_res: 4.918e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.584e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.116767\n",
      "mean_grad_bcs: 0.032656\n",
      "It: 34700, Loss: 5.844e+02, Loss_bcs: 3.393e-02, Loss_res: 4.136e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.829e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.840384\n",
      "mean_grad_bcs: 0.028287\n",
      "It: 34800, Loss: 6.336e+02, Loss_bcs: 3.480e-02, Loss_res: 4.584e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.384e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.777771\n",
      "mean_grad_bcs: 0.032746\n",
      "It: 34900, Loss: 1.476e+03, Loss_bcs: 3.185e-02, Loss_res: 1.311e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.564e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.475136\n",
      "mean_grad_bcs: 0.030095\n",
      "It: 35000, Loss: 1.035e+03, Loss_bcs: 3.209e-02, Loss_res: 8.711e-01,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.640e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.596022\n",
      "mean_grad_bcs: 0.031143\n",
      "It: 35100, Loss: 6.394e+02, Loss_bcs: 3.525e-02, Loss_res: 4.619e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.814e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.596730\n",
      "mean_grad_bcs: 0.026439\n",
      "It: 35200, Loss: 5.785e+02, Loss_bcs: 3.324e-02, Loss_res: 4.112e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.937e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.094791\n",
      "mean_grad_bcs: 0.030041\n",
      "It: 35300, Loss: 6.252e+02, Loss_bcs: 3.516e-02, Loss_res: 4.482e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.780e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.696297\n",
      "mean_grad_bcs: 0.029515\n",
      "It: 35400, Loss: 6.935e+02, Loss_bcs: 3.295e-02, Loss_res: 5.271e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.748e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.190064\n",
      "mean_grad_bcs: 0.029675\n",
      "It: 35500, Loss: 7.957e+02, Loss_bcs: 3.843e-02, Loss_res: 6.017e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.948e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.797790\n",
      "mean_grad_bcs: 0.032694\n",
      "It: 35600, Loss: 6.942e+02, Loss_bcs: 3.730e-02, Loss_res: 5.063e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.900e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.615323\n",
      "mean_grad_bcs: 0.028638\n",
      "It: 35700, Loss: 8.180e+02, Loss_bcs: 3.050e-02, Loss_res: 6.631e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.781e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.597347\n",
      "mean_grad_bcs: 0.033232\n",
      "It: 35800, Loss: 6.617e+02, Loss_bcs: 3.151e-02, Loss_res: 5.025e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.165e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.091063\n",
      "mean_grad_bcs: 0.031246\n",
      "It: 35900, Loss: 6.690e+02, Loss_bcs: 3.322e-02, Loss_res: 5.014e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.150e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.712989\n",
      "mean_grad_bcs: 0.030203\n",
      "It: 36000, Loss: 6.852e+02, Loss_bcs: 3.449e-02, Loss_res: 5.112e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.566e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.086018\n",
      "mean_grad_bcs: 0.023570\n",
      "It: 36100, Loss: 6.712e+02, Loss_bcs: 3.364e-02, Loss_res: 5.015e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.724e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.640205\n",
      "mean_grad_bcs: 0.024814\n",
      "It: 36200, Loss: 6.571e+02, Loss_bcs: 3.147e-02, Loss_res: 4.982e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.013e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.133768\n",
      "mean_grad_bcs: 0.026496\n",
      "It: 36300, Loss: 7.729e+02, Loss_bcs: 3.440e-02, Loss_res: 5.989e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.722e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.192774\n",
      "mean_grad_bcs: 0.024713\n",
      "It: 36400, Loss: 8.702e+02, Loss_bcs: 3.144e-02, Loss_res: 7.104e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.176e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.273103\n",
      "mean_grad_bcs: 0.035181\n",
      "It: 36500, Loss: 5.821e+02, Loss_bcs: 3.152e-02, Loss_res: 4.234e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.840e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.906110\n",
      "mean_grad_bcs: 0.028098\n",
      "It: 36600, Loss: 6.289e+02, Loss_bcs: 3.737e-02, Loss_res: 4.410e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.290e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.377558\n",
      "mean_grad_bcs: 0.029718\n",
      "It: 36700, Loss: 5.717e+02, Loss_bcs: 3.617e-02, Loss_res: 3.900e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.538e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.525476\n",
      "mean_grad_bcs: 0.026159\n",
      "It: 36800, Loss: 7.751e+02, Loss_bcs: 3.617e-02, Loss_res: 5.925e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.703e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.083011\n",
      "mean_grad_bcs: 0.028836\n",
      "It: 36900, Loss: 1.227e+03, Loss_bcs: 3.122e-02, Loss_res: 1.066e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.220e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.153429\n",
      "mean_grad_bcs: 0.028927\n",
      "It: 37000, Loss: 5.375e+02, Loss_bcs: 3.415e-02, Loss_res: 3.660e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.043e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.398934\n",
      "mean_grad_bcs: 0.038453\n",
      "It: 37100, Loss: 8.650e+02, Loss_bcs: 3.210e-02, Loss_res: 7.019e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.577e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.389603\n",
      "mean_grad_bcs: 0.028140\n",
      "It: 37200, Loss: 6.201e+02, Loss_bcs: 3.155e-02, Loss_res: 4.609e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.012e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.214183\n",
      "mean_grad_bcs: 0.037576\n",
      "It: 37300, Loss: 8.150e+02, Loss_bcs: 3.585e-02, Loss_res: 6.337e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.020e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.295553\n",
      "mean_grad_bcs: 0.036989\n",
      "It: 37400, Loss: 6.210e+02, Loss_bcs: 3.305e-02, Loss_res: 4.544e-01,Time: 0.00\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.174e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.170686\n",
      "mean_grad_bcs: 0.031174\n",
      "It: 37500, Loss: 5.598e+02, Loss_bcs: 3.101e-02, Loss_res: 4.037e-01,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.859e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.858613\n",
      "mean_grad_bcs: 0.028213\n",
      "It: 37600, Loss: 6.123e+02, Loss_bcs: 3.240e-02, Loss_res: 4.490e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.025e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.974872\n",
      "mean_grad_bcs: 0.037081\n",
      "It: 37700, Loss: 7.448e+02, Loss_bcs: 3.179e-02, Loss_res: 5.839e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.506e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.913412\n",
      "mean_grad_bcs: 0.025222\n",
      "It: 37800, Loss: 6.353e+02, Loss_bcs: 2.921e-02, Loss_res: 4.877e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.037e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.949230\n",
      "mean_grad_bcs: 0.026704\n",
      "It: 37900, Loss: 1.072e+03, Loss_bcs: 3.964e-02, Loss_res: 8.709e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 8.301e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 6.003813\n",
      "mean_grad_bcs: 0.040779\n",
      "It: 38000, Loss: 1.360e+03, Loss_bcs: 3.408e-02, Loss_res: 1.185e+00,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 6.190e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 5.856117\n",
      "mean_grad_bcs: 0.030006\n",
      "It: 38100, Loss: 5.453e+02, Loss_bcs: 3.415e-02, Loss_res: 3.737e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.862e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.102401\n",
      "mean_grad_bcs: 0.026754\n",
      "It: 38200, Loss: 6.833e+02, Loss_bcs: 3.726e-02, Loss_res: 4.956e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.770e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.708395\n",
      "mean_grad_bcs: 0.028469\n",
      "It: 38300, Loss: 6.236e+02, Loss_bcs: 3.549e-02, Loss_res: 4.450e-01,Time: 0.02\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.219e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.727950\n",
      "mean_grad_bcs: 0.028834\n",
      "It: 38400, Loss: 7.757e+02, Loss_bcs: 3.373e-02, Loss_res: 6.050e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.817e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.713546\n",
      "mean_grad_bcs: 0.032773\n",
      "It: 38500, Loss: 5.644e+02, Loss_bcs: 3.621e-02, Loss_res: 3.825e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.547e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.829881\n",
      "mean_grad_bcs: 0.027190\n",
      "It: 38600, Loss: 6.334e+02, Loss_bcs: 3.359e-02, Loss_res: 4.641e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.080e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.015761\n",
      "mean_grad_bcs: 0.027290\n",
      "It: 38700, Loss: 7.130e+02, Loss_bcs: 2.995e-02, Loss_res: 5.614e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.039e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.445255\n",
      "mean_grad_bcs: 0.027442\n",
      "It: 38800, Loss: 8.324e+02, Loss_bcs: 3.866e-02, Loss_res: 6.370e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 7.312e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.329457\n",
      "mean_grad_bcs: 0.029234\n",
      "It: 38900, Loss: 6.277e+02, Loss_bcs: 3.034e-02, Loss_res: 4.745e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.925e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.786499\n",
      "mean_grad_bcs: 0.025002\n",
      "It: 39000, Loss: 6.150e+02, Loss_bcs: 3.676e-02, Loss_res: 4.302e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.367e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.096522\n",
      "mean_grad_bcs: 0.030776\n",
      "It: 39100, Loss: 7.346e+02, Loss_bcs: 3.434e-02, Loss_res: 5.612e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.231e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.575480\n",
      "mean_grad_bcs: 0.025661\n",
      "It: 39200, Loss: 6.088e+02, Loss_bcs: 3.470e-02, Loss_res: 4.342e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.368e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 1.818946\n",
      "mean_grad_bcs: 0.034267\n",
      "It: 39300, Loss: 6.222e+02, Loss_bcs: 3.474e-02, Loss_res: 4.473e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.652e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.048906\n",
      "mean_grad_bcs: 0.031139\n",
      "It: 39400, Loss: 7.657e+02, Loss_bcs: 3.476e-02, Loss_res: 5.900e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 5.067e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 4.089428\n",
      "mean_grad_bcs: 0.030441\n",
      "It: 39500, Loss: 5.139e+02, Loss_bcs: 3.612e-02, Loss_res: 3.328e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.562e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.749943\n",
      "mean_grad_bcs: 0.039470\n",
      "It: 39600, Loss: 7.438e+02, Loss_bcs: 3.486e-02, Loss_res: 5.677e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.704e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.404221\n",
      "mean_grad_bcs: 0.031570\n",
      "It: 39700, Loss: 5.249e+02, Loss_bcs: 3.587e-02, Loss_res: 3.449e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 1.829e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.618160\n",
      "mean_grad_bcs: 0.027993\n",
      "It: 39800, Loss: 1.008e+03, Loss_bcs: 3.784e-02, Loss_res: 8.162e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 4.847e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 3.710537\n",
      "mean_grad_bcs: 0.033447\n",
      "It: 39900, Loss: 5.677e+02, Loss_bcs: 3.258e-02, Loss_res: 4.037e-01,Time: 0.01\n",
      " np.max([max_grad_bcs , max_grad_res]): 2.568e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.742768\n",
      "mean_grad_bcs: 0.041381\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000, Loss: 5.837e+02, Loss_bcs: 3.442e-02, Loss_res: 4.106e-01,Time: 0.45\n",
      " np.max([max_grad_bcs , max_grad_res]): 3.818e+01\n",
      "adaptive_bcs_val: 4.970e+03\n",
      "adaptive_res_val: 1.005e+03\n",
      "mean_grad_res: 2.922009\n",
      "mean_grad_bcs: 0.025166\n",
      "Relative L2 error_u: 7.02e-02\n",
      "Relative L2 error_f: 1.08e-02\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Jan-16-2024_14-25-58-092649_M2\n",
      "Final loss total loss: 5.893403e+02\n",
      "Final loss loss_res: 4.161685e-01\n",
      "Final loss loss_bc1: 1.238242e-02\n",
      "Final loss loss_bc2: 8.395938e-03\n",
      "Final loss loss_bc3: 6.932987e-03\n",
      "Final loss loss_bc4: 6.715083e-03\n",
      "average lambda_bc4.9699e+03\n",
      "average lambda_res1.0\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:3.1759e+02\n",
      "average of error_u_list:7.0223e-02\n",
      "average of error_v_list:1.0761e-02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "a_1 = 1\n",
    "a_2 = 4\n",
    "\n",
    "# Parameter\n",
    "lam = 1.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[-1.0, -1.0], [1.0, -1.0]])\n",
    "bc2_coords = np.array([[1.0, -1.0], [1.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 1.0], [-1.0, 1.0]])\n",
    "bc4_coords = np.array([[-1.0, 1.0], [-1.0, -1.0]])\n",
    "\n",
    "dom_coords = np.array([[-1.0, -1.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "x1 = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x2 = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "X_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star, a_1, a_2)\n",
    "f_star = f(X_star, a_1, a_2, lam)\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = None\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'            # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False    # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "layers = [2, 50, 50, 50, 1]\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: u(x, a_1, a_2), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, a_1, a_2, lam), name='Forcing')\n",
    "\n",
    "        # [elapsed, error_u , error_f ,  mode] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , \n",
    "        #                                                                stiff_ratio , X_star ,u_star , f_star , nIter ,bcbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "#test_method(mtd , layers, operator, ics_sampler, bcs_sampler , res_sampler ,lam , mode , stiff_ratio , X_star ,u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            model = Helmholtz2D(layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    " #def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, lam, mode, sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                model.trainmb(nIter, batch_size=mbbatch_size )\n",
    "            else:\n",
    "                model.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            f_pred = model.predict_r(X_star)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "            model.plot_grad()\n",
    "            model.plot_lambda()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( x1 , x2 , X_star , u_star , u_pred , f_star , f_pred)\n",
    "\n",
    "            model.print(\"average lambda_bc\" , np.average(model.adpative_bcs_log))\n",
    "            model.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "            time_list.append(elapsed)\n",
    "            error_u_list.append(error_u)\n",
    "            error_f_list.append(error_f)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_v_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_f_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_Helmholtz_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1408950603.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_78693/1408950603.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    average of time_list:1.7314e+02\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Method: mini_batch\n",
    "\n",
    "average of time_list:1.7314e+02\n",
    "average of error_u_list:5.2605e-03\n",
    "average of error_v_list:9.5779e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21548/2533309064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Predicted solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mU_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mF_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'u_pred' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot ###\n",
    "\n",
    "# Exact solution & Predicted solution\n",
    "# Exact soluton\n",
    "U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "# Predicted solution\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(x1, x2, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Exact $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(x1, x2, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Predicted $u(x)$')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual loss & Boundary loss\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "gradients_res_list = []\n",
    "gradients_bcs_list = []\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "for j in range(num_hidden_layers):\n",
    "    gradient_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradient_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    gradients_res_list.append(gradient_res)\n",
    "    gradients_bcs_list.append(gradient_bcs)\n",
    "\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    \n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlim([-3.0, 3.0])\n",
    "    ax.set_ylim([0,100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = mode.eigenvalue_log\n",
    "    eigenvalues_bcs_list = mode.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = mode.eigenvalue_res_log\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
