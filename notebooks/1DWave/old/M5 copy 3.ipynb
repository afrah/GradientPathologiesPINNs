{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkCgnRiYQSY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import timeit\n",
        "from scipy.interpolate import griddata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
        "import timeit\n",
        "\n",
        "import sys\n",
        "\n",
        "import scipy\n",
        "import scipy.io\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "import os.path\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "class Sampler:\n",
        "    # Initialize the class\n",
        "    def __init__(self, dim, coords, func, name = None):\n",
        "        self.dim = dim\n",
        "        self.coords = coords\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "    def sample(self, N):\n",
        "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
        "        y = self.func(x)\n",
        "        return x, y\n",
        "\n",
        "# Define the exact solution and its derivatives\n",
        "def u(x, a, c):\n",
        "    \"\"\"\n",
        "    :param x: x = (t, x)\n",
        "    \"\"\"\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    return np.sin(np.pi * x) * np.cos(c * np.pi * t) + a * np.sin(2 * c * np.pi* x) * np.cos(4 * c  * np.pi * t)\n",
        "\n",
        "def u_t(x,a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_t = -  c * np.pi * np.sin(np.pi * x) * np.sin(c * np.pi * t) -  a * 4 * c * np.pi * np.sin(2 * c * np.pi* x) * np.sin(4 * c * np.pi * t)\n",
        "    return u_t\n",
        "\n",
        "def u_tt(x, a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_tt = -(c * np.pi)**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) - a * (4 * c * np.pi)**2 *  np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
        "    return u_tt\n",
        "\n",
        "def u_xx(x, a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_xx = - np.pi**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) -  a * (2 * c * np.pi)** 2 * np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
        "    return  u_xx\n",
        "\n",
        "\n",
        "def r(x, a, c):\n",
        "    return u_tt(x, a, c) - c**2 * u_xx(x, a, c)\n",
        "\n",
        "def operator(u, t, x, c, sigma_t=1.0, sigma_x=1.0):\n",
        "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
        "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
        "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
        "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
        "    residual = u_tt - c**2 * u_xx\n",
        "    return residual\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y7cHTcJfBTR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SDqDWN3nfSAg"
      },
      "outputs": [],
      "source": [
        "class PINN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, c ,mode ,  sess):\n",
        "        # Normalization \n",
        "\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        self.dirname, logpath = self.make_output_dir()\n",
        "        self.logger = self.get_logger(logpath)     \n",
        "\n",
        "        X, _ = res_sampler.sample(np.int32(1e5))\n",
        "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
        "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
        "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
        "\n",
        "        self.activFun = 'tanh'\n",
        "        # Samplers\n",
        "        self.operator = operator\n",
        "        self.ics_sampler = ics_sampler\n",
        "        self.bcs_sampler = bcs_sampler\n",
        "        self.res_sampler = res_sampler\n",
        "\n",
        "        self.sess = sess\n",
        "        # Initialize network weights and biases\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "        \n",
        "        # weights\n",
        "        self.loss_list = [ \"loss_bc1\", \"loss_bc2\", \"loss_ics_u\" ]#, \"loss_ics_ut\" ] \n",
        "\n",
        "        self.lam_u_val = np.array([2.0])\n",
        "        # self.lam_ut_val = np.array([2.0])\n",
        "        # self.lam_res_val = np.array(1.0)\n",
        "        self.lam_bc1_val = np.array([2.0])\n",
        "        self.lam_bc2_val = np.array([2.0])\n",
        "        self.lambda_val_list ={  \"loss_bc1\" : self.lam_bc1_val , \"loss_bc2\" : self.lam_bc2_val , \"loss_ics_u\" :self.lam_u_val }#, \"loss_ics_ut\" :self.lam_ut_val }\n",
        "\n",
        "        # Wave constant\n",
        "        self.c = tf.constant(c, dtype=tf.float32)\n",
        "        \n",
        "        self.T = 1\n",
        "        # Define Tensorflow session\n",
        "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
        "\n",
        "        # Define placeholders and computational graph\n",
        "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        \n",
        "        self.lam_u_tf = tf.placeholder(tf.float32, shape=self.lam_u_val.shape)\n",
        "        self.lam_ut_tf = tf.placeholder(tf.float32, shape=self.lam_u_val.shape)\n",
        "        # self.lam_r_tf = tf.placeholder(tf.float32, shape=self.lam_u_val.shape)\n",
        "        self.lam_bc1_tf = tf.placeholder(tf.float32, shape=self.lam_bc1_val.shape)\n",
        "        self.lam_bc2_tf = tf.placeholder(tf.float32, shape=self.lam_bc2_val.shape)\n",
        "        self.lam_res_tf = tf.placeholder(tf.float32, shape=self.lam_bc2_val.shape)\n",
        "\n",
        "\n",
        "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
        "\n",
        "\n",
        "        # Evaluate predictions\n",
        "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
        "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
        "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
        "\n",
        "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
        "        \n",
        "        \n",
        "        # Boundary loss and Initial loss\n",
        "        self.loss_ics_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred)) + tf.reduce_mean(tf.square( tf.gradients(self.u_ics_pred, self.t_ics_tf)[0] / self.sigma_t) )\n",
        "        # self.loss_ics_ut = tf.reduce_mean(tf.square( tf.gradients(self.u_ics_pred, self.t_ics_tf)[0] / self.sigma_t) )\n",
        "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred)) #+ tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
        "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
        "\n",
        "        # Residual loss\n",
        "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred))\n",
        "\n",
        "        # Total loss\n",
        "        self.loss =  self.loss_res + self.lam_bc1_tf * self.loss_bc1 +  self.lam_bc2_tf * self.loss_bc2 + self.lam_u_tf * self.loss_ics_u \n",
        "\n",
        "        self.prev_loss = 0.0\n",
        "        # Define optimizer with learning rate schedule\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 1e-3\n",
        "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,  1000, 0.9, staircase=False)\n",
        "        # Passing global_step to minimize() will increment it at each step.\n",
        "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
        "        \n",
        "\n",
        "        self.loss_tensor_list = [ self.loss ,  self.loss_res,  self.loss_bc1 , self.loss_bc2 , self.loss_ics_u] \n",
        "\n",
        "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
        "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
        "\n",
        "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
        "        self.dict_gradients_bcs1_layers = self.generate_grad_dict()\n",
        "        self.dict_gradients_bcs2_layers = self.generate_grad_dict()\n",
        "        self.dict_gradients_ics_layers = self.generate_grad_dict()\n",
        "\n",
        "        # Gradients Storage\n",
        "        self.grad_res = []\n",
        "        self.grad_ics_u = []\n",
        "        self.grad_ics_ut = []\n",
        "        self.grad_bcs1 = []\n",
        "        self.grad_bcs2 = []\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
        "            self.grad_bcs1.append(tf.gradients(self.loss_bc1, self.weights[i])[0])\n",
        "            self.grad_bcs2.append(tf.gradients(self.loss_bc2, self.weights[i])[0])\n",
        "            self.grad_ics_u.append(tf.gradients(self.loss_ics_u, self.weights[i])[0])\n",
        "            # self.grad_ics_ut.append(tf.gradients(self.loss_ics_ut, self.weights[i])[0])\n",
        "\n",
        "        self.mean_grad_res_list = []\n",
        "        self.mean_grad_bcs1_list = []\n",
        "        self.mean_grad_bcs2_list = []\n",
        "        self.mean_grad_ics_u_list = []\n",
        "        # self.mean_grad_ics_ut_list = []\n",
        "\n",
        "        self.mean_grad_res_log = []\n",
        "        self.mean_grad_bcs1_log = []\n",
        "        self.mean_grad_bcs2_log = []\n",
        "        self.mean_grad_ics_u_log = []\n",
        "        # self.mean_grad_ics_ut_log = []\n",
        "\n",
        "        self.adaptive_constant_bcs1_log  = []\n",
        "        self.adaptive_constant_bcs2_log  = []\n",
        "        self.adaptive_constant_ics_u_log  = []\n",
        "        # self.adaptive_constant_ics_ut_log  = []\n",
        "        self.adaptive_constant_res_log  = []\n",
        "    \n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.mean_grad_res_list.append(tf.math.reduce_max(tf.abs(self.grad_res[i]))) \n",
        "            self.mean_grad_bcs1_list.append(tf.math.reduce_mean(tf.abs(self.grad_bcs1[i])))\n",
        "            self.mean_grad_bcs2_list.append(tf.math.reduce_mean(tf.abs(self.grad_bcs2[i])))\n",
        "            self.mean_grad_ics_u_list.append(tf.math.reduce_mean(tf.abs(self.grad_ics_u[i])))\n",
        "            # self.mean_grad_ics_ut_list.append(tf.math.reduce_mean(tf.abs(self.grad_ics_ut[i])))\n",
        "\n",
        "        self.mean_grad_res = tf.math.reduce_max(tf.stack(self.mean_grad_res_list))\n",
        "        self.mean_grad_bcs1 = tf.math.reduce_mean(tf.stack(self.mean_grad_bcs1_list))\n",
        "        self.mean_grad_bcs2 = tf.math.reduce_mean(tf.stack(self.mean_grad_bcs2_list))\n",
        "        self.mean_grad_ics_u = tf.math.reduce_mean(tf.stack(self.mean_grad_ics_u_list))\n",
        "        # self.mean_grad_ics_ut = tf.math.reduce_mean(tf.stack(self.mean_grad_ics_ut_list))\n",
        "        \n",
        "        self.adaptive_constant_bcs1 =  self.mean_grad_res / self.mean_grad_bcs1\n",
        "        self.adaptive_constant_bcs2 = self.mean_grad_res / self.mean_grad_bcs2\n",
        "        self.adaptive_constant_u_ics = self.mean_grad_res / self.mean_grad_ics_u\n",
        "        # self.adaptive_constant_ut_ics = self.mean_grad_res / self.mean_grad_ics_ut\n",
        "        self.adaptive_constant_res = self.mean_grad_res \n",
        "\n",
        "        self.lambda_list = [  self.adaptive_constant_bcs1 , self.adaptive_constant_bcs2 , self.adaptive_constant_u_ics]#, self.adaptive_constant_ut_ics] \n",
        "        self.lambda_list_history = dict((loss, lambda1 ) for loss , lambda1 in zip(self.loss_list ,  self.lambda_list))\n",
        "\n",
        "         # Initialize Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    # Initialize network weights and biases using Xavier initialization\n",
        "    def initialize_NN(self, layers):\n",
        "        # Xavier initialization\n",
        "        def xavier_init(size):\n",
        "            in_dim = size[0]\n",
        "            out_dim = size[1]\n",
        "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "            return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,  dtype=tf.float32)\n",
        "\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = xavier_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    # Evaluates the forward pass\n",
        "    def forward_pass(self, H, layers, weights, biases):\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        H = tf.add(tf.matmul(H, W), b)\n",
        "        return H\n",
        "       \n",
        "################################################################################################\n",
        "    # Forward pass for u\n",
        "    def net_u(self, t, x):\n",
        "        u = self.forward_pass(tf.concat([t, x], 1),  self.layers, self.weights, self.biases)\n",
        "        return u\n",
        "\n",
        "    # Forward pass for du/dt\n",
        "    def net_u_t(self, t, x):\n",
        "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
        "        return u_t\n",
        "\n",
        "    # Forward pass for the residual\n",
        "    def net_r(self, t, x):\n",
        "        u = self.net_u(t, x)\n",
        "        residual = self.operator(u, t, x, self.c, self.sigma_t,  self.sigma_x)\n",
        "        return residual\n",
        "\n",
        "    def fetch_minibatch(self, sampler, N):\n",
        "        X, Y = sampler.sample(N)\n",
        "        X = (X - self.mu_X) / self.sigma_X\n",
        "        return X, Y\n",
        "\n",
        "        # Trains the model by minimizing the MSE loss\n",
        "\n",
        "    def lambda_balance(self  , term  ):\n",
        "                m = len(self.loss_list)\n",
        "                num = np.exp(np.mean(self.loss_history[term]))#/(self.T * histoy_mean))\n",
        "                denum = 0 \n",
        "\n",
        "                for  key in self.loss_list:\n",
        "                    denum +=  np.exp(np.mean(self.loss_history[key]))\n",
        "                return  (num / denum)\n",
        "        \n",
        "    def trainmb(self, nIter=10000, batch_size=128 ):\n",
        "\n",
        "        \n",
        "        itValues = [1,100,1000,39999]\n",
        "        start_time = timeit.default_timer()\n",
        "        for it in range(1 , nIter):\n",
        "            # Fetch boundary mini-batches\n",
        "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size // 3)\n",
        "            X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], batch_size // 3)\n",
        "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size // 3)\n",
        "            \n",
        "            # Fetch residual mini-batch\n",
        "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
        "            # Define a dictionary for associating placeholders with data\n",
        "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
        "                       self.u_ics_tf: u_ics_batch,\n",
        "                       self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
        "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
        "                       self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
        "                       self.lam_u_tf: self.lam_u_val,\n",
        "                    #    self.lam_ut_tf: self.lam_ut_val,\n",
        "                       self.lam_bc1_tf: self.lam_bc1_val,\n",
        "                       self.lam_bc2_tf: self.lam_bc2_val\n",
        "                       }\n",
        "\n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "            _, batch_losses = self.sess.run([self.train_op, self.loss_tensor_list] ,tf_dict)\n",
        "            [ loss ,  loss_res,  loss_bc1 , loss_bc2 , loss_ics_u]   = batch_losses\n",
        "\n",
        "            # for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
        "            #     self.epoch_loss[key] = loss_values\n",
        "\n",
        "            # for key in self.loss_list:\n",
        "            self.loss_history[\"loss_bc1\"].append(loss_bc1)\n",
        "            self.loss_history[\"loss_bc2\"].append(loss_bc2)\n",
        "            self.loss_history[\"loss_ics_u\"].append(loss_ics_u)\n",
        "            # self.loss_history[\"loss_ics_ut\"].append(loss_ics_ut)\n",
        "\n",
        "            # Print\n",
        "            if  it % 100 == 0:\n",
        "              \n",
        "                    elapsed = timeit.default_timer() - start_time\n",
        "                # loss ,  loss_res , loss_ics_ut  = self.sess.run([self.loss, self.loss_res ,  self.loss_ics_ut ] ,tf_dict)\n",
        "\n",
        "                    self.print('It: %d| Loss: %.3e|  loss_bc1: %.3e|  loss_bc2: %.3e| loss_ics_u: %.3e|   Loss_res: %.3e , Time: %.2f' %(it, loss, loss_bc1 , loss_bc2 , loss_ics_u , loss_res , elapsed))\n",
        "\n",
        "            if  it % 100 == 0:\n",
        "                    adaptive_constant_lambda_list =  dict((loss, []) for loss  in self.loss_list )\n",
        "                    total = 0.0\n",
        "                    for term in self.loss_list:\n",
        "                        update  = self.lambda_balance(term)\n",
        "                        self.print( \"lambda_balance \" , term , \": \" , update)\n",
        "                        # if  update > 0.1:\n",
        "                        adaptive_constant_lambda = self.sess.run( [self.lambda_list_history[term] ], tf_dict)\n",
        "                        adaptive_constant_lambda_list[term] = [update , adaptive_constant_lambda]\n",
        "                        total += np.array(adaptive_constant_lambda)*np.array(update)\n",
        "                    for term in self.loss_list:\n",
        "\n",
        "                        self.lambda_val_list[term] = (np.array(adaptive_constant_lambda_list[term][0])*adaptive_constant_lambda_list[term][1]) /total# ( 1.0 - 0.9)*  np.array( adaptive_constant_lambda) + 0.9 *self.lambda_val_list[term] #  #* ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
        "                        self.print('updating  lamda for loss ' , term , ':   ', self.lambda_val_list[term])\n",
        "\n",
        "\n",
        "                    # for term in self.loss_list:\n",
        "                    #         self.print('updating  lamda for loss ' , term , ':   ', self.lambda_val_list[term])\n",
        "\n",
        "\n",
        "                    self.lam_u_val  = np.array(self.lambda_val_list[\"loss_ics_u\"])\n",
        "                    # self.lam_ut_val  = np.array(self.lambda_val_list[\"loss_ics_ut\"])\n",
        "                    self.lam_bc1_val  = np.array(self.lambda_val_list[\"loss_bc1\"])\n",
        "                    self.lam_bc2_val =np.array(self.lambda_val_list[\"loss_bc2\"])\n",
        "            \n",
        "                    self.adaptive_constant_bcs1_log.append( self.lambda_val_list[\"loss_bc1\"] )\n",
        "                    self.adaptive_constant_bcs2_log.append( self.lambda_val_list[\"loss_bc2\"]) #\"loss_bc2\", \"loss_ics_u\" \n",
        "                    self.adaptive_constant_ics_u_log.append( self.lambda_val_list[\"loss_ics_u\"])\n",
        "                    # self.adaptive_constant_ics_ut_log.append( self.lambda_val_list[\"loss_ics_ut\"])\n",
        "\n",
        "            mean_grad_res , mean_grad_bcs1, mean_grad_bcs2, mean_grad_ics = self.sess.run( [self.mean_grad_res , self.mean_grad_bcs1 ,  self.mean_grad_bcs2 ,  self.mean_grad_ics_u ], tf_dict)\n",
        "\n",
        "            self.mean_grad_res_log.append( mean_grad_res)\n",
        "            self.mean_grad_bcs1_log.append( mean_grad_bcs1)\n",
        "            self.mean_grad_bcs2_log.append( mean_grad_bcs2)\n",
        "            self.mean_grad_ics_u_log.append( mean_grad_ics)\n",
        "            # self.mean_grad_ics_ut_log.append( mean_grad_ics_ut)\n",
        "\n",
        "            start_time =  timeit.default_timer()\n",
        "            if it in itValues:\n",
        "                    self.plot_layerLoss(tf_dict , it)\n",
        "                    self.print(\"Gradients information stored ...\")\n",
        "\n",
        "            sys.stdout.flush()\n",
        " \n",
        "    # Evaluates predictions at test points\n",
        "    def predict_u(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        return u_star\n",
        "\n",
        "        # Evaluates predictions at test points\n",
        "\n",
        "    def predict_r(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
        "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
        "        return r_star\n",
        "    \n",
        "   ###############################################################################################################################################\n",
        "   # \n",
        "   # ###############################################################################################################################################\n",
        "   # \n",
        "   # ###############################################################################################################################################\n",
        "   # \n",
        "   #  \n",
        "\n",
        "\n",
        "   #  \n",
        "    def plot_layerLoss(self , tf_dict , epoch):\n",
        "        ## Gradients #\n",
        "        num_layers = len(self.layers)\n",
        "        for i in range(num_layers - 1):\n",
        "            grad_res , grad_bc1, grad_bc2  , grad_ics  = self.sess.run([ self.grad_res[i],self.grad_bcs1[i],self.grad_bcs2[i],self.grad_ics_u[i]], feed_dict=tf_dict)\n",
        "\n",
        "            # save gradients of loss_r and loss_u\n",
        "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
        "            self.dict_gradients_bcs1_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
        "            self.dict_gradients_bcs2_layers['layer_' + str(i + 1)].append(grad_bc2.flatten())\n",
        "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics.flatten())\n",
        "\n",
        "        num_hidden_layers = num_layers -1\n",
        "        cnt = 1\n",
        "        fig = plt.figure(4, figsize=(13, 4))\n",
        "        for j in range(num_hidden_layers):\n",
        "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
        "            ax.set_title('Layer {}'.format(j + 1))\n",
        "            ax.set_yscale('symlog')\n",
        "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
        "            gradients_bc1 = self.dict_gradients_bcs1_layers['layer_' + str(j + 1)][-1]\n",
        "            gradients_bc2 = self.dict_gradients_bcs2_layers['layer_' + str(j + 1)][-1]\n",
        "            gradients_ics = self.dict_gradients_ics_layers['layer_' + str(j + 1)][-1]\n",
        "\n",
        "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
        "\n",
        "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
        "            sns.distplot(gradients_bc2, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc2}}$')\n",
        "            sns.distplot(gradients_ics, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{ics}}$')\n",
        "\n",
        "            #ax.get_legend().remove()\n",
        "            ax.set_xlim([-1.0, 1.0])\n",
        "            #ax.set_ylim([0, 150])\n",
        "            cnt += 1\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=4)\n",
        "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
        "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    # #########################\n",
        "    # def make_output_dir(self):\n",
        "        \n",
        "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
        "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
        "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "    #     os.mkdir(dirname)\n",
        "    #     text = 'output.log'\n",
        "    #     logpath = os.path.join(dirname, text)\n",
        "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
        "\n",
        "    #     return dirname, logpath\n",
        "    \n",
        "    # # ###########################################################\n",
        "    def make_output_dir(self):\n",
        "        \n",
        "        if not os.path.exists(\"checkpoints\"):\n",
        "            os.mkdir(\"checkpoints\")\n",
        "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "        os.mkdir(dirname)\n",
        "        text = 'output.log'\n",
        "        logpath = os.path.join(dirname, text)\n",
        "        shutil.copyfile('M5.ipynb', os.path.join(dirname, 'M5.ipynb'))\n",
        "        return dirname, logpath\n",
        "    \n",
        "\n",
        "    def get_logger(self, logpath):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "        sh = logging.StreamHandler()\n",
        "        sh.setLevel(logging.DEBUG)        \n",
        "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
        "        fh = logging.FileHandler(logpath)\n",
        "        logger.addHandler(sh)\n",
        "        logger.addHandler(fh)\n",
        "        return logger\n",
        "\n",
        "\n",
        "   \n",
        "    def print(self, *args):\n",
        "        for word in args:\n",
        "            if len(args) == 1:\n",
        "                self.logger.info(word)\n",
        "            elif word != args[-1]:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "            else:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\\n\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "\n",
        "\n",
        "    def plot_loss_history(self , path):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        for key in self.loss_history:\n",
        "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
        "            ax.semilogy(self.loss_history[key], label=key)\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        plt.savefig(path)\n",
        "        #plt.show()\n",
        "       #######################\n",
        "    def save_NN(self):\n",
        "\n",
        "        uv_weights = self.sess.run(self.weights)\n",
        "        uv_biases = self.sess.run(self.biases)\n",
        "\n",
        "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
        "            pickle.dump([uv_weights, uv_biases], f)\n",
        "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
        "\n",
        "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
        "        #     pickle.dump(self.loss_rec, f)\n",
        "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
        "            self.plot_loss_history(f)\n",
        "\n",
        "    def assign_batch_losses(self, batch_losses):\n",
        "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
        "            self.epoch_loss[key] = loss_values\n",
        "\n",
        "\n",
        "    def generate_grad_dict(self):\n",
        "        num = len(self.layers) - 1\n",
        "        grad_dict = {}\n",
        "        for i in range(num):\n",
        "            grad_dict['layer_{}'.format(i + 1)] = []\n",
        "        return grad_dict\n",
        "    \n",
        "    def assign_batch_losses(self, batch_losses):\n",
        "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
        "            self.epoch_loss[key] = loss_values\n",
        "\n",
        "#            mode.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
        "\n",
        "    def plt_prediction(self , t , x , X_star , u_star , u_pred , r_star , r_pred):\n",
        "        \n",
        "        U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
        "        r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
        "        U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
        "        R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(18, 9))\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.pcolor(t, x, U_star, cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$x_1$')\n",
        "        plt.ylabel('$x_2$')\n",
        "        plt.title('Exact u(t, x)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.pcolor(t, x, U_pred, cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Predicted u(t, x)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Absolute error')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 4)\n",
        "        plt.pcolor(t, x, r_star, cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Exact r(t, x)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.pcolor(t, x, R_pred, cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Predicted r(t, x)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Absolute error')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    def plot_lambda(self ):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        ax.semilogy(self.adaptive_constant_bcs1_log, label=\"adaptive_constant_bcs1_log\")\n",
        "        ax.semilogy(self.adaptive_constant_bcs2_log, label=\"adaptive_constant_bcs2_log\")\n",
        "        ax.semilogy(self.adaptive_constant_ics_u_log, label=\"adaptive_constant_ics_u_log\")\n",
        "        # ax.semilogy(self.adaptive_constant_ics_ut_log, label=\"adaptive_constant_ics_ut_log\")\n",
        "        ax.semilogy(self.adaptive_constant_res_log, label=\"adaptive_constant_res_log\")\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        path = os.path.join(self.dirname,'lambda_history.png')\n",
        "        plt.savefig(path)\n",
        "\n",
        "    def plot_grad(self ):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        ax.semilogy(self.mean_grad_bcs1_log, label=\"mean_grad_bcs1_log\")\n",
        "        ax.semilogy(self.mean_grad_bcs2_log, label=\"mean_grad_bcs2_log\")\n",
        "        ax.semilogy(self.mean_grad_res_log, label=\"mean_grad_res_log\")\n",
        "        ax.semilogy(self.mean_grad_ics_u_log, label=\"mean_grad_ics_u_log\")\n",
        "        # ax.semilogy(self.mean_grad_ics_ut_log, label=\"mean_grad_ics_ut_log\")\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        path = os.path.join(self.dirname,'grad_history.png')\n",
        "        plt.savefig(path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
        "def test_method(method , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "        # sess.run(init)\n",
        "\n",
        "        model = PINN(layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess)\n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "\n",
        "        if method ==\"full_batch\":\n",
        "            print(\"full_batch method is used\")\n",
        "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "        elif method ==\"mini_batch\":\n",
        "            print(\"mini_batch method is used\")\n",
        "            model.trainmb(nIter, mbbatch_size)\n",
        "        else:\n",
        "            print(\"unknown method!\")\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Predictions\n",
        "        u_pred = model.predict_u(X_star)\n",
        "        r_pred = model.predict_u(X_star)\n",
        "        # Predictions\n",
        "\n",
        "        sess.close()   \n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "    error_r = np.linalg.norm(r_star - r_pred, 2) #/ np.linalg.norm(u_star, 2)\n",
        "\n",
        "    print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "    print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "\n",
        "    return [elapsed, error_u , error_r , model]\n",
        "\n",
        "###############################################################################################################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method:  mini_batch\n",
            "Epoch:  1\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17385/2255654768.py:66: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17385/2255654768.py:67: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17385/2255654768.py:68: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17385/2255654768.py:68: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-19 19:16:24.697434: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-19 19:16:24.723143: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
            "2023-12-19 19:16:24.724068: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5578005922f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-12-19 19:16:24.724083: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-12-19 19:16:24.740594: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_17385/1708299940.py:47: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17385/1708299940.py:98: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17385/1708299940.py:100: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_17385/1708299940.py:168: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "mini_batch method is used\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 100| Loss: 8.048e-01|  loss_bc1: 5.448e-02|  loss_bc2: 3.291e-02| loss_ics_u: 3.130e-01|   Loss_res: 3.978e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 2.9708e-01\n",
            "lambda_balance loss_bc2: 2.9089e-01\n",
            "lambda_balance loss_ics_u: 4.1203e-01\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:293: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:435: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "updating  lamda for loss loss_bc1:   [0.3142174]\n",
            "updating  lamda for loss loss_bc2:   [0.48653755]\n",
            "updating  lamda for loss loss_ics_u:   [0.19924505]\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 200| Loss: 8.341e-02|  loss_bc1: 2.679e-02|  loss_bc2: 1.649e-02| loss_ics_u: 3.273e-01|   Loss_res: 1.764e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 2.9727e-01\n",
            "lambda_balance loss_bc2: 2.9231e-01\n",
            "lambda_balance loss_ics_u: 4.1042e-01\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:293: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:435: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "updating  lamda for loss loss_bc1:   [0.37532708]\n",
            "updating  lamda for loss loss_bc2:   [0.4235689]\n",
            "updating  lamda for loss loss_ics_u:   [0.201104]\n",
            "It: 300| Loss: 8.085e-02|  loss_bc1: 3.107e-02|  loss_bc2: 1.505e-02| loss_ics_u: 3.038e-01|   Loss_res: 1.717e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 2.9746e-01\n",
            "lambda_balance loss_bc2: 2.9319e-01\n",
            "lambda_balance loss_ics_u: 4.0936e-01\n",
            "updating  lamda for loss loss_bc1:   [0.32140642]\n",
            "updating  lamda for loss loss_bc2:   [0.484795]\n",
            "updating  lamda for loss loss_ics_u:   [0.19379859]\n",
            "It: 400| Loss: 8.163e-02|  loss_bc1: 3.229e-02|  loss_bc2: 1.480e-02| loss_ics_u: 3.238e-01|   Loss_res: 1.336e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 2.9822e-01\n",
            "lambda_balance loss_bc2: 2.9383e-01\n",
            "lambda_balance loss_ics_u: 4.0795e-01\n",
            "updating  lamda for loss loss_bc1:   [0.33896205]\n",
            "updating  lamda for loss loss_bc2:   [0.4437212]\n",
            "updating  lamda for loss loss_ics_u:   [0.21731675]\n",
            "It: 500| Loss: 9.223e-02|  loss_bc1: 5.069e-02|  loss_bc2: 1.754e-02| loss_ics_u: 3.038e-01|   Loss_res: 1.244e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 2.9944e-01\n",
            "lambda_balance loss_bc2: 2.9481e-01\n",
            "lambda_balance loss_ics_u: 4.0576e-01\n",
            "updating  lamda for loss loss_bc1:   [0.28553092]\n",
            "updating  lamda for loss loss_bc2:   [0.5059153]\n",
            "updating  lamda for loss loss_ics_u:   [0.20855382]\n",
            "It: 600| Loss: 6.583e-02|  loss_bc1: 4.602e-02|  loss_bc2: 8.305e-03| loss_ics_u: 2.225e-01|   Loss_res: 2.097e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.0095e-01\n",
            "lambda_balance loss_bc2: 2.9569e-01\n",
            "lambda_balance loss_ics_u: 4.0336e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22450198]\n",
            "updating  lamda for loss loss_bc2:   [0.5218407]\n",
            "updating  lamda for loss loss_ics_u:   [0.2536573]\n",
            "It: 700| Loss: 7.604e-02|  loss_bc1: 5.112e-02|  loss_bc2: 8.629e-03| loss_ics_u: 2.341e-01|   Loss_res: 6.786e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.0319e-01\n",
            "lambda_balance loss_bc2: 2.9679e-01\n",
            "lambda_balance loss_ics_u: 4.0002e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17312725]\n",
            "updating  lamda for loss loss_bc2:   [0.36807188]\n",
            "updating  lamda for loss loss_ics_u:   [0.4588009]\n",
            "It: 800| Loss: 1.280e-01|  loss_bc1: 8.462e-02|  loss_bc2: 1.612e-02| loss_ics_u: 1.926e-01|   Loss_res: 1.908e-02 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.0625e-01\n",
            "lambda_balance loss_bc2: 2.9798e-01\n",
            "lambda_balance loss_ics_u: 3.9577e-01\n",
            "updating  lamda for loss loss_bc1:   [0.23821044]\n",
            "updating  lamda for loss loss_bc2:   [0.42997077]\n",
            "updating  lamda for loss loss_ics_u:   [0.33181873]\n",
            "It: 900| Loss: 8.640e-02|  loss_bc1: 4.689e-02|  loss_bc2: 6.735e-03| loss_ics_u: 2.155e-01|   Loss_res: 8.313e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.0792e-01\n",
            "lambda_balance loss_bc2: 2.9901e-01\n",
            "lambda_balance loss_ics_u: 3.9307e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17895961]\n",
            "updating  lamda for loss loss_bc2:   [0.4603832]\n",
            "updating  lamda for loss loss_ics_u:   [0.36065722]\n",
            "It: 1000| Loss: 1.022e-01|  loss_bc1: 6.140e-02|  loss_bc2: 5.246e-03| loss_ics_u: 1.879e-01|   Loss_res: 2.107e-02 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.0960e-01\n",
            "lambda_balance loss_bc2: 3.0010e-01\n",
            "lambda_balance loss_ics_u: 3.9030e-01\n",
            "updating  lamda for loss loss_bc1:   [0.13445963]\n",
            "updating  lamda for loss loss_bc2:   [0.49949566]\n",
            "updating  lamda for loss loss_ics_u:   [0.36604476]\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 1100| Loss: 1.263e-01|  loss_bc1: 5.278e-02|  loss_bc2: 5.450e-03| loss_ics_u: 1.712e-01|   Loss_res: 5.383e-02 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1124e-01\n",
            "lambda_balance loss_bc2: 3.0099e-01\n",
            "lambda_balance loss_ics_u: 3.8776e-01\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:293: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:435: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "updating  lamda for loss loss_bc1:   [0.14285955]\n",
            "updating  lamda for loss loss_bc2:   [0.54416275]\n",
            "updating  lamda for loss loss_ics_u:   [0.31297773]\n",
            "It: 1200| Loss: 5.726e-02|  loss_bc1: 5.027e-02|  loss_bc2: 6.336e-03| loss_ics_u: 1.447e-01|   Loss_res: 1.335e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1259e-01\n",
            "lambda_balance loss_bc2: 3.0183e-01\n",
            "lambda_balance loss_ics_u: 3.8558e-01\n",
            "updating  lamda for loss loss_bc1:   [0.12717912]\n",
            "updating  lamda for loss loss_bc2:   [0.80865294]\n",
            "updating  lamda for loss loss_ics_u:   [0.06416793]\n",
            "It: 1300| Loss: 1.966e-02|  loss_bc1: 3.153e-02|  loss_bc2: 3.970e-03| loss_ics_u: 1.886e-01|   Loss_res: 3.322e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1328e-01\n",
            "lambda_balance loss_bc2: 3.0252e-01\n",
            "lambda_balance loss_ics_u: 3.8421e-01\n",
            "updating  lamda for loss loss_bc1:   [0.10867052]\n",
            "updating  lamda for loss loss_bc2:   [0.84381175]\n",
            "updating  lamda for loss loss_ics_u:   [0.0475177]\n",
            "It: 1400| Loss: 1.542e-02|  loss_bc1: 3.060e-02|  loss_bc2: 2.094e-03| loss_ics_u: 2.128e-01|   Loss_res: 2.206e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1364e-01\n",
            "lambda_balance loss_bc2: 3.0296e-01\n",
            "lambda_balance loss_ics_u: 3.8340e-01\n",
            "updating  lamda for loss loss_bc1:   [0.13734913]\n",
            "updating  lamda for loss loss_bc2:   [0.8147362]\n",
            "updating  lamda for loss loss_ics_u:   [0.04791468]\n",
            "It: 1500| Loss: 1.376e-02|  loss_bc1: 2.369e-02|  loss_bc2: 2.183e-03| loss_ics_u: 1.756e-01|   Loss_res: 3.132e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1376e-01\n",
            "lambda_balance loss_bc2: 3.0325e-01\n",
            "lambda_balance loss_ics_u: 3.8300e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2627527]\n",
            "updating  lamda for loss loss_bc2:   [0.6710473]\n",
            "updating  lamda for loss loss_ics_u:   [0.06619994]\n",
            "It: 1600| Loss: 2.056e-02|  loss_bc1: 1.996e-02|  loss_bc2: 3.043e-03| loss_ics_u: 1.953e-01|   Loss_res: 3.421e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1369e-01\n",
            "lambda_balance loss_bc2: 3.0349e-01\n",
            "lambda_balance loss_ics_u: 3.8282e-01\n",
            "updating  lamda for loss loss_bc1:   [0.25200617]\n",
            "updating  lamda for loss loss_bc2:   [0.64371955]\n",
            "updating  lamda for loss loss_ics_u:   [0.1042743]\n",
            "It: 1700| Loss: 2.726e-02|  loss_bc1: 1.989e-02|  loss_bc2: 3.497e-03| loss_ics_u: 1.856e-01|   Loss_res: 6.389e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1376e-01\n",
            "lambda_balance loss_bc2: 3.0378e-01\n",
            "lambda_balance loss_ics_u: 3.8246e-01\n",
            "updating  lamda for loss loss_bc1:   [0.26483417]\n",
            "updating  lamda for loss loss_bc2:   [0.6031517]\n",
            "updating  lamda for loss loss_ics_u:   [0.13201413]\n",
            "It: 1800| Loss: 5.403e-02|  loss_bc1: 2.603e-02|  loss_bc2: 7.180e-03| loss_ics_u: 1.865e-01|   Loss_res: 1.819e-02 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1396e-01\n",
            "lambda_balance loss_bc2: 3.0414e-01\n",
            "lambda_balance loss_ics_u: 3.8189e-01\n",
            "updating  lamda for loss loss_bc1:   [0.18154801]\n",
            "updating  lamda for loss loss_bc2:   [0.6638963]\n",
            "updating  lamda for loss loss_ics_u:   [0.15455571]\n",
            "It: 1900| Loss: 4.053e-02|  loss_bc1: 3.447e-02|  loss_bc2: 4.330e-03| loss_ics_u: 1.870e-01|   Loss_res: 2.495e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1436e-01\n",
            "lambda_balance loss_bc2: 3.0453e-01\n",
            "lambda_balance loss_ics_u: 3.8111e-01\n",
            "updating  lamda for loss loss_bc1:   [0.18855672]\n",
            "updating  lamda for loss loss_bc2:   [0.6987974]\n",
            "updating  lamda for loss loss_ics_u:   [0.11264587]\n",
            "It: 2000| Loss: 2.788e-02|  loss_bc1: 2.708e-02|  loss_bc2: 2.822e-03| loss_ics_u: 1.709e-01|   Loss_res: 1.556e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1464e-01\n",
            "lambda_balance loss_bc2: 3.0485e-01\n",
            "lambda_balance loss_ics_u: 3.8051e-01\n",
            "updating  lamda for loss loss_bc1:   [0.16121697]\n",
            "updating  lamda for loss loss_bc2:   [0.7785603]\n",
            "updating  lamda for loss loss_ics_u:   [0.06022271]\n",
            "It: 2100| Loss: 1.735e-02|  loss_bc1: 2.627e-02|  loss_bc2: 3.136e-03| loss_ics_u: 1.728e-01|   Loss_res: 2.676e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1474e-01\n",
            "lambda_balance loss_bc2: 3.0507e-01\n",
            "lambda_balance loss_ics_u: 3.8018e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2739859]\n",
            "updating  lamda for loss loss_bc2:   [0.62970763]\n",
            "updating  lamda for loss loss_ics_u:   [0.09630641]\n",
            "It: 2200| Loss: 2.793e-02|  loss_bc1: 2.556e-02|  loss_bc2: 2.795e-03| loss_ics_u: 1.800e-01|   Loss_res: 1.832e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1480e-01\n",
            "lambda_balance loss_bc2: 3.0529e-01\n",
            "lambda_balance loss_ics_u: 3.7991e-01\n",
            "updating  lamda for loss loss_bc1:   [0.26660407]\n",
            "updating  lamda for loss loss_bc2:   [0.6206065]\n",
            "updating  lamda for loss loss_ics_u:   [0.11278948]\n",
            "It: 2300| Loss: 3.089e-02|  loss_bc1: 2.651e-02|  loss_bc2: 5.785e-03| loss_ics_u: 1.600e-01|   Loss_res: 2.183e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1491e-01\n",
            "lambda_balance loss_bc2: 3.0554e-01\n",
            "lambda_balance loss_ics_u: 3.7954e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17708656]\n",
            "updating  lamda for loss loss_bc2:   [0.6490075]\n",
            "updating  lamda for loss loss_ics_u:   [0.1739059]\n",
            "It: 2400| Loss: 5.049e-02|  loss_bc1: 4.529e-02|  loss_bc2: 4.962e-03| loss_ics_u: 1.782e-01|   Loss_res: 8.268e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1522e-01\n",
            "lambda_balance loss_bc2: 3.0581e-01\n",
            "lambda_balance loss_ics_u: 3.7897e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17242497]\n",
            "updating  lamda for loss loss_bc2:   [0.7694803]\n",
            "updating  lamda for loss loss_ics_u:   [0.05809475]\n",
            "It: 2500| Loss: 1.709e-02|  loss_bc1: 2.192e-02|  loss_bc2: 3.438e-03| loss_ics_u: 1.801e-01|   Loss_res: 1.995e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1523e-01\n",
            "lambda_balance loss_bc2: 3.0594e-01\n",
            "lambda_balance loss_ics_u: 3.7882e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3226997]\n",
            "updating  lamda for loss loss_bc2:   [0.55587274]\n",
            "updating  lamda for loss loss_ics_u:   [0.12142754]\n",
            "It: 2600| Loss: 3.472e-02|  loss_bc1: 2.554e-02|  loss_bc2: 5.328e-03| loss_ics_u: 1.887e-01|   Loss_res: 6.075e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1529e-01\n",
            "lambda_balance loss_bc2: 3.0615e-01\n",
            "lambda_balance loss_ics_u: 3.7856e-01\n",
            "updating  lamda for loss loss_bc1:   [0.36147696]\n",
            "updating  lamda for loss loss_bc2:   [0.5605374]\n",
            "updating  lamda for loss loss_ics_u:   [0.07798557]\n",
            "It: 2700| Loss: 2.514e-02|  loss_bc1: 1.456e-02|  loss_bc2: 5.621e-03| loss_ics_u: 2.110e-01|   Loss_res: 2.682e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1523e-01\n",
            "lambda_balance loss_bc2: 3.0629e-01\n",
            "lambda_balance loss_ics_u: 3.7848e-01\n",
            "updating  lamda for loss loss_bc1:   [0.44933146]\n",
            "updating  lamda for loss loss_bc2:   [0.4284114]\n",
            "updating  lamda for loss loss_ics_u:   [0.1222572]\n",
            "It: 2800| Loss: 2.856e-02|  loss_bc1: 1.161e-02|  loss_bc2: 6.781e-03| loss_ics_u: 1.477e-01|   Loss_res: 2.377e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1520e-01\n",
            "lambda_balance loss_bc2: 3.0648e-01\n",
            "lambda_balance loss_ics_u: 3.7832e-01\n",
            "updating  lamda for loss loss_bc1:   [0.30255416]\n",
            "updating  lamda for loss loss_bc2:   [0.49190512]\n",
            "updating  lamda for loss loss_ics_u:   [0.20554072]\n",
            "It: 2900| Loss: 5.141e-02|  loss_bc1: 3.668e-02|  loss_bc2: 1.189e-02| loss_ics_u: 1.509e-01|   Loss_res: 3.444e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1536e-01\n",
            "lambda_balance loss_bc2: 3.0673e-01\n",
            "lambda_balance loss_ics_u: 3.7791e-01\n",
            "updating  lamda for loss loss_bc1:   [0.25126028]\n",
            "updating  lamda for loss loss_bc2:   [0.26871535]\n",
            "updating  lamda for loss loss_ics_u:   [0.4800244]\n",
            "It: 3000| Loss: 9.818e-02|  loss_bc1: 4.383e-02|  loss_bc2: 9.348e-03| loss_ics_u: 1.751e-01|   Loss_res: 6.120e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1525e-01\n",
            "lambda_balance loss_bc2: 3.0663e-01\n",
            "lambda_balance loss_ics_u: 3.7813e-01\n",
            "updating  lamda for loss loss_bc1:   [0.19101678]\n",
            "updating  lamda for loss loss_bc2:   [0.46201074]\n",
            "updating  lamda for loss loss_ics_u:   [0.3469725]\n",
            "It: 3100| Loss: 7.725e-02|  loss_bc1: 3.514e-02|  loss_bc2: 6.594e-03| loss_ics_u: 1.726e-01|   Loss_res: 7.592e-03 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1556e-01\n",
            "lambda_balance loss_bc2: 3.0687e-01\n",
            "lambda_balance loss_ics_u: 3.7756e-01\n",
            "updating  lamda for loss loss_bc1:   [0.18970339]\n",
            "updating  lamda for loss loss_bc2:   [0.614573]\n",
            "updating  lamda for loss loss_ics_u:   [0.19572362]\n",
            "It: 3200| Loss: 3.816e-02|  loss_bc1: 3.399e-02|  loss_bc2: 3.811e-03| loss_ics_u: 1.480e-01|   Loss_res: 3.932e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1583e-01\n",
            "lambda_balance loss_bc2: 3.0711e-01\n",
            "lambda_balance loss_ics_u: 3.7706e-01\n",
            "updating  lamda for loss loss_bc1:   [0.19132103]\n",
            "updating  lamda for loss loss_bc2:   [0.6849971]\n",
            "updating  lamda for loss loss_ics_u:   [0.12368184]\n",
            "It: 3300| Loss: 3.436e-02|  loss_bc1: 3.310e-02|  loss_bc2: 3.930e-03| loss_ics_u: 1.918e-01|   Loss_res: 1.612e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1603e-01\n",
            "lambda_balance loss_bc2: 3.0732e-01\n",
            "lambda_balance loss_ics_u: 3.7665e-01\n",
            "updating  lamda for loss loss_bc1:   [0.13350986]\n",
            "updating  lamda for loss loss_bc2:   [0.79133224]\n",
            "updating  lamda for loss loss_ics_u:   [0.07515792]\n",
            "It: 3400| Loss: 2.059e-02|  loss_bc1: 3.627e-02|  loss_bc2: 2.795e-03| loss_ics_u: 1.454e-01|   Loss_res: 2.602e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1622e-01\n",
            "lambda_balance loss_bc2: 3.0749e-01\n",
            "lambda_balance loss_ics_u: 3.7628e-01\n",
            "updating  lamda for loss loss_bc1:   [0.10335287]\n",
            "updating  lamda for loss loss_bc2:   [0.82081634]\n",
            "updating  lamda for loss loss_ics_u:   [0.0758308]\n",
            "It: 3500| Loss: 1.756e-02|  loss_bc1: 3.460e-02|  loss_bc2: 2.578e-03| loss_ics_u: 1.300e-01|   Loss_res: 2.013e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1641e-01\n",
            "lambda_balance loss_bc2: 3.0764e-01\n",
            "lambda_balance loss_ics_u: 3.7595e-01\n",
            "updating  lamda for loss loss_bc1:   [0.33723098]\n",
            "updating  lamda for loss loss_bc2:   [0.4477161]\n",
            "updating  lamda for loss loss_ics_u:   [0.215053]\n",
            "It: 3600| Loss: 5.660e-02|  loss_bc1: 2.640e-02|  loss_bc2: 5.665e-03| loss_ics_u: 1.562e-01|   Loss_res: 1.157e-02 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1655e-01\n",
            "lambda_balance loss_bc2: 3.0782e-01\n",
            "lambda_balance loss_ics_u: 3.7563e-01\n",
            "updating  lamda for loss loss_bc1:   [0.19952542]\n",
            "updating  lamda for loss loss_bc2:   [0.6650516]\n",
            "updating  lamda for loss loss_ics_u:   [0.13542305]\n",
            "It: 3700| Loss: 2.743e-02|  loss_bc1: 3.044e-02|  loss_bc2: 3.874e-03| loss_ics_u: 1.318e-01|   Loss_res: 9.267e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1671e-01\n",
            "lambda_balance loss_bc2: 3.0800e-01\n",
            "lambda_balance loss_ics_u: 3.7529e-01\n",
            "updating  lamda for loss loss_bc1:   [0.21797808]\n",
            "updating  lamda for loss loss_bc2:   [0.6579229]\n",
            "updating  lamda for loss loss_ics_u:   [0.12409898]\n",
            "It: 3800| Loss: 3.137e-02|  loss_bc1: 2.985e-02|  loss_bc2: 5.523e-03| loss_ics_u: 1.553e-01|   Loss_res: 1.948e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1684e-01\n",
            "lambda_balance loss_bc2: 3.0816e-01\n",
            "lambda_balance loss_ics_u: 3.7500e-01\n",
            "updating  lamda for loss loss_bc1:   [0.1773327]\n",
            "updating  lamda for loss loss_bc2:   [0.7519139]\n",
            "updating  lamda for loss loss_ics_u:   [0.07075337]\n",
            "It: 3900| Loss: 1.732e-02|  loss_bc1: 2.643e-02|  loss_bc2: 2.973e-03| loss_ics_u: 1.427e-01|   Loss_res: 2.982e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1692e-01\n",
            "lambda_balance loss_bc2: 3.0828e-01\n",
            "lambda_balance loss_ics_u: 3.7480e-01\n",
            "updating  lamda for loss loss_bc1:   [0.1135742]\n",
            "updating  lamda for loss loss_bc2:   [0.83611995]\n",
            "updating  lamda for loss loss_ics_u:   [0.05030583]\n",
            "It: 4000| Loss: 1.552e-02|  loss_bc1: 2.467e-02|  loss_bc2: 1.685e-03| loss_ics_u: 1.300e-01|   Loss_res: 4.772e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1701e-01\n",
            "lambda_balance loss_bc2: 3.0839e-01\n",
            "lambda_balance loss_ics_u: 3.7461e-01\n",
            "updating  lamda for loss loss_bc1:   [0.21299852]\n",
            "updating  lamda for loss loss_bc2:   [0.6877905]\n",
            "updating  lamda for loss loss_ics_u:   [0.09921091]\n",
            "It: 4100| Loss: 2.549e-02|  loss_bc1: 2.820e-02|  loss_bc2: 5.096e-03| loss_ics_u: 1.493e-01|   Loss_res: 1.165e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1707e-01\n",
            "lambda_balance loss_bc2: 3.0848e-01\n",
            "lambda_balance loss_ics_u: 3.7444e-01\n",
            "updating  lamda for loss loss_bc1:   [0.30214345]\n",
            "updating  lamda for loss loss_bc2:   [0.57009244]\n",
            "updating  lamda for loss loss_ics_u:   [0.12776417]\n",
            "It: 4200| Loss: 4.716e-02|  loss_bc1: 2.522e-02|  loss_bc2: 6.460e-03| loss_ics_u: 1.688e-01|   Loss_res: 1.429e-02 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1714e-01\n",
            "lambda_balance loss_bc2: 3.0860e-01\n",
            "lambda_balance loss_ics_u: 3.7426e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4090873]\n",
            "updating  lamda for loss loss_bc2:   [0.44359967]\n",
            "updating  lamda for loss loss_ics_u:   [0.14731306]\n",
            "It: 4300| Loss: 4.041e-02|  loss_bc1: 1.973e-02|  loss_bc2: 5.327e-03| loss_ics_u: 1.772e-01|   Loss_res: 3.875e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1716e-01\n",
            "lambda_balance loss_bc2: 3.0873e-01\n",
            "lambda_balance loss_ics_u: 3.7411e-01\n",
            "updating  lamda for loss loss_bc1:   [0.36821297]\n",
            "updating  lamda for loss loss_bc2:   [0.5157168]\n",
            "updating  lamda for loss loss_ics_u:   [0.11607027]\n",
            "It: 4400| Loss: 3.395e-02|  loss_bc1: 1.808e-02|  loss_bc2: 4.600e-03| loss_ics_u: 1.776e-01|   Loss_res: 4.310e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1717e-01\n",
            "lambda_balance loss_bc2: 3.0884e-01\n",
            "lambda_balance loss_ics_u: 3.7400e-01\n",
            "updating  lamda for loss loss_bc1:   [0.25680208]\n",
            "updating  lamda for loss loss_bc2:   [0.6604551]\n",
            "updating  lamda for loss loss_ics_u:   [0.08274285]\n",
            "It: 4500| Loss: 2.629e-02|  loss_bc1: 1.960e-02|  loss_bc2: 4.228e-03| loss_ics_u: 2.191e-01|   Loss_res: 3.298e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1715e-01\n",
            "lambda_balance loss_bc2: 3.0891e-01\n",
            "lambda_balance loss_ics_u: 3.7395e-01\n",
            "updating  lamda for loss loss_bc1:   [0.37933874]\n",
            "updating  lamda for loss loss_bc2:   [0.49911183]\n",
            "updating  lamda for loss loss_ics_u:   [0.12154943]\n",
            "It: 4600| Loss: 3.366e-02|  loss_bc1: 1.395e-02|  loss_bc2: 6.799e-03| loss_ics_u: 1.932e-01|   Loss_res: 1.492e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1715e-01\n",
            "lambda_balance loss_bc2: 3.0900e-01\n",
            "lambda_balance loss_ics_u: 3.7385e-01\n",
            "updating  lamda for loss loss_bc1:   [0.39777955]\n",
            "updating  lamda for loss loss_bc2:   [0.5073886]\n",
            "updating  lamda for loss loss_ics_u:   [0.09483194]\n",
            "It: 4700| Loss: 2.414e-02|  loss_bc1: 1.549e-02|  loss_bc2: 5.744e-03| loss_ics_u: 1.536e-01|   Loss_res: 4.893e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1712e-01\n",
            "lambda_balance loss_bc2: 3.0909e-01\n",
            "lambda_balance loss_ics_u: 3.7379e-01\n",
            "updating  lamda for loss loss_bc1:   [0.35686797]\n",
            "updating  lamda for loss loss_bc2:   [0.4813537]\n",
            "updating  lamda for loss loss_ics_u:   [0.1617783]\n",
            "It: 4800| Loss: 3.372e-02|  loss_bc1: 1.852e-02|  loss_bc2: 5.958e-03| loss_ics_u: 1.422e-01|   Loss_res: 1.249e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1715e-01\n",
            "lambda_balance loss_bc2: 3.0920e-01\n",
            "lambda_balance loss_ics_u: 3.7365e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22199634]\n",
            "updating  lamda for loss loss_bc2:   [0.6615221]\n",
            "updating  lamda for loss loss_ics_u:   [0.11648157]\n",
            "It: 4900| Loss: 3.101e-02|  loss_bc1: 2.181e-02|  loss_bc2: 4.199e-03| loss_ics_u: 1.981e-01|   Loss_res: 3.097e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1720e-01\n",
            "lambda_balance loss_bc2: 3.0930e-01\n",
            "lambda_balance loss_ics_u: 3.7349e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17869991]\n",
            "updating  lamda for loss loss_bc2:   [0.75246954]\n",
            "updating  lamda for loss loss_ics_u:   [0.06883063]\n",
            "It: 5000| Loss: 1.850e-02|  loss_bc1: 1.747e-02|  loss_bc2: 3.016e-03| loss_ics_u: 1.823e-01|   Loss_res: 5.669e-04 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1724e-01\n",
            "lambda_balance loss_bc2: 3.0939e-01\n",
            "lambda_balance loss_ics_u: 3.7337e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14078481]\n",
            "updating  lamda for loss loss_bc2:   [0.822287]\n",
            "updating  lamda for loss loss_ics_u:   [0.03692819]\n",
            "It: 5100| Loss: 1.427e-02|  loss_bc1: 1.807e-02|  loss_bc2: 3.832e-03| loss_ics_u: 2.068e-01|   Loss_res: 9.386e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1722e-01\n",
            "lambda_balance loss_bc2: 3.0942e-01\n",
            "lambda_balance loss_ics_u: 3.7335e-01\n",
            "updating  lamda for loss loss_bc1:   [0.35262212]\n",
            "updating  lamda for loss loss_bc2:   [0.57045114]\n",
            "updating  lamda for loss loss_ics_u:   [0.07692675]\n",
            "It: 5200| Loss: 2.200e-02|  loss_bc1: 1.354e-02|  loss_bc2: 3.555e-03| loss_ics_u: 1.667e-01|   Loss_res: 2.372e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1716e-01\n",
            "lambda_balance loss_bc2: 3.0945e-01\n",
            "lambda_balance loss_ics_u: 3.7339e-01\n",
            "updating  lamda for loss loss_bc1:   [0.45715725]\n",
            "updating  lamda for loss loss_bc2:   [0.43466052]\n",
            "updating  lamda for loss loss_ics_u:   [0.10818217]\n",
            "It: 5300| Loss: 2.748e-02|  loss_bc1: 1.180e-02|  loss_bc2: 5.631e-03| loss_ics_u: 1.540e-01|   Loss_res: 2.980e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1712e-01\n",
            "lambda_balance loss_bc2: 3.0950e-01\n",
            "lambda_balance loss_ics_u: 3.7338e-01\n",
            "updating  lamda for loss loss_bc1:   [0.38454598]\n",
            "updating  lamda for loss loss_bc2:   [0.5448853]\n",
            "updating  lamda for loss loss_ics_u:   [0.07056879]\n",
            "It: 5400| Loss: 2.190e-02|  loss_bc1: 1.190e-02|  loss_bc2: 3.779e-03| loss_ics_u: 1.925e-01|   Loss_res: 1.678e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1706e-01\n",
            "lambda_balance loss_bc2: 3.0954e-01\n",
            "lambda_balance loss_ics_u: 3.7339e-01\n",
            "updating  lamda for loss loss_bc1:   [0.68795824]\n",
            "updating  lamda for loss loss_bc2:   [0.26415613]\n",
            "updating  lamda for loss loss_ics_u:   [0.04788566]\n",
            "It: 5500| Loss: 1.769e-02|  loss_bc1: 5.404e-03|  loss_bc2: 8.714e-03| loss_ics_u: 2.151e-01|   Loss_res: 1.371e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1692e-01\n",
            "lambda_balance loss_bc2: 3.0954e-01\n",
            "lambda_balance loss_ics_u: 3.7354e-01\n",
            "updating  lamda for loss loss_bc1:   [0.66285336]\n",
            "updating  lamda for loss loss_bc2:   [0.29183197]\n",
            "updating  lamda for loss loss_ics_u:   [0.04531467]\n",
            "It: 5600| Loss: 1.809e-02|  loss_bc1: 4.684e-03|  loss_bc2: 5.635e-03| loss_ics_u: 2.750e-01|   Loss_res: 8.848e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1676e-01\n",
            "lambda_balance loss_bc2: 3.0951e-01\n",
            "lambda_balance loss_ics_u: 3.7373e-01\n",
            "updating  lamda for loss loss_bc1:   [0.69281936]\n",
            "updating  lamda for loss loss_bc2:   [0.27825272]\n",
            "updating  lamda for loss loss_ics_u:   [0.02892797]\n",
            "It: 5700| Loss: 1.095e-02|  loss_bc1: 2.904e-03|  loss_bc2: 4.577e-03| loss_ics_u: 2.570e-01|   Loss_res: 2.278e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1654e-01\n",
            "lambda_balance loss_bc2: 3.0944e-01\n",
            "lambda_balance loss_ics_u: 3.7402e-01\n",
            "updating  lamda for loss loss_bc1:   [0.52647823]\n",
            "updating  lamda for loss loss_bc2:   [0.4315005]\n",
            "updating  lamda for loss loss_ics_u:   [0.04202126]\n",
            "It: 5800| Loss: 1.433e-02|  loss_bc1: 4.947e-03|  loss_bc2: 3.985e-03| loss_ics_u: 2.086e-01|   Loss_res: 1.242e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1635e-01\n",
            "lambda_balance loss_bc2: 3.0937e-01\n",
            "lambda_balance loss_ics_u: 3.7427e-01\n",
            "updating  lamda for loss loss_bc1:   [0.48941654]\n",
            "updating  lamda for loss loss_bc2:   [0.4704081]\n",
            "updating  lamda for loss loss_ics_u:   [0.04017536]\n",
            "It: 5900| Loss: 1.384e-02|  loss_bc1: 4.462e-03|  loss_bc2: 4.376e-03| loss_ics_u: 2.292e-01|   Loss_res: 3.916e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1617e-01\n",
            "lambda_balance loss_bc2: 3.0931e-01\n",
            "lambda_balance loss_ics_u: 3.7452e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3516576]\n",
            "updating  lamda for loss loss_bc2:   [0.61460656]\n",
            "updating  lamda for loss loss_ics_u:   [0.0337358]\n",
            "It: 6000| Loss: 1.327e-02|  loss_bc1: 4.757e-03|  loss_bc2: 2.738e-03| loss_ics_u: 2.908e-01|   Loss_res: 1.035e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1599e-01\n",
            "lambda_balance loss_bc2: 3.0923e-01\n",
            "lambda_balance loss_ics_u: 3.7478e-01\n",
            "updating  lamda for loss loss_bc1:   [0.26584637]\n",
            "updating  lamda for loss loss_bc2:   [0.7089684]\n",
            "updating  lamda for loss loss_ics_u:   [0.02518513]\n",
            "It: 6100| Loss: 9.003e-03|  loss_bc1: 5.399e-03|  loss_bc2: 1.608e-03| loss_ics_u: 2.314e-01|   Loss_res: 5.990e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1580e-01\n",
            "lambda_balance loss_bc2: 3.0914e-01\n",
            "lambda_balance loss_ics_u: 3.7506e-01\n",
            "updating  lamda for loss loss_bc1:   [0.41948587]\n",
            "updating  lamda for loss loss_bc2:   [0.51256293]\n",
            "updating  lamda for loss loss_ics_u:   [0.06795126]\n",
            "It: 6200| Loss: 1.995e-02|  loss_bc1: 8.859e-03|  loss_bc2: 4.525e-03| loss_ics_u: 1.882e-01|   Loss_res: 1.128e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1568e-01\n",
            "lambda_balance loss_bc2: 3.0911e-01\n",
            "lambda_balance loss_ics_u: 3.7521e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4453024]\n",
            "updating  lamda for loss loss_bc2:   [0.47973207]\n",
            "updating  lamda for loss loss_ics_u:   [0.07496554]\n",
            "It: 6300| Loss: 4.737e-02|  loss_bc1: 1.039e-02|  loss_bc2: 1.012e-02| loss_ics_u: 2.539e-01|   Loss_res: 1.885e-02 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1560e-01\n",
            "lambda_balance loss_bc2: 3.0912e-01\n",
            "lambda_balance loss_ics_u: 3.7529e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3584399]\n",
            "updating  lamda for loss loss_bc2:   [0.56710404]\n",
            "updating  lamda for loss loss_ics_u:   [0.07445613]\n",
            "It: 6400| Loss: 1.984e-02|  loss_bc1: 8.091e-03|  loss_bc2: 3.288e-03| loss_ics_u: 1.964e-01|   Loss_res: 4.533e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1554e-01\n",
            "lambda_balance loss_bc2: 3.0913e-01\n",
            "lambda_balance loss_ics_u: 3.7532e-01\n",
            "updating  lamda for loss loss_bc1:   [0.39993283]\n",
            "updating  lamda for loss loss_bc2:   [0.5269456]\n",
            "updating  lamda for loss loss_ics_u:   [0.07312158]\n",
            "It: 6500| Loss: 1.982e-02|  loss_bc1: 8.008e-03|  loss_bc2: 4.522e-03| loss_ics_u: 1.887e-01|   Loss_res: 4.333e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1549e-01\n",
            "lambda_balance loss_bc2: 3.0916e-01\n",
            "lambda_balance loss_ics_u: 3.7535e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4342341]\n",
            "updating  lamda for loss loss_bc2:   [0.4806783]\n",
            "updating  lamda for loss loss_ics_u:   [0.08508759]\n",
            "It: 6600| Loss: 2.707e-02|  loss_bc1: 1.005e-02|  loss_bc2: 5.164e-03| loss_ics_u: 1.914e-01|   Loss_res: 3.941e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1546e-01\n",
            "lambda_balance loss_bc2: 3.0920e-01\n",
            "lambda_balance loss_ics_u: 3.7534e-01\n",
            "updating  lamda for loss loss_bc1:   [0.48121044]\n",
            "updating  lamda for loss loss_bc2:   [0.38053256]\n",
            "updating  lamda for loss loss_ics_u:   [0.13825704]\n",
            "It: 6700| Loss: 3.115e-02|  loss_bc1: 1.061e-02|  loss_bc2: 7.584e-03| loss_ics_u: 1.327e-01|   Loss_res: 4.810e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1546e-01\n",
            "lambda_balance loss_bc2: 3.0927e-01\n",
            "lambda_balance loss_ics_u: 3.7528e-01\n",
            "updating  lamda for loss loss_bc1:   [0.29208556]\n",
            "updating  lamda for loss loss_bc2:   [0.640705]\n",
            "updating  lamda for loss loss_ics_u:   [0.06720942]\n",
            "It: 6800| Loss: 1.895e-02|  loss_bc1: 1.131e-02|  loss_bc2: 3.708e-03| loss_ics_u: 1.765e-01|   Loss_res: 1.400e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1543e-01\n",
            "lambda_balance loss_bc2: 3.0931e-01\n",
            "lambda_balance loss_ics_u: 3.7526e-01\n",
            "updating  lamda for loss loss_bc1:   [0.37009358]\n",
            "updating  lamda for loss loss_bc2:   [0.51634336]\n",
            "updating  lamda for loss loss_ics_u:   [0.11356307]\n",
            "It: 6900| Loss: 3.342e-02|  loss_bc1: 1.507e-02|  loss_bc2: 5.576e-03| loss_ics_u: 1.813e-01|   Loss_res: 4.370e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1544e-01\n",
            "lambda_balance loss_bc2: 3.0937e-01\n",
            "lambda_balance loss_ics_u: 3.7519e-01\n",
            "updating  lamda for loss loss_bc1:   [0.18494181]\n",
            "updating  lamda for loss loss_bc2:   [0.77102596]\n",
            "updating  lamda for loss loss_ics_u:   [0.04403226]\n",
            "It: 7000| Loss: 1.330e-02|  loss_bc1: 1.164e-02|  loss_bc2: 2.422e-03| loss_ics_u: 1.886e-01|   Loss_res: 9.722e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1544e-01\n",
            "lambda_balance loss_bc2: 3.0941e-01\n",
            "lambda_balance loss_ics_u: 3.7516e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3445573]\n",
            "updating  lamda for loss loss_bc2:   [0.55987066]\n",
            "updating  lamda for loss loss_ics_u:   [0.0955721]\n",
            "It: 7100| Loss: 5.146e-02|  loss_bc1: 1.375e-02|  loss_bc2: 4.455e-03| loss_ics_u: 1.955e-01|   Loss_res: 2.554e-02 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1543e-01\n",
            "lambda_balance loss_bc2: 3.0945e-01\n",
            "lambda_balance loss_ics_u: 3.7511e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4361236]\n",
            "updating  lamda for loss loss_bc2:   [0.4501482]\n",
            "updating  lamda for loss loss_ics_u:   [0.1137282]\n",
            "It: 7200| Loss: 2.822e-02|  loss_bc1: 1.084e-02|  loss_bc2: 5.527e-03| loss_ics_u: 1.795e-01|   Loss_res: 5.868e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1541e-01\n",
            "lambda_balance loss_bc2: 3.0949e-01\n",
            "lambda_balance loss_ics_u: 3.7509e-01\n",
            "updating  lamda for loss loss_bc1:   [0.44524238]\n",
            "updating  lamda for loss loss_bc2:   [0.45206916]\n",
            "updating  lamda for loss loss_ics_u:   [0.10268845]\n",
            "It: 7300| Loss: 3.292e-02|  loss_bc1: 9.522e-03|  loss_bc2: 4.627e-03| loss_ics_u: 2.499e-01|   Loss_res: 9.248e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1541e-01\n",
            "lambda_balance loss_bc2: 3.0955e-01\n",
            "lambda_balance loss_ics_u: 3.7504e-01\n",
            "updating  lamda for loss loss_bc1:   [0.28706434]\n",
            "updating  lamda for loss loss_bc2:   [0.6602453]\n",
            "updating  lamda for loss loss_ics_u:   [0.05269041]\n",
            "It: 7400| Loss: 1.324e-02|  loss_bc1: 1.275e-02|  loss_bc2: 2.486e-03| loss_ics_u: 1.471e-01|   Loss_res: 1.891e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1540e-01\n",
            "lambda_balance loss_bc2: 3.0959e-01\n",
            "lambda_balance loss_ics_u: 3.7501e-01\n",
            "updating  lamda for loss loss_bc1:   [0.51957923]\n",
            "updating  lamda for loss loss_bc2:   [0.40312353]\n",
            "updating  lamda for loss loss_ics_u:   [0.07729729]\n",
            "It: 7500| Loss: 2.133e-02|  loss_bc1: 8.570e-03|  loss_bc2: 4.856e-03| loss_ics_u: 1.745e-01|   Loss_res: 1.432e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1537e-01\n",
            "lambda_balance loss_bc2: 3.0962e-01\n",
            "lambda_balance loss_ics_u: 3.7502e-01\n",
            "updating  lamda for loss loss_bc1:   [0.35432488]\n",
            "updating  lamda for loss loss_bc2:   [0.5802464]\n",
            "updating  lamda for loss loss_ics_u:   [0.06542873]\n",
            "It: 7600| Loss: 2.589e-02|  loss_bc1: 1.004e-02|  loss_bc2: 3.817e-03| loss_ics_u: 2.304e-01|   Loss_res: 5.052e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1535e-01\n",
            "lambda_balance loss_bc2: 3.0965e-01\n",
            "lambda_balance loss_ics_u: 3.7500e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3163193]\n",
            "updating  lamda for loss loss_bc2:   [0.5910147]\n",
            "updating  lamda for loss loss_ics_u:   [0.09266598]\n",
            "It: 7700| Loss: 2.594e-02|  loss_bc1: 1.057e-02|  loss_bc2: 4.114e-03| loss_ics_u: 2.108e-01|   Loss_res: 6.298e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1535e-01\n",
            "lambda_balance loss_bc2: 3.0970e-01\n",
            "lambda_balance loss_ics_u: 3.7495e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3163174]\n",
            "updating  lamda for loss loss_bc2:   [0.6032904]\n",
            "updating  lamda for loss loss_ics_u:   [0.08039228]\n",
            "It: 7800| Loss: 1.990e-02|  loss_bc1: 1.280e-02|  loss_bc2: 3.352e-03| loss_ics_u: 1.659e-01|   Loss_res: 4.963e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1536e-01\n",
            "lambda_balance loss_bc2: 3.0974e-01\n",
            "lambda_balance loss_ics_u: 3.7490e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2965789]\n",
            "updating  lamda for loss loss_bc2:   [0.6193104]\n",
            "updating  lamda for loss loss_ics_u:   [0.08411067]\n",
            "It: 7900| Loss: 2.287e-02|  loss_bc1: 1.403e-02|  loss_bc2: 3.452e-03| loss_ics_u: 1.856e-01|   Loss_res: 9.648e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1537e-01\n",
            "lambda_balance loss_bc2: 3.0979e-01\n",
            "lambda_balance loss_ics_u: 3.7484e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22638875]\n",
            "updating  lamda for loss loss_bc2:   [0.700625]\n",
            "updating  lamda for loss loss_ics_u:   [0.07298625]\n",
            "It: 8000| Loss: 1.854e-02|  loss_bc1: 1.561e-02|  loss_bc2: 2.816e-03| loss_ics_u: 1.655e-01|   Loss_res: 9.598e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1539e-01\n",
            "lambda_balance loss_bc2: 3.0984e-01\n",
            "lambda_balance loss_ics_u: 3.7477e-01\n",
            "updating  lamda for loss loss_bc1:   [0.21383609]\n",
            "updating  lamda for loss loss_bc2:   [0.70120734]\n",
            "updating  lamda for loss loss_ics_u:   [0.08495661]\n",
            "It: 8100| Loss: 2.460e-02|  loss_bc1: 1.634e-02|  loss_bc2: 5.806e-03| loss_ics_u: 1.691e-01|   Loss_res: 2.666e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1542e-01\n",
            "lambda_balance loss_bc2: 3.0988e-01\n",
            "lambda_balance loss_ics_u: 3.7470e-01\n",
            "updating  lamda for loss loss_bc1:   [0.13232115]\n",
            "updating  lamda for loss loss_bc2:   [0.80181646]\n",
            "updating  lamda for loss loss_ics_u:   [0.0658624]\n",
            "It: 8200| Loss: 1.442e-02|  loss_bc1: 1.642e-02|  loss_bc2: 2.177e-03| loss_ics_u: 1.461e-01|   Loss_res: 8.805e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1546e-01\n",
            "lambda_balance loss_bc2: 3.0992e-01\n",
            "lambda_balance loss_ics_u: 3.7462e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14866233]\n",
            "updating  lamda for loss loss_bc2:   [0.7866812]\n",
            "updating  lamda for loss loss_ics_u:   [0.06465655]\n",
            "It: 8300| Loss: 1.689e-02|  loss_bc1: 1.693e-02|  loss_bc2: 2.467e-03| loss_ics_u: 1.610e-01|   Loss_res: 2.027e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1550e-01\n",
            "lambda_balance loss_bc2: 3.0997e-01\n",
            "lambda_balance loss_ics_u: 3.7453e-01\n",
            "updating  lamda for loss loss_bc1:   [0.1667663]\n",
            "updating  lamda for loss loss_bc2:   [0.75213414]\n",
            "updating  lamda for loss loss_ics_u:   [0.08109949]\n",
            "It: 8400| Loss: 2.195e-02|  loss_bc1: 2.499e-02|  loss_bc2: 4.724e-03| loss_ics_u: 1.723e-01|   Loss_res: 2.509e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1554e-01\n",
            "lambda_balance loss_bc2: 3.1001e-01\n",
            "lambda_balance loss_ics_u: 3.7445e-01\n",
            "updating  lamda for loss loss_bc1:   [0.31659836]\n",
            "updating  lamda for loss loss_bc2:   [0.54898953]\n",
            "updating  lamda for loss loss_ics_u:   [0.13441202]\n",
            "It: 8500| Loss: 2.918e-02|  loss_bc1: 1.564e-02|  loss_bc2: 2.747e-03| loss_ics_u: 1.507e-01|   Loss_res: 2.457e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1559e-01\n",
            "lambda_balance loss_bc2: 3.1007e-01\n",
            "lambda_balance loss_ics_u: 3.7434e-01\n",
            "updating  lamda for loss loss_bc1:   [0.460005]\n",
            "updating  lamda for loss loss_bc2:   [0.35129517]\n",
            "updating  lamda for loss loss_ics_u:   [0.18869978]\n",
            "It: 8600| Loss: 5.516e-02|  loss_bc1: 1.137e-02|  loss_bc2: 6.481e-03| loss_ics_u: 1.752e-01|   Loss_res: 1.459e-02 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1563e-01\n",
            "lambda_balance loss_bc2: 3.1014e-01\n",
            "lambda_balance loss_ics_u: 3.7423e-01\n",
            "updating  lamda for loss loss_bc1:   [0.60085714]\n",
            "updating  lamda for loss loss_bc2:   [0.31416476]\n",
            "updating  lamda for loss loss_ics_u:   [0.08497805]\n",
            "It: 8700| Loss: 2.217e-02|  loss_bc1: 9.181e-03|  loss_bc2: 7.676e-03| loss_ics_u: 1.655e-01|   Loss_res: 1.746e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1562e-01\n",
            "lambda_balance loss_bc2: 3.1019e-01\n",
            "lambda_balance loss_ics_u: 3.7419e-01\n",
            "updating  lamda for loss loss_bc1:   [0.44895756]\n",
            "updating  lamda for loss loss_bc2:   [0.48596752]\n",
            "updating  lamda for loss loss_ics_u:   [0.06507491]\n",
            "It: 8800| Loss: 1.541e-02|  loss_bc1: 6.100e-03|  loss_bc2: 3.813e-03| loss_ics_u: 1.626e-01|   Loss_res: 2.382e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1561e-01\n",
            "lambda_balance loss_bc2: 3.1022e-01\n",
            "lambda_balance loss_ics_u: 3.7417e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5567424]\n",
            "updating  lamda for loss loss_bc2:   [0.42214411]\n",
            "updating  lamda for loss loss_ics_u:   [0.02111344]\n",
            "It: 8900| Loss: 1.007e-02|  loss_bc1: 4.039e-03|  loss_bc2: 2.399e-03| loss_ics_u: 2.674e-01|   Loss_res: 1.162e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1551e-01\n",
            "lambda_balance loss_bc2: 3.1018e-01\n",
            "lambda_balance loss_ics_u: 3.7431e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4979189]\n",
            "updating  lamda for loss loss_bc2:   [0.47296607]\n",
            "updating  lamda for loss loss_ics_u:   [0.02911504]\n",
            "It: 9000| Loss: 9.494e-03|  loss_bc1: 4.159e-03|  loss_bc2: 2.268e-03| loss_ics_u: 2.134e-01|   Loss_res: 1.359e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1541e-01\n",
            "lambda_balance loss_bc2: 3.1014e-01\n",
            "lambda_balance loss_ics_u: 3.7445e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4082582]\n",
            "updating  lamda for loss loss_bc2:   [0.5487933]\n",
            "updating  lamda for loss loss_ics_u:   [0.04294847]\n",
            "It: 9100| Loss: 1.293e-02|  loss_bc1: 4.928e-03|  loss_bc2: 2.028e-03| loss_ics_u: 2.207e-01|   Loss_res: 3.298e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1534e-01\n",
            "lambda_balance loss_bc2: 3.1012e-01\n",
            "lambda_balance loss_ics_u: 3.7454e-01\n",
            "updating  lamda for loss loss_bc1:   [0.41517714]\n",
            "updating  lamda for loss loss_bc2:   [0.55027276]\n",
            "updating  lamda for loss loss_ics_u:   [0.03455015]\n",
            "It: 9200| Loss: 1.091e-02|  loss_bc1: 5.166e-03|  loss_bc2: 1.836e-03| loss_ics_u: 2.086e-01|   Loss_res: 5.478e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1527e-01\n",
            "lambda_balance loss_bc2: 3.1009e-01\n",
            "lambda_balance loss_ics_u: 3.7464e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5815725]\n",
            "updating  lamda for loss loss_bc2:   [0.38201615]\n",
            "updating  lamda for loss loss_ics_u:   [0.03641134]\n",
            "It: 9300| Loss: 1.323e-02|  loss_bc1: 4.096e-03|  loss_bc2: 3.671e-03| loss_ics_u: 2.575e-01|   Loss_res: 7.388e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1519e-01\n",
            "lambda_balance loss_bc2: 3.1006e-01\n",
            "lambda_balance loss_ics_u: 3.7475e-01\n",
            "updating  lamda for loss loss_bc1:   [0.55080825]\n",
            "updating  lamda for loss loss_bc2:   [0.37507024]\n",
            "updating  lamda for loss loss_ics_u:   [0.07412161]\n",
            "It: 9400| Loss: 2.074e-02|  loss_bc1: 7.346e-03|  loss_bc2: 6.004e-03| loss_ics_u: 1.891e-01|   Loss_res: 4.262e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1514e-01\n",
            "lambda_balance loss_bc2: 3.1007e-01\n",
            "lambda_balance loss_ics_u: 3.7479e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3735852]\n",
            "updating  lamda for loss loss_bc2:   [0.5341501]\n",
            "updating  lamda for loss loss_ics_u:   [0.09226464]\n",
            "It: 9500| Loss: 2.834e-02|  loss_bc1: 1.019e-02|  loss_bc2: 3.403e-03| loss_ics_u: 1.520e-01|   Loss_res: 8.689e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1514e-01\n",
            "lambda_balance loss_bc2: 3.1011e-01\n",
            "lambda_balance loss_ics_u: 3.7474e-01\n",
            "updating  lamda for loss loss_bc1:   [0.26936513]\n",
            "updating  lamda for loss loss_bc2:   [0.66275007]\n",
            "updating  lamda for loss loss_ics_u:   [0.06788478]\n",
            "It: 9600| Loss: 1.687e-02|  loss_bc1: 8.755e-03|  loss_bc2: 2.091e-03| loss_ics_u: 1.902e-01|   Loss_res: 2.179e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1514e-01\n",
            "lambda_balance loss_bc2: 3.1014e-01\n",
            "lambda_balance loss_ics_u: 3.7472e-01\n",
            "updating  lamda for loss loss_bc1:   [0.13956934]\n",
            "updating  lamda for loss loss_bc2:   [0.83253217]\n",
            "updating  lamda for loss loss_ics_u:   [0.0278984]\n",
            "It: 9700| Loss: 8.617e-03|  loss_bc1: 9.757e-03|  loss_bc2: 1.047e-03| loss_ics_u: 2.243e-01|   Loss_res: 1.266e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1512e-01\n",
            "lambda_balance loss_bc2: 3.1015e-01\n",
            "lambda_balance loss_ics_u: 3.7474e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3752489]\n",
            "updating  lamda for loss loss_bc2:   [0.55405873]\n",
            "updating  lamda for loss loss_ics_u:   [0.07069236]\n",
            "It: 9800| Loss: 1.792e-02|  loss_bc1: 7.072e-03|  loss_bc2: 2.616e-03| loss_ics_u: 1.928e-01|   Loss_res: 1.873e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1510e-01\n",
            "lambda_balance loss_bc2: 3.1016e-01\n",
            "lambda_balance loss_ics_u: 3.7474e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3403937]\n",
            "updating  lamda for loss loss_bc2:   [0.6218514]\n",
            "updating  lamda for loss loss_ics_u:   [0.03775492]\n",
            "It: 9900| Loss: 1.213e-02|  loss_bc1: 6.130e-03|  loss_bc2: 1.634e-03| loss_ics_u: 2.317e-01|   Loss_res: 2.774e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1507e-01\n",
            "lambda_balance loss_bc2: 3.1017e-01\n",
            "lambda_balance loss_ics_u: 3.7476e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3269461]\n",
            "updating  lamda for loss loss_bc2:   [0.6266732]\n",
            "updating  lamda for loss loss_ics_u:   [0.04638066]\n",
            "It: 10000| Loss: 1.298e-02|  loss_bc1: 7.244e-03|  loss_bc2: 1.412e-03| loss_ics_u: 1.837e-01|   Loss_res: 1.202e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1504e-01\n",
            "lambda_balance loss_bc2: 3.1017e-01\n",
            "lambda_balance loss_ics_u: 3.7478e-01\n",
            "updating  lamda for loss loss_bc1:   [0.13663308]\n",
            "updating  lamda for loss loss_bc2:   [0.8343277]\n",
            "updating  lamda for loss loss_ics_u:   [0.02903924]\n",
            "It: 10100| Loss: 8.090e-03|  loss_bc1: 7.886e-03|  loss_bc2: 9.154e-04| loss_ics_u: 2.078e-01|   Loss_res: 2.139e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1503e-01\n",
            "lambda_balance loss_bc2: 3.1018e-01\n",
            "lambda_balance loss_ics_u: 3.7480e-01\n",
            "updating  lamda for loss loss_bc1:   [0.16608924]\n",
            "updating  lamda for loss loss_bc2:   [0.8029931]\n",
            "updating  lamda for loss loss_ics_u:   [0.03091773]\n",
            "It: 10200| Loss: 1.027e-02|  loss_bc1: 8.225e-03|  loss_bc2: 2.585e-03| loss_ics_u: 1.853e-01|   Loss_res: 1.094e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1501e-01\n",
            "lambda_balance loss_bc2: 3.1018e-01\n",
            "lambda_balance loss_ics_u: 3.7481e-01\n",
            "updating  lamda for loss loss_bc1:   [0.12171204]\n",
            "updating  lamda for loss loss_bc2:   [0.84012264]\n",
            "updating  lamda for loss loss_ics_u:   [0.0381653]\n",
            "It: 10300| Loss: 1.104e-02|  loss_bc1: 1.568e-02|  loss_bc2: 1.461e-03| loss_ics_u: 1.686e-01|   Loss_res: 1.468e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1501e-01\n",
            "lambda_balance loss_bc2: 3.1020e-01\n",
            "lambda_balance loss_ics_u: 3.7480e-01\n",
            "updating  lamda for loss loss_bc1:   [0.40645868]\n",
            "updating  lamda for loss loss_bc2:   [0.48694822]\n",
            "updating  lamda for loss loss_ics_u:   [0.10659308]\n",
            "It: 10400| Loss: 2.424e-02|  loss_bc1: 1.369e-02|  loss_bc2: 3.714e-03| loss_ics_u: 1.505e-01|   Loss_res: 8.255e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1502e-01\n",
            "lambda_balance loss_bc2: 3.1023e-01\n",
            "lambda_balance loss_ics_u: 3.7475e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3185737]\n",
            "updating  lamda for loss loss_bc2:   [0.5477721]\n",
            "updating  lamda for loss loss_ics_u:   [0.13365424]\n",
            "It: 10500| Loss: 2.898e-02|  loss_bc1: 1.289e-02|  loss_bc2: 3.481e-03| loss_ics_u: 1.669e-01|   Loss_res: 6.596e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1505e-01\n",
            "lambda_balance loss_bc2: 3.1028e-01\n",
            "lambda_balance loss_ics_u: 3.7467e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3454349]\n",
            "updating  lamda for loss loss_bc2:   [0.5463268]\n",
            "updating  lamda for loss loss_ics_u:   [0.10823826]\n",
            "It: 10600| Loss: 2.345e-02|  loss_bc1: 1.328e-02|  loss_bc2: 5.184e-03| loss_ics_u: 1.460e-01|   Loss_res: 2.200e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1508e-01\n",
            "lambda_balance loss_bc2: 3.1033e-01\n",
            "lambda_balance loss_ics_u: 3.7459e-01\n",
            "updating  lamda for loss loss_bc1:   [0.33954948]\n",
            "updating  lamda for loss loss_bc2:   [0.5954418]\n",
            "updating  lamda for loss loss_ics_u:   [0.06500875]\n",
            "It: 10700| Loss: 1.337e-02|  loss_bc1: 9.243e-03|  loss_bc2: 2.418e-03| loss_ics_u: 1.284e-01|   Loss_res: 4.476e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1510e-01\n",
            "lambda_balance loss_bc2: 3.1037e-01\n",
            "lambda_balance loss_ics_u: 3.7453e-01\n",
            "updating  lamda for loss loss_bc1:   [0.12723193]\n",
            "updating  lamda for loss loss_bc2:   [0.8561677]\n",
            "updating  lamda for loss loss_ics_u:   [0.01660039]\n",
            "It: 10800| Loss: 6.790e-03|  loss_bc1: 8.565e-03|  loss_bc2: 2.391e-03| loss_ics_u: 2.078e-01|   Loss_res: 2.038e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1509e-01\n",
            "lambda_balance loss_bc2: 3.1038e-01\n",
            "lambda_balance loss_ics_u: 3.7453e-01\n",
            "updating  lamda for loss loss_bc1:   [0.47209072]\n",
            "updating  lamda for loss loss_bc2:   [0.48645052]\n",
            "updating  lamda for loss loss_ics_u:   [0.0414587]\n",
            "It: 10900| Loss: 1.116e-02|  loss_bc1: 5.491e-03|  loss_bc2: 2.242e-03| loss_ics_u: 1.763e-01|   Loss_res: 1.654e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1505e-01\n",
            "lambda_balance loss_bc2: 3.1038e-01\n",
            "lambda_balance loss_ics_u: 3.7456e-01\n",
            "updating  lamda for loss loss_bc1:   [0.41801515]\n",
            "updating  lamda for loss loss_bc2:   [0.5295524]\n",
            "updating  lamda for loss loss_ics_u:   [0.05243249]\n",
            "It: 11000| Loss: 1.338e-02|  loss_bc1: 4.667e-03|  loss_bc2: 2.702e-03| loss_ics_u: 1.799e-01|   Loss_res: 5.651e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1504e-01\n",
            "lambda_balance loss_bc2: 3.1039e-01\n",
            "lambda_balance loss_ics_u: 3.7457e-01\n",
            "updating  lamda for loss loss_bc1:   [0.37284955]\n",
            "updating  lamda for loss loss_bc2:   [0.5872327]\n",
            "updating  lamda for loss loss_ics_u:   [0.03991767]\n",
            "It: 11100| Loss: 1.063e-02|  loss_bc1: 4.738e-03|  loss_bc2: 1.383e-03| loss_ics_u: 1.860e-01|   Loss_res: 6.292e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1501e-01\n",
            "lambda_balance loss_bc2: 3.1040e-01\n",
            "lambda_balance loss_ics_u: 3.7458e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6150792]\n",
            "updating  lamda for loss loss_bc2:   [0.34562445]\n",
            "updating  lamda for loss loss_ics_u:   [0.03929634]\n",
            "It: 11200| Loss: 1.133e-02|  loss_bc1: 3.508e-03|  loss_bc2: 2.992e-03| loss_ics_u: 1.974e-01|   Loss_res: 3.804e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1497e-01\n",
            "lambda_balance loss_bc2: 3.1040e-01\n",
            "lambda_balance loss_ics_u: 3.7463e-01\n",
            "updating  lamda for loss loss_bc1:   [0.51779443]\n",
            "updating  lamda for loss loss_bc2:   [0.44931793]\n",
            "updating  lamda for loss loss_ics_u:   [0.03288763]\n",
            "It: 11300| Loss: 9.537e-03|  loss_bc1: 2.540e-03|  loss_bc2: 1.977e-03| loss_ics_u: 2.112e-01|   Loss_res: 3.886e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1492e-01\n",
            "lambda_balance loss_bc2: 3.1039e-01\n",
            "lambda_balance loss_ics_u: 3.7469e-01\n",
            "updating  lamda for loss loss_bc1:   [0.41341949]\n",
            "updating  lamda for loss loss_bc2:   [0.555768]\n",
            "updating  lamda for loss loss_ics_u:   [0.03081246]\n",
            "It: 11400| Loss: 7.346e-03|  loss_bc1: 3.257e-03|  loss_bc2: 1.575e-03| loss_ics_u: 1.613e-01|   Loss_res: 1.541e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1487e-01\n",
            "lambda_balance loss_bc2: 3.1038e-01\n",
            "lambda_balance loss_ics_u: 3.7475e-01\n",
            "updating  lamda for loss loss_bc1:   [0.54968256]\n",
            "updating  lamda for loss loss_bc2:   [0.3890146]\n",
            "updating  lamda for loss loss_ics_u:   [0.0613029]\n",
            "It: 11500| Loss: 1.326e-02|  loss_bc1: 3.835e-03|  loss_bc2: 3.258e-03| loss_ics_u: 1.470e-01|   Loss_res: 8.725e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1485e-01\n",
            "lambda_balance loss_bc2: 3.1039e-01\n",
            "lambda_balance loss_ics_u: 3.7476e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5252611]\n",
            "updating  lamda for loss loss_bc2:   [0.41607454]\n",
            "updating  lamda for loss loss_ics_u:   [0.05866437]\n",
            "It: 11600| Loss: 1.439e-02|  loss_bc1: 3.510e-03|  loss_bc2: 2.958e-03| loss_ics_u: 1.886e-01|   Loss_res: 2.520e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1483e-01\n",
            "lambda_balance loss_bc2: 3.1041e-01\n",
            "lambda_balance loss_ics_u: 3.7476e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5185449]\n",
            "updating  lamda for loss loss_bc2:   [0.4469361]\n",
            "updating  lamda for loss loss_ics_u:   [0.03451904]\n",
            "It: 11700| Loss: 2.676e-02|  loss_bc1: 3.274e-03|  loss_bc2: 3.336e-03| loss_ics_u: 1.624e-01|   Loss_res: 1.796e-02 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1480e-01\n",
            "lambda_balance loss_bc2: 3.1042e-01\n",
            "lambda_balance loss_ics_u: 3.7478e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2509056]\n",
            "updating  lamda for loss loss_bc2:   [0.7288416]\n",
            "updating  lamda for loss loss_ics_u:   [0.02025278]\n",
            "It: 11800| Loss: 6.292e-03|  loss_bc1: 2.467e-03|  loss_bc2: 7.267e-04| loss_ics_u: 2.473e-01|   Loss_res: 1.346e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1475e-01\n",
            "lambda_balance loss_bc2: 3.1041e-01\n",
            "lambda_balance loss_ics_u: 3.7484e-01\n",
            "updating  lamda for loss loss_bc1:   [0.18720311]\n",
            "updating  lamda for loss loss_bc2:   [0.79899406]\n",
            "updating  lamda for loss loss_ics_u:   [0.01380282]\n",
            "It: 11900| Loss: 4.906e-03|  loss_bc1: 2.677e-03|  loss_bc2: 5.910e-04| loss_ics_u: 2.800e-01|   Loss_res: 6.767e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1470e-01\n",
            "lambda_balance loss_bc2: 3.1038e-01\n",
            "lambda_balance loss_ics_u: 3.7492e-01\n",
            "updating  lamda for loss loss_bc1:   [0.25834697]\n",
            "updating  lamda for loss loss_bc2:   [0.7271915]\n",
            "updating  lamda for loss loss_ics_u:   [0.01446158]\n",
            "It: 12000| Loss: 5.124e-03|  loss_bc1: 2.680e-03|  loss_bc2: 8.236e-04| loss_ics_u: 2.311e-01|   Loss_res: 4.913e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1464e-01\n",
            "lambda_balance loss_bc2: 3.1036e-01\n",
            "lambda_balance loss_ics_u: 3.7500e-01\n",
            "updating  lamda for loss loss_bc1:   [0.31639487]\n",
            "updating  lamda for loss loss_bc2:   [0.66551095]\n",
            "updating  lamda for loss loss_ics_u:   [0.01809414]\n",
            "It: 12100| Loss: 5.107e-03|  loss_bc1: 1.699e-03|  loss_bc2: 1.194e-03| loss_ics_u: 2.006e-01|   Loss_res: 1.455e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1458e-01\n",
            "lambda_balance loss_bc2: 3.1033e-01\n",
            "lambda_balance loss_ics_u: 3.7510e-01\n",
            "updating  lamda for loss loss_bc1:   [0.16596147]\n",
            "updating  lamda for loss loss_bc2:   [0.82050323]\n",
            "updating  lamda for loss loss_ics_u:   [0.01353529]\n",
            "It: 12200| Loss: 4.960e-03|  loss_bc1: 2.900e-03|  loss_bc2: 5.540e-04| loss_ics_u: 2.390e-01|   Loss_res: 7.888e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1453e-01\n",
            "lambda_balance loss_bc2: 3.1031e-01\n",
            "lambda_balance loss_ics_u: 3.7517e-01\n",
            "updating  lamda for loss loss_bc1:   [0.11301083]\n",
            "updating  lamda for loss loss_bc2:   [0.8786946]\n",
            "updating  lamda for loss loss_ics_u:   [0.00829457]\n",
            "It: 12300| Loss: 3.410e-03|  loss_bc1: 2.344e-03|  loss_bc2: 6.679e-04| loss_ics_u: 2.875e-01|   Loss_res: 1.737e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1447e-01\n",
            "lambda_balance loss_bc2: 3.1028e-01\n",
            "lambda_balance loss_ics_u: 3.7526e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5146668]\n",
            "updating  lamda for loss loss_bc2:   [0.4629287]\n",
            "updating  lamda for loss loss_ics_u:   [0.02240448]\n",
            "It: 12400| Loss: 6.372e-03|  loss_bc1: 1.308e-03|  loss_bc2: 9.399e-04| loss_ics_u: 2.128e-01|   Loss_res: 4.969e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1441e-01\n",
            "lambda_balance loss_bc2: 3.1025e-01\n",
            "lambda_balance loss_ics_u: 3.7535e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6898509]\n",
            "updating  lamda for loss loss_bc2:   [0.28023472]\n",
            "updating  lamda for loss loss_ics_u:   [0.02991432]\n",
            "It: 12500| Loss: 9.659e-03|  loss_bc1: 1.019e-03|  loss_bc2: 1.892e-03| loss_ics_u: 2.531e-01|   Loss_res: 8.551e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1436e-01\n",
            "lambda_balance loss_bc2: 3.1023e-01\n",
            "lambda_balance loss_ics_u: 3.7541e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5230948]\n",
            "updating  lamda for loss loss_bc2:   [0.4214813]\n",
            "updating  lamda for loss loss_ics_u:   [0.05542389]\n",
            "It: 12600| Loss: 1.310e-02|  loss_bc1: 2.765e-03|  loss_bc2: 2.860e-03| loss_ics_u: 1.779e-01|   Loss_res: 5.907e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1434e-01\n",
            "lambda_balance loss_bc2: 3.1025e-01\n",
            "lambda_balance loss_ics_u: 3.7541e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5505135]\n",
            "updating  lamda for loss loss_bc2:   [0.37591836]\n",
            "updating  lamda for loss loss_ics_u:   [0.07356807]\n",
            "It: 12700| Loss: 1.680e-02|  loss_bc1: 3.044e-03|  loss_bc2: 3.910e-03| loss_ics_u: 1.323e-01|   Loss_res: 3.925e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1434e-01\n",
            "lambda_balance loss_bc2: 3.1028e-01\n",
            "lambda_balance loss_ics_u: 3.7539e-01\n",
            "updating  lamda for loss loss_bc1:   [0.77200156]\n",
            "updating  lamda for loss loss_bc2:   [0.19027963]\n",
            "updating  lamda for loss loss_ics_u:   [0.03771881]\n",
            "It: 12800| Loss: 8.889e-03|  loss_bc1: 1.918e-03|  loss_bc2: 5.552e-03| loss_ics_u: 1.657e-01|   Loss_res: 1.020e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1432e-01\n",
            "lambda_balance loss_bc2: 3.1031e-01\n",
            "lambda_balance loss_ics_u: 3.7537e-01\n",
            "updating  lamda for loss loss_bc1:   [0.556312]\n",
            "updating  lamda for loss loss_bc2:   [0.3615906]\n",
            "updating  lamda for loss loss_ics_u:   [0.08209731]\n",
            "It: 12900| Loss: 2.174e-02|  loss_bc1: 1.294e-03|  loss_bc2: 3.811e-03| loss_ics_u: 2.136e-01|   Loss_res: 2.108e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1433e-01\n",
            "lambda_balance loss_bc2: 3.1035e-01\n",
            "lambda_balance loss_ics_u: 3.7533e-01\n",
            "updating  lamda for loss loss_bc1:   [0.23802337]\n",
            "updating  lamda for loss loss_bc2:   [0.6890032]\n",
            "updating  lamda for loss loss_ics_u:   [0.07297341]\n",
            "It: 13000| Loss: 1.693e-02|  loss_bc1: 3.826e-03|  loss_bc2: 2.443e-03| loss_ics_u: 1.917e-01|   Loss_res: 3.477e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1435e-01\n",
            "lambda_balance loss_bc2: 3.1039e-01\n",
            "lambda_balance loss_ics_u: 3.7526e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22089508]\n",
            "updating  lamda for loss loss_bc2:   [0.7286812]\n",
            "updating  lamda for loss loss_ics_u:   [0.05042371]\n",
            "It: 13100| Loss: 1.151e-02|  loss_bc1: 4.531e-03|  loss_bc2: 1.304e-03| loss_ics_u: 1.783e-01|   Loss_res: 5.638e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1437e-01\n",
            "lambda_balance loss_bc2: 3.1043e-01\n",
            "lambda_balance loss_ics_u: 3.7520e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3321668]\n",
            "updating  lamda for loss loss_bc2:   [0.61599255]\n",
            "updating  lamda for loss loss_ics_u:   [0.05184066]\n",
            "It: 13200| Loss: 1.187e-02|  loss_bc1: 3.370e-03|  loss_bc2: 1.365e-03| loss_ics_u: 1.873e-01|   Loss_res: 1.994e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1438e-01\n",
            "lambda_balance loss_bc2: 3.1047e-01\n",
            "lambda_balance loss_ics_u: 3.7515e-01\n",
            "updating  lamda for loss loss_bc1:   [0.44329]\n",
            "updating  lamda for loss loss_bc2:   [0.48479995]\n",
            "updating  lamda for loss loss_ics_u:   [0.07191008]\n",
            "It: 13300| Loss: 1.990e-02|  loss_bc1: 2.522e-03|  loss_bc2: 1.834e-03| loss_ics_u: 1.600e-01|   Loss_res: 6.391e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1440e-01\n",
            "lambda_balance loss_bc2: 3.1051e-01\n",
            "lambda_balance loss_ics_u: 3.7510e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2210216]\n",
            "updating  lamda for loss loss_bc2:   [0.736318]\n",
            "updating  lamda for loss loss_ics_u:   [0.04266046]\n",
            "It: 13400| Loss: 1.004e-02|  loss_bc1: 4.535e-03|  loss_bc2: 8.640e-04| loss_ics_u: 1.884e-01|   Loss_res: 3.681e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1441e-01\n",
            "lambda_balance loss_bc2: 3.1054e-01\n",
            "lambda_balance loss_ics_u: 3.7505e-01\n",
            "updating  lamda for loss loss_bc1:   [0.33926013]\n",
            "updating  lamda for loss loss_bc2:   [0.6024829]\n",
            "updating  lamda for loss loss_ics_u:   [0.05825688]\n",
            "It: 13500| Loss: 1.307e-02|  loss_bc1: 4.745e-03|  loss_bc2: 1.687e-03| loss_ics_u: 1.694e-01|   Loss_res: 5.733e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1442e-01\n",
            "lambda_balance loss_bc2: 3.1058e-01\n",
            "lambda_balance loss_ics_u: 3.7500e-01\n",
            "updating  lamda for loss loss_bc1:   [0.15008524]\n",
            "updating  lamda for loss loss_bc2:   [0.833626]\n",
            "updating  lamda for loss loss_ics_u:   [0.01628881]\n",
            "It: 13600| Loss: 8.313e-03|  loss_bc1: 3.348e-03|  loss_bc2: 7.757e-04| loss_ics_u: 1.748e-01|   Loss_res: 4.317e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1443e-01\n",
            "lambda_balance loss_bc2: 3.1061e-01\n",
            "lambda_balance loss_ics_u: 3.7496e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17871958]\n",
            "updating  lamda for loss loss_bc2:   [0.7918909]\n",
            "updating  lamda for loss loss_ics_u:   [0.0293896]\n",
            "It: 13700| Loss: 7.216e-03|  loss_bc1: 4.827e-03|  loss_bc2: 6.937e-04| loss_ics_u: 1.923e-01|   Loss_res: 1.514e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1444e-01\n",
            "lambda_balance loss_bc2: 3.1064e-01\n",
            "lambda_balance loss_ics_u: 3.7493e-01\n",
            "updating  lamda for loss loss_bc1:   [0.23462473]\n",
            "updating  lamda for loss loss_bc2:   [0.7059729]\n",
            "updating  lamda for loss loss_ics_u:   [0.0594024]\n",
            "It: 13800| Loss: 1.129e-02|  loss_bc1: 5.666e-03|  loss_bc2: 1.443e-03| loss_ics_u: 1.468e-01|   Loss_res: 2.143e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1445e-01\n",
            "lambda_balance loss_bc2: 3.1067e-01\n",
            "lambda_balance loss_ics_u: 3.7487e-01\n",
            "updating  lamda for loss loss_bc1:   [0.38827547]\n",
            "updating  lamda for loss loss_bc2:   [0.5400722]\n",
            "updating  lamda for loss loss_ics_u:   [0.07165233]\n",
            "It: 13900| Loss: 1.329e-02|  loss_bc1: 3.621e-03|  loss_bc2: 1.627e-03| loss_ics_u: 1.329e-01|   Loss_res: 1.487e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1448e-01\n",
            "lambda_balance loss_bc2: 3.1072e-01\n",
            "lambda_balance loss_ics_u: 3.7481e-01\n",
            "updating  lamda for loss loss_bc1:   [0.16706878]\n",
            "updating  lamda for loss loss_bc2:   [0.790684]\n",
            "updating  lamda for loss loss_ics_u:   [0.04224717]\n",
            "It: 14000| Loss: 8.448e-03|  loss_bc1: 5.803e-03|  loss_bc2: 9.208e-04| loss_ics_u: 1.449e-01|   Loss_res: 6.289e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1450e-01\n",
            "lambda_balance loss_bc2: 3.1076e-01\n",
            "lambda_balance loss_ics_u: 3.7475e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14098871]\n",
            "updating  lamda for loss loss_bc2:   [0.82198334]\n",
            "updating  lamda for loss loss_ics_u:   [0.03702795]\n",
            "It: 14100| Loss: 7.069e-03|  loss_bc1: 6.632e-03|  loss_bc2: 5.788e-04| loss_ics_u: 1.431e-01|   Loss_res: 3.586e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1452e-01\n",
            "lambda_balance loss_bc2: 3.1080e-01\n",
            "lambda_balance loss_ics_u: 3.7468e-01\n",
            "updating  lamda for loss loss_bc1:   [0.32521898]\n",
            "updating  lamda for loss loss_bc2:   [0.53534484]\n",
            "updating  lamda for loss loss_ics_u:   [0.13943617]\n",
            "It: 14200| Loss: 4.515e-02|  loss_bc1: 6.387e-03|  loss_bc2: 1.399e-03| loss_ics_u: 1.416e-01|   Loss_res: 2.258e-02 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1455e-01\n",
            "lambda_balance loss_bc2: 3.1084e-01\n",
            "lambda_balance loss_ics_u: 3.7461e-01\n",
            "updating  lamda for loss loss_bc1:   [0.62163424]\n",
            "updating  lamda for loss loss_bc2:   [0.34030938]\n",
            "updating  lamda for loss loss_ics_u:   [0.0380564]\n",
            "It: 14300| Loss: 8.102e-03|  loss_bc1: 1.284e-03|  loss_bc2: 1.339e-03| loss_ics_u: 1.757e-01|   Loss_res: 1.603e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1456e-01\n",
            "lambda_balance loss_bc2: 3.1087e-01\n",
            "lambda_balance loss_ics_u: 3.7457e-01\n",
            "updating  lamda for loss loss_bc1:   [0.38907498]\n",
            "updating  lamda for loss loss_bc2:   [0.57117045]\n",
            "updating  lamda for loss loss_ics_u:   [0.03975464]\n",
            "It: 14400| Loss: 8.406e-03|  loss_bc1: 2.277e-03|  loss_bc2: 1.074e-03| loss_ics_u: 1.606e-01|   Loss_res: 5.240e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1457e-01\n",
            "lambda_balance loss_bc2: 3.1091e-01\n",
            "lambda_balance loss_ics_u: 3.7452e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4269137]\n",
            "updating  lamda for loss loss_bc2:   [0.5048211]\n",
            "updating  lamda for loss loss_ics_u:   [0.06826518]\n",
            "It: 14500| Loss: 1.141e-02|  loss_bc1: 3.056e-03|  loss_bc2: 1.170e-03| loss_ics_u: 1.158e-01|   Loss_res: 1.606e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1459e-01\n",
            "lambda_balance loss_bc2: 3.1095e-01\n",
            "lambda_balance loss_ics_u: 3.7446e-01\n",
            "updating  lamda for loss loss_bc1:   [0.28812164]\n",
            "updating  lamda for loss loss_bc2:   [0.66428685]\n",
            "updating  lamda for loss loss_ics_u:   [0.04759159]\n",
            "It: 14600| Loss: 9.271e-03|  loss_bc1: 2.995e-03|  loss_bc2: 5.665e-04| loss_ics_u: 1.533e-01|   Loss_res: 7.344e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1461e-01\n",
            "lambda_balance loss_bc2: 3.1099e-01\n",
            "lambda_balance loss_ics_u: 3.7440e-01\n",
            "updating  lamda for loss loss_bc1:   [0.44140092]\n",
            "updating  lamda for loss loss_bc2:   [0.4973627]\n",
            "updating  lamda for loss loss_ics_u:   [0.06123644]\n",
            "It: 14700| Loss: 1.317e-02|  loss_bc1: 1.122e-03|  loss_bc2: 1.068e-03| loss_ics_u: 1.569e-01|   Loss_res: 2.531e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1462e-01\n",
            "lambda_balance loss_bc2: 3.1103e-01\n",
            "lambda_balance loss_ics_u: 3.7435e-01\n",
            "updating  lamda for loss loss_bc1:   [0.31717294]\n",
            "updating  lamda for loss loss_bc2:   [0.65305835]\n",
            "updating  lamda for loss loss_ics_u:   [0.02976877]\n",
            "It: 14800| Loss: 4.800e-03|  loss_bc1: 1.267e-03|  loss_bc2: 4.023e-04| loss_ics_u: 1.328e-01|   Loss_res: 1.829e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1464e-01\n",
            "lambda_balance loss_bc2: 3.1106e-01\n",
            "lambda_balance loss_ics_u: 3.7430e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6168999]\n",
            "updating  lamda for loss loss_bc2:   [0.34898955]\n",
            "updating  lamda for loss loss_ics_u:   [0.03411056]\n",
            "It: 14900| Loss: 8.622e-03|  loss_bc1: 6.741e-04|  loss_bc2: 9.758e-04| loss_ics_u: 1.596e-01|   Loss_res: 2.423e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1465e-01\n",
            "lambda_balance loss_bc2: 3.1110e-01\n",
            "lambda_balance loss_ics_u: 3.7425e-01\n",
            "updating  lamda for loss loss_bc1:   [0.32433158]\n",
            "updating  lamda for loss loss_bc2:   [0.64743537]\n",
            "updating  lamda for loss loss_ics_u:   [0.02823303]\n",
            "It: 15000| Loss: 7.161e-03|  loss_bc1: 7.503e-04|  loss_bc2: 4.886e-04| loss_ics_u: 1.703e-01|   Loss_res: 1.794e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1466e-01\n",
            "lambda_balance loss_bc2: 3.1113e-01\n",
            "lambda_balance loss_ics_u: 3.7421e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2638852]\n",
            "updating  lamda for loss loss_bc2:   [0.7209422]\n",
            "updating  lamda for loss loss_ics_u:   [0.0151726]\n",
            "It: 15100| Loss: 3.331e-03|  loss_bc1: 8.566e-04|  loss_bc2: 3.416e-04| loss_ics_u: 1.687e-01|   Loss_res: 2.992e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1467e-01\n",
            "lambda_balance loss_bc2: 3.1116e-01\n",
            "lambda_balance loss_ics_u: 3.7417e-01\n",
            "updating  lamda for loss loss_bc1:   [0.30999407]\n",
            "updating  lamda for loss loss_bc2:   [0.6668454]\n",
            "updating  lamda for loss loss_ics_u:   [0.02316059]\n",
            "It: 15200| Loss: 4.382e-03|  loss_bc1: 1.053e-03|  loss_bc2: 3.133e-04| loss_ics_u: 1.441e-01|   Loss_res: 5.096e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1467e-01\n",
            "lambda_balance loss_bc2: 3.1119e-01\n",
            "lambda_balance loss_ics_u: 3.7413e-01\n",
            "updating  lamda for loss loss_bc1:   [0.15677634]\n",
            "updating  lamda for loss loss_bc2:   [0.82618845]\n",
            "updating  lamda for loss loss_ics_u:   [0.01703516]\n",
            "It: 15300| Loss: 3.056e-03|  loss_bc1: 8.770e-04|  loss_bc2: 1.440e-04| loss_ics_u: 1.526e-01|   Loss_res: 1.998e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1469e-01\n",
            "lambda_balance loss_bc2: 3.1122e-01\n",
            "lambda_balance loss_ics_u: 3.7409e-01\n",
            "updating  lamda for loss loss_bc1:   [0.68012244]\n",
            "updating  lamda for loss loss_bc2:   [0.2992554]\n",
            "updating  lamda for loss loss_ics_u:   [0.02062217]\n",
            "It: 15400| Loss: 4.024e-03|  loss_bc1: 4.013e-04|  loss_bc2: 3.907e-04| loss_ics_u: 1.642e-01|   Loss_res: 2.475e-04 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1469e-01\n",
            "lambda_balance loss_bc2: 3.1125e-01\n",
            "lambda_balance loss_ics_u: 3.7406e-01\n",
            "updating  lamda for loss loss_bc1:   [0.54543924]\n",
            "updating  lamda for loss loss_bc2:   [0.4219408]\n",
            "updating  lamda for loss loss_ics_u:   [0.03261993]\n",
            "It: 15500| Loss: 1.203e-02|  loss_bc1: 9.345e-04|  loss_bc2: 1.687e-03| loss_ics_u: 1.532e-01|   Loss_res: 5.814e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1470e-01\n",
            "lambda_balance loss_bc2: 3.1128e-01\n",
            "lambda_balance loss_ics_u: 3.7403e-01\n",
            "updating  lamda for loss loss_bc1:   [0.71619064]\n",
            "updating  lamda for loss loss_bc2:   [0.25565478]\n",
            "updating  lamda for loss loss_ics_u:   [0.02815467]\n",
            "It: 15600| Loss: 5.784e-03|  loss_bc1: 2.122e-04|  loss_bc2: 7.296e-04| loss_ics_u: 1.826e-01|   Loss_res: 3.031e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1470e-01\n",
            "lambda_balance loss_bc2: 3.1131e-01\n",
            "lambda_balance loss_ics_u: 3.7399e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6233791]\n",
            "updating  lamda for loss loss_bc2:   [0.35013303]\n",
            "updating  lamda for loss loss_ics_u:   [0.02648785]\n",
            "It: 15700| Loss: 4.688e-03|  loss_bc1: 4.256e-04|  loss_bc2: 9.313e-04| loss_ics_u: 1.398e-01|   Loss_res: 3.927e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1471e-01\n",
            "lambda_balance loss_bc2: 3.1134e-01\n",
            "lambda_balance loss_ics_u: 3.7394e-01\n",
            "updating  lamda for loss loss_bc1:   [0.57070184]\n",
            "updating  lamda for loss loss_bc2:   [0.40582687]\n",
            "updating  lamda for loss loss_ics_u:   [0.02347125]\n",
            "It: 15800| Loss: 3.957e-03|  loss_bc1: 3.319e-04|  loss_bc2: 4.197e-04| loss_ics_u: 1.486e-01|   Loss_res: 1.091e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1472e-01\n",
            "lambda_balance loss_bc2: 3.1137e-01\n",
            "lambda_balance loss_ics_u: 3.7390e-01\n",
            "updating  lamda for loss loss_bc1:   [0.29282853]\n",
            "updating  lamda for loss loss_bc2:   [0.69705105]\n",
            "updating  lamda for loss loss_ics_u:   [0.01012045]\n",
            "It: 15900| Loss: 4.280e-03|  loss_bc1: 4.770e-04|  loss_bc2: 6.758e-04| loss_ics_u: 1.527e-01|   Loss_res: 2.123e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1473e-01\n",
            "lambda_balance loss_bc2: 3.1140e-01\n",
            "lambda_balance loss_ics_u: 3.7386e-01\n",
            "updating  lamda for loss loss_bc1:   [0.31716278]\n",
            "updating  lamda for loss loss_bc2:   [0.67307985]\n",
            "updating  lamda for loss loss_ics_u:   [0.00975734]\n",
            "It: 16000| Loss: 2.100e-03|  loss_bc1: 3.259e-04|  loss_bc2: 1.433e-04| loss_ics_u: 1.546e-01|   Loss_res: 3.925e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1474e-01\n",
            "lambda_balance loss_bc2: 3.1143e-01\n",
            "lambda_balance loss_ics_u: 3.7384e-01\n",
            "updating  lamda for loss loss_bc1:   [0.77121496]\n",
            "updating  lamda for loss loss_bc2:   [0.20864075]\n",
            "updating  lamda for loss loss_ics_u:   [0.02014427]\n",
            "It: 16100| Loss: 3.228e-03|  loss_bc1: 3.990e-04|  loss_bc2: 5.342e-04| loss_ics_u: 1.334e-01|   Loss_res: 1.210e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1474e-01\n",
            "lambda_balance loss_bc2: 3.1145e-01\n",
            "lambda_balance loss_ics_u: 3.7380e-01\n",
            "updating  lamda for loss loss_bc1:   [0.45348686]\n",
            "updating  lamda for loss loss_bc2:   [0.5019963]\n",
            "updating  lamda for loss loss_ics_u:   [0.0445168]\n",
            "It: 16200| Loss: 1.189e-02|  loss_bc1: 6.913e-04|  loss_bc2: 1.348e-03| loss_ics_u: 1.497e-01|   Loss_res: 4.234e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1476e-01\n",
            "lambda_balance loss_bc2: 3.1149e-01\n",
            "lambda_balance loss_ics_u: 3.7376e-01\n",
            "updating  lamda for loss loss_bc1:   [0.37435868]\n",
            "updating  lamda for loss loss_bc2:   [0.547829]\n",
            "updating  lamda for loss loss_ics_u:   [0.07781229]\n",
            "It: 16300| Loss: 1.144e-02|  loss_bc1: 2.805e-03|  loss_bc2: 7.972e-04| loss_ics_u: 1.205e-01|   Loss_res: 5.787e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1478e-01\n",
            "lambda_balance loss_bc2: 3.1152e-01\n",
            "lambda_balance loss_ics_u: 3.7370e-01\n",
            "updating  lamda for loss loss_bc1:   [0.38557237]\n",
            "updating  lamda for loss loss_bc2:   [0.5664035]\n",
            "updating  lamda for loss loss_ics_u:   [0.0480241]\n",
            "It: 16400| Loss: 9.207e-03|  loss_bc1: 2.403e-03|  loss_bc2: 5.832e-04| loss_ics_u: 1.494e-01|   Loss_res: 7.755e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1480e-01\n",
            "lambda_balance loss_bc2: 3.1156e-01\n",
            "lambda_balance loss_ics_u: 3.7364e-01\n",
            "updating  lamda for loss loss_bc1:   [0.19356956]\n",
            "updating  lamda for loss loss_bc2:   [0.78057647]\n",
            "updating  lamda for loss loss_ics_u:   [0.02585396]\n",
            "It: 16500| Loss: 4.862e-03|  loss_bc1: 7.284e-04|  loss_bc2: 1.567e-04| loss_ics_u: 1.544e-01|   Loss_res: 6.066e-04 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1481e-01\n",
            "lambda_balance loss_bc2: 3.1160e-01\n",
            "lambda_balance loss_ics_u: 3.7359e-01\n",
            "updating  lamda for loss loss_bc1:   [0.25192398]\n",
            "updating  lamda for loss loss_bc2:   [0.72442394]\n",
            "updating  lamda for loss loss_ics_u:   [0.02365205]\n",
            "It: 16600| Loss: 4.064e-03|  loss_bc1: 6.333e-04|  loss_bc2: 1.487e-04| loss_ics_u: 1.425e-01|   Loss_res: 4.262e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1483e-01\n",
            "lambda_balance loss_bc2: 3.1163e-01\n",
            "lambda_balance loss_ics_u: 3.7354e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5170991]\n",
            "updating  lamda for loss loss_bc2:   [0.4385458]\n",
            "updating  lamda for loss loss_ics_u:   [0.04435513]\n",
            "It: 16700| Loss: 6.187e-03|  loss_bc1: 4.273e-04|  loss_bc2: 4.669e-04| loss_ics_u: 1.215e-01|   Loss_res: 3.724e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1485e-01\n",
            "lambda_balance loss_bc2: 3.1167e-01\n",
            "lambda_balance loss_ics_u: 3.7348e-01\n",
            "updating  lamda for loss loss_bc1:   [0.56755465]\n",
            "updating  lamda for loss loss_bc2:   [0.36939064]\n",
            "updating  lamda for loss loss_ics_u:   [0.06305467]\n",
            "It: 16800| Loss: 9.460e-03|  loss_bc1: 5.170e-04|  loss_bc2: 4.769e-04| loss_ics_u: 1.348e-01|   Loss_res: 4.914e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1487e-01\n",
            "lambda_balance loss_bc2: 3.1171e-01\n",
            "lambda_balance loss_ics_u: 3.7342e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3645394]\n",
            "updating  lamda for loss loss_bc2:   [0.60172856]\n",
            "updating  lamda for loss loss_ics_u:   [0.03373199]\n",
            "It: 16900| Loss: 5.909e-03|  loss_bc1: 4.922e-04|  loss_bc2: 2.129e-04| loss_ics_u: 1.319e-01|   Loss_res: 1.153e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1489e-01\n",
            "lambda_balance loss_bc2: 3.1175e-01\n",
            "lambda_balance loss_ics_u: 3.7337e-01\n",
            "updating  lamda for loss loss_bc1:   [0.44747984]\n",
            "updating  lamda for loss loss_bc2:   [0.5307227]\n",
            "updating  lamda for loss loss_ics_u:   [0.02179751]\n",
            "It: 17000| Loss: 3.958e-03|  loss_bc1: 2.593e-04|  loss_bc2: 1.703e-04| loss_ics_u: 1.501e-01|   Loss_res: 4.788e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1490e-01\n",
            "lambda_balance loss_bc2: 3.1178e-01\n",
            "lambda_balance loss_ics_u: 3.7331e-01\n",
            "updating  lamda for loss loss_bc1:   [0.45200762]\n",
            "updating  lamda for loss loss_bc2:   [0.52392185]\n",
            "updating  lamda for loss loss_ics_u:   [0.02407056]\n",
            "It: 17100| Loss: 3.496e-03|  loss_bc1: 2.242e-04|  loss_bc2: 1.925e-04| loss_ics_u: 1.267e-01|   Loss_res: 2.429e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1492e-01\n",
            "lambda_balance loss_bc2: 3.1182e-01\n",
            "lambda_balance loss_ics_u: 3.7326e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5159411]\n",
            "updating  lamda for loss loss_bc2:   [0.44942003]\n",
            "updating  lamda for loss loss_ics_u:   [0.0346389]\n",
            "It: 17200| Loss: 6.874e-03|  loss_bc1: 2.002e-04|  loss_bc2: 2.634e-04| loss_ics_u: 1.653e-01|   Loss_res: 9.277e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1494e-01\n",
            "lambda_balance loss_bc2: 3.1185e-01\n",
            "lambda_balance loss_ics_u: 3.7321e-01\n",
            "updating  lamda for loss loss_bc1:   [0.33254927]\n",
            "updating  lamda for loss loss_bc2:   [0.6371447]\n",
            "updating  lamda for loss loss_ics_u:   [0.03030605]\n",
            "It: 17300| Loss: 5.272e-03|  loss_bc1: 1.141e-03|  loss_bc2: 5.517e-04| loss_ics_u: 1.336e-01|   Loss_res: 4.916e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1496e-01\n",
            "lambda_balance loss_bc2: 3.1189e-01\n",
            "lambda_balance loss_ics_u: 3.7315e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22099413]\n",
            "updating  lamda for loss loss_bc2:   [0.755373]\n",
            "updating  lamda for loss loss_ics_u:   [0.02363288]\n",
            "It: 17400| Loss: 3.616e-03|  loss_bc1: 3.351e-04|  loss_bc2: 1.332e-04| loss_ics_u: 1.400e-01|   Loss_res: 1.322e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1498e-01\n",
            "lambda_balance loss_bc2: 3.1192e-01\n",
            "lambda_balance loss_ics_u: 3.7310e-01\n",
            "updating  lamda for loss loss_bc1:   [0.15319385]\n",
            "updating  lamda for loss loss_bc2:   [0.8316153]\n",
            "updating  lamda for loss loss_ics_u:   [0.01519088]\n",
            "It: 17500| Loss: 3.105e-03|  loss_bc1: 4.066e-04|  loss_bc2: 3.859e-04| loss_ics_u: 1.475e-01|   Loss_res: 4.809e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1500e-01\n",
            "lambda_balance loss_bc2: 3.1196e-01\n",
            "lambda_balance loss_ics_u: 3.7304e-01\n",
            "updating  lamda for loss loss_bc1:   [0.7950348]\n",
            "updating  lamda for loss loss_bc2:   [0.18578915]\n",
            "updating  lamda for loss loss_ics_u:   [0.019176]\n",
            "It: 17600| Loss: 3.370e-03|  loss_bc1: 7.004e-05|  loss_bc2: 1.758e-04| loss_ics_u: 1.430e-01|   Loss_res: 5.389e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1501e-01\n",
            "lambda_balance loss_bc2: 3.1199e-01\n",
            "lambda_balance loss_ics_u: 3.7300e-01\n",
            "updating  lamda for loss loss_bc1:   [0.47693142]\n",
            "updating  lamda for loss loss_bc2:   [0.49279734]\n",
            "updating  lamda for loss loss_ics_u:   [0.03027117]\n",
            "It: 17700| Loss: 5.420e-03|  loss_bc1: 3.836e-04|  loss_bc2: 1.158e-04| loss_ics_u: 1.395e-01|   Loss_res: 9.585e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1503e-01\n",
            "lambda_balance loss_bc2: 3.1203e-01\n",
            "lambda_balance loss_ics_u: 3.7295e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17129803]\n",
            "updating  lamda for loss loss_bc2:   [0.8160374]\n",
            "updating  lamda for loss loss_ics_u:   [0.0126646]\n",
            "It: 17800| Loss: 2.735e-03|  loss_bc1: 4.401e-04|  loss_bc2: 6.324e-04| loss_ics_u: 1.387e-01|   Loss_res: 3.873e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1505e-01\n",
            "lambda_balance loss_bc2: 3.1206e-01\n",
            "lambda_balance loss_ics_u: 3.7290e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6950847]\n",
            "updating  lamda for loss loss_bc2:   [0.23488611]\n",
            "updating  lamda for loss loss_ics_u:   [0.07002915]\n",
            "It: 17900| Loss: 1.170e-02|  loss_bc1: 6.278e-04|  loss_bc2: 5.291e-04| loss_ics_u: 1.481e-01|   Loss_res: 7.660e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1506e-01\n",
            "lambda_balance loss_bc2: 3.1209e-01\n",
            "lambda_balance loss_ics_u: 3.7285e-01\n",
            "updating  lamda for loss loss_bc1:   [0.24899253]\n",
            "updating  lamda for loss loss_bc2:   [0.6620664]\n",
            "updating  lamda for loss loss_ics_u:   [0.08894107]\n",
            "It: 18000| Loss: 1.306e-02|  loss_bc1: 2.828e-03|  loss_bc2: 5.765e-04| loss_ics_u: 1.320e-01|   Loss_res: 2.293e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1509e-01\n",
            "lambda_balance loss_bc2: 3.1213e-01\n",
            "lambda_balance loss_ics_u: 3.7278e-01\n",
            "updating  lamda for loss loss_bc1:   [0.19229409]\n",
            "updating  lamda for loss loss_bc2:   [0.76204026]\n",
            "updating  lamda for loss loss_ics_u:   [0.04566566]\n",
            "It: 18100| Loss: 8.099e-03|  loss_bc1: 4.535e-04|  loss_bc2: 4.381e-04| loss_ics_u: 1.575e-01|   Loss_res: 4.867e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1511e-01\n",
            "lambda_balance loss_bc2: 3.1217e-01\n",
            "lambda_balance loss_ics_u: 3.7273e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5520899]\n",
            "updating  lamda for loss loss_bc2:   [0.3995661]\n",
            "updating  lamda for loss loss_ics_u:   [0.04834391]\n",
            "It: 18200| Loss: 7.190e-03|  loss_bc1: 1.135e-03|  loss_bc2: 2.600e-04| loss_ics_u: 1.315e-01|   Loss_res: 1.007e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1513e-01\n",
            "lambda_balance loss_bc2: 3.1220e-01\n",
            "lambda_balance loss_ics_u: 3.7267e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14773771]\n",
            "updating  lamda for loss loss_bc2:   [0.80876386]\n",
            "updating  lamda for loss loss_ics_u:   [0.04349837]\n",
            "It: 18300| Loss: 6.518e-03|  loss_bc1: 1.250e-03|  loss_bc2: 1.054e-03| loss_ics_u: 1.191e-01|   Loss_res: 3.005e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1515e-01\n",
            "lambda_balance loss_bc2: 3.1224e-01\n",
            "lambda_balance loss_ics_u: 3.7261e-01\n",
            "updating  lamda for loss loss_bc1:   [0.72759104]\n",
            "updating  lamda for loss loss_bc2:   [0.21271709]\n",
            "updating  lamda for loss loss_ics_u:   [0.05969185]\n",
            "It: 18400| Loss: 7.752e-03|  loss_bc1: 2.121e-04|  loss_bc2: 3.578e-04| loss_ics_u: 1.161e-01|   Loss_res: 5.886e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1517e-01\n",
            "lambda_balance loss_bc2: 3.1227e-01\n",
            "lambda_balance loss_ics_u: 3.7256e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6228955]\n",
            "updating  lamda for loss loss_bc2:   [0.3312754]\n",
            "updating  lamda for loss loss_ics_u:   [0.04582914]\n",
            "It: 18500| Loss: 6.931e-03|  loss_bc1: 9.391e-05|  loss_bc2: 2.519e-04| loss_ics_u: 1.358e-01|   Loss_res: 5.655e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1519e-01\n",
            "lambda_balance loss_bc2: 3.1231e-01\n",
            "lambda_balance loss_ics_u: 3.7251e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3170931]\n",
            "updating  lamda for loss loss_bc2:   [0.61804914]\n",
            "updating  lamda for loss loss_ics_u:   [0.06485776]\n",
            "It: 18600| Loss: 8.257e-03|  loss_bc1: 5.679e-04|  loss_bc2: 2.643e-04| loss_ics_u: 1.210e-01|   Loss_res: 6.651e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1521e-01\n",
            "lambda_balance loss_bc2: 3.1234e-01\n",
            "lambda_balance loss_ics_u: 3.7245e-01\n",
            "updating  lamda for loss loss_bc1:   [0.7628909]\n",
            "updating  lamda for loss loss_bc2:   [0.20946315]\n",
            "updating  lamda for loss loss_ics_u:   [0.02764593]\n",
            "It: 18700| Loss: 3.925e-03|  loss_bc1: 7.178e-05|  loss_bc2: 2.088e-04| loss_ics_u: 1.319e-01|   Loss_res: 1.815e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1523e-01\n",
            "lambda_balance loss_bc2: 3.1237e-01\n",
            "lambda_balance loss_ics_u: 3.7240e-01\n",
            "updating  lamda for loss loss_bc1:   [0.53134984]\n",
            "updating  lamda for loss loss_bc2:   [0.44233385]\n",
            "updating  lamda for loss loss_ics_u:   [0.02631629]\n",
            "It: 18800| Loss: 3.814e-03|  loss_bc1: 1.020e-04|  loss_bc2: 1.306e-04| loss_ics_u: 1.355e-01|   Loss_res: 1.361e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1524e-01\n",
            "lambda_balance loss_bc2: 3.1241e-01\n",
            "lambda_balance loss_ics_u: 3.7235e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4969062]\n",
            "updating  lamda for loss loss_bc2:   [0.47399828]\n",
            "updating  lamda for loss loss_ics_u:   [0.0290955]\n",
            "It: 18900| Loss: 4.783e-03|  loss_bc1: 1.252e-04|  loss_bc2: 1.063e-04| loss_ics_u: 1.439e-01|   Loss_res: 4.840e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1526e-01\n",
            "lambda_balance loss_bc2: 3.1244e-01\n",
            "lambda_balance loss_ics_u: 3.7230e-01\n",
            "updating  lamda for loss loss_bc1:   [0.26904696]\n",
            "updating  lamda for loss loss_bc2:   [0.6478069]\n",
            "updating  lamda for loss loss_ics_u:   [0.08314615]\n",
            "It: 19000| Loss: 1.284e-02|  loss_bc1: 1.131e-03|  loss_bc2: 2.374e-04| loss_ics_u: 1.459e-01|   Loss_res: 2.476e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1528e-01\n",
            "lambda_balance loss_bc2: 3.1247e-01\n",
            "lambda_balance loss_ics_u: 3.7224e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5548476]\n",
            "updating  lamda for loss loss_bc2:   [0.38643998]\n",
            "updating  lamda for loss loss_ics_u:   [0.05871244]\n",
            "It: 19100| Loss: 8.112e-03|  loss_bc1: 9.846e-05|  loss_bc2: 2.149e-04| loss_ics_u: 1.310e-01|   Loss_res: 2.803e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1530e-01\n",
            "lambda_balance loss_bc2: 3.1251e-01\n",
            "lambda_balance loss_ics_u: 3.7219e-01\n",
            "updating  lamda for loss loss_bc1:   [0.55219424]\n",
            "updating  lamda for loss loss_bc2:   [0.41030374]\n",
            "updating  lamda for loss loss_ics_u:   [0.03750205]\n",
            "It: 19200| Loss: 5.299e-03|  loss_bc1: 5.980e-04|  loss_bc2: 1.595e-04| loss_ics_u: 1.235e-01|   Loss_res: 2.717e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1532e-01\n",
            "lambda_balance loss_bc2: 3.1254e-01\n",
            "lambda_balance loss_ics_u: 3.7214e-01\n",
            "updating  lamda for loss loss_bc1:   [0.06591602]\n",
            "updating  lamda for loss loss_bc2:   [0.9069972]\n",
            "updating  lamda for loss loss_ics_u:   [0.02708672]\n",
            "It: 19300| Loss: 3.693e-03|  loss_bc1: 7.668e-04|  loss_bc2: 7.158e-05| loss_ics_u: 1.283e-01|   Loss_res: 1.030e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1534e-01\n",
            "lambda_balance loss_bc2: 3.1257e-01\n",
            "lambda_balance loss_ics_u: 3.7209e-01\n",
            "updating  lamda for loss loss_bc1:   [0.06840418]\n",
            "updating  lamda for loss loss_bc2:   [0.9027756]\n",
            "updating  lamda for loss loss_ics_u:   [0.02882019]\n",
            "It: 19400| Loss: 4.016e-03|  loss_bc1: 6.372e-04|  loss_bc2: 5.579e-05| loss_ics_u: 1.333e-01|   Loss_res: 7.958e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1536e-01\n",
            "lambda_balance loss_bc2: 3.1261e-01\n",
            "lambda_balance loss_ics_u: 3.7203e-01\n",
            "updating  lamda for loss loss_bc1:   [0.20805988]\n",
            "updating  lamda for loss loss_bc2:   [0.71246964]\n",
            "updating  lamda for loss loss_ics_u:   [0.07947048]\n",
            "It: 19500| Loss: 9.512e-03|  loss_bc1: 6.994e-04|  loss_bc2: 2.045e-04| loss_ics_u: 1.148e-01|   Loss_res: 9.372e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1538e-01\n",
            "lambda_balance loss_bc2: 3.1264e-01\n",
            "lambda_balance loss_ics_u: 3.7198e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22899863]\n",
            "updating  lamda for loss loss_bc2:   [0.54048973]\n",
            "updating  lamda for loss loss_ics_u:   [0.2305116]\n",
            "It: 19600| Loss: 2.991e-02|  loss_bc1: 2.416e-03|  loss_bc2: 1.439e-04| loss_ics_u: 1.263e-01|   Loss_res: 1.600e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1540e-01\n",
            "lambda_balance loss_bc2: 3.1267e-01\n",
            "lambda_balance loss_ics_u: 3.7193e-01\n",
            "updating  lamda for loss loss_bc1:   [0.38141927]\n",
            "updating  lamda for loss loss_bc2:   [0.44415027]\n",
            "updating  lamda for loss loss_ics_u:   [0.17443043]\n",
            "It: 19700| Loss: 2.389e-02|  loss_bc1: 2.065e-03|  loss_bc2: 6.000e-04| loss_ics_u: 1.287e-01|   Loss_res: 3.961e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1542e-01\n",
            "lambda_balance loss_bc2: 3.1271e-01\n",
            "lambda_balance loss_ics_u: 3.7187e-01\n",
            "updating  lamda for loss loss_bc1:   [0.66751814]\n",
            "updating  lamda for loss loss_bc2:   [0.24143805]\n",
            "updating  lamda for loss loss_ics_u:   [0.09104384]\n",
            "It: 19800| Loss: 1.185e-02|  loss_bc1: 4.801e-04|  loss_bc2: 2.068e-04| loss_ics_u: 1.241e-01|   Loss_res: 1.822e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1544e-01\n",
            "lambda_balance loss_bc2: 3.1274e-01\n",
            "lambda_balance loss_ics_u: 3.7182e-01\n",
            "updating  lamda for loss loss_bc1:   [0.29972008]\n",
            "updating  lamda for loss loss_bc2:   [0.6164246]\n",
            "updating  lamda for loss loss_ics_u:   [0.0838553]\n",
            "It: 19900| Loss: 9.340e-03|  loss_bc1: 1.021e-03|  loss_bc2: 2.302e-04| loss_ics_u: 1.033e-01|   Loss_res: 2.294e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1546e-01\n",
            "lambda_balance loss_bc2: 3.1277e-01\n",
            "lambda_balance loss_ics_u: 3.7176e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5950346]\n",
            "updating  lamda for loss loss_bc2:   [0.321075]\n",
            "updating  lamda for loss loss_ics_u:   [0.08389041]\n",
            "It: 20000| Loss: 1.086e-02|  loss_bc1: 1.028e-04|  loss_bc2: 3.741e-04| loss_ics_u: 1.255e-01|   Loss_res: 1.521e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1548e-01\n",
            "lambda_balance loss_bc2: 3.1280e-01\n",
            "lambda_balance loss_ics_u: 3.7172e-01\n",
            "updating  lamda for loss loss_bc1:   [0.35502988]\n",
            "updating  lamda for loss loss_bc2:   [0.5719659]\n",
            "updating  lamda for loss loss_ics_u:   [0.07300422]\n",
            "It: 20100| Loss: 1.059e-02|  loss_bc1: 7.034e-04|  loss_bc2: 1.385e-04| loss_ics_u: 1.388e-01|   Loss_res: 1.252e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1550e-01\n",
            "lambda_balance loss_bc2: 3.1283e-01\n",
            "lambda_balance loss_ics_u: 3.7166e-01\n",
            "updating  lamda for loss loss_bc1:   [0.28491274]\n",
            "updating  lamda for loss loss_bc2:   [0.65549064]\n",
            "updating  lamda for loss loss_ics_u:   [0.0595966]\n",
            "It: 20200| Loss: 8.610e-03|  loss_bc1: 2.315e-04|  loss_bc2: 8.243e-05| loss_ics_u: 1.383e-01|   Loss_res: 2.479e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1552e-01\n",
            "lambda_balance loss_bc2: 3.1286e-01\n",
            "lambda_balance loss_ics_u: 3.7162e-01\n",
            "updating  lamda for loss loss_bc1:   [0.24872947]\n",
            "updating  lamda for loss loss_bc2:   [0.68381166]\n",
            "updating  lamda for loss loss_ics_u:   [0.0674589]\n",
            "It: 20300| Loss: 8.525e-03|  loss_bc1: 4.345e-04|  loss_bc2: 8.912e-05| loss_ics_u: 1.221e-01|   Loss_res: 1.181e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1554e-01\n",
            "lambda_balance loss_bc2: 3.1289e-01\n",
            "lambda_balance loss_ics_u: 3.7157e-01\n",
            "updating  lamda for loss loss_bc1:   [0.58041424]\n",
            "updating  lamda for loss loss_bc2:   [0.37251917]\n",
            "updating  lamda for loss loss_ics_u:   [0.04706657]\n",
            "It: 20400| Loss: 6.705e-03|  loss_bc1: 2.016e-04|  loss_bc2: 7.905e-05| loss_ics_u: 1.373e-01|   Loss_res: 9.710e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1555e-01\n",
            "lambda_balance loss_bc2: 3.1293e-01\n",
            "lambda_balance loss_ics_u: 3.7152e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4881695]\n",
            "updating  lamda for loss loss_bc2:   [0.47253472]\n",
            "updating  lamda for loss loss_ics_u:   [0.03929583]\n",
            "It: 20500| Loss: 4.982e-03|  loss_bc1: 1.597e-04|  loss_bc2: 5.099e-05| loss_ics_u: 1.158e-01|   Loss_res: 3.288e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1557e-01\n",
            "lambda_balance loss_bc2: 3.1295e-01\n",
            "lambda_balance loss_ics_u: 3.7147e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14119269]\n",
            "updating  lamda for loss loss_bc2:   [0.78222597]\n",
            "updating  lamda for loss loss_ics_u:   [0.07658134]\n",
            "It: 20600| Loss: 9.627e-03|  loss_bc1: 6.754e-04|  loss_bc2: 4.386e-04| loss_ics_u: 1.149e-01|   Loss_res: 3.875e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1559e-01\n",
            "lambda_balance loss_bc2: 3.1298e-01\n",
            "lambda_balance loss_ics_u: 3.7142e-01\n",
            "updating  lamda for loss loss_bc1:   [0.27429572]\n",
            "updating  lamda for loss loss_bc2:   [0.6434274]\n",
            "updating  lamda for loss loss_ics_u:   [0.0822769]\n",
            "It: 20700| Loss: 1.105e-02|  loss_bc1: 6.985e-04|  loss_bc2: 1.109e-04| loss_ics_u: 1.290e-01|   Loss_res: 1.710e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1561e-01\n",
            "lambda_balance loss_bc2: 3.1301e-01\n",
            "lambda_balance loss_ics_u: 3.7138e-01\n",
            "updating  lamda for loss loss_bc1:   [0.23422743]\n",
            "updating  lamda for loss loss_bc2:   [0.62029797]\n",
            "updating  lamda for loss loss_ics_u:   [0.14547458]\n",
            "It: 20800| Loss: 2.077e-02|  loss_bc1: 2.938e-03|  loss_bc2: 2.634e-04| loss_ics_u: 1.350e-01|   Loss_res: 2.815e-04 , Time: 0.09\n",
            "lambda_balance loss_bc1: 3.1563e-01\n",
            "lambda_balance loss_bc2: 3.1304e-01\n",
            "lambda_balance loss_ics_u: 3.7133e-01\n",
            "updating  lamda for loss loss_bc1:   [0.40868282]\n",
            "updating  lamda for loss loss_bc2:   [0.4479051]\n",
            "updating  lamda for loss loss_ics_u:   [0.14341214]\n",
            "It: 20900| Loss: 1.962e-02|  loss_bc1: 8.286e-04|  loss_bc2: 5.366e-04| loss_ics_u: 1.248e-01|   Loss_res: 1.141e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1565e-01\n",
            "lambda_balance loss_bc2: 3.1307e-01\n",
            "lambda_balance loss_ics_u: 3.7128e-01\n",
            "updating  lamda for loss loss_bc1:   [0.514347]\n",
            "updating  lamda for loss loss_bc2:   [0.30960467]\n",
            "updating  lamda for loss loss_ics_u:   [0.17604826]\n",
            "It: 21000| Loss: 2.148e-02|  loss_bc1: 6.983e-04|  loss_bc2: 2.516e-04| loss_ics_u: 1.168e-01|   Loss_res: 4.758e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1566e-01\n",
            "lambda_balance loss_bc2: 3.1310e-01\n",
            "lambda_balance loss_ics_u: 3.7124e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4015064]\n",
            "updating  lamda for loss loss_bc2:   [0.54536545]\n",
            "updating  lamda for loss loss_ics_u:   [0.05312822]\n",
            "It: 21100| Loss: 6.532e-03|  loss_bc1: 2.732e-04|  loss_bc2: 1.038e-04| loss_ics_u: 1.156e-01|   Loss_res: 2.247e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1568e-01\n",
            "lambda_balance loss_bc2: 3.1313e-01\n",
            "lambda_balance loss_ics_u: 3.7119e-01\n",
            "updating  lamda for loss loss_bc1:   [0.213999]\n",
            "updating  lamda for loss loss_bc2:   [0.7513198]\n",
            "updating  lamda for loss loss_ics_u:   [0.03468116]\n",
            "It: 21200| Loss: 5.019e-03|  loss_bc1: 3.842e-04|  loss_bc2: 7.828e-05| loss_ics_u: 1.352e-01|   Loss_res: 1.885e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1570e-01\n",
            "lambda_balance loss_bc2: 3.1316e-01\n",
            "lambda_balance loss_ics_u: 3.7115e-01\n",
            "updating  lamda for loss loss_bc1:   [0.26480162]\n",
            "updating  lamda for loss loss_bc2:   [0.64533335]\n",
            "updating  lamda for loss loss_ics_u:   [0.08986508]\n",
            "It: 21300| Loss: 1.286e-02|  loss_bc1: 3.769e-04|  loss_bc2: 8.315e-05| loss_ics_u: 1.296e-01|   Loss_res: 1.056e-03 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1571e-01\n",
            "lambda_balance loss_bc2: 3.1319e-01\n",
            "lambda_balance loss_ics_u: 3.7110e-01\n",
            "updating  lamda for loss loss_bc1:   [0.34085783]\n",
            "updating  lamda for loss loss_bc2:   [0.5890477]\n",
            "updating  lamda for loss loss_ics_u:   [0.07009453]\n",
            "It: 21400| Loss: 9.261e-03|  loss_bc1: 1.598e-04|  loss_bc2: 1.119e-04| loss_ics_u: 1.286e-01|   Loss_res: 1.286e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1573e-01\n",
            "lambda_balance loss_bc2: 3.1321e-01\n",
            "lambda_balance loss_ics_u: 3.7106e-01\n",
            "updating  lamda for loss loss_bc1:   [0.12027439]\n",
            "updating  lamda for loss loss_bc2:   [0.8084777]\n",
            "updating  lamda for loss loss_ics_u:   [0.07124791]\n",
            "It: 21500| Loss: 9.439e-03|  loss_bc1: 5.749e-04|  loss_bc2: 4.696e-05| loss_ics_u: 1.133e-01|   Loss_res: 1.256e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1575e-01\n",
            "lambda_balance loss_bc2: 3.1324e-01\n",
            "lambda_balance loss_ics_u: 3.7101e-01\n",
            "updating  lamda for loss loss_bc1:   [0.09148984]\n",
            "updating  lamda for loss loss_bc2:   [0.87856746]\n",
            "updating  lamda for loss loss_ics_u:   [0.02994265]\n",
            "It: 21600| Loss: 4.468e-03|  loss_bc1: 3.746e-04|  loss_bc2: 6.286e-05| loss_ics_u: 1.403e-01|   Loss_res: 1.778e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1576e-01\n",
            "lambda_balance loss_bc2: 3.1327e-01\n",
            "lambda_balance loss_ics_u: 3.7097e-01\n",
            "updating  lamda for loss loss_bc1:   [0.28409666]\n",
            "updating  lamda for loss loss_bc2:   [0.6549328]\n",
            "updating  lamda for loss loss_ics_u:   [0.0609705]\n",
            "It: 21700| Loss: 8.106e-03|  loss_bc1: 4.593e-04|  loss_bc2: 1.048e-04| loss_ics_u: 1.188e-01|   Loss_res: 6.612e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1578e-01\n",
            "lambda_balance loss_bc2: 3.1330e-01\n",
            "lambda_balance loss_ics_u: 3.7092e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3538824]\n",
            "updating  lamda for loss loss_bc2:   [0.49311036]\n",
            "updating  lamda for loss loss_ics_u:   [0.1530072]\n",
            "It: 21800| Loss: 1.988e-02|  loss_bc1: 4.937e-04|  loss_bc2: 1.181e-03| loss_ics_u: 1.231e-01|   Loss_res: 2.907e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1580e-01\n",
            "lambda_balance loss_bc2: 3.1333e-01\n",
            "lambda_balance loss_ics_u: 3.7087e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5712417]\n",
            "updating  lamda for loss loss_bc2:   [0.2307143]\n",
            "updating  lamda for loss loss_ics_u:   [0.19804403]\n",
            "It: 21900| Loss: 2.672e-02|  loss_bc1: 3.475e-03|  loss_bc2: 1.975e-04| loss_ics_u: 1.233e-01|   Loss_res: 2.651e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1582e-01\n",
            "lambda_balance loss_bc2: 3.1335e-01\n",
            "lambda_balance loss_ics_u: 3.7083e-01\n",
            "updating  lamda for loss loss_bc1:   [0.45557913]\n",
            "updating  lamda for loss loss_bc2:   [0.4220981]\n",
            "updating  lamda for loss loss_ics_u:   [0.12232272]\n",
            "It: 22000| Loss: 1.480e-02|  loss_bc1: 3.462e-04|  loss_bc2: 6.689e-04| loss_ics_u: 1.118e-01|   Loss_res: 6.854e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1583e-01\n",
            "lambda_balance loss_bc2: 3.1338e-01\n",
            "lambda_balance loss_ics_u: 3.7078e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4147058]\n",
            "updating  lamda for loss loss_bc2:   [0.46091485]\n",
            "updating  lamda for loss loss_ics_u:   [0.12437935]\n",
            "It: 22100| Loss: 1.868e-02|  loss_bc1: 5.346e-04|  loss_bc2: 1.932e-04| loss_ics_u: 1.464e-01|   Loss_res: 1.611e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1585e-01\n",
            "lambda_balance loss_bc2: 3.1341e-01\n",
            "lambda_balance loss_ics_u: 3.7074e-01\n",
            "updating  lamda for loss loss_bc1:   [0.10183411]\n",
            "updating  lamda for loss loss_bc2:   [0.79192656]\n",
            "updating  lamda for loss loss_ics_u:   [0.10623934]\n",
            "It: 22200| Loss: 1.430e-02|  loss_bc1: 1.660e-03|  loss_bc2: 6.635e-05| loss_ics_u: 1.298e-01|   Loss_res: 2.896e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1587e-01\n",
            "lambda_balance loss_bc2: 3.1343e-01\n",
            "lambda_balance loss_ics_u: 3.7070e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2699886]\n",
            "updating  lamda for loss loss_bc2:   [0.63174254]\n",
            "updating  lamda for loss loss_ics_u:   [0.09826887]\n",
            "It: 22300| Loss: 1.372e-02|  loss_bc1: 1.736e-03|  loss_bc2: 2.152e-04| loss_ics_u: 1.285e-01|   Loss_res: 4.903e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1588e-01\n",
            "lambda_balance loss_bc2: 3.1346e-01\n",
            "lambda_balance loss_ics_u: 3.7065e-01\n",
            "updating  lamda for loss loss_bc1:   [0.27008593]\n",
            "updating  lamda for loss loss_bc2:   [0.5073735]\n",
            "updating  lamda for loss loss_ics_u:   [0.22254059]\n",
            "It: 22400| Loss: 3.040e-02|  loss_bc1: 1.695e-03|  loss_bc2: 3.519e-04| loss_ics_u: 1.329e-01|   Loss_res: 1.936e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1590e-01\n",
            "lambda_balance loss_bc2: 3.1349e-01\n",
            "lambda_balance loss_ics_u: 3.7061e-01\n",
            "updating  lamda for loss loss_bc1:   [0.36602768]\n",
            "updating  lamda for loss loss_bc2:   [0.40128168]\n",
            "updating  lamda for loss loss_ics_u:   [0.23269066]\n",
            "It: 22500| Loss: 3.097e-02|  loss_bc1: 1.852e-03|  loss_bc2: 9.266e-04| loss_ics_u: 1.281e-01|   Loss_res: 1.107e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1592e-01\n",
            "lambda_balance loss_bc2: 3.1351e-01\n",
            "lambda_balance loss_ics_u: 3.7057e-01\n",
            "updating  lamda for loss loss_bc1:   [0.48458764]\n",
            "updating  lamda for loss loss_bc2:   [0.37687004]\n",
            "updating  lamda for loss loss_ics_u:   [0.13854226]\n",
            "It: 22600| Loss: 1.751e-02|  loss_bc1: 2.618e-04|  loss_bc2: 2.258e-04| loss_ics_u: 1.221e-01|   Loss_res: 3.776e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1593e-01\n",
            "lambda_balance loss_bc2: 3.1354e-01\n",
            "lambda_balance loss_ics_u: 3.7053e-01\n",
            "updating  lamda for loss loss_bc1:   [0.1763228]\n",
            "updating  lamda for loss loss_bc2:   [0.72038054]\n",
            "updating  lamda for loss loss_ics_u:   [0.10329667]\n",
            "It: 22700| Loss: 1.123e-02|  loss_bc1: 3.257e-04|  loss_bc2: 1.713e-04| loss_ics_u: 1.025e-01|   Loss_res: 4.649e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1595e-01\n",
            "lambda_balance loss_bc2: 3.1356e-01\n",
            "lambda_balance loss_ics_u: 3.7049e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5285588]\n",
            "updating  lamda for loss loss_bc2:   [0.42759022]\n",
            "updating  lamda for loss loss_ics_u:   [0.04385099]\n",
            "It: 22800| Loss: 6.130e-03|  loss_bc1: 1.056e-04|  loss_bc2: 1.533e-04| loss_ics_u: 1.272e-01|   Loss_res: 4.304e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1596e-01\n",
            "lambda_balance loss_bc2: 3.1359e-01\n",
            "lambda_balance loss_ics_u: 3.7045e-01\n",
            "updating  lamda for loss loss_bc1:   [0.44859606]\n",
            "updating  lamda for loss loss_bc2:   [0.5106707]\n",
            "updating  lamda for loss loss_ics_u:   [0.04073327]\n",
            "It: 22900| Loss: 5.685e-03|  loss_bc1: 6.256e-05|  loss_bc2: 5.654e-05| loss_ics_u: 1.296e-01|   Loss_res: 3.488e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1598e-01\n",
            "lambda_balance loss_bc2: 3.1361e-01\n",
            "lambda_balance loss_ics_u: 3.7041e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22399767]\n",
            "updating  lamda for loss loss_bc2:   [0.76040286]\n",
            "updating  lamda for loss loss_ics_u:   [0.01559949]\n",
            "It: 23000| Loss: 2.297e-03|  loss_bc1: 1.346e-04|  loss_bc2: 2.288e-05| loss_ics_u: 1.302e-01|   Loss_res: 2.186e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1599e-01\n",
            "lambda_balance loss_bc2: 3.1364e-01\n",
            "lambda_balance loss_ics_u: 3.7037e-01\n",
            "updating  lamda for loss loss_bc1:   [0.24962515]\n",
            "updating  lamda for loss loss_bc2:   [0.6893261]\n",
            "updating  lamda for loss loss_ics_u:   [0.06104869]\n",
            "It: 23100| Loss: 7.304e-03|  loss_bc1: 4.819e-04|  loss_bc2: 3.138e-05| loss_ics_u: 1.136e-01|   Loss_res: 2.270e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1601e-01\n",
            "lambda_balance loss_bc2: 3.1366e-01\n",
            "lambda_balance loss_ics_u: 3.7033e-01\n",
            "updating  lamda for loss loss_bc1:   [0.11358591]\n",
            "updating  lamda for loss loss_bc2:   [0.85995466]\n",
            "updating  lamda for loss loss_ics_u:   [0.02645949]\n",
            "It: 23200| Loss: 5.104e-03|  loss_bc1: 3.815e-04|  loss_bc2: 2.830e-05| loss_ics_u: 1.342e-01|   Loss_res: 1.486e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1602e-01\n",
            "lambda_balance loss_bc2: 3.1369e-01\n",
            "lambda_balance loss_ics_u: 3.7029e-01\n",
            "updating  lamda for loss loss_bc1:   [0.13299643]\n",
            "updating  lamda for loss loss_bc2:   [0.80399084]\n",
            "updating  lamda for loss loss_ics_u:   [0.06301272]\n",
            "It: 23300| Loss: 8.156e-03|  loss_bc1: 7.961e-04|  loss_bc2: 5.982e-05| loss_ics_u: 1.253e-01|   Loss_res: 1.031e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1604e-01\n",
            "lambda_balance loss_bc2: 3.1371e-01\n",
            "lambda_balance loss_ics_u: 3.7025e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17460735]\n",
            "updating  lamda for loss loss_bc2:   [0.7648612]\n",
            "updating  lamda for loss loss_ics_u:   [0.06053144]\n",
            "It: 23400| Loss: 7.984e-03|  loss_bc1: 5.043e-04|  loss_bc2: 2.901e-05| loss_ics_u: 1.271e-01|   Loss_res: 1.776e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1605e-01\n",
            "lambda_balance loss_bc2: 3.1373e-01\n",
            "lambda_balance loss_ics_u: 3.7022e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2187997]\n",
            "updating  lamda for loss loss_bc2:   [0.7413791]\n",
            "updating  lamda for loss loss_ics_u:   [0.03982126]\n",
            "It: 23500| Loss: 5.088e-03|  loss_bc1: 1.639e-04|  loss_bc2: 1.140e-04| loss_ics_u: 1.208e-01|   Loss_res: 1.592e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1606e-01\n",
            "lambda_balance loss_bc2: 3.1376e-01\n",
            "lambda_balance loss_ics_u: 3.7018e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2927736]\n",
            "updating  lamda for loss loss_bc2:   [0.64922804]\n",
            "updating  lamda for loss loss_ics_u:   [0.05799837]\n",
            "It: 23600| Loss: 7.717e-03|  loss_bc1: 1.513e-04|  loss_bc2: 7.908e-05| loss_ics_u: 1.295e-01|   Loss_res: 1.127e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1608e-01\n",
            "lambda_balance loss_bc2: 3.1378e-01\n",
            "lambda_balance loss_ics_u: 3.7014e-01\n",
            "updating  lamda for loss loss_bc1:   [0.13569775]\n",
            "updating  lamda for loss loss_bc2:   [0.8490935]\n",
            "updating  lamda for loss loss_ics_u:   [0.01520873]\n",
            "It: 23700| Loss: 2.068e-03|  loss_bc1: 9.620e-05|  loss_bc2: 1.173e-05| loss_ics_u: 1.274e-01|   Loss_res: 1.082e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1609e-01\n",
            "lambda_balance loss_bc2: 3.1380e-01\n",
            "lambda_balance loss_ics_u: 3.7011e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4780443]\n",
            "updating  lamda for loss loss_bc2:   [0.4819826]\n",
            "updating  lamda for loss loss_ics_u:   [0.03997307]\n",
            "It: 23800| Loss: 4.968e-03|  loss_bc1: 8.435e-05|  loss_bc2: 3.496e-05| loss_ics_u: 1.174e-01|   Loss_res: 2.180e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1610e-01\n",
            "lambda_balance loss_bc2: 3.1383e-01\n",
            "lambda_balance loss_ics_u: 3.7007e-01\n",
            "updating  lamda for loss loss_bc1:   [0.08313446]\n",
            "updating  lamda for loss loss_bc2:   [0.8886701]\n",
            "updating  lamda for loss loss_ics_u:   [0.02819549]\n",
            "It: 23900| Loss: 3.463e-03|  loss_bc1: 4.122e-04|  loss_bc2: 2.077e-05| loss_ics_u: 1.155e-01|   Loss_res: 1.538e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1612e-01\n",
            "lambda_balance loss_bc2: 3.1385e-01\n",
            "lambda_balance loss_ics_u: 3.7003e-01\n",
            "updating  lamda for loss loss_bc1:   [0.30875218]\n",
            "updating  lamda for loss loss_bc2:   [0.625132]\n",
            "updating  lamda for loss loss_ics_u:   [0.0661159]\n",
            "It: 24000| Loss: 8.091e-03|  loss_bc1: 2.599e-04|  loss_bc2: 1.283e-04| loss_ics_u: 1.182e-01|   Loss_res: 1.127e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1613e-01\n",
            "lambda_balance loss_bc2: 3.1387e-01\n",
            "lambda_balance loss_ics_u: 3.7000e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6676747]\n",
            "updating  lamda for loss loss_bc2:   [0.2889889]\n",
            "updating  lamda for loss loss_ics_u:   [0.04333637]\n",
            "It: 24100| Loss: 5.006e-03|  loss_bc1: 1.117e-04|  loss_bc2: 1.080e-04| loss_ics_u: 1.060e-01|   Loss_res: 3.069e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1615e-01\n",
            "lambda_balance loss_bc2: 3.1389e-01\n",
            "lambda_balance loss_ics_u: 3.6996e-01\n",
            "updating  lamda for loss loss_bc1:   [0.48077756]\n",
            "updating  lamda for loss loss_bc2:   [0.4741057]\n",
            "updating  lamda for loss loss_ics_u:   [0.04511674]\n",
            "It: 24200| Loss: 6.579e-03|  loss_bc1: 1.031e-04|  loss_bc2: 1.463e-04| loss_ics_u: 1.352e-01|   Loss_res: 3.616e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1616e-01\n",
            "lambda_balance loss_bc2: 3.1391e-01\n",
            "lambda_balance loss_ics_u: 3.6993e-01\n",
            "updating  lamda for loss loss_bc1:   [0.55847245]\n",
            "updating  lamda for loss loss_bc2:   [0.4087395]\n",
            "updating  lamda for loss loss_ics_u:   [0.03278811]\n",
            "It: 24300| Loss: 4.951e-03|  loss_bc1: 6.314e-05|  loss_bc2: 4.180e-05| loss_ics_u: 1.465e-01|   Loss_res: 9.581e-05 , Time: 0.06\n",
            "lambda_balance loss_bc1: 3.1617e-01\n",
            "lambda_balance loss_bc2: 3.1394e-01\n",
            "lambda_balance loss_ics_u: 3.6989e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5591162]\n",
            "updating  lamda for loss loss_bc2:   [0.36633095]\n",
            "updating  lamda for loss loss_ics_u:   [0.0745529]\n",
            "It: 24400| Loss: 9.761e-03|  loss_bc1: 1.362e-04|  loss_bc2: 9.546e-05| loss_ics_u: 1.202e-01|   Loss_res: 6.859e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1618e-01\n",
            "lambda_balance loss_bc2: 3.1396e-01\n",
            "lambda_balance loss_ics_u: 3.6986e-01\n",
            "updating  lamda for loss loss_bc1:   [0.41435012]\n",
            "updating  lamda for loss loss_bc2:   [0.49420696]\n",
            "updating  lamda for loss loss_ics_u:   [0.09144291]\n",
            "It: 24500| Loss: 1.354e-02|  loss_bc1: 1.012e-03|  loss_bc2: 1.408e-03| loss_ics_u: 1.341e-01|   Loss_res: 1.671e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1620e-01\n",
            "lambda_balance loss_bc2: 3.1398e-01\n",
            "lambda_balance loss_ics_u: 3.6982e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3755353]\n",
            "updating  lamda for loss loss_bc2:   [0.56568635]\n",
            "updating  lamda for loss loss_ics_u:   [0.05877835]\n",
            "It: 24600| Loss: 8.445e-03|  loss_bc1: 3.213e-04|  loss_bc2: 4.665e-05| loss_ics_u: 1.380e-01|   Loss_res: 1.886e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1621e-01\n",
            "lambda_balance loss_bc2: 3.1400e-01\n",
            "lambda_balance loss_ics_u: 3.6979e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22299789]\n",
            "updating  lamda for loss loss_bc2:   [0.70587623]\n",
            "updating  lamda for loss loss_ics_u:   [0.0711259]\n",
            "It: 24700| Loss: 9.643e-03|  loss_bc1: 1.615e-03|  loss_bc2: 9.252e-05| loss_ics_u: 1.286e-01|   Loss_res: 7.192e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1622e-01\n",
            "lambda_balance loss_bc2: 3.1402e-01\n",
            "lambda_balance loss_ics_u: 3.6976e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17504326]\n",
            "updating  lamda for loss loss_bc2:   [0.7273421]\n",
            "updating  lamda for loss loss_ics_u:   [0.09761461]\n",
            "It: 24800| Loss: 1.173e-02|  loss_bc1: 5.516e-04|  loss_bc2: 1.613e-04| loss_ics_u: 1.162e-01|   Loss_res: 1.788e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1624e-01\n",
            "lambda_balance loss_bc2: 3.1404e-01\n",
            "lambda_balance loss_ics_u: 3.6972e-01\n",
            "updating  lamda for loss loss_bc1:   [0.08829231]\n",
            "updating  lamda for loss loss_bc2:   [0.8851227]\n",
            "updating  lamda for loss loss_ics_u:   [0.02658493]\n",
            "It: 24900| Loss: 3.926e-03|  loss_bc1: 5.513e-04|  loss_bc2: 2.200e-05| loss_ics_u: 1.352e-01|   Loss_res: 2.640e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1625e-01\n",
            "lambda_balance loss_bc2: 3.1407e-01\n",
            "lambda_balance loss_ics_u: 3.6969e-01\n",
            "updating  lamda for loss loss_bc1:   [0.18743785]\n",
            "updating  lamda for loss loss_bc2:   [0.72113913]\n",
            "updating  lamda for loss loss_ics_u:   [0.09142295]\n",
            "It: 25000| Loss: 1.125e-02|  loss_bc1: 1.305e-03|  loss_bc2: 6.010e-05| loss_ics_u: 1.182e-01|   Loss_res: 1.637e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1626e-01\n",
            "lambda_balance loss_bc2: 3.1409e-01\n",
            "lambda_balance loss_ics_u: 3.6965e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17373528]\n",
            "updating  lamda for loss loss_bc2:   [0.7468279]\n",
            "updating  lamda for loss loss_ics_u:   [0.07943677]\n",
            "It: 25100| Loss: 1.031e-02|  loss_bc1: 3.032e-04|  loss_bc2: 3.309e-05| loss_ics_u: 1.240e-01|   Loss_res: 3.836e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1628e-01\n",
            "lambda_balance loss_bc2: 3.1411e-01\n",
            "lambda_balance loss_ics_u: 3.6962e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14543462]\n",
            "updating  lamda for loss loss_bc2:   [0.7907321]\n",
            "updating  lamda for loss loss_ics_u:   [0.06383327]\n",
            "It: 25200| Loss: 1.072e-02|  loss_bc1: 1.322e-03|  loss_bc2: 2.030e-05| loss_ics_u: 1.591e-01|   Loss_res: 3.536e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1629e-01\n",
            "lambda_balance loss_bc2: 3.1413e-01\n",
            "lambda_balance loss_ics_u: 3.6958e-01\n",
            "updating  lamda for loss loss_bc1:   [0.11450295]\n",
            "updating  lamda for loss loss_bc2:   [0.8266074]\n",
            "updating  lamda for loss loss_ics_u:   [0.0588896]\n",
            "It: 25300| Loss: 7.170e-03|  loss_bc1: 9.673e-04|  loss_bc2: 6.590e-05| loss_ics_u: 1.163e-01|   Loss_res: 1.558e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1630e-01\n",
            "lambda_balance loss_bc2: 3.1415e-01\n",
            "lambda_balance loss_ics_u: 3.6955e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17099215]\n",
            "updating  lamda for loss loss_bc2:   [0.75645494]\n",
            "updating  lamda for loss loss_ics_u:   [0.07255295]\n",
            "It: 25400| Loss: 9.009e-03|  loss_bc1: 6.500e-04|  loss_bc2: 4.652e-05| loss_ics_u: 1.210e-01|   Loss_res: 8.287e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1632e-01\n",
            "lambda_balance loss_bc2: 3.1417e-01\n",
            "lambda_balance loss_ics_u: 3.6952e-01\n",
            "updating  lamda for loss loss_bc1:   [0.09273799]\n",
            "updating  lamda for loss loss_bc2:   [0.8706315]\n",
            "updating  lamda for loss loss_ics_u:   [0.03663046]\n",
            "It: 25500| Loss: 4.244e-03|  loss_bc1: 1.338e-03|  loss_bc2: 5.588e-05| loss_ics_u: 1.065e-01|   Loss_res: 1.704e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1633e-01\n",
            "lambda_balance loss_bc2: 3.1419e-01\n",
            "lambda_balance loss_ics_u: 3.6948e-01\n",
            "updating  lamda for loss loss_bc1:   [0.09810089]\n",
            "updating  lamda for loss loss_bc2:   [0.8462905]\n",
            "updating  lamda for loss loss_ics_u:   [0.05560858]\n",
            "It: 25600| Loss: 7.055e-03|  loss_bc1: 8.937e-04|  loss_bc2: 2.641e-05| loss_ics_u: 1.214e-01|   Loss_res: 1.936e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1634e-01\n",
            "lambda_balance loss_bc2: 3.1421e-01\n",
            "lambda_balance loss_ics_u: 3.6945e-01\n",
            "updating  lamda for loss loss_bc1:   [0.16567257]\n",
            "updating  lamda for loss loss_bc2:   [0.7748089]\n",
            "updating  lamda for loss loss_ics_u:   [0.05951854]\n",
            "It: 25700| Loss: 6.855e-03|  loss_bc1: 5.011e-04|  loss_bc2: 5.416e-05| loss_ics_u: 1.116e-01|   Loss_res: 8.860e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1635e-01\n",
            "lambda_balance loss_bc2: 3.1423e-01\n",
            "lambda_balance loss_ics_u: 3.6942e-01\n",
            "updating  lamda for loss loss_bc1:   [0.11909821]\n",
            "updating  lamda for loss loss_bc2:   [0.8215566]\n",
            "updating  lamda for loss loss_ics_u:   [0.05934512]\n",
            "It: 25800| Loss: 5.930e-03|  loss_bc1: 9.930e-04|  loss_bc2: 1.617e-05| loss_ics_u: 9.418e-02|   Loss_res: 2.090e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1637e-01\n",
            "lambda_balance loss_bc2: 3.1425e-01\n",
            "lambda_balance loss_ics_u: 3.6939e-01\n",
            "updating  lamda for loss loss_bc1:   [0.30815992]\n",
            "updating  lamda for loss loss_bc2:   [0.5263442]\n",
            "updating  lamda for loss loss_ics_u:   [0.16549586]\n",
            "It: 25900| Loss: 1.971e-02|  loss_bc1: 3.942e-04|  loss_bc2: 3.891e-04| loss_ics_u: 1.166e-01|   Loss_res: 8.412e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1638e-01\n",
            "lambda_balance loss_bc2: 3.1427e-01\n",
            "lambda_balance loss_ics_u: 3.6935e-01\n",
            "updating  lamda for loss loss_bc1:   [0.45782372]\n",
            "updating  lamda for loss loss_bc2:   [0.2954077]\n",
            "updating  lamda for loss loss_ics_u:   [0.24676852]\n",
            "It: 26000| Loss: 3.125e-02|  loss_bc1: 2.014e-03|  loss_bc2: 3.696e-03| loss_ics_u: 1.178e-01|   Loss_res: 1.567e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1639e-01\n",
            "lambda_balance loss_bc2: 3.1429e-01\n",
            "lambda_balance loss_ics_u: 3.6932e-01\n",
            "updating  lamda for loss loss_bc1:   [0.42025757]\n",
            "updating  lamda for loss loss_bc2:   [0.26452172]\n",
            "updating  lamda for loss loss_ics_u:   [0.31522068]\n",
            "It: 26100| Loss: 4.284e-02|  loss_bc1: 1.001e-03|  loss_bc2: 1.540e-03| loss_ics_u: 1.320e-01|   Loss_res: 4.033e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1640e-01\n",
            "lambda_balance loss_bc2: 3.1431e-01\n",
            "lambda_balance loss_ics_u: 3.6929e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5022556]\n",
            "updating  lamda for loss loss_bc2:   [0.31682748]\n",
            "updating  lamda for loss loss_ics_u:   [0.18091685]\n",
            "It: 26200| Loss: 2.350e-02|  loss_bc1: 3.603e-04|  loss_bc2: 4.378e-04| loss_ics_u: 1.275e-01|   Loss_res: 1.140e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1642e-01\n",
            "lambda_balance loss_bc2: 3.1433e-01\n",
            "lambda_balance loss_ics_u: 3.6925e-01\n",
            "updating  lamda for loss loss_bc1:   [0.60717064]\n",
            "updating  lamda for loss loss_bc2:   [0.26330778]\n",
            "updating  lamda for loss loss_ics_u:   [0.1295216]\n",
            "It: 26300| Loss: 1.897e-02|  loss_bc1: 2.045e-04|  loss_bc2: 4.448e-04| loss_ics_u: 1.440e-01|   Loss_res: 8.157e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1643e-01\n",
            "lambda_balance loss_bc2: 3.1435e-01\n",
            "lambda_balance loss_ics_u: 3.6922e-01\n",
            "updating  lamda for loss loss_bc1:   [0.46990985]\n",
            "updating  lamda for loss loss_bc2:   [0.4181539]\n",
            "updating  lamda for loss loss_ics_u:   [0.11193625]\n",
            "It: 26400| Loss: 1.562e-02|  loss_bc1: 7.686e-04|  loss_bc2: 1.263e-04| loss_ics_u: 1.333e-01|   Loss_res: 2.871e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1644e-01\n",
            "lambda_balance loss_bc2: 3.1437e-01\n",
            "lambda_balance loss_ics_u: 3.6919e-01\n",
            "updating  lamda for loss loss_bc1:   [0.35205257]\n",
            "updating  lamda for loss loss_bc2:   [0.4935817]\n",
            "updating  lamda for loss loss_ics_u:   [0.15436573]\n",
            "It: 26500| Loss: 2.050e-02|  loss_bc1: 4.188e-04|  loss_bc2: 4.498e-04| loss_ics_u: 1.279e-01|   Loss_res: 3.830e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1645e-01\n",
            "lambda_balance loss_bc2: 3.1439e-01\n",
            "lambda_balance loss_ics_u: 3.6916e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4344143]\n",
            "updating  lamda for loss loss_bc2:   [0.4645015]\n",
            "updating  lamda for loss loss_ics_u:   [0.1010842]\n",
            "It: 26600| Loss: 1.179e-02|  loss_bc1: 1.563e-04|  loss_bc2: 2.679e-04| loss_ics_u: 1.136e-01|   Loss_res: 1.173e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1646e-01\n",
            "lambda_balance loss_bc2: 3.1441e-01\n",
            "lambda_balance loss_ics_u: 3.6913e-01\n",
            "updating  lamda for loss loss_bc1:   [0.63637793]\n",
            "updating  lamda for loss loss_bc2:   [0.30052787]\n",
            "updating  lamda for loss loss_ics_u:   [0.06309421]\n",
            "It: 26700| Loss: 9.052e-03|  loss_bc1: 4.207e-04|  loss_bc2: 7.604e-05| loss_ics_u: 1.340e-01|   Loss_res: 3.058e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1647e-01\n",
            "lambda_balance loss_bc2: 3.1443e-01\n",
            "lambda_balance loss_ics_u: 3.6910e-01\n",
            "updating  lamda for loss loss_bc1:   [0.16412233]\n",
            "updating  lamda for loss loss_bc2:   [0.7819512]\n",
            "updating  lamda for loss loss_ics_u:   [0.05392649]\n",
            "It: 26800| Loss: 5.998e-03|  loss_bc1: 5.634e-04|  loss_bc2: 5.018e-05| loss_ics_u: 1.075e-01|   Loss_res: 7.126e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1649e-01\n",
            "lambda_balance loss_bc2: 3.1444e-01\n",
            "lambda_balance loss_ics_u: 3.6907e-01\n",
            "updating  lamda for loss loss_bc1:   [0.19244175]\n",
            "updating  lamda for loss loss_bc2:   [0.7509985]\n",
            "updating  lamda for loss loss_ics_u:   [0.05655981]\n",
            "It: 26900| Loss: 6.893e-03|  loss_bc1: 3.079e-04|  loss_bc2: 6.283e-05| loss_ics_u: 1.187e-01|   Loss_res: 7.247e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1650e-01\n",
            "lambda_balance loss_bc2: 3.1446e-01\n",
            "lambda_balance loss_ics_u: 3.6904e-01\n",
            "updating  lamda for loss loss_bc1:   [0.06407461]\n",
            "updating  lamda for loss loss_bc2:   [0.8962898]\n",
            "updating  lamda for loss loss_ics_u:   [0.03963555]\n",
            "It: 27000| Loss: 4.789e-03|  loss_bc1: 1.143e-03|  loss_bc2: 3.002e-05| loss_ics_u: 1.121e-01|   Loss_res: 2.440e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1651e-01\n",
            "lambda_balance loss_bc2: 3.1448e-01\n",
            "lambda_balance loss_ics_u: 3.6901e-01\n",
            "updating  lamda for loss loss_bc1:   [0.0444727]\n",
            "updating  lamda for loss loss_bc2:   [0.9301314]\n",
            "updating  lamda for loss loss_ics_u:   [0.02539592]\n",
            "It: 27100| Loss: 3.256e-03|  loss_bc1: 1.033e-03|  loss_bc2: 3.708e-05| loss_ics_u: 1.218e-01|   Loss_res: 8.156e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1652e-01\n",
            "lambda_balance loss_bc2: 3.1450e-01\n",
            "lambda_balance loss_ics_u: 3.6898e-01\n",
            "updating  lamda for loss loss_bc1:   [0.09527129]\n",
            "updating  lamda for loss loss_bc2:   [0.8547341]\n",
            "updating  lamda for loss loss_ics_u:   [0.04999457]\n",
            "It: 27200| Loss: 6.211e-03|  loss_bc1: 5.705e-04|  loss_bc2: 3.146e-05| loss_ics_u: 1.213e-01|   Loss_res: 6.728e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1653e-01\n",
            "lambda_balance loss_bc2: 3.1452e-01\n",
            "lambda_balance loss_ics_u: 3.6895e-01\n",
            "updating  lamda for loss loss_bc1:   [0.05810722]\n",
            "updating  lamda for loss loss_bc2:   [0.90662163]\n",
            "updating  lamda for loss loss_ics_u:   [0.0352712]\n",
            "It: 27300| Loss: 4.781e-03|  loss_bc1: 1.646e-03|  loss_bc2: 2.949e-05| loss_ics_u: 1.274e-01|   Loss_res: 1.640e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1654e-01\n",
            "lambda_balance loss_bc2: 3.1453e-01\n",
            "lambda_balance loss_ics_u: 3.6892e-01\n",
            "updating  lamda for loss loss_bc1:   [0.08146759]\n",
            "updating  lamda for loss loss_bc2:   [0.88804775]\n",
            "updating  lamda for loss loss_ics_u:   [0.03048461]\n",
            "It: 27400| Loss: 4.080e-03|  loss_bc1: 4.644e-04|  loss_bc2: 1.947e-05| loss_ics_u: 1.281e-01|   Loss_res: 1.191e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1655e-01\n",
            "lambda_balance loss_bc2: 3.1455e-01\n",
            "lambda_balance loss_ics_u: 3.6889e-01\n",
            "updating  lamda for loss loss_bc1:   [0.09660145]\n",
            "updating  lamda for loss loss_bc2:   [0.8843493]\n",
            "updating  lamda for loss loss_ics_u:   [0.01904926]\n",
            "It: 27500| Loss: 2.253e-03|  loss_bc1: 4.544e-04|  loss_bc2: 1.391e-05| loss_ics_u: 1.129e-01|   Loss_res: 4.650e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1657e-01\n",
            "lambda_balance loss_bc2: 3.1457e-01\n",
            "lambda_balance loss_ics_u: 3.6886e-01\n",
            "updating  lamda for loss loss_bc1:   [0.10077151]\n",
            "updating  lamda for loss loss_bc2:   [0.8539115]\n",
            "updating  lamda for loss loss_ics_u:   [0.04531695]\n",
            "It: 27600| Loss: 5.047e-03|  loss_bc1: 1.377e-03|  loss_bc2: 4.370e-05| loss_ics_u: 1.049e-01|   Loss_res: 1.151e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1658e-01\n",
            "lambda_balance loss_bc2: 3.1459e-01\n",
            "lambda_balance loss_ics_u: 3.6884e-01\n",
            "updating  lamda for loss loss_bc1:   [0.150053]\n",
            "updating  lamda for loss loss_bc2:   [0.7545963]\n",
            "updating  lamda for loss loss_ics_u:   [0.0953507]\n",
            "It: 27700| Loss: 1.347e-02|  loss_bc1: 5.203e-04|  loss_bc2: 1.567e-04| loss_ics_u: 1.150e-01|   Loss_res: 2.312e-03 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1659e-01\n",
            "lambda_balance loss_bc2: 3.1460e-01\n",
            "lambda_balance loss_ics_u: 3.6881e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17285994]\n",
            "updating  lamda for loss loss_bc2:   [0.7157652]\n",
            "updating  lamda for loss loss_ics_u:   [0.11137495]\n",
            "It: 27800| Loss: 1.362e-02|  loss_bc1: 1.166e-03|  loss_bc2: 3.708e-05| loss_ics_u: 1.197e-01|   Loss_res: 6.374e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1660e-01\n",
            "lambda_balance loss_bc2: 3.1462e-01\n",
            "lambda_balance loss_ics_u: 3.6878e-01\n",
            "updating  lamda for loss loss_bc1:   [0.11576513]\n",
            "updating  lamda for loss loss_bc2:   [0.8357718]\n",
            "updating  lamda for loss loss_ics_u:   [0.04846303]\n",
            "It: 27900| Loss: 6.597e-03|  loss_bc1: 7.755e-04|  loss_bc2: 5.387e-05| loss_ics_u: 1.324e-01|   Loss_res: 4.497e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1661e-01\n",
            "lambda_balance loss_bc2: 3.1464e-01\n",
            "lambda_balance loss_ics_u: 3.6875e-01\n",
            "updating  lamda for loss loss_bc1:   [0.16801715]\n",
            "updating  lamda for loss loss_bc2:   [0.7638244]\n",
            "updating  lamda for loss loss_ics_u:   [0.06815836]\n",
            "It: 28000| Loss: 8.021e-03|  loss_bc1: 7.097e-04|  loss_bc2: 7.099e-05| loss_ics_u: 1.137e-01|   Loss_res: 1.014e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1662e-01\n",
            "lambda_balance loss_bc2: 3.1466e-01\n",
            "lambda_balance loss_ics_u: 3.6872e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2783899]\n",
            "updating  lamda for loss loss_bc2:   [0.6467149]\n",
            "updating  lamda for loss loss_ics_u:   [0.07489516]\n",
            "It: 28100| Loss: 1.014e-02|  loss_bc1: 9.777e-04|  loss_bc2: 7.953e-05| loss_ics_u: 1.296e-01|   Loss_res: 1.174e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1663e-01\n",
            "lambda_balance loss_bc2: 3.1467e-01\n",
            "lambda_balance loss_ics_u: 3.6869e-01\n",
            "updating  lamda for loss loss_bc1:   [0.21689549]\n",
            "updating  lamda for loss loss_bc2:   [0.68380517]\n",
            "updating  lamda for loss loss_ics_u:   [0.09929942]\n",
            "It: 28200| Loss: 1.370e-02|  loss_bc1: 1.174e-03|  loss_bc2: 1.271e-04| loss_ics_u: 1.332e-01|   Loss_res: 1.267e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1664e-01\n",
            "lambda_balance loss_bc2: 3.1469e-01\n",
            "lambda_balance loss_ics_u: 3.6867e-01\n",
            "updating  lamda for loss loss_bc1:   [0.1290407]\n",
            "updating  lamda for loss loss_bc2:   [0.813198]\n",
            "updating  lamda for loss loss_ics_u:   [0.05776134]\n",
            "It: 28300| Loss: 7.285e-03|  loss_bc1: 8.036e-04|  loss_bc2: 4.099e-05| loss_ics_u: 1.228e-01|   Loss_res: 5.679e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1665e-01\n",
            "lambda_balance loss_bc2: 3.1471e-01\n",
            "lambda_balance loss_ics_u: 3.6864e-01\n",
            "updating  lamda for loss loss_bc1:   [0.11497956]\n",
            "updating  lamda for loss loss_bc2:   [0.74172884]\n",
            "updating  lamda for loss loss_ics_u:   [0.14329162]\n",
            "It: 28400| Loss: 1.919e-02|  loss_bc1: 1.723e-03|  loss_bc2: 1.084e-04| loss_ics_u: 1.310e-01|   Loss_res: 1.326e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1666e-01\n",
            "lambda_balance loss_bc2: 3.1472e-01\n",
            "lambda_balance loss_ics_u: 3.6861e-01\n",
            "updating  lamda for loss loss_bc1:   [0.08291116]\n",
            "updating  lamda for loss loss_bc2:   [0.8747244]\n",
            "updating  lamda for loss loss_ics_u:   [0.0423644]\n",
            "It: 28500| Loss: 5.478e-03|  loss_bc1: 8.164e-04|  loss_bc2: 2.623e-05| loss_ics_u: 1.243e-01|   Loss_res: 1.213e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1668e-01\n",
            "lambda_balance loss_bc2: 3.1474e-01\n",
            "lambda_balance loss_ics_u: 3.6858e-01\n",
            "updating  lamda for loss loss_bc1:   [0.15877108]\n",
            "updating  lamda for loss loss_bc2:   [0.7925514]\n",
            "updating  lamda for loss loss_ics_u:   [0.04867756]\n",
            "It: 28600| Loss: 6.677e-03|  loss_bc1: 4.528e-04|  loss_bc2: 1.200e-05| loss_ics_u: 1.344e-01|   Loss_res: 5.334e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1669e-01\n",
            "lambda_balance loss_bc2: 3.1476e-01\n",
            "lambda_balance loss_ics_u: 3.6856e-01\n",
            "updating  lamda for loss loss_bc1:   [0.30759326]\n",
            "updating  lamda for loss loss_bc2:   [0.6045917]\n",
            "updating  lamda for loss loss_ics_u:   [0.08781493]\n",
            "It: 28700| Loss: 1.092e-02|  loss_bc1: 3.476e-04|  loss_bc2: 1.312e-04| loss_ics_u: 1.208e-01|   Loss_res: 1.213e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1670e-01\n",
            "lambda_balance loss_bc2: 3.1477e-01\n",
            "lambda_balance loss_ics_u: 3.6853e-01\n",
            "updating  lamda for loss loss_bc1:   [0.58805376]\n",
            "updating  lamda for loss loss_bc2:   [0.3385949]\n",
            "updating  lamda for loss loss_ics_u:   [0.0733513]\n",
            "It: 28800| Loss: 1.076e-02|  loss_bc1: 2.741e-04|  loss_bc2: 6.496e-05| loss_ics_u: 1.402e-01|   Loss_res: 2.890e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1671e-01\n",
            "lambda_balance loss_bc2: 3.1479e-01\n",
            "lambda_balance loss_ics_u: 3.6851e-01\n",
            "updating  lamda for loss loss_bc1:   [0.12704973]\n",
            "updating  lamda for loss loss_bc2:   [0.8409633]\n",
            "updating  lamda for loss loss_ics_u:   [0.03198686]\n",
            "It: 28900| Loss: 4.194e-03|  loss_bc1: 5.795e-04|  loss_bc2: 1.821e-05| loss_ics_u: 1.264e-01|   Loss_res: 6.011e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1672e-01\n",
            "lambda_balance loss_bc2: 3.1480e-01\n",
            "lambda_balance loss_ics_u: 3.6848e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17878288]\n",
            "updating  lamda for loss loss_bc2:   [0.7771062]\n",
            "updating  lamda for loss loss_ics_u:   [0.04411095]\n",
            "It: 29000| Loss: 5.754e-03|  loss_bc1: 4.272e-04|  loss_bc2: 1.782e-05| loss_ics_u: 1.268e-01|   Loss_res: 7.148e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1672e-01\n",
            "lambda_balance loss_bc2: 3.1482e-01\n",
            "lambda_balance loss_ics_u: 3.6846e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14956157]\n",
            "updating  lamda for loss loss_bc2:   [0.81902575]\n",
            "updating  lamda for loss loss_ics_u:   [0.03141274]\n",
            "It: 29100| Loss: 4.534e-03|  loss_bc1: 2.599e-04|  loss_bc2: 3.136e-05| loss_ics_u: 1.395e-01|   Loss_res: 8.785e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1673e-01\n",
            "lambda_balance loss_bc2: 3.1483e-01\n",
            "lambda_balance loss_ics_u: 3.6843e-01\n",
            "updating  lamda for loss loss_bc1:   [0.33659223]\n",
            "updating  lamda for loss loss_bc2:   [0.6103801]\n",
            "updating  lamda for loss loss_ics_u:   [0.05302756]\n",
            "It: 29200| Loss: 6.811e-03|  loss_bc1: 2.369e-04|  loss_bc2: 2.753e-05| loss_ics_u: 1.254e-01|   Loss_res: 6.211e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1674e-01\n",
            "lambda_balance loss_bc2: 3.1485e-01\n",
            "lambda_balance loss_ics_u: 3.6841e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4571256]\n",
            "updating  lamda for loss loss_bc2:   [0.45395827]\n",
            "updating  lamda for loss loss_ics_u:   [0.08891617]\n",
            "It: 29300| Loss: 1.128e-02|  loss_bc1: 9.312e-04|  loss_bc2: 4.900e-05| loss_ics_u: 1.207e-01|   Loss_res: 1.001e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1675e-01\n",
            "lambda_balance loss_bc2: 3.1487e-01\n",
            "lambda_balance loss_ics_u: 3.6838e-01\n",
            "updating  lamda for loss loss_bc1:   [0.08684047]\n",
            "updating  lamda for loss loss_bc2:   [0.8715626]\n",
            "updating  lamda for loss loss_ics_u:   [0.04159693]\n",
            "It: 29400| Loss: 5.798e-03|  loss_bc1: 1.964e-04|  loss_bc2: 4.432e-05| loss_ics_u: 1.360e-01|   Loss_res: 8.382e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1676e-01\n",
            "lambda_balance loss_bc2: 3.1488e-01\n",
            "lambda_balance loss_ics_u: 3.6836e-01\n",
            "updating  lamda for loss loss_bc1:   [0.13870613]\n",
            "updating  lamda for loss loss_bc2:   [0.8336223]\n",
            "updating  lamda for loss loss_ics_u:   [0.0276716]\n",
            "It: 29500| Loss: 3.765e-03|  loss_bc1: 2.228e-04|  loss_bc2: 1.193e-05| loss_ics_u: 1.329e-01|   Loss_res: 4.586e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1677e-01\n",
            "lambda_balance loss_bc2: 3.1490e-01\n",
            "lambda_balance loss_ics_u: 3.6833e-01\n",
            "updating  lamda for loss loss_bc1:   [0.04831463]\n",
            "updating  lamda for loss loss_bc2:   [0.9467168]\n",
            "updating  lamda for loss loss_ics_u:   [0.00496852]\n",
            "It: 29600| Loss: 7.715e-04|  loss_bc1: 1.892e-04|  loss_bc2: 2.339e-05| loss_ics_u: 1.347e-01|   Loss_res: 7.110e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1678e-01\n",
            "lambda_balance loss_bc2: 3.1491e-01\n",
            "lambda_balance loss_ics_u: 3.6831e-01\n",
            "updating  lamda for loss loss_bc1:   [0.21139047]\n",
            "updating  lamda for loss loss_bc2:   [0.7702288]\n",
            "updating  lamda for loss loss_ics_u:   [0.01838076]\n",
            "It: 29700| Loss: 2.368e-03|  loss_bc1: 9.028e-05|  loss_bc2: 8.416e-06| loss_ics_u: 1.250e-01|   Loss_res: 4.592e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1679e-01\n",
            "lambda_balance loss_bc2: 3.1493e-01\n",
            "lambda_balance loss_ics_u: 3.6829e-01\n",
            "updating  lamda for loss loss_bc1:   [0.67460203]\n",
            "updating  lamda for loss loss_bc2:   [0.29104167]\n",
            "updating  lamda for loss loss_ics_u:   [0.03435634]\n",
            "It: 29800| Loss: 4.733e-03|  loss_bc1: 8.015e-05|  loss_bc2: 3.654e-05| loss_ics_u: 1.323e-01|   Loss_res: 1.233e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1680e-01\n",
            "lambda_balance loss_bc2: 3.1494e-01\n",
            "lambda_balance loss_ics_u: 3.6827e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6142268]\n",
            "updating  lamda for loss loss_bc2:   [0.36988774]\n",
            "updating  lamda for loss loss_ics_u:   [0.01588549]\n",
            "It: 29900| Loss: 2.302e-03|  loss_bc1: 4.796e-05|  loss_bc2: 2.917e-05| loss_ics_u: 1.319e-01|   Loss_res: 1.657e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1680e-01\n",
            "lambda_balance loss_bc2: 3.1495e-01\n",
            "lambda_balance loss_ics_u: 3.6824e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6086053]\n",
            "updating  lamda for loss loss_bc2:   [0.38119137]\n",
            "updating  lamda for loss loss_ics_u:   [0.01020329]\n",
            "It: 30000| Loss: 1.285e-03|  loss_bc1: 1.689e-05|  loss_bc2: 1.727e-05| loss_ics_u: 1.189e-01|   Loss_res: 5.453e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1681e-01\n",
            "lambda_balance loss_bc2: 3.1497e-01\n",
            "lambda_balance loss_ics_u: 3.6822e-01\n",
            "updating  lamda for loss loss_bc1:   [0.67489386]\n",
            "updating  lamda for loss loss_bc2:   [0.30950838]\n",
            "updating  lamda for loss loss_ics_u:   [0.01559771]\n",
            "It: 30100| Loss: 1.840e-03|  loss_bc1: 2.217e-05|  loss_bc2: 3.370e-05| loss_ics_u: 1.117e-01|   Loss_res: 7.169e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1682e-01\n",
            "lambda_balance loss_bc2: 3.1498e-01\n",
            "lambda_balance loss_ics_u: 3.6820e-01\n",
            "updating  lamda for loss loss_bc1:   [0.70199615]\n",
            "updating  lamda for loss loss_bc2:   [0.27736032]\n",
            "updating  lamda for loss loss_ics_u:   [0.02064358]\n",
            "It: 30200| Loss: 2.763e-03|  loss_bc1: 3.847e-05|  loss_bc2: 2.034e-05| loss_ics_u: 1.287e-01|   Loss_res: 7.287e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1683e-01\n",
            "lambda_balance loss_bc2: 3.1499e-01\n",
            "lambda_balance loss_ics_u: 3.6818e-01\n",
            "updating  lamda for loss loss_bc1:   [0.72085583]\n",
            "updating  lamda for loss loss_bc2:   [0.22687994]\n",
            "updating  lamda for loss loss_ics_u:   [0.05226427]\n",
            "It: 30300| Loss: 6.424e-03|  loss_bc1: 9.269e-05|  loss_bc2: 1.767e-04| loss_ics_u: 1.174e-01|   Loss_res: 1.835e-04 , Time: 0.06\n",
            "lambda_balance loss_bc1: 3.1683e-01\n",
            "lambda_balance loss_bc2: 3.1501e-01\n",
            "lambda_balance loss_ics_u: 3.6816e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5875018]\n",
            "updating  lamda for loss loss_bc2:   [0.38667756]\n",
            "updating  lamda for loss loss_ics_u:   [0.02582063]\n",
            "It: 30400| Loss: 3.622e-03|  loss_bc1: 7.554e-05|  loss_bc2: 6.570e-05| loss_ics_u: 1.287e-01|   Loss_res: 2.297e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1684e-01\n",
            "lambda_balance loss_bc2: 3.1502e-01\n",
            "lambda_balance loss_ics_u: 3.6814e-01\n",
            "updating  lamda for loss loss_bc1:   [0.77513987]\n",
            "updating  lamda for loss loss_bc2:   [0.19849357]\n",
            "updating  lamda for loss loss_ics_u:   [0.02636656]\n",
            "It: 30500| Loss: 3.385e-03|  loss_bc1: 8.184e-05|  loss_bc2: 6.499e-05| loss_ics_u: 1.237e-01|   Loss_res: 4.785e-05 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1685e-01\n",
            "lambda_balance loss_bc2: 3.1504e-01\n",
            "lambda_balance loss_ics_u: 3.6811e-01\n",
            "updating  lamda for loss loss_bc1:   [0.28890347]\n",
            "updating  lamda for loss loss_bc2:   [0.6862524]\n",
            "updating  lamda for loss loss_ics_u:   [0.02484411]\n",
            "It: 30600| Loss: 3.463e-03|  loss_bc1: 4.319e-04|  loss_bc2: 4.906e-05| loss_ics_u: 1.289e-01|   Loss_res: 1.004e-04 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1686e-01\n",
            "lambda_balance loss_bc2: 3.1505e-01\n",
            "lambda_balance loss_ics_u: 3.6809e-01\n",
            "updating  lamda for loss loss_bc1:   [0.36997202]\n",
            "updating  lamda for loss loss_bc2:   [0.56078976]\n",
            "updating  lamda for loss loss_ics_u:   [0.06923822]\n",
            "It: 30700| Loss: 9.043e-03|  loss_bc1: 5.746e-04|  loss_bc2: 6.434e-05| loss_ics_u: 1.261e-01|   Loss_res: 6.516e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1687e-01\n",
            "lambda_balance loss_bc2: 3.1506e-01\n",
            "lambda_balance loss_ics_u: 3.6807e-01\n",
            "updating  lamda for loss loss_bc1:   [0.40941983]\n",
            "updating  lamda for loss loss_bc2:   [0.52668047]\n",
            "updating  lamda for loss loss_ics_u:   [0.06389968]\n",
            "It: 30800| Loss: 8.094e-03|  loss_bc1: 5.131e-04|  loss_bc2: 4.342e-05| loss_ics_u: 1.217e-01|   Loss_res: 8.605e-05 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1687e-01\n",
            "lambda_balance loss_bc2: 3.1508e-01\n",
            "lambda_balance loss_ics_u: 3.6805e-01\n",
            "updating  lamda for loss loss_bc1:   [0.24500121]\n",
            "updating  lamda for loss loss_bc2:   [0.6481229]\n",
            "updating  lamda for loss loss_ics_u:   [0.1068759]\n",
            "It: 30900| Loss: 1.353e-02|  loss_bc1: 9.342e-04|  loss_bc2: 1.453e-04| loss_ics_u: 1.224e-01|   Loss_res: 1.302e-04 , Time: 0.09\n",
            "lambda_balance loss_bc1: 3.1688e-01\n",
            "lambda_balance loss_bc2: 3.1509e-01\n",
            "lambda_balance loss_ics_u: 3.6803e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4703179]\n",
            "updating  lamda for loss loss_bc2:   [0.444989]\n",
            "updating  lamda for loss loss_ics_u:   [0.08469316]\n",
            "It: 31000| Loss: 1.145e-02|  loss_bc1: 1.818e-04|  loss_bc2: 2.105e-04| loss_ics_u: 1.319e-01|   Loss_res: 9.342e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1689e-01\n",
            "lambda_balance loss_bc2: 3.1511e-01\n",
            "lambda_balance loss_ics_u: 3.6800e-01\n",
            "updating  lamda for loss loss_bc1:   [0.08488017]\n",
            "updating  lamda for loss loss_bc2:   [0.8872012]\n",
            "updating  lamda for loss loss_ics_u:   [0.02791863]\n",
            "It: 31100| Loss: 3.996e-03|  loss_bc1: 6.548e-04|  loss_bc2: 1.495e-05| loss_ics_u: 1.389e-01|   Loss_res: 4.949e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1690e-01\n",
            "lambda_balance loss_bc2: 3.1512e-01\n",
            "lambda_balance loss_ics_u: 3.6798e-01\n",
            "updating  lamda for loss loss_bc1:   [0.09633043]\n",
            "updating  lamda for loss loss_bc2:   [0.8518078]\n",
            "updating  lamda for loss loss_ics_u:   [0.05186188]\n",
            "It: 31200| Loss: 7.095e-03|  loss_bc1: 5.065e-04|  loss_bc2: 6.103e-05| loss_ics_u: 1.338e-01|   Loss_res: 5.428e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1691e-01\n",
            "lambda_balance loss_bc2: 3.1513e-01\n",
            "lambda_balance loss_ics_u: 3.6796e-01\n",
            "updating  lamda for loss loss_bc1:   [0.16999096]\n",
            "updating  lamda for loss loss_bc2:   [0.7881214]\n",
            "updating  lamda for loss loss_ics_u:   [0.04188755]\n",
            "It: 31300| Loss: 4.926e-03|  loss_bc1: 2.801e-04|  loss_bc2: 3.982e-05| loss_ics_u: 1.141e-01|   Loss_res: 6.800e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1692e-01\n",
            "lambda_balance loss_bc2: 3.1515e-01\n",
            "lambda_balance loss_ics_u: 3.6794e-01\n",
            "updating  lamda for loss loss_bc1:   [0.33010808]\n",
            "updating  lamda for loss loss_bc2:   [0.61520183]\n",
            "updating  lamda for loss loss_ics_u:   [0.05469007]\n",
            "It: 31400| Loss: 7.184e-03|  loss_bc1: 2.813e-04|  loss_bc2: 1.972e-05| loss_ics_u: 1.285e-01|   Loss_res: 5.422e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1692e-01\n",
            "lambda_balance loss_bc2: 3.1516e-01\n",
            "lambda_balance loss_ics_u: 3.6792e-01\n",
            "updating  lamda for loss loss_bc1:   [0.12227179]\n",
            "updating  lamda for loss loss_bc2:   [0.8526209]\n",
            "updating  lamda for loss loss_ics_u:   [0.02510731]\n",
            "It: 31500| Loss: 3.535e-03|  loss_bc1: 1.786e-04|  loss_bc2: 1.963e-05| loss_ics_u: 1.337e-01|   Loss_res: 1.401e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1693e-01\n",
            "lambda_balance loss_bc2: 3.1517e-01\n",
            "lambda_balance loss_ics_u: 3.6790e-01\n",
            "updating  lamda for loss loss_bc1:   [0.15376262]\n",
            "updating  lamda for loss loss_bc2:   [0.8273009]\n",
            "updating  lamda for loss loss_ics_u:   [0.01893648]\n",
            "It: 31600| Loss: 2.380e-03|  loss_bc1: 2.086e-04|  loss_bc2: 1.273e-05| loss_ics_u: 1.200e-01|   Loss_res: 6.403e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1694e-01\n",
            "lambda_balance loss_bc2: 3.1518e-01\n",
            "lambda_balance loss_ics_u: 3.6788e-01\n",
            "updating  lamda for loss loss_bc1:   [0.11646564]\n",
            "updating  lamda for loss loss_bc2:   [0.8671907]\n",
            "updating  lamda for loss loss_ics_u:   [0.01634359]\n",
            "It: 31700| Loss: 2.256e-03|  loss_bc1: 1.014e-04|  loss_bc2: 1.007e-05| loss_ics_u: 1.312e-01|   Loss_res: 9.151e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1695e-01\n",
            "lambda_balance loss_bc2: 3.1520e-01\n",
            "lambda_balance loss_ics_u: 3.6785e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14885613]\n",
            "updating  lamda for loss loss_bc2:   [0.83466715]\n",
            "updating  lamda for loss loss_ics_u:   [0.01647674]\n",
            "It: 31800| Loss: 2.069e-03|  loss_bc1: 1.445e-04|  loss_bc2: 9.755e-06| loss_ics_u: 1.208e-01|   Loss_res: 4.875e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1695e-01\n",
            "lambda_balance loss_bc2: 3.1521e-01\n",
            "lambda_balance loss_ics_u: 3.6784e-01\n",
            "updating  lamda for loss loss_bc1:   [0.04323898]\n",
            "updating  lamda for loss loss_bc2:   [0.943066]\n",
            "updating  lamda for loss loss_ics_u:   [0.01369504]\n",
            "It: 31900| Loss: 1.698e-03|  loss_bc1: 3.686e-04|  loss_bc2: 8.777e-06| loss_ics_u: 1.173e-01|   Loss_res: 6.754e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1696e-01\n",
            "lambda_balance loss_bc2: 3.1522e-01\n",
            "lambda_balance loss_ics_u: 3.6781e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22631092]\n",
            "updating  lamda for loss loss_bc2:   [0.738727]\n",
            "updating  lamda for loss loss_ics_u:   [0.03496212]\n",
            "It: 32000| Loss: 4.959e-03|  loss_bc1: 2.130e-04|  loss_bc2: 2.374e-04| loss_ics_u: 1.329e-01|   Loss_res: 8.890e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1697e-01\n",
            "lambda_balance loss_bc2: 3.1524e-01\n",
            "lambda_balance loss_ics_u: 3.6779e-01\n",
            "updating  lamda for loss loss_bc1:   [0.23620747]\n",
            "updating  lamda for loss loss_bc2:   [0.73024553]\n",
            "updating  lamda for loss loss_ics_u:   [0.03354703]\n",
            "It: 32100| Loss: 4.345e-03|  loss_bc1: 1.239e-04|  loss_bc2: 6.431e-05| loss_ics_u: 1.247e-01|   Loss_res: 8.591e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1698e-01\n",
            "lambda_balance loss_bc2: 3.1525e-01\n",
            "lambda_balance loss_ics_u: 3.6777e-01\n",
            "updating  lamda for loss loss_bc1:   [0.423328]\n",
            "updating  lamda for loss loss_bc2:   [0.5224633]\n",
            "updating  lamda for loss loss_ics_u:   [0.0542086]\n",
            "It: 32200| Loss: 7.224e-03|  loss_bc1: 2.851e-04|  loss_bc2: 2.663e-05| loss_ics_u: 1.296e-01|   Loss_res: 6.341e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1698e-01\n",
            "lambda_balance loss_bc2: 3.1526e-01\n",
            "lambda_balance loss_ics_u: 3.6775e-01\n",
            "updating  lamda for loss loss_bc1:   [0.29463583]\n",
            "updating  lamda for loss loss_bc2:   [0.6571489]\n",
            "updating  lamda for loss loss_ics_u:   [0.0482153]\n",
            "It: 32300| Loss: 5.576e-03|  loss_bc1: 1.236e-04|  loss_bc2: 8.382e-05| loss_ics_u: 1.096e-01|   Loss_res: 2.023e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1699e-01\n",
            "lambda_balance loss_bc2: 3.1527e-01\n",
            "lambda_balance loss_ics_u: 3.6773e-01\n",
            "updating  lamda for loss loss_bc1:   [0.43439755]\n",
            "updating  lamda for loss loss_bc2:   [0.5184387]\n",
            "updating  lamda for loss loss_ics_u:   [0.04716372]\n",
            "It: 32400| Loss: 6.109e-03|  loss_bc1: 1.401e-04|  loss_bc2: 2.496e-05| loss_ics_u: 1.265e-01|   Loss_res: 6.802e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1700e-01\n",
            "lambda_balance loss_bc2: 3.1529e-01\n",
            "lambda_balance loss_ics_u: 3.6771e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6385172]\n",
            "updating  lamda for loss loss_bc2:   [0.31473005]\n",
            "updating  lamda for loss loss_ics_u:   [0.04675274]\n",
            "It: 32500| Loss: 5.401e-03|  loss_bc1: 1.345e-04|  loss_bc2: 2.696e-05| loss_ics_u: 1.123e-01|   Loss_res: 5.777e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1701e-01\n",
            "lambda_balance loss_bc2: 3.1530e-01\n",
            "lambda_balance loss_ics_u: 3.6770e-01\n",
            "updating  lamda for loss loss_bc1:   [0.1372754]\n",
            "updating  lamda for loss loss_bc2:   [0.8144444]\n",
            "updating  lamda for loss loss_ics_u:   [0.04828019]\n",
            "It: 32600| Loss: 6.173e-03|  loss_bc1: 1.905e-04|  loss_bc2: 2.011e-04| loss_ics_u: 1.223e-01|   Loss_res: 7.775e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1701e-01\n",
            "lambda_balance loss_bc2: 3.1531e-01\n",
            "lambda_balance loss_ics_u: 3.6767e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3338382]\n",
            "updating  lamda for loss loss_bc2:   [0.6234998]\n",
            "updating  lamda for loss loss_ics_u:   [0.04266199]\n",
            "It: 32700| Loss: 5.030e-03|  loss_bc1: 2.624e-04|  loss_bc2: 2.051e-05| loss_ics_u: 1.144e-01|   Loss_res: 4.928e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1702e-01\n",
            "lambda_balance loss_bc2: 3.1532e-01\n",
            "lambda_balance loss_ics_u: 3.6766e-01\n",
            "updating  lamda for loss loss_bc1:   [0.40758106]\n",
            "updating  lamda for loss loss_bc2:   [0.54347044]\n",
            "updating  lamda for loss loss_ics_u:   [0.04894853]\n",
            "It: 32800| Loss: 6.451e-03|  loss_bc1: 1.128e-04|  loss_bc2: 1.456e-04| loss_ics_u: 1.286e-01|   Loss_res: 2.971e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1703e-01\n",
            "lambda_balance loss_bc2: 3.1534e-01\n",
            "lambda_balance loss_ics_u: 3.6764e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6945562]\n",
            "updating  lamda for loss loss_bc2:   [0.26513022]\n",
            "updating  lamda for loss loss_ics_u:   [0.0403136]\n",
            "It: 32900| Loss: 4.983e-03|  loss_bc1: 1.525e-04|  loss_bc2: 9.089e-05| loss_ics_u: 1.191e-01|   Loss_res: 4.994e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1704e-01\n",
            "lambda_balance loss_bc2: 3.1535e-01\n",
            "lambda_balance loss_ics_u: 3.6762e-01\n",
            "updating  lamda for loss loss_bc1:   [0.47997046]\n",
            "updating  lamda for loss loss_bc2:   [0.4455619]\n",
            "updating  lamda for loss loss_ics_u:   [0.07446767]\n",
            "It: 33000| Loss: 9.259e-03|  loss_bc1: 7.030e-05|  loss_bc2: 1.911e-04| loss_ics_u: 1.220e-01|   Loss_res: 5.158e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1704e-01\n",
            "lambda_balance loss_bc2: 3.1536e-01\n",
            "lambda_balance loss_ics_u: 3.6760e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5225165]\n",
            "updating  lamda for loss loss_bc2:   [0.42034987]\n",
            "updating  lamda for loss loss_ics_u:   [0.05713363]\n",
            "It: 33100| Loss: 7.321e-03|  loss_bc1: 1.259e-04|  loss_bc2: 5.829e-05| loss_ics_u: 1.251e-01|   Loss_res: 8.352e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1705e-01\n",
            "lambda_balance loss_bc2: 3.1537e-01\n",
            "lambda_balance loss_ics_u: 3.6758e-01\n",
            "updating  lamda for loss loss_bc1:   [0.31489307]\n",
            "updating  lamda for loss loss_bc2:   [0.62107825]\n",
            "updating  lamda for loss loss_ics_u:   [0.06402863]\n",
            "It: 33200| Loss: 8.578e-03|  loss_bc1: 1.281e-04|  loss_bc2: 7.609e-05| loss_ics_u: 1.315e-01|   Loss_res: 6.750e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1706e-01\n",
            "lambda_balance loss_bc2: 3.1538e-01\n",
            "lambda_balance loss_ics_u: 3.6756e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4636348]\n",
            "updating  lamda for loss loss_bc2:   [0.49425367]\n",
            "updating  lamda for loss loss_ics_u:   [0.04211152]\n",
            "It: 33300| Loss: 5.497e-03|  loss_bc1: 5.788e-05|  loss_bc2: 4.767e-05| loss_ics_u: 1.280e-01|   Loss_res: 5.562e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1706e-01\n",
            "lambda_balance loss_bc2: 3.1539e-01\n",
            "lambda_balance loss_ics_u: 3.6754e-01\n",
            "updating  lamda for loss loss_bc1:   [0.619098]\n",
            "updating  lamda for loss loss_bc2:   [0.35369095]\n",
            "updating  lamda for loss loss_ics_u:   [0.02721109]\n",
            "It: 33400| Loss: 3.658e-03|  loss_bc1: 4.550e-05|  loss_bc2: 2.041e-05| loss_ics_u: 1.310e-01|   Loss_res: 5.796e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1707e-01\n",
            "lambda_balance loss_bc2: 3.1541e-01\n",
            "lambda_balance loss_ics_u: 3.6753e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14304179]\n",
            "updating  lamda for loss loss_bc2:   [0.8449345]\n",
            "updating  lamda for loss loss_ics_u:   [0.01202363]\n",
            "It: 33500| Loss: 1.531e-03|  loss_bc1: 6.194e-05|  loss_bc2: 2.972e-05| loss_ics_u: 1.188e-01|   Loss_res: 6.866e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1707e-01\n",
            "lambda_balance loss_bc2: 3.1542e-01\n",
            "lambda_balance loss_ics_u: 3.6751e-01\n",
            "updating  lamda for loss loss_bc1:   [0.09844401]\n",
            "updating  lamda for loss loss_bc2:   [0.8915672]\n",
            "updating  lamda for loss loss_ics_u:   [0.00998873]\n",
            "It: 33600| Loss: 1.186e-03|  loss_bc1: 1.120e-04|  loss_bc2: 1.510e-05| loss_ics_u: 1.099e-01|   Loss_res: 6.347e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1708e-01\n",
            "lambda_balance loss_bc2: 3.1543e-01\n",
            "lambda_balance loss_ics_u: 3.6749e-01\n",
            "updating  lamda for loss loss_bc1:   [0.31263682]\n",
            "updating  lamda for loss loss_bc2:   [0.6486731]\n",
            "updating  lamda for loss loss_ics_u:   [0.03869001]\n",
            "It: 33700| Loss: 5.270e-03|  loss_bc1: 8.123e-05|  loss_bc2: 3.286e-05| loss_ics_u: 1.340e-01|   Loss_res: 3.814e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1709e-01\n",
            "lambda_balance loss_bc2: 3.1544e-01\n",
            "lambda_balance loss_ics_u: 3.6747e-01\n",
            "updating  lamda for loss loss_bc1:   [0.27294213]\n",
            "updating  lamda for loss loss_bc2:   [0.67499673]\n",
            "updating  lamda for loss loss_ics_u:   [0.05206108]\n",
            "It: 33800| Loss: 7.417e-03|  loss_bc1: 2.001e-04|  loss_bc2: 4.036e-05| loss_ics_u: 1.398e-01|   Loss_res: 5.543e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1709e-01\n",
            "lambda_balance loss_bc2: 3.1545e-01\n",
            "lambda_balance loss_ics_u: 3.6745e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5893632]\n",
            "updating  lamda for loss loss_bc2:   [0.36353087]\n",
            "updating  lamda for loss loss_ics_u:   [0.04710587]\n",
            "It: 33900| Loss: 6.097e-03|  loss_bc1: 7.062e-05|  loss_bc2: 4.574e-05| loss_ics_u: 1.261e-01|   Loss_res: 9.761e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1710e-01\n",
            "lambda_balance loss_bc2: 3.1546e-01\n",
            "lambda_balance loss_ics_u: 3.6744e-01\n",
            "updating  lamda for loss loss_bc1:   [0.51670676]\n",
            "updating  lamda for loss loss_bc2:   [0.44337094]\n",
            "updating  lamda for loss loss_ics_u:   [0.03992231]\n",
            "It: 34000| Loss: 5.422e-03|  loss_bc1: 6.118e-05|  loss_bc2: 4.789e-05| loss_ics_u: 1.215e-01|   Loss_res: 5.179e-04 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1711e-01\n",
            "lambda_balance loss_bc2: 3.1547e-01\n",
            "lambda_balance loss_ics_u: 3.6742e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5734229]\n",
            "updating  lamda for loss loss_bc2:   [0.4106165]\n",
            "updating  lamda for loss loss_ics_u:   [0.01596051]\n",
            "It: 34100| Loss: 1.990e-03|  loss_bc1: 2.772e-05|  loss_bc2: 2.173e-05| loss_ics_u: 1.206e-01|   Loss_res: 4.033e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1711e-01\n",
            "lambda_balance loss_bc2: 3.1548e-01\n",
            "lambda_balance loss_ics_u: 3.6740e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6743445]\n",
            "updating  lamda for loss loss_bc2:   [0.3079272]\n",
            "updating  lamda for loss loss_ics_u:   [0.01772838]\n",
            "It: 34200| Loss: 2.316e-03|  loss_bc1: 3.230e-05|  loss_bc2: 1.483e-05| loss_ics_u: 1.251e-01|   Loss_res: 7.115e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1712e-01\n",
            "lambda_balance loss_bc2: 3.1549e-01\n",
            "lambda_balance loss_ics_u: 3.6739e-01\n",
            "updating  lamda for loss loss_bc1:   [0.19596304]\n",
            "updating  lamda for loss loss_bc2:   [0.799118]\n",
            "updating  lamda for loss loss_ics_u:   [0.004919]\n",
            "It: 34300| Loss: 7.023e-04|  loss_bc1: 3.100e-05|  loss_bc2: 1.652e-05| loss_ics_u: 1.323e-01|   Loss_res: 3.203e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1713e-01\n",
            "lambda_balance loss_bc2: 3.1551e-01\n",
            "lambda_balance loss_ics_u: 3.6737e-01\n",
            "updating  lamda for loss loss_bc1:   [0.4423734]\n",
            "updating  lamda for loss loss_bc2:   [0.5471344]\n",
            "updating  lamda for loss loss_ics_u:   [0.01049224]\n",
            "It: 34400| Loss: 1.411e-03|  loss_bc1: 1.820e-05|  loss_bc2: 2.098e-05| loss_ics_u: 1.286e-01|   Loss_res: 4.168e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1713e-01\n",
            "lambda_balance loss_bc2: 3.1552e-01\n",
            "lambda_balance loss_ics_u: 3.6735e-01\n",
            "updating  lamda for loss loss_bc1:   [0.75980544]\n",
            "updating  lamda for loss loss_bc2:   [0.22527716]\n",
            "updating  lamda for loss loss_ics_u:   [0.01491743]\n",
            "It: 34500| Loss: 2.031e-03|  loss_bc1: 1.603e-05|  loss_bc2: 2.003e-05| loss_ics_u: 1.330e-01|   Loss_res: 3.008e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1714e-01\n",
            "lambda_balance loss_bc2: 3.1553e-01\n",
            "lambda_balance loss_ics_u: 3.6734e-01\n",
            "updating  lamda for loss loss_bc1:   [0.79579854]\n",
            "updating  lamda for loss loss_bc2:   [0.19071178]\n",
            "updating  lamda for loss loss_ics_u:   [0.01348967]\n",
            "It: 34600| Loss: 1.773e-03|  loss_bc1: 1.868e-05|  loss_bc2: 3.156e-05| loss_ics_u: 1.268e-01|   Loss_res: 4.199e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1714e-01\n",
            "lambda_balance loss_bc2: 3.1554e-01\n",
            "lambda_balance loss_ics_u: 3.6732e-01\n",
            "updating  lamda for loss loss_bc1:   [0.8072284]\n",
            "updating  lamda for loss loss_bc2:   [0.17526776]\n",
            "updating  lamda for loss loss_ics_u:   [0.01750378]\n",
            "It: 34700| Loss: 2.197e-03|  loss_bc1: 1.574e-05|  loss_bc2: 4.400e-05| loss_ics_u: 1.214e-01|   Loss_res: 5.134e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1715e-01\n",
            "lambda_balance loss_bc2: 3.1555e-01\n",
            "lambda_balance loss_ics_u: 3.6730e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5440578]\n",
            "updating  lamda for loss loss_bc2:   [0.43119767]\n",
            "updating  lamda for loss loss_ics_u:   [0.02474449]\n",
            "It: 34800| Loss: 3.305e-03|  loss_bc1: 6.682e-05|  loss_bc2: 2.508e-05| loss_ics_u: 1.286e-01|   Loss_res: 7.598e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1715e-01\n",
            "lambda_balance loss_bc2: 3.1556e-01\n",
            "lambda_balance loss_ics_u: 3.6729e-01\n",
            "updating  lamda for loss loss_bc1:   [0.42635357]\n",
            "updating  lamda for loss loss_bc2:   [0.5577931]\n",
            "updating  lamda for loss loss_ics_u:   [0.01585328]\n",
            "It: 34900| Loss: 2.167e-03|  loss_bc1: 2.241e-05|  loss_bc2: 1.324e-05| loss_ics_u: 1.317e-01|   Loss_res: 6.194e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1716e-01\n",
            "lambda_balance loss_bc2: 3.1557e-01\n",
            "lambda_balance loss_ics_u: 3.6727e-01\n",
            "updating  lamda for loss loss_bc1:   [0.37881607]\n",
            "updating  lamda for loss loss_bc2:   [0.61379045]\n",
            "updating  lamda for loss loss_ics_u:   [0.00739346]\n",
            "It: 35000| Loss: 1.006e-03|  loss_bc1: 1.646e-05|  loss_bc2: 1.540e-05| loss_ics_u: 1.244e-01|   Loss_res: 7.027e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1717e-01\n",
            "lambda_balance loss_bc2: 3.1558e-01\n",
            "lambda_balance loss_ics_u: 3.6726e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5739607]\n",
            "updating  lamda for loss loss_bc2:   [0.3885336]\n",
            "updating  lamda for loss loss_ics_u:   [0.0375057]\n",
            "It: 35100| Loss: 4.407e-03|  loss_bc1: 6.368e-05|  loss_bc2: 5.683e-05| loss_ics_u: 1.150e-01|   Loss_res: 3.557e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1717e-01\n",
            "lambda_balance loss_bc2: 3.1559e-01\n",
            "lambda_balance loss_ics_u: 3.6724e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5872012]\n",
            "updating  lamda for loss loss_bc2:   [0.3802991]\n",
            "updating  lamda for loss loss_ics_u:   [0.03249973]\n",
            "It: 35200| Loss: 3.844e-03|  loss_bc1: 1.547e-04|  loss_bc2: 3.402e-05| loss_ics_u: 1.134e-01|   Loss_res: 5.536e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1718e-01\n",
            "lambda_balance loss_bc2: 3.1560e-01\n",
            "lambda_balance loss_ics_u: 3.6722e-01\n",
            "updating  lamda for loss loss_bc1:   [0.29893565]\n",
            "updating  lamda for loss loss_bc2:   [0.6685572]\n",
            "updating  lamda for loss loss_ics_u:   [0.03250712]\n",
            "It: 35300| Loss: 4.491e-03|  loss_bc1: 3.922e-04|  loss_bc2: 3.763e-05| loss_ics_u: 1.330e-01|   Loss_res: 2.510e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1718e-01\n",
            "lambda_balance loss_bc2: 3.1561e-01\n",
            "lambda_balance loss_ics_u: 3.6721e-01\n",
            "updating  lamda for loss loss_bc1:   [0.10523254]\n",
            "updating  lamda for loss loss_bc2:   [0.8753074]\n",
            "updating  lamda for loss loss_ics_u:   [0.01946005]\n",
            "It: 35400| Loss: 2.361e-03|  loss_bc1: 2.089e-04|  loss_bc2: 2.404e-05| loss_ics_u: 1.171e-01|   Loss_res: 3.927e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1719e-01\n",
            "lambda_balance loss_bc2: 3.1562e-01\n",
            "lambda_balance loss_ics_u: 3.6719e-01\n",
            "updating  lamda for loss loss_bc1:   [0.411728]\n",
            "updating  lamda for loss loss_bc2:   [0.5443337]\n",
            "updating  lamda for loss loss_ics_u:   [0.04393835]\n",
            "It: 35500| Loss: 4.641e-03|  loss_bc1: 6.927e-05|  loss_bc2: 4.418e-05| loss_ics_u: 1.032e-01|   Loss_res: 5.200e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1720e-01\n",
            "lambda_balance loss_bc2: 3.1563e-01\n",
            "lambda_balance loss_ics_u: 3.6717e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5231318]\n",
            "updating  lamda for loss loss_bc2:   [0.42389283]\n",
            "updating  lamda for loss loss_ics_u:   [0.05297539]\n",
            "It: 35600| Loss: 6.744e-03|  loss_bc1: 8.499e-05|  loss_bc2: 1.161e-04| loss_ics_u: 1.248e-01|   Loss_res: 3.888e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1720e-01\n",
            "lambda_balance loss_bc2: 3.1564e-01\n",
            "lambda_balance loss_ics_u: 3.6716e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5506121]\n",
            "updating  lamda for loss loss_bc2:   [0.42351195]\n",
            "updating  lamda for loss loss_ics_u:   [0.02587594]\n",
            "It: 35700| Loss: 3.686e-03|  loss_bc1: 6.010e-05|  loss_bc2: 3.273e-05| loss_ics_u: 1.395e-01|   Loss_res: 3.061e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1721e-01\n",
            "lambda_balance loss_bc2: 3.1565e-01\n",
            "lambda_balance loss_ics_u: 3.6714e-01\n",
            "updating  lamda for loss loss_bc1:   [0.29248813]\n",
            "updating  lamda for loss loss_bc2:   [0.6791701]\n",
            "updating  lamda for loss loss_ics_u:   [0.02834184]\n",
            "It: 35800| Loss: 3.469e-03|  loss_bc1: 1.653e-04|  loss_bc2: 2.495e-05| loss_ics_u: 1.184e-01|   Loss_res: 4.725e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1721e-01\n",
            "lambda_balance loss_bc2: 3.1566e-01\n",
            "lambda_balance loss_ics_u: 3.6713e-01\n",
            "updating  lamda for loss loss_bc1:   [0.18598747]\n",
            "updating  lamda for loss loss_bc2:   [0.78824055]\n",
            "updating  lamda for loss loss_ics_u:   [0.02577198]\n",
            "It: 35900| Loss: 3.357e-03|  loss_bc1: 1.863e-04|  loss_bc2: 1.218e-05| loss_ics_u: 1.272e-01|   Loss_res: 3.407e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1722e-01\n",
            "lambda_balance loss_bc2: 3.1567e-01\n",
            "lambda_balance loss_ics_u: 3.6711e-01\n",
            "updating  lamda for loss loss_bc1:   [0.23716065]\n",
            "updating  lamda for loss loss_bc2:   [0.7339237]\n",
            "updating  lamda for loss loss_ics_u:   [0.0289157]\n",
            "It: 36000| Loss: 3.936e-03|  loss_bc1: 6.930e-05|  loss_bc2: 3.095e-05| loss_ics_u: 1.324e-01|   Loss_res: 6.787e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1722e-01\n",
            "lambda_balance loss_bc2: 3.1568e-01\n",
            "lambda_balance loss_ics_u: 3.6710e-01\n",
            "updating  lamda for loss loss_bc1:   [0.51574546]\n",
            "updating  lamda for loss loss_bc2:   [0.44013578]\n",
            "updating  lamda for loss loss_ics_u:   [0.04411878]\n",
            "It: 36100| Loss: 5.336e-03|  loss_bc1: 5.631e-05|  loss_bc2: 8.707e-05| loss_ics_u: 1.180e-01|   Loss_res: 6.064e-05 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1723e-01\n",
            "lambda_balance loss_bc2: 3.1569e-01\n",
            "lambda_balance loss_ics_u: 3.6708e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22016259]\n",
            "updating  lamda for loss loss_bc2:   [0.7323054]\n",
            "updating  lamda for loss loss_ics_u:   [0.04753195]\n",
            "It: 36200| Loss: 6.075e-03|  loss_bc1: 3.352e-04|  loss_bc2: 2.029e-05| loss_ics_u: 1.249e-01|   Loss_res: 4.939e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1724e-01\n",
            "lambda_balance loss_bc2: 3.1570e-01\n",
            "lambda_balance loss_ics_u: 3.6706e-01\n",
            "updating  lamda for loss loss_bc1:   [0.22394867]\n",
            "updating  lamda for loss loss_bc2:   [0.68587303]\n",
            "updating  lamda for loss loss_ics_u:   [0.09017827]\n",
            "It: 36300| Loss: 1.173e-02|  loss_bc1: 6.347e-04|  loss_bc2: 1.114e-04| loss_ics_u: 1.271e-01|   Loss_res: 5.431e-05 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1724e-01\n",
            "lambda_balance loss_bc2: 3.1571e-01\n",
            "lambda_balance loss_ics_u: 3.6705e-01\n",
            "updating  lamda for loss loss_bc1:   [0.18325084]\n",
            "updating  lamda for loss loss_bc2:   [0.72151434]\n",
            "updating  lamda for loss loss_ics_u:   [0.09523483]\n",
            "It: 36400| Loss: 1.270e-02|  loss_bc1: 4.397e-04|  loss_bc2: 1.504e-04| loss_ics_u: 1.309e-01|   Loss_res: 4.390e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1725e-01\n",
            "lambda_balance loss_bc2: 3.1572e-01\n",
            "lambda_balance loss_ics_u: 3.6703e-01\n",
            "updating  lamda for loss loss_bc1:   [0.1483059]\n",
            "updating  lamda for loss loss_bc2:   [0.8001305]\n",
            "updating  lamda for loss loss_ics_u:   [0.0515636]\n",
            "It: 36500| Loss: 5.389e-03|  loss_bc1: 7.174e-04|  loss_bc2: 2.734e-05| loss_ics_u: 1.015e-01|   Loss_res: 2.563e-05 , Time: 0.03\n",
            "lambda_balance loss_bc1: 3.1726e-01\n",
            "lambda_balance loss_bc2: 3.1573e-01\n",
            "lambda_balance loss_ics_u: 3.6701e-01\n",
            "updating  lamda for loss loss_bc1:   [0.07382178]\n",
            "updating  lamda for loss loss_bc2:   [0.88694143]\n",
            "updating  lamda for loss loss_ics_u:   [0.0392368]\n",
            "It: 36600| Loss: 5.006e-03|  loss_bc1: 5.431e-04|  loss_bc2: 6.190e-05| loss_ics_u: 1.241e-01|   Loss_res: 4.102e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1726e-01\n",
            "lambda_balance loss_bc2: 3.1574e-01\n",
            "lambda_balance loss_ics_u: 3.6699e-01\n",
            "updating  lamda for loss loss_bc1:   [0.17905352]\n",
            "updating  lamda for loss loss_bc2:   [0.7084002]\n",
            "updating  lamda for loss loss_ics_u:   [0.11254636]\n",
            "It: 36700| Loss: 1.534e-02|  loss_bc1: 1.746e-03|  loss_bc2: 1.194e-04| loss_ics_u: 1.323e-01|   Loss_res: 5.484e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1727e-01\n",
            "lambda_balance loss_bc2: 3.1575e-01\n",
            "lambda_balance loss_ics_u: 3.6698e-01\n",
            "updating  lamda for loss loss_bc1:   [0.05618083]\n",
            "updating  lamda for loss loss_bc2:   [0.88711447]\n",
            "updating  lamda for loss loss_ics_u:   [0.0567047]\n",
            "It: 36800| Loss: 6.827e-03|  loss_bc1: 8.958e-04|  loss_bc2: 6.269e-05| loss_ics_u: 1.171e-01|   Loss_res: 8.294e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1728e-01\n",
            "lambda_balance loss_bc2: 3.1576e-01\n",
            "lambda_balance loss_ics_u: 3.6696e-01\n",
            "updating  lamda for loss loss_bc1:   [0.32874668]\n",
            "updating  lamda for loss loss_bc2:   [0.61475724]\n",
            "updating  lamda for loss loss_ics_u:   [0.05649609]\n",
            "It: 36900| Loss: 7.078e-03|  loss_bc1: 1.868e-04|  loss_bc2: 9.902e-05| loss_ics_u: 1.224e-01|   Loss_res: 4.285e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1728e-01\n",
            "lambda_balance loss_bc2: 3.1577e-01\n",
            "lambda_balance loss_ics_u: 3.6695e-01\n",
            "updating  lamda for loss loss_bc1:   [0.55445987]\n",
            "updating  lamda for loss loss_bc2:   [0.39444283]\n",
            "updating  lamda for loss loss_ics_u:   [0.05109719]\n",
            "It: 37000| Loss: 6.215e-03|  loss_bc1: 1.075e-04|  loss_bc2: 7.958e-05| loss_ics_u: 1.190e-01|   Loss_res: 4.376e-05 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1729e-01\n",
            "lambda_balance loss_bc2: 3.1578e-01\n",
            "lambda_balance loss_ics_u: 3.6693e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3642657]\n",
            "updating  lamda for loss loss_bc2:   [0.5772183]\n",
            "updating  lamda for loss loss_ics_u:   [0.05851606]\n",
            "It: 37100| Loss: 7.462e-03|  loss_bc1: 2.146e-04|  loss_bc2: 5.782e-05| loss_ics_u: 1.246e-01|   Loss_res: 5.979e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1729e-01\n",
            "lambda_balance loss_bc2: 3.1579e-01\n",
            "lambda_balance loss_ics_u: 3.6692e-01\n",
            "updating  lamda for loss loss_bc1:   [0.39545825]\n",
            "updating  lamda for loss loss_bc2:   [0.5493513]\n",
            "updating  lamda for loss loss_ics_u:   [0.0551905]\n",
            "It: 37200| Loss: 7.304e-03|  loss_bc1: 1.335e-04|  loss_bc2: 1.186e-04| loss_ics_u: 1.295e-01|   Loss_res: 3.991e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1730e-01\n",
            "lambda_balance loss_bc2: 3.1580e-01\n",
            "lambda_balance loss_ics_u: 3.6690e-01\n",
            "updating  lamda for loss loss_bc1:   [0.33739376]\n",
            "updating  lamda for loss loss_bc2:   [0.6101983]\n",
            "updating  lamda for loss loss_ics_u:   [0.05240794]\n",
            "It: 37300| Loss: 6.413e-03|  loss_bc1: 1.959e-04|  loss_bc2: 6.803e-05| loss_ics_u: 1.197e-01|   Loss_res: 3.223e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1730e-01\n",
            "lambda_balance loss_bc2: 3.1581e-01\n",
            "lambda_balance loss_ics_u: 3.6689e-01\n",
            "updating  lamda for loss loss_bc1:   [0.48925212]\n",
            "updating  lamda for loss loss_bc2:   [0.46172592]\n",
            "updating  lamda for loss loss_ics_u:   [0.04902199]\n",
            "It: 37400| Loss: 6.632e-03|  loss_bc1: 7.052e-05|  loss_bc2: 7.369e-05| loss_ics_u: 1.322e-01|   Loss_res: 8.338e-05 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1731e-01\n",
            "lambda_balance loss_bc2: 3.1582e-01\n",
            "lambda_balance loss_ics_u: 3.6687e-01\n",
            "updating  lamda for loss loss_bc1:   [0.65358585]\n",
            "updating  lamda for loss loss_bc2:   [0.3161841]\n",
            "updating  lamda for loss loss_ics_u:   [0.03023004]\n",
            "It: 37500| Loss: 4.093e-03|  loss_bc1: 5.154e-05|  loss_bc2: 2.516e-05| loss_ics_u: 1.324e-01|   Loss_res: 4.816e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1731e-01\n",
            "lambda_balance loss_bc2: 3.1583e-01\n",
            "lambda_balance loss_ics_u: 3.6686e-01\n",
            "updating  lamda for loss loss_bc1:   [0.51764077]\n",
            "updating  lamda for loss loss_bc2:   [0.44039202]\n",
            "updating  lamda for loss loss_ics_u:   [0.04196726]\n",
            "It: 37600| Loss: 5.154e-03|  loss_bc1: 5.134e-05|  loss_bc2: 1.817e-05| loss_ics_u: 1.207e-01|   Loss_res: 5.209e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1732e-01\n",
            "lambda_balance loss_bc2: 3.1584e-01\n",
            "lambda_balance loss_ics_u: 3.6684e-01\n",
            "updating  lamda for loss loss_bc1:   [0.3360403]\n",
            "updating  lamda for loss loss_bc2:   [0.6206824]\n",
            "updating  lamda for loss loss_ics_u:   [0.04327734]\n",
            "It: 37700| Loss: 6.070e-03|  loss_bc1: 3.265e-04|  loss_bc2: 2.984e-05| loss_ics_u: 1.361e-01|   Loss_res: 5.250e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1733e-01\n",
            "lambda_balance loss_bc2: 3.1585e-01\n",
            "lambda_balance loss_ics_u: 3.6683e-01\n",
            "updating  lamda for loss loss_bc1:   [0.07811242]\n",
            "updating  lamda for loss loss_bc2:   [0.89995843]\n",
            "updating  lamda for loss loss_ics_u:   [0.02192918]\n",
            "It: 37800| Loss: 2.892e-03|  loss_bc1: 2.298e-04|  loss_bc2: 1.675e-05| loss_ics_u: 1.281e-01|   Loss_res: 5.017e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1733e-01\n",
            "lambda_balance loss_bc2: 3.1585e-01\n",
            "lambda_balance loss_ics_u: 3.6681e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2797935]\n",
            "updating  lamda for loss loss_bc2:   [0.67571557]\n",
            "updating  lamda for loss loss_ics_u:   [0.04449096]\n",
            "It: 37900| Loss: 5.411e-03|  loss_bc1: 1.168e-04|  loss_bc2: 5.672e-05| loss_ics_u: 1.191e-01|   Loss_res: 3.946e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1734e-01\n",
            "lambda_balance loss_bc2: 3.1586e-01\n",
            "lambda_balance loss_ics_u: 3.6680e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5365523]\n",
            "updating  lamda for loss loss_bc2:   [0.41630277]\n",
            "updating  lamda for loss loss_ics_u:   [0.04714492]\n",
            "It: 38000| Loss: 5.624e-03|  loss_bc1: 5.656e-05|  loss_bc2: 4.291e-05| loss_ics_u: 1.172e-01|   Loss_res: 5.212e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1734e-01\n",
            "lambda_balance loss_bc2: 3.1587e-01\n",
            "lambda_balance loss_ics_u: 3.6679e-01\n",
            "updating  lamda for loss loss_bc1:   [0.40521747]\n",
            "updating  lamda for loss loss_bc2:   [0.55975574]\n",
            "updating  lamda for loss loss_ics_u:   [0.0350268]\n",
            "It: 38100| Loss: 4.799e-03|  loss_bc1: 6.777e-05|  loss_bc2: 4.223e-05| loss_ics_u: 1.348e-01|   Loss_res: 2.600e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1735e-01\n",
            "lambda_balance loss_bc2: 3.1588e-01\n",
            "lambda_balance loss_ics_u: 3.6677e-01\n",
            "updating  lamda for loss loss_bc1:   [0.57965475]\n",
            "updating  lamda for loss loss_bc2:   [0.37106916]\n",
            "updating  lamda for loss loss_ics_u:   [0.04927606]\n",
            "It: 38200| Loss: 6.350e-03|  loss_bc1: 1.101e-04|  loss_bc2: 1.239e-04| loss_ics_u: 1.250e-01|   Loss_res: 8.096e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1735e-01\n",
            "lambda_balance loss_bc2: 3.1589e-01\n",
            "lambda_balance loss_ics_u: 3.6676e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5788446]\n",
            "updating  lamda for loss loss_bc2:   [0.34078896]\n",
            "updating  lamda for loss loss_ics_u:   [0.08036646]\n",
            "It: 38300| Loss: 1.058e-02|  loss_bc1: 4.737e-04|  loss_bc2: 7.918e-05| loss_ics_u: 1.268e-01|   Loss_res: 9.035e-05 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1736e-01\n",
            "lambda_balance loss_bc2: 3.1590e-01\n",
            "lambda_balance loss_ics_u: 3.6674e-01\n",
            "updating  lamda for loss loss_bc1:   [0.34833094]\n",
            "updating  lamda for loss loss_bc2:   [0.59803164]\n",
            "updating  lamda for loss loss_ics_u:   [0.05363741]\n",
            "It: 38400| Loss: 6.520e-03|  loss_bc1: 7.235e-05|  loss_bc2: 6.647e-05| loss_ics_u: 1.197e-01|   Loss_res: 3.468e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1736e-01\n",
            "lambda_balance loss_bc2: 3.1591e-01\n",
            "lambda_balance loss_ics_u: 3.6673e-01\n",
            "updating  lamda for loss loss_bc1:   [0.66760314]\n",
            "updating  lamda for loss loss_bc2:   [0.3001993]\n",
            "updating  lamda for loss loss_ics_u:   [0.03219753]\n",
            "It: 38500| Loss: 3.761e-03|  loss_bc1: 4.194e-05|  loss_bc2: 2.865e-05| loss_ics_u: 1.146e-01|   Loss_res: 3.514e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1737e-01\n",
            "lambda_balance loss_bc2: 3.1592e-01\n",
            "lambda_balance loss_ics_u: 3.6672e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5577534]\n",
            "updating  lamda for loss loss_bc2:   [0.42167652]\n",
            "updating  lamda for loss loss_ics_u:   [0.02057005]\n",
            "It: 38600| Loss: 2.644e-03|  loss_bc1: 2.196e-05|  loss_bc2: 4.421e-05| loss_ics_u: 1.246e-01|   Loss_res: 4.960e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1737e-01\n",
            "lambda_balance loss_bc2: 3.1592e-01\n",
            "lambda_balance loss_ics_u: 3.6670e-01\n",
            "updating  lamda for loss loss_bc1:   [0.556017]\n",
            "updating  lamda for loss loss_bc2:   [0.4194965]\n",
            "updating  lamda for loss loss_ics_u:   [0.02448652]\n",
            "It: 38700| Loss: 3.379e-03|  loss_bc1: 3.125e-05|  loss_bc2: 3.313e-05| loss_ics_u: 1.339e-01|   Loss_res: 6.882e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1738e-01\n",
            "lambda_balance loss_bc2: 3.1593e-01\n",
            "lambda_balance loss_ics_u: 3.6669e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6857373]\n",
            "updating  lamda for loss loss_bc2:   [0.2997296]\n",
            "updating  lamda for loss loss_ics_u:   [0.01453312]\n",
            "It: 38800| Loss: 2.006e-03|  loss_bc1: 2.302e-05|  loss_bc2: 2.147e-05| loss_ics_u: 1.348e-01|   Loss_res: 2.502e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1738e-01\n",
            "lambda_balance loss_bc2: 3.1594e-01\n",
            "lambda_balance loss_ics_u: 3.6668e-01\n",
            "updating  lamda for loss loss_bc1:   [0.61589235]\n",
            "updating  lamda for loss loss_bc2:   [0.36937866]\n",
            "updating  lamda for loss loss_ics_u:   [0.01472907]\n",
            "It: 38900| Loss: 2.058e-03|  loss_bc1: 3.467e-05|  loss_bc2: 9.098e-06| loss_ics_u: 1.331e-01|   Loss_res: 7.215e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1738e-01\n",
            "lambda_balance loss_bc2: 3.1595e-01\n",
            "lambda_balance loss_ics_u: 3.6667e-01\n",
            "updating  lamda for loss loss_bc1:   [0.10949182]\n",
            "updating  lamda for loss loss_bc2:   [0.88319]\n",
            "updating  lamda for loss loss_ics_u:   [0.00731821]\n",
            "It: 39000| Loss: 1.009e-03|  loss_bc1: 3.497e-05|  loss_bc2: 1.133e-05| loss_ics_u: 1.308e-01|   Loss_res: 3.836e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1739e-01\n",
            "lambda_balance loss_bc2: 3.1596e-01\n",
            "lambda_balance loss_ics_u: 3.6665e-01\n",
            "updating  lamda for loss loss_bc1:   [0.14934275]\n",
            "updating  lamda for loss loss_bc2:   [0.83879066]\n",
            "updating  lamda for loss loss_ics_u:   [0.01186661]\n",
            "It: 39100| Loss: 1.664e-03|  loss_bc1: 5.598e-05|  loss_bc2: 1.922e-05| loss_ics_u: 1.353e-01|   Loss_res: 3.479e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1739e-01\n",
            "lambda_balance loss_bc2: 3.1596e-01\n",
            "lambda_balance loss_ics_u: 3.6664e-01\n",
            "updating  lamda for loss loss_bc1:   [0.51297355]\n",
            "updating  lamda for loss loss_bc2:   [0.4719386]\n",
            "updating  lamda for loss loss_ics_u:   [0.01508788]\n",
            "It: 39200| Loss: 1.948e-03|  loss_bc1: 3.570e-05|  loss_bc2: 1.248e-05| loss_ics_u: 1.256e-01|   Loss_res: 2.902e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1740e-01\n",
            "lambda_balance loss_bc2: 3.1597e-01\n",
            "lambda_balance loss_ics_u: 3.6663e-01\n",
            "updating  lamda for loss loss_bc1:   [0.2964926]\n",
            "updating  lamda for loss loss_bc2:   [0.67159736]\n",
            "updating  lamda for loss loss_ics_u:   [0.03190994]\n",
            "It: 39300| Loss: 3.910e-03|  loss_bc1: 3.617e-05|  loss_bc2: 3.668e-05| loss_ics_u: 1.193e-01|   Loss_res: 6.836e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1740e-01\n",
            "lambda_balance loss_bc2: 3.1598e-01\n",
            "lambda_balance loss_ics_u: 3.6662e-01\n",
            "updating  lamda for loss loss_bc1:   [0.5951971]\n",
            "updating  lamda for loss loss_bc2:   [0.37536728]\n",
            "updating  lamda for loss loss_ics_u:   [0.02943564]\n",
            "It: 39400| Loss: 3.898e-03|  loss_bc1: 6.285e-05|  loss_bc2: 3.631e-05| loss_ics_u: 1.296e-01|   Loss_res: 3.202e-05 , Time: 0.05\n",
            "lambda_balance loss_bc1: 3.1741e-01\n",
            "lambda_balance loss_bc2: 3.1599e-01\n",
            "lambda_balance loss_ics_u: 3.6660e-01\n",
            "updating  lamda for loss loss_bc1:   [0.6132343]\n",
            "updating  lamda for loss loss_bc2:   [0.3524845]\n",
            "updating  lamda for loss loss_ics_u:   [0.03428128]\n",
            "It: 39500| Loss: 4.063e-03|  loss_bc1: 9.330e-05|  loss_bc2: 7.667e-05| loss_ics_u: 1.150e-01|   Loss_res: 3.516e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1741e-01\n",
            "lambda_balance loss_bc2: 3.1600e-01\n",
            "lambda_balance loss_ics_u: 3.6659e-01\n",
            "updating  lamda for loss loss_bc1:   [0.42849278]\n",
            "updating  lamda for loss loss_bc2:   [0.52394146]\n",
            "updating  lamda for loss loss_ics_u:   [0.04756567]\n",
            "It: 39600| Loss: 5.358e-03|  loss_bc1: 1.719e-04|  loss_bc2: 3.597e-05| loss_ics_u: 1.101e-01|   Loss_res: 2.985e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1742e-01\n",
            "lambda_balance loss_bc2: 3.1601e-01\n",
            "lambda_balance loss_ics_u: 3.6658e-01\n",
            "updating  lamda for loss loss_bc1:   [0.24956226]\n",
            "updating  lamda for loss loss_bc2:   [0.7141019]\n",
            "updating  lamda for loss loss_ics_u:   [0.03633579]\n",
            "It: 39700| Loss: 4.960e-03|  loss_bc1: 7.171e-05|  loss_bc2: 3.197e-05| loss_ics_u: 1.343e-01|   Loss_res: 3.743e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1742e-01\n",
            "lambda_balance loss_bc2: 3.1601e-01\n",
            "lambda_balance loss_ics_u: 3.6656e-01\n",
            "updating  lamda for loss loss_bc1:   [0.68634087]\n",
            "updating  lamda for loss loss_bc2:   [0.2921222]\n",
            "updating  lamda for loss loss_ics_u:   [0.02153692]\n",
            "It: 39800| Loss: 2.827e-03|  loss_bc1: 2.951e-05|  loss_bc2: 9.628e-05| loss_ics_u: 1.279e-01|   Loss_res: 2.378e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1743e-01\n",
            "lambda_balance loss_bc2: 3.1602e-01\n",
            "lambda_balance loss_ics_u: 3.6655e-01\n",
            "updating  lamda for loss loss_bc1:   [0.75787723]\n",
            "updating  lamda for loss loss_bc2:   [0.21173391]\n",
            "updating  lamda for loss loss_ics_u:   [0.03038892]\n",
            "It: 39900| Loss: 4.187e-03|  loss_bc1: 2.416e-05|  loss_bc2: 1.880e-04| loss_ics_u: 1.346e-01|   Loss_res: 3.794e-05 , Time: 0.04\n",
            "lambda_balance loss_bc1: 3.1743e-01\n",
            "lambda_balance loss_bc2: 3.1603e-01\n",
            "lambda_balance loss_ics_u: 3.6654e-01\n",
            "updating  lamda for loss loss_bc1:   [0.65653104]\n",
            "updating  lamda for loss loss_bc2:   [0.28325415]\n",
            "updating  lamda for loss loss_ics_u:   [0.06021476]\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 40000| Loss: 6.962e-03|  loss_bc1: 5.105e-05|  loss_bc2: 9.782e-05| loss_ics_u: 1.138e-01|   Loss_res: 4.661e-05 , Time: 4.42\n",
            "lambda_balance loss_bc1: 3.1744e-01\n",
            "lambda_balance loss_bc2: 3.1604e-01\n",
            "lambda_balance loss_ics_u: 3.6653e-01\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:293: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:435: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "updating  lamda for loss loss_bc1:   [0.39516]\n",
            "updating  lamda for loss loss_bc2:   [0.5387596]\n",
            "updating  lamda for loss loss_ics_u:   [0.06608038]\n",
            "average adaptive_constant_bcs1_log : 3.4857e-01\n",
            "average adaptive_constant_bcs2_log : 5.8213e-01\n",
            "average adaptive_constant_ics_u_log : 6.9298e-02\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "elapsed: 3.68e+03\n",
            "Relative L2 error_u: 4.50e-01\n",
            "Relative L2 error_r: 1.16e-02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Save uv NN parameters successfully in %s ...checkpoints/Dec-19-2023_19-16-24-740962_M5\n",
            "Final loss loss_bc1: 5.104672e-05\n",
            "Final loss loss_bc2: 9.781888e-05\n",
            "Final loss loss_ics_u: 1.138217e-01\n",
            "\n",
            "\n",
            "Method: mini_batch\n",
            "\n",
            "average of time_list:3.6832e+03\n",
            "average of error_u_list:4.5001e-01\n",
            "average of error_r_list:1.1559e-02\n"
          ]
        }
      ],
      "source": [
        "# Define PINN model\n",
        "a = 0.5\n",
        "c = 2\n",
        "\n",
        "kernel_size = 300\n",
        "\n",
        "# Domain boundaries\n",
        "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
        "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
        "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
        "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
        "\n",
        "# Create initial conditions samplers\n",
        "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
        "\n",
        "# Create boundary conditions samplers\n",
        "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
        "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
        "bcs_sampler = [bc1, bc2]\n",
        "\n",
        "# Create residual sampler\n",
        "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
        "coll_sampler = Sampler(2, dom_coords, lambda x: u(x, a, c), name='coll')\n",
        "\n",
        "\n",
        "\n",
        "nIter =40001\n",
        "bcbatch_size = 500\n",
        "ubatch_size = 5000\n",
        "mbbatch_size = 300\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "mode = 'M5'\n",
        "layers = [2, 500, 500, 500, 1]\n",
        "\n",
        "\n",
        "nn = 200\n",
        "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
        "t, x = np.meshgrid(t, x)\n",
        "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
        "\n",
        "u_star = u(X_star, a,c)\n",
        "r_star = r(X_star, a, c)\n",
        "\n",
        "iterations = 1\n",
        "methods = [  \"mini_batch\"]\n",
        "\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "for mtd in methods:\n",
        "    print(\"Method: \", mtd)\n",
        "    time_list = []\n",
        "    error_u_list = []\n",
        "    error_r_list = []\n",
        "    \n",
        "    for index in range(iterations):\n",
        "\n",
        "        print(\"Epoch: \", str(index+1))\n",
        "\n",
        "        # Create residual sampler\n",
        "\n",
        "        # [elapsed, error_u , model] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
        "        tf.reset_default_graph()\n",
        "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "            # sess.run(init)\n",
        "\n",
        "            model = PINN(layers, operator ,  ics_sampler, bcs_sampler, res_sampler, c , mode , sess)\n",
        "            # Train model\n",
        "            start_time = time.time()\n",
        "\n",
        "            if mtd ==\"full_batch\":\n",
        "                print(\"full_batch method is used\")\n",
        "                model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "            elif mtd ==\"mini_batch\":\n",
        "                print(\"mini_batch method is used\")\n",
        "                model.trainmb(nIter, mbbatch_size)\n",
        "            else:\n",
        "                print(\"unknown method!\")\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            # Predictions\n",
        "            u_pred = model.predict_u(X_star)\n",
        "            r_pred = model.predict_r(X_star)\n",
        "            # Predictions\n",
        "\n",
        "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "            error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "            print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "            print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "            print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "\n",
        "\n",
        "            model.print(\"average adaptive_constant_bcs1_log : \" , np.average(model.adaptive_constant_bcs1_log))\n",
        "            model.print(\"average adaptive_constant_bcs2_log : \" ,  np.average(model.adaptive_constant_bcs2_log))\n",
        "            model.print(\"average adaptive_constant_ics_u_log : \" ,  np.average(model.adaptive_constant_ics_u_log))\n",
        "            # model.print(\"average adaptive_constant_ics_ut_log : \" ,  np.average(model.adaptive_constant_ics_ut_log))\n",
        "            # sess.close()  \n",
        "            model.plot_lambda()\n",
        "            model.plot_grad()\n",
        "            model.save_NN()\n",
        "            model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
        "            # sess.close()  \n",
        "\n",
        "        time_list.append(elapsed)\n",
        "        error_u_list.append(error_u)\n",
        "        error_r_list.append(error_r)\n",
        "\n",
        "    model.print(\"\\n\\nMethod: \", mtd)\n",
        "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "    model.print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "    result_dict[mtd] = [time_list ,error_u_list,error_r_list]\n",
        "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
        "\n",
        "###############################################################################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "average adaptive_constant_bcs1_log : 6.8604e+06\n",
            "average adaptive_constant_bcs2_log : 1.1612e+09\n",
            "average adaptive_constant_ics_u_log : 9.4952e+05\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Attempted to use a closed Session.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_14308/692465048.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt_prediction\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mu_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mr_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mr_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# sess.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_14308/3633663037.py\u001b[0m in \u001b[0;36msave_NN\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0muv_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0muv_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "model.print(\"average adaptive_constant_bcs1_log : \" , np.average(model.adaptive_constant_bcs1_log))\n",
        "model.print(\"average adaptive_constant_bcs2_log : \" ,  np.average(model.adaptive_constant_bcs2_log))\n",
        "model.print(\"average adaptive_constant_ics_u_log : \" ,  np.average(model.adaptive_constant_ics_u_log))\n",
        "# model.print(\"average adaptive_constant_ics_ut_log : \" ,  np.average(model.adaptive_constant_ics_ut_log))\n",
        "# sess.close()  \n",
        "model.plot_lambda()\n",
        "model.plot_grad()\n",
        "model.save_NN()\n",
        "model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
        "# sess.close()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for term in model.loss_list:\n",
        "    model.print('updating  lamda for loss ' , term , ':   ', np.array(model.lambda_val_list[term][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'loss_bc1': [0.015686769,\n",
              "  2.5778625,\n",
              "  0.26360357,\n",
              "  0.17325027,\n",
              "  0.39980224,\n",
              "  0.28063843,\n",
              "  0.06424001,\n",
              "  0.039986998,\n",
              "  0.2505087,\n",
              "  0.50829256,\n",
              "  0.5602535,\n",
              "  0.34757835,\n",
              "  0.13427413,\n",
              "  0.057680443,\n",
              "  0.053158194,\n",
              "  0.10446495,\n",
              "  0.05880063,\n",
              "  0.028127143,\n",
              "  0.00018533632,\n",
              "  0.025134519,\n",
              "  0.1023542,\n",
              "  0.20549442,\n",
              "  0.27953562,\n",
              "  0.31343406,\n",
              "  0.28867707,\n",
              "  0.2264886,\n",
              "  0.14996916,\n",
              "  0.08111393,\n",
              "  0.039163448,\n",
              "  0.01667751,\n",
              "  0.0077827806,\n",
              "  0.0049439343,\n",
              "  0.004865125,\n",
              "  0.009129898,\n",
              "  0.012720677,\n",
              "  0.011769869,\n",
              "  0.017200775,\n",
              "  0.021844115,\n",
              "  0.030505192,\n",
              "  0.041285247,\n",
              "  0.055376228,\n",
              "  0.07347159,\n",
              "  0.08100958,\n",
              "  0.08863105,\n",
              "  0.09826657,\n",
              "  0.102411725,\n",
              "  0.095383756,\n",
              "  0.10358077,\n",
              "  0.08040479,\n",
              "  0.07541983,\n",
              "  0.062275838,\n",
              "  0.05203336,\n",
              "  0.042465135,\n",
              "  0.036939293,\n",
              "  0.033731963,\n",
              "  0.03144329,\n",
              "  0.036140393,\n",
              "  0.03841101,\n",
              "  0.0381078,\n",
              "  0.040206183,\n",
              "  0.039643973,\n",
              "  0.03781557,\n",
              "  0.039549112,\n",
              "  0.039058886,\n",
              "  0.043113746,\n",
              "  0.048954602,\n",
              "  0.05735177,\n",
              "  0.061008114,\n",
              "  0.056940977,\n",
              "  0.05888206,\n",
              "  0.05935356,\n",
              "  0.054877788,\n",
              "  0.061391693,\n",
              "  0.055639826,\n",
              "  0.05044438,\n",
              "  0.055052225,\n",
              "  0.05077411,\n",
              "  0.047524728,\n",
              "  0.04278309,\n",
              "  0.04053913,\n",
              "  0.03777103,\n",
              "  0.042142957,\n",
              "  0.043206014,\n",
              "  0.04364509,\n",
              "  0.04852133,\n",
              "  0.04485381,\n",
              "  0.04358181,\n",
              "  0.048274912,\n",
              "  0.04765662,\n",
              "  0.05224763,\n",
              "  0.05868283,\n",
              "  0.068632886,\n",
              "  0.069667235,\n",
              "  0.06370346,\n",
              "  0.059879,\n",
              "  0.059955824,\n",
              "  0.057493214,\n",
              "  0.04929123,\n",
              "  0.051424537,\n",
              "  0.04837225,\n",
              "  0.05228642,\n",
              "  0.047477312,\n",
              "  0.045566894,\n",
              "  0.039195098,\n",
              "  0.040574625,\n",
              "  0.03752403,\n",
              "  0.032769658,\n",
              "  0.029971052,\n",
              "  0.02919843,\n",
              "  0.025753489,\n",
              "  0.022045307,\n",
              "  0.026294727,\n",
              "  0.024313107,\n",
              "  0.020466395,\n",
              "  0.020022653,\n",
              "  0.023612142,\n",
              "  0.02382143,\n",
              "  0.019265283,\n",
              "  0.022006264,\n",
              "  0.022889758,\n",
              "  0.02358129,\n",
              "  0.023678632,\n",
              "  0.021022806,\n",
              "  0.019014603,\n",
              "  0.021131752,\n",
              "  0.016944215,\n",
              "  0.01651194,\n",
              "  0.015646957,\n",
              "  0.017933311,\n",
              "  0.017613312,\n",
              "  0.016929787,\n",
              "  0.018197525,\n",
              "  0.017047608,\n",
              "  0.018665662,\n",
              "  0.018053984,\n",
              "  0.019833652,\n",
              "  0.020116888,\n",
              "  0.020036902,\n",
              "  0.022137666,\n",
              "  0.024079628,\n",
              "  0.02140131,\n",
              "  0.024655098,\n",
              "  0.025560947,\n",
              "  0.020084115,\n",
              "  0.023388283,\n",
              "  0.020481016,\n",
              "  0.021056253,\n",
              "  0.024452025,\n",
              "  0.020684086,\n",
              "  0.023915954,\n",
              "  0.019420743,\n",
              "  0.022652874,\n",
              "  0.020866724,\n",
              "  0.02210177,\n",
              "  0.026049383,\n",
              "  0.020631801,\n",
              "  0.022789521,\n",
              "  0.029288642,\n",
              "  0.03041765,\n",
              "  0.03034348,\n",
              "  0.02781049,\n",
              "  0.027724572,\n",
              "  0.024638688,\n",
              "  0.02263065,\n",
              "  0.022617998,\n",
              "  0.021551842,\n",
              "  0.020214014,\n",
              "  0.020541891,\n",
              "  0.022148421,\n",
              "  0.021204643,\n",
              "  0.023353253,\n",
              "  0.01809203,\n",
              "  0.019746881,\n",
              "  0.023193888,\n",
              "  0.022664316,\n",
              "  0.02664749,\n",
              "  0.021593517,\n",
              "  0.021919379,\n",
              "  0.020242821,\n",
              "  0.019318689,\n",
              "  0.020997424,\n",
              "  0.020499386,\n",
              "  0.017798131,\n",
              "  0.016856547,\n",
              "  0.020395162,\n",
              "  0.02414984,\n",
              "  0.020497337,\n",
              "  0.02360372,\n",
              "  0.0224618,\n",
              "  0.021703877,\n",
              "  0.023353409,\n",
              "  0.020516295,\n",
              "  0.023770152,\n",
              "  0.021236848,\n",
              "  0.020399652,\n",
              "  0.01814341,\n",
              "  0.022523861,\n",
              "  0.019005729,\n",
              "  0.02179551],\n",
              " 'loss_bc2': [0.0190239,\n",
              "  2.0764785,\n",
              "  0.072038956,\n",
              "  0.33196396,\n",
              "  1.0295914,\n",
              "  0.68396103,\n",
              "  0.28877214,\n",
              "  0.032057326,\n",
              "  0.15225257,\n",
              "  0.2782787,\n",
              "  0.24070293,\n",
              "  0.08225706,\n",
              "  0.031085545,\n",
              "  0.1377828,\n",
              "  0.29176632,\n",
              "  0.39780846,\n",
              "  0.3741992,\n",
              "  0.25600022,\n",
              "  0.12695661,\n",
              "  0.036142495,\n",
              "  0.0011727773,\n",
              "  0.008331849,\n",
              "  0.026837826,\n",
              "  0.038304973,\n",
              "  0.027552273,\n",
              "  0.011564608,\n",
              "  0.0041314526,\n",
              "  0.014862445,\n",
              "  0.038930632,\n",
              "  0.07402875,\n",
              "  0.10650106,\n",
              "  0.12457615,\n",
              "  0.13974033,\n",
              "  0.13584918,\n",
              "  0.13194579,\n",
              "  0.10290692,\n",
              "  0.08311662,\n",
              "  0.07052091,\n",
              "  0.04515871,\n",
              "  0.030864263,\n",
              "  0.018916316,\n",
              "  0.011129674,\n",
              "  0.006624071,\n",
              "  0.005084564,\n",
              "  0.00457777,\n",
              "  0.004968962,\n",
              "  0.008405469,\n",
              "  0.009828042,\n",
              "  0.011294028,\n",
              "  0.015687715,\n",
              "  0.017291365,\n",
              "  0.020867381,\n",
              "  0.023176821,\n",
              "  0.031494368,\n",
              "  0.034086633,\n",
              "  0.038219277,\n",
              "  0.040660568,\n",
              "  0.035717577,\n",
              "  0.04037715,\n",
              "  0.034878325,\n",
              "  0.033420663,\n",
              "  0.035872236,\n",
              "  0.027559381,\n",
              "  0.025587553,\n",
              "  0.023001187,\n",
              "  0.020083541,\n",
              "  0.019473247,\n",
              "  0.01899372,\n",
              "  0.016411148,\n",
              "  0.0154236695,\n",
              "  0.015687807,\n",
              "  0.016154993,\n",
              "  0.015391065,\n",
              "  0.015801702,\n",
              "  0.021625338,\n",
              "  0.02040939,\n",
              "  0.021408703,\n",
              "  0.021170754,\n",
              "  0.023441058,\n",
              "  0.023384681,\n",
              "  0.030042812,\n",
              "  0.02885791,\n",
              "  0.028789433,\n",
              "  0.035849933,\n",
              "  0.034608625,\n",
              "  0.03521502,\n",
              "  0.033576842,\n",
              "  0.034288302,\n",
              "  0.03176683,\n",
              "  0.02671451,\n",
              "  0.029465327,\n",
              "  0.029339133,\n",
              "  0.022998195,\n",
              "  0.02412519,\n",
              "  0.022980487,\n",
              "  0.022228584,\n",
              "  0.025939088,\n",
              "  0.0241674,\n",
              "  0.026192969,\n",
              "  0.02757751,\n",
              "  0.02272612,\n",
              "  0.022413064,\n",
              "  0.021526491,\n",
              "  0.024464844,\n",
              "  0.026272902,\n",
              "  0.019376082,\n",
              "  0.020178836,\n",
              "  0.014582262,\n",
              "  0.015828632,\n",
              "  0.012543145,\n",
              "  0.013591948,\n",
              "  0.009959502,\n",
              "  0.009278263,\n",
              "  0.0106248995,\n",
              "  0.008208278,\n",
              "  0.00763587,\n",
              "  0.008935779,\n",
              "  0.00864407,\n",
              "  0.0067056906,\n",
              "  0.009277158,\n",
              "  0.009872914,\n",
              "  0.007906604,\n",
              "  0.008501554,\n",
              "  0.006831936,\n",
              "  0.009449872,\n",
              "  0.007394147,\n",
              "  0.009786775,\n",
              "  0.007847198,\n",
              "  0.00864431,\n",
              "  0.007500513,\n",
              "  0.00959943,\n",
              "  0.009383598,\n",
              "  0.009146958,\n",
              "  0.010104258,\n",
              "  0.010732626,\n",
              "  0.008702243,\n",
              "  0.008763095,\n",
              "  0.009494107,\n",
              "  0.011589773,\n",
              "  0.011193693,\n",
              "  0.01090111,\n",
              "  0.011278408,\n",
              "  0.015080272,\n",
              "  0.013038068,\n",
              "  0.0110204695,\n",
              "  0.009812891,\n",
              "  0.014351547,\n",
              "  0.012746538,\n",
              "  0.012100761,\n",
              "  0.009805627,\n",
              "  0.010996175,\n",
              "  0.009895049,\n",
              "  0.010945797,\n",
              "  0.010633537,\n",
              "  0.009206348,\n",
              "  0.011164743,\n",
              "  0.011568961,\n",
              "  0.012348141,\n",
              "  0.009844369,\n",
              "  0.011317595,\n",
              "  0.010098171,\n",
              "  0.009982837,\n",
              "  0.010347811,\n",
              "  0.009451268,\n",
              "  0.010292039,\n",
              "  0.0099853445,\n",
              "  0.007604133,\n",
              "  0.010889504,\n",
              "  0.0105938725,\n",
              "  0.009838026,\n",
              "  0.009165163,\n",
              "  0.010316342,\n",
              "  0.012961869,\n",
              "  0.011082823,\n",
              "  0.008302458,\n",
              "  0.010158713,\n",
              "  0.009654768,\n",
              "  0.011652925,\n",
              "  0.010582533,\n",
              "  0.008259008,\n",
              "  0.009747542,\n",
              "  0.008880122,\n",
              "  0.009010612,\n",
              "  0.0106331585,\n",
              "  0.011893685,\n",
              "  0.010908382,\n",
              "  0.011009867,\n",
              "  0.012352401,\n",
              "  0.012884705,\n",
              "  0.010666329,\n",
              "  0.012961889,\n",
              "  0.011499286,\n",
              "  0.011180529,\n",
              "  0.00872769,\n",
              "  0.01044507,\n",
              "  0.010639451,\n",
              "  0.012485648,\n",
              "  0.012258518,\n",
              "  0.010370298],\n",
              " 'loss_ics_u': [0.41820595,\n",
              "  5.340099,\n",
              "  1.0301033,\n",
              "  0.36987555,\n",
              "  0.8598682,\n",
              "  0.3679113,\n",
              "  0.3871922,\n",
              "  0.89825135,\n",
              "  1.0862451,\n",
              "  1.0510787,\n",
              "  0.526028,\n",
              "  0.31463832,\n",
              "  0.24082871,\n",
              "  0.20705864,\n",
              "  0.323376,\n",
              "  0.3981423,\n",
              "  0.42171288,\n",
              "  0.43758598,\n",
              "  0.44227314,\n",
              "  0.47325787,\n",
              "  0.36528245,\n",
              "  0.3482347,\n",
              "  0.27261254,\n",
              "  0.35836342,\n",
              "  0.43142387,\n",
              "  0.34956723,\n",
              "  0.31823957,\n",
              "  0.39166275,\n",
              "  0.38127312,\n",
              "  0.4671138,\n",
              "  0.35520524,\n",
              "  0.37705365,\n",
              "  0.39238465,\n",
              "  0.3529647,\n",
              "  0.37339723,\n",
              "  0.32510883,\n",
              "  0.25435445,\n",
              "  0.32114914,\n",
              "  0.34491608,\n",
              "  0.37036982,\n",
              "  0.3223784,\n",
              "  0.3732583,\n",
              "  0.37502706,\n",
              "  0.36056876,\n",
              "  0.34888062,\n",
              "  0.35018212,\n",
              "  0.3355171,\n",
              "  0.33149838,\n",
              "  0.34548515,\n",
              "  0.2985716,\n",
              "  0.3763668,\n",
              "  0.32562158,\n",
              "  0.33892348,\n",
              "  0.37583748,\n",
              "  0.37505105,\n",
              "  0.32190597,\n",
              "  0.32789773,\n",
              "  0.32705352,\n",
              "  0.3151423,\n",
              "  0.35954586,\n",
              "  0.31079388,\n",
              "  0.34872776,\n",
              "  0.38377175,\n",
              "  0.29780006,\n",
              "  0.33056277,\n",
              "  0.3437229,\n",
              "  0.33928508,\n",
              "  0.3384837,\n",
              "  0.29945177,\n",
              "  0.3530985,\n",
              "  0.40143195,\n",
              "  0.3019611,\n",
              "  0.2781233,\n",
              "  0.32542324,\n",
              "  0.35115415,\n",
              "  0.25307426,\n",
              "  0.38764068,\n",
              "  0.35315746,\n",
              "  0.24801555,\n",
              "  0.35514012,\n",
              "  0.32619575,\n",
              "  0.3649379,\n",
              "  0.40656608,\n",
              "  0.35101715,\n",
              "  0.26442406,\n",
              "  0.3824257,\n",
              "  0.32921928,\n",
              "  0.25113314,\n",
              "  0.35863054,\n",
              "  0.3867927,\n",
              "  0.388163,\n",
              "  0.28150073,\n",
              "  0.35721245,\n",
              "  0.2569131,\n",
              "  0.34386986,\n",
              "  0.3070869,\n",
              "  0.26215032,\n",
              "  0.29696837,\n",
              "  0.34309193,\n",
              "  0.30020165,\n",
              "  0.2807905,\n",
              "  0.33727214,\n",
              "  0.31080684,\n",
              "  0.32969034,\n",
              "  0.35651076,\n",
              "  0.31802493,\n",
              "  0.368641,\n",
              "  0.35372204,\n",
              "  0.3510576,\n",
              "  0.38267422,\n",
              "  0.38814652,\n",
              "  0.4257723,\n",
              "  0.3305326,\n",
              "  0.27079326,\n",
              "  0.4160693,\n",
              "  0.3631781,\n",
              "  0.35755774,\n",
              "  0.4824585,\n",
              "  0.40434277,\n",
              "  0.5070867,\n",
              "  0.32864735,\n",
              "  0.3265656,\n",
              "  0.40598547,\n",
              "  0.37486258,\n",
              "  0.35825616,\n",
              "  0.4104063,\n",
              "  0.37402183,\n",
              "  0.42722824,\n",
              "  0.5233129,\n",
              "  0.42227668,\n",
              "  0.26644433,\n",
              "  0.4250258,\n",
              "  0.42177612,\n",
              "  0.42330098,\n",
              "  0.40745962,\n",
              "  0.4353906,\n",
              "  0.38591477,\n",
              "  0.425365,\n",
              "  0.41412532,\n",
              "  0.45421764,\n",
              "  0.4165976,\n",
              "  0.40171722,\n",
              "  0.35421348,\n",
              "  0.33636108,\n",
              "  0.45330483,\n",
              "  0.4485381,\n",
              "  0.2750708,\n",
              "  0.32847923,\n",
              "  0.32291472,\n",
              "  0.4304811,\n",
              "  0.3764,\n",
              "  0.35663453,\n",
              "  0.3721557,\n",
              "  0.4249852,\n",
              "  0.41195008,\n",
              "  0.35656014,\n",
              "  0.43165153,\n",
              "  0.36203057,\n",
              "  0.37631974,\n",
              "  0.33619195,\n",
              "  0.32275742,\n",
              "  0.33477727,\n",
              "  0.4038484,\n",
              "  0.4226867,\n",
              "  0.39968285,\n",
              "  0.35818726,\n",
              "  0.3287065,\n",
              "  0.43191513,\n",
              "  0.38966697,\n",
              "  0.43564093,\n",
              "  0.37291884,\n",
              "  0.3542125,\n",
              "  0.33110213,\n",
              "  0.37170875,\n",
              "  0.3526497,\n",
              "  0.35634598,\n",
              "  0.40786752,\n",
              "  0.37585166,\n",
              "  0.43477058,\n",
              "  0.39940667,\n",
              "  0.38575456,\n",
              "  0.3467967,\n",
              "  0.42424053,\n",
              "  0.40571046,\n",
              "  0.37266132,\n",
              "  0.40296143,\n",
              "  0.4519216,\n",
              "  0.43367395,\n",
              "  0.40555847,\n",
              "  0.36600044,\n",
              "  0.3619426,\n",
              "  0.36179107,\n",
              "  0.3495037,\n",
              "  0.3933011,\n",
              "  0.32543635,\n",
              "  0.43935382,\n",
              "  0.41101924,\n",
              "  0.37052858,\n",
              "  0.48240098],\n",
              " 'loss_ics_ut': [0.17354092,\n",
              "  9.01247,\n",
              "  0.4138926,\n",
              "  2.079203,\n",
              "  3.4584644,\n",
              "  1.6005683,\n",
              "  0.10758175,\n",
              "  0.29348347,\n",
              "  0.78117967,\n",
              "  0.47923172,\n",
              "  0.05752197,\n",
              "  0.0533395,\n",
              "  0.32215324,\n",
              "  0.4943439,\n",
              "  0.40216124,\n",
              "  0.21261646,\n",
              "  0.0620305,\n",
              "  0.006440753,\n",
              "  0.0010868908,\n",
              "  0.0029174353,\n",
              "  5.471116e-05,\n",
              "  0.005728276,\n",
              "  0.024777532,\n",
              "  0.04446825,\n",
              "  0.054113492,\n",
              "  0.049024682,\n",
              "  0.032876752,\n",
              "  0.017403102,\n",
              "  0.00962772,\n",
              "  0.0076879906,\n",
              "  0.0103488,\n",
              "  0.017935669,\n",
              "  0.031683568,\n",
              "  0.048512507,\n",
              "  0.060732096,\n",
              "  0.06330827,\n",
              "  0.056983873,\n",
              "  0.042488325,\n",
              "  0.027499562,\n",
              "  0.016621763,\n",
              "  0.010681635,\n",
              "  0.0078678895,\n",
              "  0.0078047155,\n",
              "  0.01045686,\n",
              "  0.017293906,\n",
              "  0.026050579,\n",
              "  0.035090562,\n",
              "  0.03928765,\n",
              "  0.036614295,\n",
              "  0.030194782,\n",
              "  0.019959517,\n",
              "  0.012097437,\n",
              "  0.00801091,\n",
              "  0.008137774,\n",
              "  0.01216347,\n",
              "  0.019612348,\n",
              "  0.028537722,\n",
              "  0.034238875,\n",
              "  0.035649464,\n",
              "  0.031516153,\n",
              "  0.026919493,\n",
              "  0.020159261,\n",
              "  0.014851806,\n",
              "  0.014266491,\n",
              "  0.016489359,\n",
              "  0.021602033,\n",
              "  0.024595475,\n",
              "  0.026276194,\n",
              "  0.022008566,\n",
              "  0.017777905,\n",
              "  0.014750451,\n",
              "  0.0140508,\n",
              "  0.014887133,\n",
              "  0.015645476,\n",
              "  0.017269686,\n",
              "  0.019351974,\n",
              "  0.01849434,\n",
              "  0.019876951,\n",
              "  0.021803295,\n",
              "  0.021006148,\n",
              "  0.021961655,\n",
              "  0.024298038,\n",
              "  0.027783502,\n",
              "  0.03257349,\n",
              "  0.032800242,\n",
              "  0.026129304,\n",
              "  0.020453896,\n",
              "  0.017072855,\n",
              "  0.012889676,\n",
              "  0.011487272,\n",
              "  0.015050667,\n",
              "  0.021098338,\n",
              "  0.02418242,\n",
              "  0.025056096,\n",
              "  0.021057129,\n",
              "  0.019568527,\n",
              "  0.016626894,\n",
              "  0.01120183,\n",
              "  0.009925517,\n",
              "  0.0102095315,\n",
              "  0.013016316,\n",
              "  0.013279068,\n",
              "  0.014543898,\n",
              "  0.016360834,\n",
              "  0.013615062,\n",
              "  0.0113083115,\n",
              "  0.008593789,\n",
              "  0.00713196,\n",
              "  0.0055809473,\n",
              "  0.0044547976,\n",
              "  0.0040707425,\n",
              "  0.004247714,\n",
              "  0.0043459455,\n",
              "  0.005586891,\n",
              "  0.006011909,\n",
              "  0.007462054,\n",
              "  0.007173325,\n",
              "  0.008906987,\n",
              "  0.010969822,\n",
              "  0.010252566,\n",
              "  0.014316921,\n",
              "  0.014887302,\n",
              "  0.013169131,\n",
              "  0.011682062,\n",
              "  0.013428658,\n",
              "  0.010404484,\n",
              "  0.008891311,\n",
              "  0.00727135,\n",
              "  0.007870759,\n",
              "  0.0067531574,\n",
              "  0.008119837,\n",
              "  0.008622922,\n",
              "  0.011647153,\n",
              "  0.0111414865,\n",
              "  0.01017731,\n",
              "  0.008824741,\n",
              "  0.011150353,\n",
              "  0.010534925,\n",
              "  0.011449492,\n",
              "  0.011911423,\n",
              "  0.012142172,\n",
              "  0.011502463,\n",
              "  0.0108986115,\n",
              "  0.009582367,\n",
              "  0.008385885,\n",
              "  0.008774151,\n",
              "  0.0071995435,\n",
              "  0.0060303644,\n",
              "  0.006981618,\n",
              "  0.0066484036,\n",
              "  0.005339396,\n",
              "  0.005208457,\n",
              "  0.004468784,\n",
              "  0.0059400056,\n",
              "  0.006400534,\n",
              "  0.0070361807,\n",
              "  0.00966879,\n",
              "  0.010644703,\n",
              "  0.0124714915,\n",
              "  0.013559568,\n",
              "  0.012326804,\n",
              "  0.010319059,\n",
              "  0.009480416,\n",
              "  0.0061059236,\n",
              "  0.0052618133,\n",
              "  0.0062561375,\n",
              "  0.005755594,\n",
              "  0.005488199,\n",
              "  0.0058847815,\n",
              "  0.0060380874,\n",
              "  0.006656642,\n",
              "  0.0090588415,\n",
              "  0.00860318,\n",
              "  0.010777319,\n",
              "  0.010822055,\n",
              "  0.009526034,\n",
              "  0.008487633,\n",
              "  0.008376563,\n",
              "  0.007977006,\n",
              "  0.0059454427,\n",
              "  0.0072281165,\n",
              "  0.0059843124,\n",
              "  0.0050743627,\n",
              "  0.006783968,\n",
              "  0.0075468575,\n",
              "  0.0062382175,\n",
              "  0.0097196065,\n",
              "  0.010285113,\n",
              "  0.012761683,\n",
              "  0.010960012,\n",
              "  0.010425487,\n",
              "  0.009669252,\n",
              "  0.010799926,\n",
              "  0.0067974734,\n",
              "  0.005424909,\n",
              "  0.0058831195,\n",
              "  0.006138505,\n",
              "  0.005432287,\n",
              "  0.0061350972]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictions\n",
        "# u_pred = model.predict_u(X_star)\n",
        "# r_pred = model.predict_r(X_star)\n",
        "\n",
        "# Predictions\n",
        "\n",
        "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "# print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "\n",
        "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
        "r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
        "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
        "# R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.pcolor(t, x, U_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.title('Exact u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.pcolor(t, x, U_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.pcolor(t, x, r_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Exact r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.pcolor(t, x, r_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.pcolor(t, x, np.abs(r_star - r_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGFobW0EatXj"
      },
      "outputs": [],
      "source": [
        "# Define PINN model\n",
        "a = 0.5\n",
        "c = 2\n",
        "\n",
        "kernel_size = 300\n",
        "\n",
        "# Domain boundaries\n",
        "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
        "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
        "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
        "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
        "\n",
        "# Create initial conditions samplers\n",
        "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
        "\n",
        "# Create boundary conditions samplers\n",
        "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
        "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
        "bcs_sampler = [bc1, bc2]\n",
        "\n",
        "# Create residual sampler\n",
        "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
        "\n",
        "\n",
        "\n",
        "nIter =40000\n",
        "bcbatch_size = 500\n",
        "ubatch_size = 5000\n",
        "mbbatch_size = 300\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "mode = 'M4'\n",
        "layers = [2, 500, 500, 500, 1]\n",
        "\n",
        "\n",
        "nn = 200\n",
        "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
        "t, x = np.meshgrid(t, x)\n",
        "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
        "\n",
        "u_star = u(X_star, a,c)\n",
        "r_star = r(X_star, a, c)\n",
        "\n",
        "iterations = 1\n",
        "methods = [  \"mini_batch\"]\n",
        "\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "for mtd in methods:\n",
        "    print(\"Method: \", mtd)\n",
        "    time_list = []\n",
        "    error_u_list = []\n",
        "    \n",
        "    for index in range(iterations):\n",
        "\n",
        "        print(\"Epoch: \", str(index+1))\n",
        "\n",
        "        # Create residual sampler\n",
        "\n",
        "        [elapsed, error_u] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
        "\n",
        "\n",
        "        print('elapsed: {:.2e}'.format(elapsed))\n",
        "        print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "\n",
        "        time_list.append(elapsed)\n",
        "        error_u_list.append(error_u)\n",
        "\n",
        "    print(\"\\n\\nMethod: \", mtd)\n",
        "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "    # print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "    result_dict[mtd] = [time_list ,error_u_list]\n",
        "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "    scipy.io.savemat(\"./1DWave_database/\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
        "\n",
        "###############################################################################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define PINN model\n",
        "a = 0.5\n",
        "c = 2\n",
        "\n",
        "kernel_size = 300\n",
        "\n",
        "# Domain boundaries\n",
        "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
        "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
        "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
        "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
        "\n",
        "# Create initial conditions samplers\n",
        "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
        "\n",
        "# Create boundary conditions samplers\n",
        "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
        "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
        "bcs_sampler = [bc1, bc2]\n",
        "\n",
        "# Create residual sampler\n",
        "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
        "\n",
        "\n",
        "\n",
        "nIter =40000\n",
        "bcbatch_size = 500\n",
        "ubatch_size = 5000\n",
        "mbbatch_size = 300\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "mode = 'M4'\n",
        "layers = [2, 500, 500, 500, 1]\n",
        "\n",
        "\n",
        "nn = 200\n",
        "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
        "t, x = np.meshgrid(t, x)\n",
        "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
        "\n",
        "u_star = u(X_star, a,c)\n",
        "r_star = r(X_star, a, c)\n",
        "\n",
        "iterations = 1\n",
        "methods = [  \"mini_batch\"]\n",
        "\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "for mtd in methods:\n",
        "    print(\"Method: \", mtd)\n",
        "    time_list = []\n",
        "    error_u_list = []\n",
        "    \n",
        "    for index in range(iterations):\n",
        "\n",
        "        print(\"Epoch: \", str(index+1))\n",
        "\n",
        "        # Create residual sampler\n",
        "\n",
        "        [elapsed, error_u] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
        "\n",
        "\n",
        "        print('elapsed: {:.2e}'.format(elapsed))\n",
        "        print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "\n",
        "        time_list.append(elapsed)\n",
        "        error_u_list.append(error_u)\n",
        "\n",
        "    print(\"\\n\\nMethod: \", mtd)\n",
        "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "    # print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "    result_dict[mtd] = [time_list ,error_u_list]\n",
        "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "    scipy.io.savemat(\"./1DWave_database/\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
        "\n",
        "###############################################################################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZtWEM9-brXF",
        "outputId": "371feba5-fed5-41cb-9e3e-5c8497c4fbe6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import scipy.io\n",
        "\n",
        "mode = 'M4'\n",
        "mbbatch_size = 128\n",
        "ubatch_size = 5000\n",
        "bcbatch_size = 500\n",
        "iterations = 40000\n",
        "\n",
        "time_list = []\n",
        "error_u_list = []\n",
        "error_v_list = []\n",
        "error_p_list = []\n",
        "    \n",
        "methods = [\"mini_batch\" , \"full_batch\"]\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "##Mini Batch\n",
        "time_list = []\n",
        "error_u_list = [ ]\n",
        "\n",
        "\n",
        "result_dict[\"mini_batch\"] = [time_list ,error_u_list]\n",
        "\n",
        "print(\"\\n\\nMethod: \", mtd)\n",
        "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "\n",
        "##Full Batch\n",
        "time_list = []\n",
        "error_u_list = [ ]\n",
        "error_v_list = []\n",
        "error_p_list = []\n",
        "\n",
        "result_dict[\"full_batch\"] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
        "\n",
        "print(\"\\n\\nMethod: \", mtd)\n",
        "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
        "print(\"average of error_p_list:\" , sum(error_p_list) / len(error_p_list) )\n",
        "\n",
        "\n",
        "scipy.io.savemat(\"./dataset/1DWave_database_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF1hwPUobyPE"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "itertaions = 80001\n",
        "log_NTK = True # Compute and store NTK matrix during training\n",
        "update_lam = True # Compute and update the loss weights using the NTK \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiyikOwBjRoZ"
      },
      "source": [
        "**Training Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Fw807UNzhu5z",
        "outputId": "f4551313-ffbc-49b5-8fd3-1296fc1641fe"
      },
      "outputs": [],
      "source": [
        "loss_res = model.loss_res_log\n",
        "loss_bcs = model.loss_bcs_log\n",
        "loss_u_t_ics = model.loss_ut_ics_log\n",
        "\n",
        "fig = plt.figure(figsize=(6, 5))\n",
        "plt.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
        "plt.plot(loss_bcs, label='$\\mathcal{L}_{u}$')\n",
        "plt.plot(loss_u_t_ics, label='$\\mathcal{L}_{u_t}$')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFLIBq5xjZ3v"
      },
      "source": [
        "**Model Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To0PDN17cc0v",
        "outputId": "1f47f288-322a-46b5-f173-45485191a68d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Predictions\n",
        "u_pred = model.predict_u(X_star)\n",
        "r_pred = model.predict_r(X_star)\n",
        "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "print('Relative L2 error_u: %e' % (error_u))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "K428lOuXhdc8",
        "outputId": "015f591b-d8a4-4e47-8020-84fcf219d7ca"
      },
      "outputs": [],
      "source": [
        "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
        "r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
        "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
        "R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.pcolor(t, x, U_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.title('Exact u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.pcolor(t, x, U_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.pcolor(t, x, r_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Exact r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.pcolor(t, x, R_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYdfKGLj6h0"
      },
      "source": [
        "**NTK Eigenvalues**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3dByeQjhBYj"
      },
      "outputs": [],
      "source": [
        "# Create empty lists for storing the eigenvalues of NTK\n",
        "lam_K_u_log = []\n",
        "lam_K_ut_log = []\n",
        "lam_K_r_log = []\n",
        "\n",
        "# Restore the NTK\n",
        "K_u_list = model.K_u_log\n",
        "K_ut_list = model.K_ut_log\n",
        "K_r_list = model.K_r_log\n",
        "\n",
        "K_list = []\n",
        "    \n",
        "for k in range(len(K_u_list)):\n",
        "    K_u = K_u_list[k]\n",
        "    K_ut = K_ut_list[k]\n",
        "    K_r = K_r_list[k]\n",
        "    \n",
        "    # Compute eigenvalues\n",
        "    lam_K_u, _ = np.linalg.eig(K_u)\n",
        "    lam_K_ut, _ = np.linalg.eig(K_ut)\n",
        "    lam_K_r, _ = np.linalg.eig(K_r)\n",
        "    # Sort in descresing order\n",
        "    lam_K_u = np.sort(np.real(lam_K_u))[::-1]\n",
        "    lam_K_ut = np.sort(np.real(lam_K_ut))[::-1]\n",
        "    lam_K_r = np.sort(np.real(lam_K_r))[::-1]\n",
        "    \n",
        "    # Store eigenvalues\n",
        "    lam_K_u_log.append(lam_K_u)\n",
        "    lam_K_ut_log.append(lam_K_ut)\n",
        "    lam_K_r_log.append(lam_K_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "vSn3Q_1IhisN",
        "outputId": "886908b3-c316-48d6-933f-81b1180ff954"
      },
      "outputs": [],
      "source": [
        "#  Eigenvalues of NTK\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "plt.subplot(1,3,1)\n",
        "\n",
        "plt.plot(lam_K_u_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_u_log[1], '--', label = '$n=10,000$')\n",
        "plt.plot(lam_K_u_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_u_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.title(r'Eigenvalues of ${K}_u$')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(lam_K_ut_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_ut_log[1], '--',label = '$n=10,000$')\n",
        "plt.plot(lam_K_ut_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_ut_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(r'Eigenvalues of ${K}_{u_t}$')\n",
        "\n",
        "ax =plt.subplot(1,3,3)\n",
        "plt.plot(lam_K_r_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_r_log[1], '--', label = '$n=10,000$')\n",
        "plt.plot(lam_K_r_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_r_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(r'Eigenvalues of ${K}_{r}$')\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.02),\n",
        "            borderaxespad=0, bbox_transform=fig.transFigure, ncol=4)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbUn_fcowojl"
      },
      "source": [
        "**Evolution of NTK Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYbzkhfMjJ8k"
      },
      "outputs": [],
      "source": [
        "if update_lam == True:\n",
        "\n",
        "  lam_u_log = model.lam_u_log\n",
        "  lam_ut_log = model.lam_ut_log\n",
        "  lam_r_log = model.lam_r_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "xzFzPCA2w1ML",
        "outputId": "71452cf9-3ebb-4aeb-9708-c7664b88e65d"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 5))\n",
        "plt.plot(lam_u_log, label='$\\lambda_u$')\n",
        "plt.plot(lam_ut_log, label='$\\lambda_{u_t}$')\n",
        "plt.plot(lam_r_log, label='$\\lambda_{r}$')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('$\\lambda$')\n",
        "plt.yscale('log')\n",
        "plt.legend( )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mimIv2Z5xlip"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINNsNTK_Wave.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
