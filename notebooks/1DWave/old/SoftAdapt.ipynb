{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkCgnRiYQSY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import timeit\n",
        "from scipy.interpolate import griddata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
        "import timeit\n",
        "\n",
        "import sys\n",
        "\n",
        "import scipy\n",
        "import scipy.io\n",
        "import time\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "import os.path\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "class Sampler:\n",
        "    # Initialize the class\n",
        "    def __init__(self, dim, coords, func, name = None):\n",
        "        self.dim = dim\n",
        "        self.coords = coords\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "    def sample(self, N):\n",
        "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
        "        y = self.func(x)\n",
        "        return x, y\n",
        "\n",
        "# Define the exact solution and its derivatives\n",
        "def u(x, a, c):\n",
        "    \"\"\"\n",
        "    :param x: x = (t, x)\n",
        "    \"\"\"\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    return np.sin(np.pi * x) * np.cos(c * np.pi * t) + a * np.sin(2 * c * np.pi* x) * np.cos(4 * c  * np.pi * t)\n",
        "\n",
        "def u_t(x,a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_t = -  c * np.pi * np.sin(np.pi * x) * np.sin(c * np.pi * t) -  a * 4 * c * np.pi * np.sin(2 * c * np.pi* x) * np.sin(4 * c * np.pi * t)\n",
        "    return u_t\n",
        "\n",
        "def u_tt(x, a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_tt = -(c * np.pi)**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) - a * (4 * c * np.pi)**2 *  np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
        "    return u_tt\n",
        "\n",
        "def u_xx(x, a, c):\n",
        "    t = x[:,0:1]\n",
        "    x = x[:,1:2]\n",
        "    u_xx = - np.pi**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) -  a * (2 * c * np.pi)** 2 * np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
        "    return  u_xx\n",
        "\n",
        "\n",
        "def r(x, a, c):\n",
        "    return u_tt(x, a, c) - c**2 * u_xx(x, a, c)\n",
        "\n",
        "def operator(u, t, x, c, sigma_t=1.0, sigma_x=1.0):\n",
        "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
        "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
        "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
        "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
        "    residual = u_tt - c**2 * u_xx\n",
        "    return residual\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SDqDWN3nfSAg"
      },
      "outputs": [],
      "source": [
        "class PINN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, c , mode ,  sess):\n",
        "        # Normalization \n",
        "#            model = PINN(layers, operator,coll_sampler ,  ics_sampler, bcs_sampler, res_sampler, c , mode , sess)\n",
        "\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        self.dirname, logpath = self.make_output_dir()\n",
        "        self.logger = self.get_logger(logpath)     \n",
        "\n",
        "        X, _ = res_sampler.sample(np.int32(1e5))\n",
        "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
        "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
        "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
        "\n",
        "        # Samplers\n",
        "        self.operator = operator\n",
        "        self.ics_sampler = ics_sampler\n",
        "        self.bcs_sampler = bcs_sampler\n",
        "        self.res_sampler = res_sampler\n",
        "\n",
        "        self.sess = sess\n",
        "        # Initialize network weights and biases\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "        \n",
        "        # weights\n",
        "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
        "        self.adaptive_constant_ics_val = np.array(1.0)\n",
        "        self.adaptive_constant_res_val = np.array(1.0)\n",
        "        self.rate = 0.9\n",
        "\n",
        "        # Wave constant\n",
        "        self.c = tf.constant(c, dtype=tf.float32)\n",
        "        \n",
        "        # self.kernel_size = kernel_size # Size of the NTK matrix\n",
        "\n",
        "        # Define Tensorflow session\n",
        "        self.sess = sess #tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
        "\n",
        "        # Define placeholders and computational graph\n",
        "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        \n",
        "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
        "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
        "        self.adaptive_constant_res_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_val.shape)\n",
        "        \n",
        "\n",
        "        # Evaluate predictions\n",
        "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
        "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
        "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
        "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
        "\n",
        "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
        "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
        "        \n",
        "\n",
        "        # Boundary loss and Initial loss\n",
        "        self.loss_ics_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
        "        self.loss_ics_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
        "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred))\n",
        "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
        "\n",
        "        self.loss_bcs = self.loss_ics_u + self.loss_bc1 + self.loss_bc2\n",
        "\n",
        "        # Residual loss\n",
        "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred))\n",
        "\n",
        "        # Total loss\n",
        "        self.loss =  self.adaptive_constant_res_tf * self.loss_res + self.adaptive_constant_bcs_tf * self.loss_bcs + self.adaptive_constant_ics_tf * self.loss_ics_u_t \n",
        "\n",
        "        # Define optimizer with learning rate schedule\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 1e-3\n",
        "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
        "        # Passing global_step to minimize() will increment it at each step.\n",
        "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
        "\n",
        "\n",
        "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 , self.loss_bc2 , self.loss_ics_u] \n",
        "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bc1\", \"loss_bc2\", \"loss_ics_u\"] \n",
        "\n",
        "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
        "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
        "        # Logger\n",
        "        self.loss_u_log = []\n",
        "        self.loss_r_log = []\n",
        "\n",
        "        # self.saver = tf.train.Saver()\n",
        "\n",
        "        # # Generate dicts for gradients storage\n",
        "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
        "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
        "        self.dict_gradients_ics_layers = self.generate_grad_dict()\n",
        "        \n",
        "   # Gradients Storage\n",
        "        self.grad_res = []\n",
        "        self.grad_ics = []\n",
        "        self.grad_bcs = []\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
        "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
        "            self.grad_ics.append(tf.gradients(self.loss_ics_u_t, self.weights[i])[0])\n",
        "\n",
        "           \n",
        "        self.max_grad_res_list = []\n",
        "        self.mean_grad_bcs_list = []\n",
        "        self.mean_grad_ics_list = []\n",
        "\n",
        "        self.adaptive_constant_bcs_log = []\n",
        "        self.adaptive_constant_ics_log = []\n",
        "        self.adaptive_constant_res_log = []\n",
        "\n",
        "        self.max_grad_res_log = []\n",
        "        self.mean_grad_bcs_log = []\n",
        "        self.mean_grad_ics_log = []\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.max_grad_res_list.append(tf.math.reduce_max(tf.abs(self.grad_res[i]))) \n",
        "            self.mean_grad_bcs_list.append(tf.math.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
        "            self.mean_grad_ics_list.append(tf.math.reduce_mean(tf.abs(self.grad_ics[i])))\n",
        "        \n",
        "        self.max_grad_res = tf.math.reduce_mean(tf.stack(self.max_grad_res_list))\n",
        "        self.mean_grad_bcs = tf.math.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
        "        self.mean_grad_ics = tf.math.reduce_mean(tf.stack(self.mean_grad_ics_list))\n",
        "        \n",
        "        self.adaptive_constant_bcs = self.max_grad_res  / self.mean_grad_bcs\n",
        "        self.adaptive_constant_ics = self.max_grad_res  / self.mean_grad_ics\n",
        "        self.adaptive_constant_res = self.max_grad_res\n",
        "\n",
        "\n",
        "        self.prev_loss_ics_u = 0\n",
        "        self.prev_loss_ics_u_t = 0\n",
        "        self.prev_loss_bc1  =0\n",
        "        self.prev_loss_bc2 = 0\n",
        "\n",
        "        self.prev2_loss_ics_u = 0\n",
        "        self.prev2_loss_ics_u_t = 0\n",
        "        self.prev2_loss_bc1  = 0\n",
        "        self.prev2_loss_bc2 = 0\n",
        "\n",
        "         # Initialize Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    # Initialize network weights and biases using Xavier initialization\n",
        "    def initialize_NN(self, layers):\n",
        "        # Xavier initialization\n",
        "        def xavier_init(size):\n",
        "            in_dim = size[0]\n",
        "            out_dim = size[1]\n",
        "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "            return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
        "\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = xavier_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    # Evaluates the forward pass\n",
        "    def forward_pass(self, H, layers, weights, biases):\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        H = tf.add(tf.matmul(H, W), b)\n",
        "        return H\n",
        "\n",
        "    # Forward pass for u\n",
        "    def net_u(self, t, x):\n",
        "        u = self.forward_pass(tf.concat([t, x], 1), self.layers, self.weights,  self.biases)\n",
        "        return u\n",
        "\n",
        "    # Forward pass for du/dt\n",
        "    def net_u_t(self, t, x):\n",
        "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
        "        return u_t\n",
        "\n",
        "    # Forward pass for the residual\n",
        "    def net_r(self, t, x):\n",
        "        u = self.net_u(t, x)\n",
        "        residual = self.operator(u, t, x, self.c, self.sigma_t,  self.sigma_x)\n",
        "        return residual\n",
        "    \n",
        "    def fetch_minibatch(self, sampler, N):\n",
        "        X, Y = sampler.sample(N)\n",
        "        X = (X - self.mu_X) / self.sigma_X\n",
        "        return X, Y\n",
        "\n",
        "        # Trains the model by minimizing the MSE loss\n",
        "\n",
        "    def trainmb(self, nIter=10000, batch_size=128, log_NTK=False, update_lam=False):\n",
        "\n",
        "        itValues = [1,100,1000,39999]\n",
        "\n",
        "        start_time = timeit.default_timer()\n",
        "        for it in range(1, nIter):\n",
        "            # Fetch boundary mini-batches\n",
        "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size // 3)\n",
        "            X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], batch_size // 3)\n",
        "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size // 3)\n",
        "            \n",
        "            # Fetch residual mini-batch\n",
        "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
        "\n",
        "            # Define a dictionary for associating placeholders with data\n",
        "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
        "                       self.x_ics_tf: X_ics_batch[:, 1:2],\n",
        "                       self.u_ics_tf: u_ics_batch,\n",
        "                       self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
        "                        self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
        "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
        "                       self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
        "                       self.t_r_tf: X_res_batch[:, 0:1], \n",
        "                       self.x_r_tf: X_res_batch[:, 1:2],\n",
        "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
        "                       self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
        "                       self.adaptive_constant_res_tf: self.adaptive_constant_res_val\n",
        "                       }#self.lam_r_val}\n",
        "\n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
        "\n",
        "            # self.print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = timeit.default_timer() - start_time\n",
        "\n",
        "                loss = self.sess.run(self.loss, tf_dict)\n",
        "                loss_bc1 = self.sess.run(self.loss_bc1, tf_dict)\n",
        "                loss_bc2 = self.sess.run(self.loss_bc2, tf_dict)\n",
        "                loss_ics_u = self.sess.run(self.loss_ics_u, tf_dict)\n",
        "                loss_ics_u_t = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
        "                loss_res = self.sess.run(self.loss_res, tf_dict)\n",
        "\n",
        "                self.print('It: %d |  Loss: %.3e |  Loss_res: %.3e |   Loss_bcs1: %.3e | Loss_bc2s: %.3e |  loss_ics_u: %.3e |  Loss_ut_ics: %.3e |  Time: %.2f' %(it ,  \n",
        "                                                                                                                                                                   loss ,\n",
        "                                                                                                                                                                       loss_res ,\n",
        "                                                                                                                                                                           loss_bc1 ,\n",
        "                                                                                                                                                                               loss_bc2  , \n",
        "                                                                                                                                                                                 loss_ics_u ,\n",
        "                                                                                                                                                                                     loss_ics_u_t  ,\n",
        "                                                                                                                                                                                         elapsed))\n",
        "\n",
        "\n",
        "                # batch_losses = [loss ,  loss_res,  loss_bc1 , loss_bc2 , loss_ics_u] \n",
        "            if it % 100 == 0:\n",
        "                norm = np.exp(self.rate * ((loss_bc1 - 2.0 * self.prev_loss_bc1 + self.prev2_loss_bc1 ) + (loss_bc2 - 2.0* self.prev_loss_bc2 + self.prev2_loss_bc2) +\\\n",
        "                              ( loss_ics_u - 2.0* self.prev_loss_ics_u + self.prev2_loss_ics_u ) +( loss_ics_u_t - 2.0* self.prev_loss_ics_u_t +  self.prev2_loss_ics_u_t)))\n",
        "                # Compute and self.print adaptive weights during training\n",
        "                    # Compute the adaptive constant\n",
        "                # adaptive_constant_res , adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [ self.adaptive_constant_res , self.adaptive_constant_bcs, self.adaptive_constant_ics  ], tf_dict)\n",
        "                # self.print adaptive weights during training\n",
        "                self.adaptive_constant_res_val =  1.0\n",
        "                self.adaptive_constant_ics_val = 1000 * np.exp(self.rate * (( loss_ics_u - 2.0* self.prev_loss_ics_u + self.prev2_loss_ics_u ) +( loss_ics_u_t - 2.0* self.prev_loss_ics_u_t + self.prev2_loss_ics_u_t))) /norm\n",
        "                self.adaptive_constant_bcs_val = 1000 * np.exp(self.rate * ((loss_bc1 - 2.0 * self.prev_loss_bc1 + self.prev2_loss_bc1 ) + (loss_bc2 - 2.0* self.prev_loss_bc2 + self.prev2_loss_bc2) ))/norm\n",
        "\n",
        "\n",
        "                self.print('adaptive_constant_res_val: {:.3e}'.format(  self.adaptive_constant_res_val))\n",
        "                self.print('adaptive_constant_ics_val: {:.3e}'.format(  self.adaptive_constant_ics_val))\n",
        "                self.print('adaptive_constant_bcs_val: {:.3e}'.format(  self.adaptive_constant_bcs_val))\n",
        "                \n",
        "                self.adaptive_constant_res_log.append( self.adaptive_constant_res_val)\n",
        "                self.adaptive_constant_bcs_log.append( self.adaptive_constant_bcs_val)\n",
        "                self.adaptive_constant_ics_log.append( self.adaptive_constant_ics_val)\n",
        "\n",
        "                self.prev2_loss_ics_u = self.prev_loss_ics_u\n",
        "                self.prev2_loss_ics_u_t = self.prev_loss_ics_u_t \n",
        "                self.prev2_loss_bc1  = self.prev_loss_bc1 \n",
        "                self.prev2_loss_bc2 = self.prev_loss_bc2\n",
        "\n",
        "                self.prev_loss_ics_u = loss_ics_u\n",
        "                self.prev_loss_ics_u_t = loss_ics_u_t\n",
        "                self.prev_loss_bc1  = loss_bc1\n",
        "                self.prev_loss_bc2 = loss_bc2\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "            max_grad_res , mean_grad_bcs, mean_grad_ics = self.sess.run( [ self.max_grad_res , self.mean_grad_bcs, self.mean_grad_ics  ], tf_dict)\n",
        "            self.max_grad_res_log.append(max_grad_res)\n",
        "            self.mean_grad_bcs_log.append(mean_grad_bcs)\n",
        "            self.mean_grad_ics_log.append(mean_grad_ics)\n",
        "\n",
        "            sys.stdout.flush()\n",
        "            start_time = timeit.default_timer()\n",
        "            \n",
        "            if it in itValues:\n",
        "                    self.plot_layerLoss(tf_dict , it)\n",
        "                    self.print(\"Gradients information stored ...\")\n",
        "\n",
        "            sys.stdout.flush()\n",
        "            self.assign_batch_losses(batch_losses)\n",
        "            for key in self.loss_history:\n",
        "                self.loss_history[key].append(self.epoch_loss[key])\n",
        "\n",
        "\n",
        "    def train(self, nIter , bcbatch_size , ubatch_size):\n",
        "\n",
        "        start_time = timeit.default_timer()\n",
        "        # Fetch boundary mini-batches\n",
        "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, bcbatch_size)\n",
        "        X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
        "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
        "        \n",
        "        # Fetch residual mini-batch\n",
        "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
        "\n",
        "        # Define a dictionary for associating placeholders with data\n",
        "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
        "                    self.x_ics_tf: X_ics_batch[:, 1:2],\n",
        "                    self.u_ics_tf: u_ics_batch,\n",
        "                    self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
        "                    self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
        "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
        "                    self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
        "                    self.t_r_tf: X_res_batch[:, 0:1], \n",
        "                    self.x_r_tf: X_res_batch[:, 1:2],\n",
        "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
        "                    self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val\n",
        "                    }#self.lam_r_val}\n",
        "        \n",
        "        for it in range(nIter):\n",
        "\n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "            self.sess.run(self.train_op, tf_dict)\n",
        "\n",
        "            # self.print\n",
        "            if it % 1000 == 0:\n",
        "                elapsed = timeit.default_timer() - start_time\n",
        "\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
        "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
        "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
        "\n",
        "                self.print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
        "                \n",
        "                # Compute and self.print adaptive weights during training\n",
        "                    # Compute the adaptive constant\n",
        "                adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [self.adaptive_constant_bcs, self.adaptive_constant_ics  ], tf_dict)\n",
        "                # self.print adaptive weights during training\n",
        "                self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
        "                self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
        "\n",
        "\n",
        "                self.print('lambda_u: {:.3e}'.format(self.adaptive_constant_bcs_val))\n",
        "                self.print('lambda_ut: {:.3e}'.format(self.adaptive_constant_ics_val))\n",
        "                sys.stdout.flush()\n",
        "\n",
        "                         \n",
        "    # Evaluates predictions at test points\n",
        "    def predict_u(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        return u_star\n",
        "\n",
        "        # Evaluates predictions at test points\n",
        "\n",
        "    def predict_r(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
        "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
        "        return r_star\n",
        "    \n",
        "   ###############################################################################################################################################\n",
        "   # \n",
        "   # ###############################################################################################################################################\n",
        "   # \n",
        "   # ###############################################################################################################################################\n",
        "   # \n",
        "   #  \n",
        "    def plot_layerLoss(self , tf_dict , epoch):\n",
        "        ## Gradients #\n",
        "        num_layers = len(self.layers)\n",
        "        for i in range(num_layers - 1):\n",
        "            grad_res, grad_bc1  , grad_ics  = self.sess.run([ self.grad_res[i],self.grad_bcs[i],self.grad_ics[i]], feed_dict=tf_dict)\n",
        "\n",
        "            # save gradients of loss_r and loss_u\n",
        "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
        "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
        "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics.flatten())\n",
        "\n",
        "        num_hidden_layers = num_layers -1\n",
        "        cnt = 1\n",
        "        fig = plt.figure(4, figsize=(13, 4))\n",
        "        for j in range(num_hidden_layers):\n",
        "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
        "            ax.set_title('Layer {}'.format(j + 1))\n",
        "            ax.set_yscale('symlog')\n",
        "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
        "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
        "            gradients_ics = self.dict_gradients_ics_layers['layer_' + str(j + 1)][-1]\n",
        "\n",
        "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
        "\n",
        "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
        "            sns.distplot(gradients_ics, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{ics}}$')\n",
        "\n",
        "            #ax.get_legend().remove()\n",
        "            ax.set_xlim([-1.0, 1.0])\n",
        "            #ax.set_ylim([0, 150])\n",
        "            cnt += 1\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=3)\n",
        "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
        "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
        "        plt.close(\"all\")\n",
        "    # #########################\n",
        "    # def make_output_dir(self):\n",
        "        \n",
        "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
        "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
        "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "    #     os.mkdir(dirname)\n",
        "    #     text = 'output.log'\n",
        "    #     logpath = os.path.join(dirname, text)\n",
        "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
        "\n",
        "    #     return dirname, logpath\n",
        "    \n",
        "    # # ###########################################################\n",
        "    def make_output_dir(self):\n",
        "        \n",
        "        if not os.path.exists(\"checkpoints\"):\n",
        "            os.mkdir(\"checkpoints\")\n",
        "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "        os.mkdir(dirname)\n",
        "        text = 'output.log'\n",
        "        logpath = os.path.join(dirname, text)\n",
        "        shutil.copyfile('M1.ipynb', os.path.join(dirname, 'M1.ipynb'))\n",
        "        return dirname, logpath\n",
        "    \n",
        "\n",
        "    def get_logger(self, logpath):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "        sh = logging.StreamHandler()\n",
        "        sh.setLevel(logging.DEBUG)        \n",
        "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
        "        fh = logging.FileHandler(logpath)\n",
        "        logger.addHandler(sh)\n",
        "        logger.addHandler(fh)\n",
        "        return logger\n",
        "\n",
        "\n",
        "   \n",
        "    def print(self, *args):\n",
        "        for word in args:\n",
        "            if len(args) == 1:\n",
        "                self.logger.info(word)\n",
        "            elif word != args[-1]:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "            else:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\\n\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "\n",
        "\n",
        "    def plot_loss_history(self , path):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        for key in self.loss_history:\n",
        "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
        "            ax.semilogy(self.loss_history[key], label=key)\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        plt.savefig(path)\n",
        "        #plt.show()\n",
        "       #######################\n",
        "    def save_NN(self):\n",
        "\n",
        "        uv_weights = self.sess.run(self.weights)\n",
        "        uv_biases = self.sess.run(self.biases)\n",
        "\n",
        "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
        "            pickle.dump([uv_weights, uv_biases], f)\n",
        "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
        "\n",
        "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
        "        #     pickle.dump(self.loss_rec, f)\n",
        "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
        "            self.plot_loss_history(f)\n",
        "\n",
        "\n",
        "    def assign_batch_losses(self, batch_losses):\n",
        "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
        "            self.epoch_loss[key] = loss_values\n",
        "\n",
        "\n",
        "    def generate_grad_dict(self):\n",
        "        num = len(self.layers) - 1\n",
        "        grad_dict = {}\n",
        "        for i in range(num):\n",
        "            grad_dict['layer_{}'.format(i + 1)] = []\n",
        "        return grad_dict\n",
        "    \n",
        "    def assign_batch_losses(self, batch_losses):\n",
        "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
        "            self.epoch_loss[key] = loss_values\n",
        "            \n",
        "    def plt_prediction(self , t , x , X_star , u_star , u_pred , r_star , r_pred):\n",
        "        \n",
        "        U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
        "        r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
        "        U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
        "        R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(18, 9))\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.pcolor(t, x, U_star, cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$x_1$')\n",
        "        plt.ylabel('$x_2$')\n",
        "        plt.title('Exact u(t, x)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.pcolor(t, x, U_pred, cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Predicted u(t, x)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Absolute error')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 4)\n",
        "        plt.pcolor(t, x, r_star, cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Exact r(t, x)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.pcolor(t, x, R_pred, cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Predicted r(t, x)')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel('$t$')\n",
        "        plt.ylabel('$x$')\n",
        "        plt.title('Absolute error')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    def plot_lambda(self ):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        ax.semilogy(self.adaptive_constant_bcs_log, label=\"adaptive_constant_bcs_log\")\n",
        "        ax.semilogy(self.adaptive_constant_ics_log, label=\"adaptive_constant_ics_log\")\n",
        "        ax.semilogy(self.adaptive_constant_res_log, label=\"adaptive_constant_res_log\")\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        path = os.path.join(self.dirname,'lambda_history.png')\n",
        "        plt.savefig(path)\n",
        "\n",
        "    def plot_grad(self ):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        ax.semilogy(self.max_grad_res_log, label=\"max_grad_res_log\")\n",
        "        ax.semilogy(self.mean_grad_bcs_log, label=\"mean_grad_bcs_log\")\n",
        "        ax.semilogy(self.mean_grad_ics_log, label=\"mean_grad_ics_log\")\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        path = os.path.join(self.dirname,'grad_history.png')\n",
        "        plt.savefig(path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
        "def test_method(method , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "        # sess.run(init)\n",
        "\n",
        "        model = PINN(layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess)\n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "\n",
        "        if method ==\"full_batch\":\n",
        "            print(\"full_batch method is used\")\n",
        "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "        elif method ==\"mini_batch\":\n",
        "            print(\"mini_batch method is used\")\n",
        "            model.trainmb(nIter, mbbatch_size)\n",
        "        else:\n",
        "            print(\"unknown method!\")\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Predictions\n",
        "        u_pred = model.predict_u(X_star)\n",
        "        r_pred = model.predict_r(X_star)\n",
        "        # Predictions\n",
        "\n",
        "        sess.close()   \n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "    print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "\n",
        "\n",
        "    return [elapsed, error_u  ]\n",
        "\n",
        "###############################################################################################################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method:  mini_batch\n",
            "Epoch:  1\n",
            "WARNING:tensorflow:From /tmp/ipykernel_25834/1855831049.py:66: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_25834/1855831049.py:67: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_25834/1855831049.py:68: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_25834/1855831049.py:68: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-15 02:11:48.519323: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-15 02:11:48.559340: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
            "2023-12-15 02:11:48.560125: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56328f54c810 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-12-15 02:11:48.560168: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-12-15 02:11:48.565440: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_25834/2697325055.py:44: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_25834/2697325055.py:92: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_25834/2697325055.py:94: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_25834/2697325055.py:161: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "mini_batch method is used\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 100 |  Loss: 3.149e-01 |  Loss_res: 7.385e-03 |   Loss_bcs1: 4.686e-02 | Loss_bc2s: 1.929e-02 |  loss_ics_u: 2.326e-01 |  Loss_ut_ics: 8.739e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.422e+02\n",
            "adaptive_constant_bcs_val: 8.048e+02\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 200 |  Loss: 2.114e+02 |  Loss_res: 3.523e+01 |   Loss_bcs1: 2.571e-02 | Loss_bc2s: 1.484e-02 |  loss_ics_u: 1.776e-01 |  Loss_ut_ics: 6.672e-04 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.086e+03\n",
            "adaptive_constant_bcs_val: 1.315e+03\n",
            "It: 300 |  Loss: 2.791e+02 |  Loss_res: 5.068e+01 |   Loss_bcs1: 1.193e-02 | Loss_bc2s: 1.658e-03 |  loss_ics_u: 1.522e-01 |  Loss_ut_ics: 9.499e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 9.589e+02\n",
            "It: 400 |  Loss: 1.716e+02 |  Loss_res: 1.844e+01 |   Loss_bcs1: 1.450e-02 | Loss_bc2s: 5.876e-03 |  loss_ics_u: 1.378e-01 |  Loss_ut_ics: 1.448e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.701e+02\n",
            "adaptive_constant_bcs_val: 1.005e+03\n",
            "It: 500 |  Loss: 1.576e+02 |  Loss_res: 1.847e+01 |   Loss_bcs1: 1.712e-02 | Loss_bc2s: 5.615e-03 |  loss_ics_u: 1.141e-01 |  Loss_ut_ics: 1.577e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.004e+03\n",
            "adaptive_constant_bcs_val: 1.001e+03\n",
            "It: 600 |  Loss: 1.489e+02 |  Loss_res: 1.331e+01 |   Loss_bcs1: 1.837e-02 | Loss_bc2s: 4.792e-03 |  loss_ics_u: 1.091e-01 |  Loss_ut_ics: 3.220e-03 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.002e+03\n",
            "adaptive_constant_bcs_val: 9.821e+02\n",
            "It: 700 |  Loss: 1.775e+02 |  Loss_res: 1.373e+01 |   Loss_bcs1: 1.471e-02 | Loss_bc2s: 3.155e-03 |  loss_ics_u: 9.731e-02 |  Loss_ut_ics: 5.059e-02 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.005e+03\n",
            "adaptive_constant_bcs_val: 9.655e+02\n",
            "It: 800 |  Loss: 1.261e+02 |  Loss_res: 1.743e+01 |   Loss_bcs1: 6.513e-03 | Loss_bc2s: 9.767e-03 |  loss_ics_u: 9.411e-02 |  Loss_ut_ics: 2.110e-03 |  Time: 0.15\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.967e+02\n",
            "adaptive_constant_bcs_val: 1.082e+03\n",
            "It: 900 |  Loss: 1.871e+02 |  Loss_res: 1.351e+01 |   Loss_bcs1: 1.155e-02 | Loss_bc2s: 5.513e-03 |  loss_ics_u: 9.582e-02 |  Loss_ut_ics: 5.171e-02 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.979e+02\n",
            "adaptive_constant_bcs_val: 9.115e+02\n",
            "It: 1000 |  Loss: 1.296e+02 |  Loss_res: 8.463e+00 |   Loss_bcs1: 1.426e-02 | Loss_bc2s: 6.697e-03 |  loss_ics_u: 1.079e-01 |  Loss_ut_ics: 3.735e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.972e+02\n",
            "adaptive_constant_bcs_val: 1.082e+03\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 1100 |  Loss: 1.549e+02 |  Loss_res: 1.128e+01 |   Loss_bcs1: 1.479e-02 | Loss_bc2s: 9.901e-03 |  loss_ics_u: 1.065e-01 |  Loss_ut_ics: 1.681e-03 |  Time: 0.25\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 9.712e+02\n",
            "It: 1200 |  Loss: 1.299e+02 |  Loss_res: 1.248e+01 |   Loss_bcs1: 1.007e-02 | Loss_bc2s: 5.231e-03 |  loss_ics_u: 1.034e-01 |  Loss_ut_ics: 2.116e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.012e+03\n",
            "adaptive_constant_bcs_val: 9.993e+02\n",
            "It: 1300 |  Loss: 1.718e+02 |  Loss_res: 7.694e+00 |   Loss_bcs1: 1.872e-02 | Loss_bc2s: 9.306e-03 |  loss_ics_u: 1.022e-01 |  Loss_ut_ics: 3.352e-02 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.803e+02\n",
            "adaptive_constant_bcs_val: 9.709e+02\n",
            "It: 1400 |  Loss: 1.277e+02 |  Loss_res: 1.197e+01 |   Loss_bcs1: 5.919e-03 | Loss_bc2s: 4.908e-03 |  loss_ics_u: 1.020e-01 |  Loss_ut_ics: 6.301e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.027e+03\n",
            "adaptive_constant_bcs_val: 1.053e+03\n",
            "It: 1500 |  Loss: 1.230e+02 |  Loss_res: 8.843e+00 |   Loss_bcs1: 1.058e-02 | Loss_bc2s: 6.401e-03 |  loss_ics_u: 8.631e-02 |  Loss_ut_ics: 5.223e-03 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.792e+02\n",
            "adaptive_constant_bcs_val: 9.905e+02\n",
            "It: 1600 |  Loss: 1.403e+02 |  Loss_res: 7.796e+00 |   Loss_bcs1: 1.764e-02 | Loss_bc2s: 6.632e-03 |  loss_ics_u: 9.639e-02 |  Loss_ut_ics: 1.331e-02 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.990e+02\n",
            "adaptive_constant_bcs_val: 9.690e+02\n",
            "It: 1700 |  Loss: 1.371e+02 |  Loss_res: 8.322e+00 |   Loss_bcs1: 6.066e-03 | Loss_bc2s: 5.624e-03 |  loss_ics_u: 1.127e-01 |  Loss_ut_ics: 8.289e-03 |  Time: 0.10\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.018e+03\n",
            "adaptive_constant_bcs_val: 1.006e+03\n",
            "It: 1800 |  Loss: 1.303e+02 |  Loss_res: 9.000e+00 |   Loss_bcs1: 6.259e-03 | Loss_bc2s: 6.512e-03 |  loss_ics_u: 1.053e-01 |  Loss_ut_ics: 2.446e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.878e+02\n",
            "adaptive_constant_bcs_val: 1.022e+03\n",
            "It: 1900 |  Loss: 1.909e+02 |  Loss_res: 1.254e+01 |   Loss_bcs1: 1.570e-02 | Loss_bc2s: 9.091e-03 |  loss_ics_u: 6.744e-02 |  Loss_ut_ics: 8.515e-02 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.902e+02\n",
            "adaptive_constant_bcs_val: 9.491e+02\n",
            "It: 2000 |  Loss: 1.222e+02 |  Loss_res: 9.131e+00 |   Loss_bcs1: 1.099e-02 | Loss_bc2s: 5.787e-03 |  loss_ics_u: 8.999e-02 |  Loss_ut_ics: 1.186e-02 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.018e+03\n",
            "adaptive_constant_bcs_val: 1.090e+03\n",
            "It: 2100 |  Loss: 1.150e+02 |  Loss_res: 8.652e+00 |   Loss_bcs1: 6.697e-03 | Loss_bc2s: 6.207e-03 |  loss_ics_u: 8.014e-02 |  Loss_ut_ics: 4.847e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.963e+02\n",
            "adaptive_constant_bcs_val: 9.700e+02\n",
            "It: 2200 |  Loss: 9.265e+01 |  Loss_res: 7.077e+00 |   Loss_bcs1: 4.877e-03 | Loss_bc2s: 5.353e-03 |  loss_ics_u: 7.522e-02 |  Loss_ut_ics: 2.699e-03 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.989e+02\n",
            "adaptive_constant_bcs_val: 9.912e+02\n",
            "It: 2300 |  Loss: 1.061e+02 |  Loss_res: 9.444e+00 |   Loss_bcs1: 6.076e-03 | Loss_bc2s: 7.257e-03 |  loss_ics_u: 8.129e-02 |  Loss_ut_ics: 2.838e-03 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.948e+02\n",
            "adaptive_constant_bcs_val: 9.881e+02\n",
            "It: 2400 |  Loss: 1.166e+02 |  Loss_res: 8.829e+00 |   Loss_bcs1: 5.969e-03 | Loss_bc2s: 6.167e-03 |  loss_ics_u: 8.237e-02 |  Loss_ut_ics: 1.449e-02 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.004e+03\n",
            "adaptive_constant_bcs_val: 9.942e+02\n",
            "It: 2500 |  Loss: 1.165e+02 |  Loss_res: 8.415e+00 |   Loss_bcs1: 8.385e-03 | Loss_bc2s: 5.086e-03 |  loss_ics_u: 8.867e-02 |  Loss_ut_ics: 6.468e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.977e+02\n",
            "adaptive_constant_bcs_val: 1.013e+03\n",
            "It: 2600 |  Loss: 1.012e+02 |  Loss_res: 9.413e+00 |   Loss_bcs1: 8.832e-03 | Loss_bc2s: 5.687e-03 |  loss_ics_u: 7.540e-02 |  Loss_ut_ics: 7.229e-04 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 1.016e+03\n",
            "It: 2700 |  Loss: 1.106e+02 |  Loss_res: 1.158e+01 |   Loss_bcs1: 1.375e-02 | Loss_bc2s: 5.016e-03 |  loss_ics_u: 7.422e-02 |  Loss_ut_ics: 4.608e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.971e+02\n",
            "adaptive_constant_bcs_val: 9.806e+02\n",
            "It: 2800 |  Loss: 9.992e+01 |  Loss_res: 8.816e+00 |   Loss_bcs1: 4.760e-03 | Loss_bc2s: 5.648e-03 |  loss_ics_u: 7.397e-02 |  Loss_ut_ics: 8.389e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.011e+03\n",
            "adaptive_constant_bcs_val: 9.993e+02\n",
            "It: 2900 |  Loss: 1.169e+02 |  Loss_res: 1.447e+01 |   Loss_bcs1: 1.670e-02 | Loss_bc2s: 5.743e-03 |  loss_ics_u: 7.107e-02 |  Loss_ut_ics: 8.873e-03 |  Time: 0.15\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.818e+02\n",
            "adaptive_constant_bcs_val: 1.005e+03\n",
            "It: 3000 |  Loss: 9.792e+01 |  Loss_res: 1.163e+01 |   Loss_bcs1: 5.464e-03 | Loss_bc2s: 5.372e-03 |  loss_ics_u: 7.310e-02 |  Loss_ut_ics: 1.933e-03 |  Time: 0.16\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.022e+03\n",
            "adaptive_constant_bcs_val: 1.002e+03\n",
            "It: 3100 |  Loss: 9.037e+01 |  Loss_res: 7.721e+00 |   Loss_bcs1: 9.559e-03 | Loss_bc2s: 7.203e-03 |  loss_ics_u: 6.486e-02 |  Loss_ut_ics: 8.226e-04 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.843e+02\n",
            "adaptive_constant_bcs_val: 1.004e+03\n",
            "It: 3200 |  Loss: 1.072e+02 |  Loss_res: 8.545e+00 |   Loss_bcs1: 1.012e-02 | Loss_bc2s: 7.733e-03 |  loss_ics_u: 7.854e-02 |  Loss_ut_ics: 1.926e-03 |  Time: 0.18\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.004e+03\n",
            "adaptive_constant_bcs_val: 9.785e+02\n",
            "It: 3300 |  Loss: 1.499e+02 |  Loss_res: 9.958e+00 |   Loss_bcs1: 4.047e-03 | Loss_bc2s: 8.546e-03 |  loss_ics_u: 7.706e-02 |  Loss_ut_ics: 5.202e-02 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.006e+03\n",
            "adaptive_constant_bcs_val: 9.700e+02\n",
            "It: 3400 |  Loss: 7.672e+01 |  Loss_res: 7.452e+00 |   Loss_bcs1: 4.856e-03 | Loss_bc2s: 6.483e-03 |  loss_ics_u: 5.871e-02 |  Loss_ut_ics: 1.309e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.964e+02\n",
            "adaptive_constant_bcs_val: 1.112e+03\n",
            "It: 3500 |  Loss: 1.789e+02 |  Loss_res: 7.549e+00 |   Loss_bcs1: 1.352e-02 | Loss_bc2s: 1.180e-02 |  loss_ics_u: 7.369e-02 |  Loss_ut_ics: 6.149e-02 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.864e+02\n",
            "adaptive_constant_bcs_val: 8.783e+02\n",
            "It: 3600 |  Loss: 9.050e+01 |  Loss_res: 8.137e+00 |   Loss_bcs1: 1.027e-02 | Loss_bc2s: 5.290e-03 |  loss_ics_u: 7.663e-02 |  Loss_ut_ics: 1.412e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.022e+03\n",
            "adaptive_constant_bcs_val: 1.126e+03\n",
            "It: 3700 |  Loss: 1.020e+02 |  Loss_res: 8.410e+00 |   Loss_bcs1: 6.013e-03 | Loss_bc2s: 7.751e-03 |  loss_ics_u: 5.847e-02 |  Loss_ut_ics: 1.194e-02 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.929e+02\n",
            "adaptive_constant_bcs_val: 9.564e+02\n",
            "It: 3800 |  Loss: 1.014e+02 |  Loss_res: 8.330e+00 |   Loss_bcs1: 5.770e-03 | Loss_bc2s: 6.659e-03 |  loss_ics_u: 7.225e-02 |  Loss_ut_ics: 1.213e-02 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.996e+02\n",
            "adaptive_constant_bcs_val: 9.807e+02\n",
            "It: 3900 |  Loss: 9.456e+01 |  Loss_res: 7.710e+00 |   Loss_bcs1: 4.192e-03 | Loss_bc2s: 6.759e-03 |  loss_ics_u: 6.550e-02 |  Loss_ut_ics: 1.187e-02 |  Time: 0.14\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 1.019e+03\n",
            "It: 4000 |  Loss: 9.642e+01 |  Loss_res: 1.056e+01 |   Loss_bcs1: 3.976e-03 | Loss_bc2s: 5.023e-03 |  loss_ics_u: 6.528e-02 |  Loss_ut_ics: 1.017e-02 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 9.954e+02\n",
            "It: 4100 |  Loss: 8.646e+01 |  Loss_res: 6.649e+00 |   Loss_bcs1: 6.388e-03 | Loss_bc2s: 7.625e-03 |  loss_ics_u: 6.216e-02 |  Loss_ut_ics: 3.988e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.937e+02\n",
            "adaptive_constant_bcs_val: 1.007e+03\n",
            "It: 4200 |  Loss: 1.042e+02 |  Loss_res: 9.982e+00 |   Loss_bcs1: 6.546e-03 | Loss_bc2s: 4.925e-03 |  loss_ics_u: 6.986e-02 |  Loss_ut_ics: 1.238e-02 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.007e+03\n",
            "adaptive_constant_bcs_val: 9.774e+02\n",
            "It: 4300 |  Loss: 8.229e+01 |  Loss_res: 6.710e+00 |   Loss_bcs1: 7.060e-03 | Loss_bc2s: 6.136e-03 |  loss_ics_u: 6.320e-02 |  Loss_ut_ics: 9.000e-04 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.962e+02\n",
            "adaptive_constant_bcs_val: 1.031e+03\n",
            "It: 4400 |  Loss: 8.044e+01 |  Loss_res: 6.788e+00 |   Loss_bcs1: 6.230e-03 | Loss_bc2s: 4.709e-03 |  loss_ics_u: 5.876e-02 |  Loss_ut_ics: 1.782e-03 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.004e+03\n",
            "adaptive_constant_bcs_val: 9.870e+02\n",
            "It: 4500 |  Loss: 1.044e+02 |  Loss_res: 8.050e+00 |   Loss_bcs1: 1.283e-02 | Loss_bc2s: 5.108e-03 |  loss_ics_u: 6.017e-02 |  Loss_ut_ics: 1.920e-02 |  Time: 0.15\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.917e+02\n",
            "adaptive_constant_bcs_val: 9.800e+02\n",
            "It: 4600 |  Loss: 8.114e+01 |  Loss_res: 6.186e+00 |   Loss_bcs1: 6.814e-03 | Loss_bc2s: 5.895e-03 |  loss_ics_u: 6.105e-02 |  Loss_ut_ics: 2.681e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.011e+03\n",
            "adaptive_constant_bcs_val: 1.032e+03\n",
            "It: 4700 |  Loss: 7.566e+01 |  Loss_res: 6.645e+00 |   Loss_bcs1: 5.356e-03 | Loss_bc2s: 6.227e-03 |  loss_ics_u: 5.158e-02 |  Loss_ut_ics: 3.811e-03 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.963e+02\n",
            "adaptive_constant_bcs_val: 9.935e+02\n",
            "It: 4800 |  Loss: 9.434e+01 |  Loss_res: 4.761e+00 |   Loss_bcs1: 5.079e-03 | Loss_bc2s: 5.210e-03 |  loss_ics_u: 5.093e-02 |  Loss_ut_ics: 2.886e-02 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 9.710e+02\n",
            "It: 4900 |  Loss: 7.842e+01 |  Loss_res: 8.419e+00 |   Loss_bcs1: 6.581e-03 | Loss_bc2s: 3.990e-03 |  loss_ics_u: 5.986e-02 |  Loss_ut_ics: 1.611e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.986e+02\n",
            "adaptive_constant_bcs_val: 1.039e+03\n",
            "It: 5000 |  Loss: 2.261e+02 |  Loss_res: 9.078e+00 |   Loss_bcs1: 7.828e-03 | Loss_bc2s: 7.480e-03 |  loss_ics_u: 5.232e-02 |  Loss_ut_ics: 1.470e-01 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.960e+02\n",
            "adaptive_constant_bcs_val: 8.689e+02\n",
            "It: 5100 |  Loss: 5.904e+01 |  Loss_res: 5.209e+00 |   Loss_bcs1: 3.814e-03 | Loss_bc2s: 4.409e-03 |  loss_ics_u: 5.196e-02 |  Loss_ut_ics: 1.541e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.011e+03\n",
            "adaptive_constant_bcs_val: 1.291e+03\n",
            "It: 5200 |  Loss: 8.988e+01 |  Loss_res: 4.714e+00 |   Loss_bcs1: 6.297e-03 | Loss_bc2s: 4.436e-03 |  loss_ics_u: 4.635e-02 |  Loss_ut_ics: 1.137e-02 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.914e+02\n",
            "adaptive_constant_bcs_val: 8.737e+02\n",
            "It: 5300 |  Loss: 6.495e+01 |  Loss_res: 3.599e+00 |   Loss_bcs1: 5.016e-03 | Loss_bc2s: 5.424e-03 |  loss_ics_u: 5.117e-02 |  Loss_ut_ics: 7.588e-03 |  Time: 0.14\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.003e+03\n",
            "adaptive_constant_bcs_val: 1.003e+03\n",
            "It: 5400 |  Loss: 5.265e+01 |  Loss_res: 4.457e+00 |   Loss_bcs1: 2.564e-03 | Loss_bc2s: 5.210e-03 |  loss_ics_u: 3.728e-02 |  Loss_ut_ics: 2.995e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.002e+03\n",
            "adaptive_constant_bcs_val: 1.018e+03\n",
            "It: 5500 |  Loss: 6.929e+01 |  Loss_res: 3.697e+00 |   Loss_bcs1: 3.926e-03 | Loss_bc2s: 2.966e-03 |  loss_ics_u: 5.396e-02 |  Loss_ut_ics: 3.655e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.984e+02\n",
            "adaptive_constant_bcs_val: 9.683e+02\n",
            "It: 5600 |  Loss: 5.449e+01 |  Loss_res: 3.692e+00 |   Loss_bcs1: 3.525e-03 | Loss_bc2s: 5.783e-03 |  loss_ics_u: 4.174e-02 |  Loss_ut_ics: 1.374e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.970e+02\n",
            "adaptive_constant_bcs_val: 1.029e+03\n",
            "It: 5700 |  Loss: 6.173e+01 |  Loss_res: 4.308e+00 |   Loss_bcs1: 4.091e-03 | Loss_bc2s: 5.194e-03 |  loss_ics_u: 3.885e-02 |  Loss_ut_ics: 7.918e-03 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.002e+03\n",
            "adaptive_constant_bcs_val: 9.838e+02\n",
            "It: 5800 |  Loss: 5.944e+01 |  Loss_res: 4.210e+00 |   Loss_bcs1: 6.524e-03 | Loss_bc2s: 4.334e-03 |  loss_ics_u: 3.683e-02 |  Loss_ut_ics: 8.295e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.986e+02\n",
            "adaptive_constant_bcs_val: 1.005e+03\n",
            "It: 5900 |  Loss: 2.755e+02 |  Loss_res: 2.909e+01 |   Loss_bcs1: 9.987e-03 | Loss_bc2s: 2.816e-02 |  loss_ics_u: 5.172e-02 |  Loss_ut_ics: 1.563e-01 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.771e+02\n",
            "adaptive_constant_bcs_val: 8.623e+02\n",
            "It: 6000 |  Loss: 5.568e+01 |  Loss_res: 4.148e+00 |   Loss_bcs1: 5.686e-03 | Loss_bc2s: 4.373e-03 |  loss_ics_u: 4.755e-02 |  Loss_ut_ics: 1.896e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.051e+03\n",
            "adaptive_constant_bcs_val: 1.336e+03\n",
            "It: 6100 |  Loss: 7.497e+01 |  Loss_res: 7.453e+00 |   Loss_bcs1: 5.725e-03 | Loss_bc2s: 3.967e-03 |  loss_ics_u: 3.837e-02 |  Loss_ut_ics: 3.159e-03 |  Time: 0.15\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.754e+02\n",
            "adaptive_constant_bcs_val: 8.732e+02\n",
            "It: 6200 |  Loss: 4.990e+01 |  Loss_res: 3.582e+00 |   Loss_bcs1: 4.783e-03 | Loss_bc2s: 5.359e-03 |  loss_ics_u: 3.400e-02 |  Loss_ut_ics: 7.966e-03 |  Time: 0.13\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.993e+02\n",
            "adaptive_constant_bcs_val: 9.925e+02\n",
            "It: 6300 |  Loss: 4.855e+01 |  Loss_res: 2.719e+00 |   Loss_bcs1: 3.085e-03 | Loss_bc2s: 6.418e-03 |  loss_ics_u: 2.865e-02 |  Loss_ut_ics: 7.970e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 1.005e+03\n",
            "It: 6400 |  Loss: 5.210e+01 |  Loss_res: 2.570e+00 |   Loss_bcs1: 5.072e-03 | Loss_bc2s: 3.888e-03 |  loss_ics_u: 3.542e-02 |  Loss_ut_ics: 4.919e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.999e+02\n",
            "adaptive_constant_bcs_val: 9.919e+02\n",
            "It: 6500 |  Loss: 4.432e+01 |  Loss_res: 5.583e+00 |   Loss_bcs1: 3.859e-03 | Loss_bc2s: 4.131e-03 |  loss_ics_u: 2.937e-02 |  Loss_ut_ics: 1.680e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 1.012e+03\n",
            "It: 6600 |  Loss: 4.179e+01 |  Loss_res: 2.858e+00 |   Loss_bcs1: 4.632e-03 | Loss_bc2s: 3.868e-03 |  loss_ics_u: 2.656e-02 |  Loss_ut_ics: 3.459e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.987e+02\n",
            "adaptive_constant_bcs_val: 9.926e+02\n",
            "It: 6700 |  Loss: 7.876e+01 |  Loss_res: 3.588e+00 |   Loss_bcs1: 4.617e-03 | Loss_bc2s: 3.566e-03 |  loss_ics_u: 2.645e-02 |  Loss_ut_ics: 4.085e-02 |  Time: 0.14\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 9.661e+02\n",
            "It: 6800 |  Loss: 3.542e+01 |  Loss_res: 3.719e+00 |   Loss_bcs1: 3.617e-03 | Loss_bc2s: 4.006e-03 |  loss_ics_u: 2.345e-02 |  Loss_ut_ics: 1.682e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 1.074e+03\n",
            "It: 6900 |  Loss: 4.138e+01 |  Loss_res: 3.611e+00 |   Loss_bcs1: 3.652e-03 | Loss_bc2s: 4.868e-03 |  loss_ics_u: 2.450e-02 |  Loss_ut_ics: 2.306e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.987e+02\n",
            "adaptive_constant_bcs_val: 9.613e+02\n",
            "It: 7000 |  Loss: 4.148e+01 |  Loss_res: 4.616e+00 |   Loss_bcs1: 2.898e-03 | Loss_bc2s: 3.794e-03 |  loss_ics_u: 2.523e-02 |  Loss_ut_ics: 6.186e-03 |  Time: 0.20\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.002e+03\n",
            "adaptive_constant_bcs_val: 9.974e+02\n",
            "It: 7100 |  Loss: 5.403e+01 |  Loss_res: 3.260e+00 |   Loss_bcs1: 3.658e-03 | Loss_bc2s: 4.754e-03 |  loss_ics_u: 1.952e-02 |  Loss_ut_ics: 2.286e-02 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.968e+02\n",
            "adaptive_constant_bcs_val: 9.943e+02\n",
            "It: 7200 |  Loss: 3.902e+01 |  Loss_res: 2.705e+00 |   Loss_bcs1: 3.780e-03 | Loss_bc2s: 5.278e-03 |  loss_ics_u: 2.210e-02 |  Loss_ut_ics: 5.347e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 1.024e+03\n",
            "It: 7300 |  Loss: 9.429e+01 |  Loss_res: 2.420e+00 |   Loss_bcs1: 4.731e-03 | Loss_bc2s: 8.589e-03 |  loss_ics_u: 2.127e-02 |  Loss_ut_ics: 5.641e-02 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.968e+02\n",
            "adaptive_constant_bcs_val: 9.430e+02\n",
            "It: 7400 |  Loss: 2.924e+01 |  Loss_res: 2.200e+00 |   Loss_bcs1: 4.224e-03 | Loss_bc2s: 3.439e-03 |  loss_ics_u: 1.860e-02 |  Loss_ut_ics: 2.277e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.009e+03\n",
            "adaptive_constant_bcs_val: 1.101e+03\n",
            "It: 7500 |  Loss: 3.917e+01 |  Loss_res: 4.402e+00 |   Loss_bcs1: 3.867e-03 | Loss_bc2s: 6.149e-03 |  loss_ics_u: 1.794e-02 |  Loss_ut_ics: 3.955e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.928e+02\n",
            "adaptive_constant_bcs_val: 9.493e+02\n",
            "It: 7600 |  Loss: 3.505e+01 |  Loss_res: 4.317e+00 |   Loss_bcs1: 3.900e-03 | Loss_bc2s: 4.372e-03 |  loss_ics_u: 2.034e-02 |  Loss_ut_ics: 3.600e-03 |  Time: 0.11\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.004e+03\n",
            "adaptive_constant_bcs_val: 9.991e+02\n",
            "It: 7700 |  Loss: 3.377e+01 |  Loss_res: 2.602e+00 |   Loss_bcs1: 4.030e-03 | Loss_bc2s: 4.143e-03 |  loss_ics_u: 1.879e-02 |  Loss_ut_ics: 4.211e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.985e+02\n",
            "adaptive_constant_bcs_val: 1.003e+03\n",
            "It: 7800 |  Loss: 6.960e+01 |  Loss_res: 6.186e+00 |   Loss_bcs1: 4.609e-03 | Loss_bc2s: 4.155e-03 |  loss_ics_u: 2.174e-02 |  Loss_ut_ics: 3.288e-02 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.994e+02\n",
            "adaptive_constant_bcs_val: 9.711e+02\n",
            "It: 7900 |  Loss: 3.379e+01 |  Loss_res: 3.003e+00 |   Loss_bcs1: 4.897e-03 | Loss_bc2s: 4.114e-03 |  loss_ics_u: 2.200e-02 |  Loss_ut_ics: 6.707e-04 |  Time: 0.06\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 1.059e+03\n",
            "It: 8000 |  Loss: 3.217e+01 |  Loss_res: 2.843e+00 |   Loss_bcs1: 4.213e-03 | Loss_bc2s: 4.497e-03 |  loss_ics_u: 1.787e-02 |  Loss_ut_ics: 1.186e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 9.748e+02\n",
            "It: 8100 |  Loss: 6.058e+01 |  Loss_res: 2.820e+00 |   Loss_bcs1: 4.234e-03 | Loss_bc2s: 5.232e-03 |  loss_ics_u: 1.614e-02 |  Loss_ut_ics: 3.279e-02 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.990e+02\n",
            "adaptive_constant_bcs_val: 9.703e+02\n",
            "It: 8200 |  Loss: 2.714e+01 |  Loss_res: 2.087e+00 |   Loss_bcs1: 4.222e-03 | Loss_bc2s: 3.318e-03 |  loss_ics_u: 1.635e-02 |  Loss_ut_ics: 1.878e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.002e+03\n",
            "adaptive_constant_bcs_val: 1.056e+03\n",
            "It: 8300 |  Loss: 3.060e+01 |  Loss_res: 2.651e+00 |   Loss_bcs1: 4.261e-03 | Loss_bc2s: 5.288e-03 |  loss_ics_u: 1.528e-02 |  Loss_ut_ics: 1.725e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.965e+02\n",
            "adaptive_constant_bcs_val: 9.738e+02\n",
            "It: 8400 |  Loss: 2.845e+01 |  Loss_res: 2.098e+00 |   Loss_bcs1: 3.889e-03 | Loss_bc2s: 5.697e-03 |  loss_ics_u: 1.459e-02 |  Loss_ut_ics: 2.828e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.002e+03\n",
            "adaptive_constant_bcs_val: 9.985e+02\n",
            "It: 8500 |  Loss: 3.067e+01 |  Loss_res: 2.229e+00 |   Loss_bcs1: 4.158e-03 | Loss_bc2s: 4.738e-03 |  loss_ics_u: 1.326e-02 |  Loss_ut_ics: 6.306e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 9.984e+02\n",
            "It: 8600 |  Loss: 4.382e+01 |  Loss_res: 2.776e+00 |   Loss_bcs1: 4.668e-03 | Loss_bc2s: 5.420e-03 |  loss_ics_u: 1.552e-02 |  Loss_ut_ics: 1.546e-02 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.983e+02\n",
            "adaptive_constant_bcs_val: 9.917e+02\n",
            "It: 8700 |  Loss: 2.799e+01 |  Loss_res: 2.001e+00 |   Loss_bcs1: 3.299e-03 | Loss_bc2s: 4.300e-03 |  loss_ics_u: 1.401e-02 |  Loss_ut_ics: 4.570e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.003e+03\n",
            "adaptive_constant_bcs_val: 1.022e+03\n",
            "It: 8800 |  Loss: 2.711e+01 |  Loss_res: 2.387e+00 |   Loss_bcs1: 4.380e-03 | Loss_bc2s: 4.429e-03 |  loss_ics_u: 1.473e-02 |  Loss_ut_ics: 6.717e-04 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.967e+02\n",
            "adaptive_constant_bcs_val: 9.917e+02\n",
            "It: 8900 |  Loss: 3.090e+01 |  Loss_res: 2.345e+00 |   Loss_bcs1: 4.763e-03 | Loss_bc2s: 4.907e-03 |  loss_ics_u: 1.410e-02 |  Loss_ut_ics: 4.997e-03 |  Time: 0.06\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 9.938e+02\n",
            "It: 9000 |  Loss: 3.219e+01 |  Loss_res: 2.857e+00 |   Loss_bcs1: 4.911e-03 | Loss_bc2s: 4.322e-03 |  loss_ics_u: 1.322e-02 |  Loss_ut_ics: 7.018e-03 |  Time: 0.06\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 1.002e+03\n",
            "It: 9100 |  Loss: 2.621e+01 |  Loss_res: 1.633e+00 |   Loss_bcs1: 4.266e-03 | Loss_bc2s: 3.416e-03 |  loss_ics_u: 1.181e-02 |  Loss_ut_ics: 5.031e-03 |  Time: 0.06\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 1.004e+03\n",
            "It: 9200 |  Loss: 2.996e+01 |  Loss_res: 2.822e+00 |   Loss_bcs1: 5.075e-03 | Loss_bc2s: 4.235e-03 |  loss_ics_u: 1.317e-02 |  Loss_ut_ics: 4.568e-03 |  Time: 0.06\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.971e+02\n",
            "adaptive_constant_bcs_val: 9.962e+02\n",
            "It: 9300 |  Loss: 2.548e+01 |  Loss_res: 2.546e+00 |   Loss_bcs1: 3.411e-03 | Loss_bc2s: 3.655e-03 |  loss_ics_u: 1.306e-02 |  Loss_ut_ics: 2.896e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.003e+03\n",
            "adaptive_constant_bcs_val: 1.002e+03\n",
            "It: 9400 |  Loss: 2.683e+01 |  Loss_res: 1.699e+00 |   Loss_bcs1: 3.895e-03 | Loss_bc2s: 4.116e-03 |  loss_ics_u: 1.228e-02 |  Loss_ut_ics: 4.777e-03 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.971e+02\n",
            "adaptive_constant_bcs_val: 9.974e+02\n",
            "It: 9500 |  Loss: 2.379e+01 |  Loss_res: 2.516e+00 |   Loss_bcs1: 3.644e-03 | Loss_bc2s: 4.343e-03 |  loss_ics_u: 1.146e-02 |  Loss_ut_ics: 1.887e-03 |  Time: 0.12\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 1.004e+03\n",
            "It: 9600 |  Loss: 2.681e+01 |  Loss_res: 3.074e+00 |   Loss_bcs1: 4.081e-03 | Loss_bc2s: 3.997e-03 |  loss_ics_u: 1.124e-02 |  Loss_ut_ics: 4.329e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.999e+02\n",
            "adaptive_constant_bcs_val: 9.947e+02\n",
            "It: 9700 |  Loss: 2.957e+01 |  Loss_res: 1.568e+00 |   Loss_bcs1: 3.912e-03 | Loss_bc2s: 3.459e-03 |  loss_ics_u: 1.936e-02 |  Loss_ut_ics: 1.424e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 9.973e+02\n",
            "It: 9800 |  Loss: 2.600e+01 |  Loss_res: 2.520e+00 |   Loss_bcs1: 3.204e-03 | Loss_bc2s: 3.737e-03 |  loss_ics_u: 1.626e-02 |  Loss_ut_ics: 3.332e-04 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.997e+02\n",
            "adaptive_constant_bcs_val: 1.008e+03\n",
            "It: 9900 |  Loss: 2.235e+01 |  Loss_res: 1.884e+00 |   Loss_bcs1: 3.348e-03 | Loss_bc2s: 4.530e-03 |  loss_ics_u: 1.172e-02 |  Loss_ut_ics: 7.059e-04 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.988e+02\n",
            "adaptive_constant_bcs_val: 1.000e+03\n",
            "It: 10000 |  Loss: 2.665e+01 |  Loss_res: 2.187e+00 |   Loss_bcs1: 3.122e-03 | Loss_bc2s: 4.103e-03 |  loss_ics_u: 1.416e-02 |  Loss_ut_ics: 3.076e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 9.919e+02\n",
            "It: 10100 |  Loss: 2.125e+01 |  Loss_res: 2.117e+00 |   Loss_bcs1: 3.854e-03 | Loss_bc2s: 3.448e-03 |  loss_ics_u: 1.154e-02 |  Loss_ut_ics: 4.467e-04 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.993e+02\n",
            "adaptive_constant_bcs_val: 1.009e+03\n",
            "It: 10200 |  Loss: 2.555e+01 |  Loss_res: 2.668e+00 |   Loss_bcs1: 3.297e-03 | Loss_bc2s: 4.124e-03 |  loss_ics_u: 1.103e-02 |  Loss_ut_ics: 4.273e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 9.923e+02\n",
            "It: 10300 |  Loss: 2.255e+01 |  Loss_res: 1.853e+00 |   Loss_bcs1: 3.487e-03 | Loss_bc2s: 4.326e-03 |  loss_ics_u: 1.067e-02 |  Loss_ut_ics: 2.355e-03 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.998e+02\n",
            "adaptive_constant_bcs_val: 1.005e+03\n",
            "It: 10400 |  Loss: 2.270e+01 |  Loss_res: 2.259e+00 |   Loss_bcs1: 4.213e-03 | Loss_bc2s: 4.239e-03 |  loss_ics_u: 9.897e-03 |  Loss_ut_ics: 1.998e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.998e+02\n",
            "adaptive_constant_bcs_val: 9.990e+02\n",
            "It: 10500 |  Loss: 1.888e+01 |  Loss_res: 1.912e+00 |   Loss_bcs1: 3.913e-03 | Loss_bc2s: 4.513e-03 |  loss_ics_u: 7.323e-03 |  Loss_ut_ics: 1.234e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 1.002e+03\n",
            "It: 10600 |  Loss: 4.309e+01 |  Loss_res: 1.909e+00 |   Loss_bcs1: 5.106e-03 | Loss_bc2s: 4.276e-03 |  loss_ics_u: 1.108e-02 |  Loss_ut_ics: 2.067e-02 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.991e+02\n",
            "adaptive_constant_bcs_val: 9.764e+02\n",
            "It: 10700 |  Loss: 1.820e+01 |  Loss_res: 1.709e+00 |   Loss_bcs1: 4.045e-03 | Loss_bc2s: 3.753e-03 |  loss_ics_u: 8.465e-03 |  Loss_ut_ics: 6.162e-04 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.002e+03\n",
            "adaptive_constant_bcs_val: 1.042e+03\n",
            "It: 10800 |  Loss: 2.161e+01 |  Loss_res: 2.196e+00 |   Loss_bcs1: 4.145e-03 | Loss_bc2s: 3.462e-03 |  loss_ics_u: 9.758e-03 |  Loss_ut_ics: 1.311e-03 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.987e+02\n",
            "adaptive_constant_bcs_val: 9.781e+02\n",
            "It: 10900 |  Loss: 5.061e+01 |  Loss_res: 2.194e+00 |   Loss_bcs1: 3.668e-03 | Loss_bc2s: 5.717e-03 |  loss_ics_u: 9.494e-03 |  Loss_ut_ics: 2.998e-02 |  Time: 0.08\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.982e+02\n",
            "adaptive_constant_bcs_val: 9.765e+02\n",
            "It: 11000 |  Loss: 1.901e+01 |  Loss_res: 1.728e+00 |   Loss_bcs1: 3.693e-03 | Loss_bc2s: 3.638e-03 |  loss_ics_u: 8.393e-03 |  Loss_ut_ics: 1.935e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.003e+03\n",
            "adaptive_constant_bcs_val: 1.053e+03\n",
            "It: 11100 |  Loss: 2.748e+01 |  Loss_res: 2.734e+00 |   Loss_bcs1: 4.353e-03 | Loss_bc2s: 4.383e-03 |  loss_ics_u: 8.275e-03 |  Loss_ut_ics: 6.809e-03 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.969e+02\n",
            "adaptive_constant_bcs_val: 9.699e+02\n",
            "It: 11200 |  Loss: 1.997e+01 |  Loss_res: 1.658e+00 |   Loss_bcs1: 4.534e-03 | Loss_bc2s: 3.786e-03 |  loss_ics_u: 9.741e-03 |  Loss_ut_ics: 7.953e-04 |  Time: 0.07\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.002e+03\n",
            "adaptive_constant_bcs_val: 1.008e+03\n",
            "It: 11300 |  Loss: 1.753e+01 |  Loss_res: 1.804e+00 |   Loss_bcs1: 3.998e-03 | Loss_bc2s: 3.799e-03 |  loss_ics_u: 7.520e-03 |  Loss_ut_ics: 2.790e-04 |  Time: 0.04\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.000e+03\n",
            "adaptive_constant_bcs_val: 9.984e+02\n",
            "It: 11400 |  Loss: 3.702e+01 |  Loss_res: 4.091e+00 |   Loss_bcs1: 4.554e-03 | Loss_bc2s: 4.488e-03 |  loss_ics_u: 8.176e-03 |  Loss_ut_ics: 1.574e-02 |  Time: 0.03\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 9.984e+02\n",
            "adaptive_constant_bcs_val: 9.832e+02\n",
            "It: 11500 |  Loss: 2.211e+01 |  Loss_res: 1.656e+00 |   Loss_bcs1: 4.072e-03 | Loss_bc2s: 4.483e-03 |  loss_ics_u: 8.207e-03 |  Loss_ut_ics: 3.984e-03 |  Time: 0.04\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.002e+03\n",
            "adaptive_constant_bcs_val: 1.025e+03\n",
            "It: 11600 |  Loss: 2.233e+01 |  Loss_res: 1.832e+00 |   Loss_bcs1: 3.332e-03 | Loss_bc2s: 4.154e-03 |  loss_ics_u: 9.755e-03 |  Loss_ut_ics: 2.815e-03 |  Time: 0.03\n",
            "adaptive_constant_res_val: 1.000e+00\n",
            "adaptive_constant_ics_val: 1.001e+03\n",
            "adaptive_constant_bcs_val: 9.892e+02\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_25834/1855831049.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmtd\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"mini_batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mini_batch method is used\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainmb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmbbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown method!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_25834/2697325055.py\u001b[0m in \u001b[0;36mtrainmb\u001b[0;34m(self, nIter, batch_size, log_NTK, update_lam)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;31m# Run the Tensorflow session to minimize the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_tensor_list\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;31m# self.print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define PINN model\n",
        "a = 0.5\n",
        "c = 2\n",
        "\n",
        "kernel_size = 300\n",
        "\n",
        "# Domain boundaries\n",
        "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
        "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
        "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
        "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
        "\n",
        "# Create initial conditions samplers\n",
        "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
        "\n",
        "# Create boundary conditions samplers\n",
        "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
        "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
        "bcs_sampler = [bc1, bc2]\n",
        "\n",
        "# Create residual sampler\n",
        "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
        "coll_sampler = Sampler(2, dom_coords, lambda x: u(x, a, c), name='coll')\n",
        "\n",
        "\n",
        "\n",
        "nIter =40000\n",
        "bcbatch_size = 500\n",
        "ubatch_size = 5000\n",
        "mbbatch_size = 300\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "mode = 'SoftAdapt'\n",
        "layers = [2, 500, 500, 500, 1]\n",
        "\n",
        "\n",
        "nn = 200\n",
        "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
        "t, x = np.meshgrid(t, x)\n",
        "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
        "\n",
        "u_star = u(X_star, a,c)\n",
        "r_star = r(X_star, a, c)\n",
        "\n",
        "iterations = 1\n",
        "methods = [  \"mini_batch\"]\n",
        "\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "for mtd in methods:\n",
        "    print(\"Method: \", mtd)\n",
        "    time_list = []\n",
        "    error_u_list = []\n",
        "    error_r_list = []\n",
        "    \n",
        "    for index in range(iterations):\n",
        "\n",
        "        print(\"Epoch: \", str(index+1))\n",
        "\n",
        "        # Create residual sampler\n",
        "\n",
        "        # [elapsed, error_u , model] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
        "        tf.reset_default_graph()\n",
        "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "            # sess.run(init)\n",
        "\n",
        "            model = PINN(layers, operator ,  ics_sampler, bcs_sampler, res_sampler, c , mode , sess)\n",
        "            # Train model\n",
        "            start_time = time.time()\n",
        "\n",
        "            if mtd ==\"full_batch\":\n",
        "                print(\"full_batch method is used\")\n",
        "                model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "            elif mtd ==\"mini_batch\":\n",
        "                print(\"mini_batch method is used\")\n",
        "                model.trainmb(nIter, mbbatch_size)\n",
        "            else:\n",
        "                print(\"unknown method!\")\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            # Predictions\n",
        "            u_pred = model.predict_u(X_star)\n",
        "            r_pred = model.predict_r(X_star)\n",
        "            # Predictions\n",
        "\n",
        "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "            error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "            print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "            print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "            print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "            model.plot_lambda()\n",
        "            model.plot_grad()\n",
        "            model.save_NN()\n",
        "            model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
        "            # sess.close()  \n",
        "\n",
        "        print('elapsed: {:.2e}'.format(elapsed))\n",
        "        print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "        print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "        time_list.append(elapsed)\n",
        "        error_u_list.append(error_u)\n",
        "        error_r_list.append(error_r)\n",
        "\n",
        "    print(\"\\n\\nMethod: \", mtd)\n",
        "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "    print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "    result_dict[mtd] = [time_list ,error_u_list,error_r_list]\n",
        "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(bcbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
        "\n",
        "###############################################################################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "average lambda_res1.0000e+00\n"
          ]
        }
      ],
      "source": [
        "model.print(\"average lambda_res\" ,  np.average(model.adaptive_constant_res_log))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Method:  mini_batch\n",
        "\n",
        "# average of time_list: 1495.849406003952\n",
        "# average of error_u_list: 0.20815852255570721\n",
        "# average of error_r_list: 0.08826000007246695"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_14832/2998510943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mu_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Predictions\n",
        "u_pred = model.predict_u(X_star)\n",
        "r_pred = model.predict_r(X_star)\n",
        "# Predictions\n",
        "\n",
        "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "model.save_NN()\n",
        "model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plt_prediction(self , t , x , X_star , u_star , u_pred , r_star , r_pred):\n",
        "    \n",
        "    U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
        "    r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
        "    R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(18, 9))\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.pcolor(t, x, U_star, cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$x_1$')\n",
        "    plt.ylabel('$x_2$')\n",
        "    plt.title('Exact u(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.pcolor(t, x, U_pred, cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Predicted u(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Absolute error')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.pcolor(t, x, r_star, cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Exact r(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.pcolor(t, x, R_pred, cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Predicted r(t, x)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('$t$')\n",
        "    plt.ylabel('$x$')\n",
        "    plt.title('Absolute error')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
        "    plt.close(\"all\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200, 200)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'u_pred' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_24403/89554458.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt_prediction\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mX_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mu_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mu_pred\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mr_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mr_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'u_pred' is not defined"
          ]
        }
      ],
      "source": [
        "plt_prediction( model , t , x , X_star , u_star , u_pred , r_star , r_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFLIBq5xjZ3v"
      },
      "source": [
        "**Model Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To0PDN17cc0v",
        "outputId": "1f47f288-322a-46b5-f173-45485191a68d"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Predictions\n",
        "u_pred = model.predict_u(X_star)\n",
        "r_pred = model.predict_r(X_star)\n",
        "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "\n",
        "print('Relative L2 error_u: %e' % (error_u))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "K428lOuXhdc8",
        "outputId": "015f591b-d8a4-4e47-8020-84fcf219d7ca"
      },
      "outputs": [],
      "source": [
        "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
        "r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
        "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
        "R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.pcolor(t, x, U_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.title('Exact u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.pcolor(t, x, U_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted u(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.pcolor(t, x, r_star, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Exact r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.pcolor(t, x, R_pred, cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Predicted r(t, x)')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
        "plt.colorbar()\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$x$')\n",
        "plt.title('Absolute error')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYdfKGLj6h0"
      },
      "source": [
        "**NTK Eigenvalues**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3dByeQjhBYj"
      },
      "outputs": [],
      "source": [
        "# Create empty lists for storing the eigenvalues of NTK\n",
        "lam_K_u_log = []\n",
        "lam_K_ut_log = []\n",
        "lam_K_r_log = []\n",
        "\n",
        "# Restore the NTK\n",
        "K_u_list = model.K_u_log\n",
        "K_ut_list = model.K_ut_log\n",
        "K_r_list = model.K_r_log\n",
        "\n",
        "K_list = []\n",
        "    \n",
        "for k in range(len(K_u_list)):\n",
        "    K_u = K_u_list[k]\n",
        "    K_ut = K_ut_list[k]\n",
        "    K_r = K_r_list[k]\n",
        "    \n",
        "    # Compute eigenvalues\n",
        "    lam_K_u, _ = np.linalg.eig(K_u)\n",
        "    lam_K_ut, _ = np.linalg.eig(K_ut)\n",
        "    lam_K_r, _ = np.linalg.eig(K_r)\n",
        "    # Sort in descresing order\n",
        "    lam_K_u = np.sort(np.real(lam_K_u))[::-1]\n",
        "    lam_K_ut = np.sort(np.real(lam_K_ut))[::-1]\n",
        "    lam_K_r = np.sort(np.real(lam_K_r))[::-1]\n",
        "    \n",
        "    # Store eigenvalues\n",
        "    lam_K_u_log.append(lam_K_u)\n",
        "    lam_K_ut_log.append(lam_K_ut)\n",
        "    lam_K_r_log.append(lam_K_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "vSn3Q_1IhisN",
        "outputId": "886908b3-c316-48d6-933f-81b1180ff954"
      },
      "outputs": [],
      "source": [
        "#  Eigenvalues of NTK\n",
        "fig = plt.figure(figsize=(18, 5))\n",
        "plt.subplot(1,3,1)\n",
        "\n",
        "plt.plot(lam_K_u_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_u_log[1], '--', label = '$n=10,000$')\n",
        "plt.plot(lam_K_u_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_u_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.title(r'Eigenvalues of ${K}_u$')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(lam_K_ut_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_ut_log[1], '--',label = '$n=10,000$')\n",
        "plt.plot(lam_K_ut_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_ut_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(r'Eigenvalues of ${K}_{u_t}$')\n",
        "\n",
        "ax =plt.subplot(1,3,3)\n",
        "plt.plot(lam_K_r_log[0], label = '$n=0$')\n",
        "plt.plot(lam_K_r_log[1], '--', label = '$n=10,000$')\n",
        "plt.plot(lam_K_r_log[4], '--', label = '$n=40,000$')\n",
        "plt.plot(lam_K_r_log[-1], '--', label = '$n=80,000$')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(r'Eigenvalues of ${K}_{r}$')\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.02),\n",
        "            borderaxespad=0, bbox_transform=fig.transFigure, ncol=4)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbUn_fcowojl"
      },
      "source": [
        "**Evolution of NTK Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYbzkhfMjJ8k"
      },
      "outputs": [],
      "source": [
        "if update_lam == True:\n",
        "\n",
        "  lam_u_log = model.lam_u_log\n",
        "  lam_ut_log = model.lam_ut_log\n",
        "  lam_r_log = model.lam_r_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "xzFzPCA2w1ML",
        "outputId": "71452cf9-3ebb-4aeb-9708-c7664b88e65d"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 5))\n",
        "plt.plot(lam_u_log, label='$\\lambda_u$')\n",
        "plt.plot(lam_ut_log, label='$\\lambda_{u_t}$')\n",
        "plt.plot(lam_r_log, label='$\\lambda_{r}$')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('$\\lambda$')\n",
        "plt.yscale('log')\n",
        "plt.legend( )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mimIv2Z5xlip"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINNsNTK_Wave.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
