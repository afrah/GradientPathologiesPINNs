{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
    "import numpy as np\n",
    "import timeit\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "# Define the exact solution and its derivatives\n",
    "def u(x, a, c):\n",
    "    \"\"\"\n",
    "    :param x: x = (t, x)\n",
    "    \"\"\"\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    return np.sin(np.pi * x) * np.cos(c * np.pi * t) + a * np.sin(2 * c * np.pi* x) * np.cos(4 * c  * np.pi * t)\n",
    "\n",
    "def u_t(x,a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_t = -  c * np.pi * np.sin(np.pi * x) * np.sin(c * np.pi * t) -  a * 4 * c * np.pi * np.sin(2 * c * np.pi* x) * np.sin(4 * c * np.pi * t)\n",
    "    return u_t\n",
    "\n",
    "def u_tt(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_tt = -(c * np.pi)**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) - a * (4 * c * np.pi)**2 *  np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return u_tt\n",
    "\n",
    "def u_xx(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_xx = - np.pi**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) -  a * (2 * c * np.pi)** 2 * np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return  u_xx\n",
    "\n",
    "\n",
    "def r(x, a, c):\n",
    "    return u_tt(x, a, c) - c**2 * u_xx(x, a, c)\n",
    "\n",
    "def operator(u, t, x, c, sigma_t=1.0, sigma_x=1.0):\n",
    "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    residual = u_tt - c**2 * u_xx\n",
    "    return residual\n",
    "\n",
    "class PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, c , mode ,  sess):\n",
    "        # Normalization \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        self.sess = sess\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # weights\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "        self.adaptive_constant_ics_val = np.array(1.0)\n",
    "        self.adaptive_constant_res_val = np.array(1.0)\n",
    "        self.rate = 0.9\n",
    "\n",
    "        # Wave constant\n",
    "        self.c = tf.constant(c, dtype=tf.float32)\n",
    "        \n",
    "        # self.kernel_size = kernel_size # Size of the NTK matrix\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        \n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
    "        self.adaptive_constant_res_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_val.shape)\n",
    "        \n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
    "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
    "        \n",
    "\n",
    "        # Boundary loss and Initial loss\n",
    "        self.loss_ics_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
    "        self.loss_ics_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
    "\n",
    "        self.loss_bcs = self.loss_ics_u + self.loss_bc1 + self.loss_bc2\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss =  self.adaptive_constant_res_tf * self.loss_res + self.adaptive_constant_bcs_tf * self.loss_bcs + self.adaptive_constant_ics_tf * self.loss_ics_u_t \n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bcs,  self.loss_bc1 , self.loss_bc2 , self.loss_ics_u , self.loss_ics_u_t] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs\" , \"loss_bc1\", \"loss_bc2\", \"loss_ics_u\", \"loss_ics_u_t\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "         # # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_ics_layers = self.generate_grad_dict()\n",
    "        \n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_ics = []\n",
    "        self.grad_bcs = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "            self.grad_ics.append(tf.gradients(self.loss_ics_u_t, self.weights[i])[0])\n",
    "          \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        self.mean_grad_ics_list = []\n",
    "\n",
    "        self.adaptive_constant_bcs_log = []\n",
    "        self.adaptive_constant_ics_log = []\n",
    "        self.adaptive_constant_res_log = []\n",
    "\n",
    "        self.mean_adaptive_constant_res_log = []\n",
    "        self.mean_adaptive_constant_bcs_log = []\n",
    "        self.mean_adaptive_constant_ics_log = []\n",
    "\n",
    "        for i in range(1 , len( self.layers) - 2):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "            self.mean_grad_ics_list.append(tf.reduce_mean(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.mean_grad_ics = tf.reduce_mean(tf.stack(self.mean_grad_ics_list))\n",
    "        \n",
    "        self.adaptive_constant_bcs = self.max_grad_res/self.mean_grad_bcs\n",
    "        self.adaptive_constant_ics = self.max_grad_res/ self.mean_grad_ics\n",
    "        self.adaptive_constant_res = self.max_grad_res \n",
    "\n",
    "         # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        # Xavier initialization\n",
    "        def xavier_init(size):\n",
    "            in_dim = size[0]\n",
    "            out_dim = size[1]\n",
    "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "            return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H, layers, weights, biases):\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1),\n",
    "                              self.layers,\n",
    "                              self.weights,\n",
    "                              self.biases)\n",
    "        return u\n",
    "\n",
    "    # Forward pass for du/dt\n",
    "    def net_u_t(self, t, x):\n",
    "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
    "        return u_t\n",
    "\n",
    "    # Forward pass for the residual\n",
    "    def net_r(self, t, x):\n",
    "        u = self.net_u(t, x)\n",
    "        residual = self.operator(u, t, x,\n",
    "                                 self.c,\n",
    "                                 self.sigma_t,\n",
    "                                 self.sigma_x)\n",
    "        return residual\n",
    "    \n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "        # Trains the model by minimizing the MSE loss\n",
    "\n",
    "\n",
    "    def lambda_balance(self  , term  ):\n",
    "                histoy_mean =  np.mean(self.loss_history[term])\n",
    "                m = 3.0 #len(self.loss_list)\n",
    "                num = np.exp(10*  np.mean(self.loss_history[term][-99::]) )#/(self.T * histoy_mean)) np.exp( )\n",
    "                denum = 0 \n",
    "                loss_list = [ \"loss_res\"   , \"loss_bcs\" ,\"loss_ics_u_t\"  ]\n",
    "                for  key in loss_list:\n",
    "                    denum +=  np.exp( 10* np.mean(self.loss_history[key][-99::]) + 1e-12 )# /(self.T * histoy_mean))  np.exp(self.loss_history[key][-1] )\n",
    "\n",
    "                return m * (num / denum)\n",
    "    \n",
    "    def trainmb(self, nIter=10000, batch_size=128, log_NTK=False, update_lam=False):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range( 1, nIter):\n",
    "            # Fetch boundary mini-batches , \n",
    "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size // 3)\n",
    "            X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], batch_size // 3)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size // 3)\n",
    "            \n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                       self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                       self.u_ics_tf: u_ics_batch,\n",
    "                       self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                        self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                       self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
    "                       self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
    "                       self.adaptive_constant_res_tf: self.adaptive_constant_res_val\n",
    "                       }#self.lam_r_val}\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                loss_ics_u_t = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "\n",
    "                self.print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs, loss_ics_u_t, elapsed))\n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "            if it % 1000 == 0:\n",
    "\n",
    "                adaptive_constant_res_val, adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [self.adaptive_constant_res, self.adaptive_constant_bcs, self.adaptive_constant_ics  ], tf_dict)\n",
    "\n",
    "                update_loss_res = self.lambda_balance( \"loss_res\"  )\n",
    "                update_loss_bcs = self.lambda_balance( \"loss_bcs\"  )\n",
    "                update_loss_ics_u_t1 = self.lambda_balance( \"loss_ics_u_t\"  )\n",
    "\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_res_val = update_loss_res * adaptive_constant_res_val / adaptive_constant_res_val #  * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_res_val\n",
    "                self.adaptive_constant_ics_val = update_loss_ics_u_t1* adaptive_constant_res_val / adaptive_constant_ics_val # * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs_val = update_loss_bcs * adaptive_constant_res_val /  adaptive_constant_bcs_val # * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                self.print('update_loss_res: ' , ( update_loss_res ))\n",
    "                self.print('update_loss_ics_u_t:' , (update_loss_ics_u_t1))\n",
    "                self.print('update_loss_bcs: ' , ( update_loss_bcs))\n",
    "                \n",
    "                self.print('adaptive_constant_res_val: ' , ( self.adaptive_constant_res_val ))\n",
    "                self.print('adaptive_constant_ics_val:' , (self.adaptive_constant_ics_val))\n",
    "                self.print('adaptive_constant_bcs_val: ' , ( self.adaptive_constant_bcs_val))\n",
    "                \n",
    "                self.adaptive_constant_res_log.append(adaptive_constant_res_val)\n",
    "                self.adaptive_constant_bcs_log.append(adaptive_constant_bcs_val)\n",
    "                self.adaptive_constant_ics_log.append(adaptive_constant_ics_val)\n",
    "\n",
    "                max_grad_res , mean_grad_bcs, mean_grad_ics = self.sess.run( [ self.max_grad_res , self.mean_grad_bcs, self.mean_grad_ics  ], tf_dict)\n",
    "\n",
    "                self.mean_adaptive_constant_res_log.append( max_grad_res)\n",
    "                self.mean_adaptive_constant_bcs_log.append( mean_grad_bcs)\n",
    "                self.mean_adaptive_constant_ics_log.append( mean_grad_ics)\n",
    "\n",
    "                self.print('max_grad_res: {:.3e}'.format( max_grad_res))\n",
    "                self.print('mean_grad_ics: {:.3e}'.format( mean_grad_ics))\n",
    "                self.print('mean_grad_bcs: {:.3e}'.format( mean_grad_bcs))\n",
    "                \n",
    "                sys.stdout.flush()\n",
    "                start_time = timeit.default_timer()\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    " \n",
    "\n",
    "    def train(self, nIter , bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, bcbatch_size)\n",
    "        X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        \n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                    self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                    self.u_ics_tf: u_ics_batch,\n",
    "                    self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                    self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                    self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
    "                    self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val\n",
    "                    }#self.lam_r_val}\n",
    "        \n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 1000 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "                adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [self.adaptive_constant_bcs, self.adaptive_constant_ics  ], tf_dict)\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "\n",
    "                print('lambda_u: {:.3e}'.format(self.adaptive_constant_bcs_val))\n",
    "                print('lambda_ut: {:.3e}'.format(self.adaptive_constant_ics_val))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                         \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "        # Evaluates predictions at test points\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "    \n",
    "   ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   \n",
    "     ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1  , grad_ics  = self.sess.run([ self.grad_res[i],self.grad_bcs[i],self.grad_ics[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_ics = self.dict_gradients_ics_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "            sns.distplot(gradients_ics, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{ics}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=3)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        #plt.show()\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "  \n",
    "            \n",
    "    def plt_prediction(self , t , x , X_star , u_star , u_pred , r_star , r_pred):\n",
    "        \n",
    "        U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "        r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "        R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(18, 9))\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.pcolor(t, x, U_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$x_1$')\n",
    "        plt.ylabel('$x_2$')\n",
    "        plt.title('Exact u(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Predicted u(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Absolute error')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.pcolor(t, x, r_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Exact r(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.pcolor(t, x, R_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Predicted r(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Absolute error')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fontsize = 17\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([16,8])\n",
    "        ax.semilogy(self.mean_adaptive_constant_bcs_log, label=r'$\\bar{\\nabla_\\theta {u_{bc}}}$' , color = 'tab:green')\n",
    "        ax.semilogy(self.mean_adaptive_constant_ics_log, label=r'$\\bar{\\nabla_\\theta {u_{ics}}}$' , color = 'tab:blue')\n",
    "        ax.semilogy(self.mean_adaptive_constant_res_log, label=r'$Max{\\nabla_\\theta {u_{phy}}}$' , color = 'tab:red')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
    "        ax.tick_params(labelsize=fontsize)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
    "\n",
    "        ax2 = ax.twinx() \n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_size_inches([15,8])\n",
    "    \n",
    "        ax2.semilogy(self.adaptive_constant_bcs_log, label=r'$\\bar{\\lambda_{bc}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
    "        ax2.semilogy(self.adaptive_constant_ics_log, label=r'$\\bar{\\lambda_{ics}}$' , linestyle='dashed'  , color = 'tab:blue')\n",
    "        ax.semilogy(self.adaptive_constant_res_log, label=r'$Max{\\lambda_{phy}}$' ,  linestyle='dashed' , color = 'tab:red')\n",
    "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
    "        ax2.tick_params(labelsize=fontsize)\n",
    "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   #  \n",
    "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
    "def test_method(method , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "        # sess.run(init)\n",
    "\n",
    "        model = PINN(layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess)\n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "\n",
    "        if method ==\"full_batch\":\n",
    "            print(\"full_batch method is used\")\n",
    "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "        elif method ==\"mini_batch\":\n",
    "            print(\"mini_batch method is used\")\n",
    "            model.trainmb(nIter, mbbatch_size)\n",
    "        else:\n",
    "            print(\"unknown method!\")\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # Predictions\n",
    "        u_pred = model.predict_u(X_star)\n",
    "        r_pred = model.predict_r(X_star)\n",
    "        # Predictions\n",
    "\n",
    "        sess.close()   \n",
    "\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "\n",
    "    print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "\n",
    "    return [elapsed, error_u  ]\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_6410/181881221.py:65: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_6410/181881221.py:66: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_6410/181881221.py:67: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_6410/181881221.py:67: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 11:08:18.244351: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-24 11:08:18.273185: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-24 11:08:18.273648: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563719df8880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-24 11:08:18.273671: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-24 11:08:18.278793: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_6410/509526601.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_6410/509526601.py:174: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_6410/509526601.py:176: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_6410/509526601.py:230: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "mini_batch method is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, Loss: 5.164e-01, Loss_res: 1.928e-03,  Loss_bcs: 4.810e-01, Loss_ut_ics: 3.344e-02,, Time: 12.49\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, Loss: 3.956e-01, Loss_res: 7.477e-03,  Loss_bcs: 3.824e-01, Loss_ut_ics: 5.728e-03,, Time: 19.70\n",
      "It: 300, Loss: 3.045e-01, Loss_res: 4.149e-02,  Loss_bcs: 2.603e-01, Loss_ut_ics: 2.746e-03,, Time: 22.64\n",
      "It: 400, Loss: 1.517e+00, Loss_res: 1.227e+00,  Loss_bcs: 2.651e-01, Loss_ut_ics: 2.482e-02,, Time: 25.50\n",
      "It: 500, Loss: 2.651e-01, Loss_res: 1.950e-03,  Loss_bcs: 2.363e-01, Loss_ut_ics: 2.684e-02,, Time: 28.43\n",
      "It: 600, Loss: 2.254e-01, Loss_res: 1.889e-02,  Loss_bcs: 2.008e-01, Loss_ut_ics: 5.738e-03,, Time: 31.45\n",
      "It: 700, Loss: 2.222e-01, Loss_res: 2.953e-03,  Loss_bcs: 2.140e-01, Loss_ut_ics: 5.169e-03,, Time: 34.64\n",
      "It: 800, Loss: 2.064e-01, Loss_res: 2.056e-02,  Loss_bcs: 1.756e-01, Loss_ut_ics: 1.030e-02,, Time: 37.87\n",
      "It: 900, Loss: 1.961e-01, Loss_res: 4.574e-03,  Loss_bcs: 1.882e-01, Loss_ut_ics: 3.272e-03,, Time: 41.06\n",
      "It: 1000, Loss: 2.566e-01, Loss_res: 5.210e-02,  Loss_bcs: 1.823e-01, Loss_ut_ics: 2.221e-02,, Time: 44.33\n",
      "update_loss_res: 3.9392e-01\n",
      "update_loss_ics_u_t:4.0775e-01\n",
      "update_loss_bcs: 2.1983e+00\n",
      "adaptive_constant_res_val: 3.9392e-01\n",
      "adaptive_constant_ics_val:9.4184e-04\n",
      "adaptive_constant_bcs_val: 1.8391e-03\n",
      "max_grad_res: 1.501e+00\n",
      "mean_grad_ics: 2.310e-03\n",
      "mean_grad_bcs: 8.366e-04\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, Loss: 6.909e-04, Loss_res: 7.262e-04,  Loss_bcs: 2.074e-01, Loss_ut_ics: 2.492e-02,, Time: 6.16\n",
      "It: 1200, Loss: 6.253e-04, Loss_res: 6.490e-04,  Loss_bcs: 1.971e-01, Loss_ut_ics: 7.501e-03,, Time: 9.55\n",
      "It: 1300, Loss: 4.770e-04, Loss_res: 4.496e-04,  Loss_bcs: 1.602e-01, Loss_ut_ics: 5.588e-03,, Time: 13.02\n",
      "It: 1400, Loss: 4.517e-04, Loss_res: 2.995e-04,  Loss_bcs: 1.790e-01, Loss_ut_ics: 4.852e-03,, Time: 16.41\n",
      "It: 1500, Loss: 4.523e-04, Loss_res: 2.608e-04,  Loss_bcs: 1.877e-01, Loss_ut_ics: 4.724e-03,, Time: 19.71\n",
      "It: 1600, Loss: 4.408e-04, Loss_res: 3.013e-04,  Loss_bcs: 1.729e-01, Loss_ut_ics: 4.345e-03,, Time: 23.07\n",
      "It: 1700, Loss: 4.230e-04, Loss_res: 2.581e-04,  Loss_bcs: 1.721e-01, Loss_ut_ics: 5.014e-03,, Time: 26.43\n",
      "It: 1800, Loss: 4.095e-04, Loss_res: 1.133e-04,  Loss_bcs: 1.963e-01, Loss_ut_ics: 4.131e-03,, Time: 29.69\n",
      "It: 1900, Loss: 3.963e-04, Loss_res: 1.755e-04,  Loss_bcs: 1.756e-01, Loss_ut_ics: 4.496e-03,, Time: 33.05\n",
      "It: 2000, Loss: 4.160e-04, Loss_res: 1.827e-04,  Loss_bcs: 1.848e-01, Loss_ut_ics: 4.369e-03,, Time: 36.39\n",
      "update_loss_res: 3.7504e-01\n",
      "update_loss_ics_u_t:3.9039e-01\n",
      "update_loss_bcs: 2.2346e+00\n",
      "adaptive_constant_res_val: 3.7504e-01\n",
      "adaptive_constant_ics_val:2.3817e-04\n",
      "adaptive_constant_bcs_val: 6.9104e-04\n",
      "max_grad_res: 2.766e-02\n",
      "mean_grad_ics: 6.101e-04\n",
      "mean_grad_bcs: 3.092e-04\n",
      "It: 2100, Loss: 1.757e-04, Loss_res: 1.298e-04,  Loss_bcs: 1.822e-01, Loss_ut_ics: 4.786e-03,, Time: 3.28\n",
      "It: 2200, Loss: 1.812e-04, Loss_res: 1.334e-04,  Loss_bcs: 1.874e-01, Loss_ut_ics: 6.687e-03,, Time: 6.69\n",
      "It: 2300, Loss: 1.566e-04, Loss_res: 1.012e-04,  Loss_bcs: 1.687e-01, Loss_ut_ics: 8.570e-03,, Time: 10.05\n",
      "It: 2400, Loss: 1.634e-04, Loss_res: 8.873e-05,  Loss_bcs: 1.856e-01, Loss_ut_ics: 7.911e-03,, Time: 13.40\n",
      "It: 2500, Loss: 1.759e-04, Loss_res: 8.222e-05,  Loss_bcs: 2.076e-01, Loss_ut_ics: 6.747e-03,, Time: 16.82\n",
      "It: 2600, Loss: 1.755e-04, Loss_res: 1.249e-04,  Loss_bcs: 1.832e-01, Loss_ut_ics: 8.460e-03,, Time: 20.10\n",
      "It: 2700, Loss: 1.738e-04, Loss_res: 9.904e-05,  Loss_bcs: 1.950e-01, Loss_ut_ics: 7.867e-03,, Time: 23.54\n",
      "It: 2800, Loss: 1.530e-04, Loss_res: 6.358e-05,  Loss_bcs: 1.844e-01, Loss_ut_ics: 7.403e-03,, Time: 26.92\n",
      "It: 2900, Loss: 1.686e-04, Loss_res: 9.576e-05,  Loss_bcs: 1.897e-01, Loss_ut_ics: 7.021e-03,, Time: 30.20\n",
      "It: 3000, Loss: 1.956e-04, Loss_res: 1.852e-04,  Loss_bcs: 1.800e-01, Loss_ut_ics: 7.349e-03,, Time: 33.60\n",
      "update_loss_res: 3.6121e-01\n",
      "update_loss_ics_u_t:3.9097e-01\n",
      "update_loss_bcs: 2.2478e+00\n",
      "adaptive_constant_res_val: 3.6121e-01\n",
      "adaptive_constant_ics_val:2.7714e-04\n",
      "adaptive_constant_bcs_val: 5.3131e-04\n",
      "max_grad_res: 6.070e-02\n",
      "mean_grad_ics: 7.088e-04\n",
      "mean_grad_bcs: 2.364e-04\n",
      "It: 3100, Loss: 1.389e-04, Loss_res: 9.265e-05,  Loss_bcs: 1.946e-01, Loss_ut_ics: 7.258e-03,, Time: 3.41\n",
      "It: 3200, Loss: 1.196e-04, Loss_res: 7.491e-05,  Loss_bcs: 1.709e-01, Loss_ut_ics: 6.353e-03,, Time: 6.72\n",
      "It: 3300, Loss: 1.285e-04, Loss_res: 9.596e-05,  Loss_bcs: 1.730e-01, Loss_ut_ics: 6.743e-03,, Time: 10.05\n",
      "It: 3400, Loss: 1.447e-04, Loss_res: 8.419e-05,  Loss_bcs: 2.123e-01, Loss_ut_ics: 5.163e-03,, Time: 13.45\n",
      "It: 3500, Loss: 1.453e-04, Loss_res: 1.280e-04,  Loss_bcs: 1.837e-01, Loss_ut_ics: 5.273e-03,, Time: 16.73\n",
      "It: 3600, Loss: 1.425e-04, Loss_res: 9.603e-05,  Loss_bcs: 1.996e-01, Loss_ut_ics: 6.582e-03,, Time: 20.14\n",
      "It: 3700, Loss: 1.901e-04, Loss_res: 2.487e-04,  Loss_bcs: 1.865e-01, Loss_ut_ics: 4.032e-03,, Time: 23.50\n",
      "It: 3800, Loss: 3.908e-04, Loss_res: 7.811e-04,  Loss_bcs: 2.009e-01, Loss_ut_ics: 7.048e-03,, Time: 26.81\n",
      "It: 3900, Loss: 2.041e-04, Loss_res: 3.125e-04,  Loss_bcs: 1.685e-01, Loss_ut_ics: 6.121e-03,, Time: 30.21\n",
      "It: 4000, Loss: 1.515e-04, Loss_res: 1.168e-04,  Loss_bcs: 2.024e-01, Loss_ut_ics: 6.533e-03,, Time: 33.56\n",
      "update_loss_res: 3.4572e-01\n",
      "update_loss_ics_u_t:3.6703e-01\n",
      "update_loss_bcs: 2.2873e+00\n",
      "adaptive_constant_res_val: 3.4572e-01\n",
      "adaptive_constant_ics_val:3.7805e-04\n",
      "adaptive_constant_bcs_val: 1.7133e-03\n",
      "max_grad_res: 2.957e-02\n",
      "mean_grad_ics: 1.030e-03\n",
      "mean_grad_bcs: 7.490e-04\n",
      "It: 4100, Loss: 4.225e-04, Loss_res: 1.835e-04,  Loss_bcs: 2.073e-01, Loss_ut_ics: 1.043e-02,, Time: 3.34\n",
      "It: 4200, Loss: 3.830e-04, Loss_res: 7.865e-05,  Loss_bcs: 2.052e-01, Loss_ut_ics: 1.131e-02,, Time: 6.71\n",
      "It: 4300, Loss: 4.213e-04, Loss_res: 3.626e-04,  Loss_bcs: 1.705e-01, Loss_ut_ics: 9.899e-03,, Time: 10.06\n",
      "It: 4400, Loss: 4.029e-04, Loss_res: 7.316e-05,  Loss_bcs: 2.168e-01, Loss_ut_ics: 1.645e-02,, Time: 13.46\n",
      "It: 4500, Loss: 3.378e-04, Loss_res: 2.664e-05,  Loss_bcs: 1.894e-01, Loss_ut_ics: 1.096e-02,, Time: 16.84\n",
      "It: 4600, Loss: 3.983e-04, Loss_res: 8.343e-05,  Loss_bcs: 2.138e-01, Loss_ut_ics: 8.587e-03,, Time: 20.22\n",
      "It: 4700, Loss: 2.050e-03, Loss_res: 4.837e-03,  Loss_bcs: 2.147e-01, Loss_ut_ics: 2.777e-02,, Time: 23.62\n",
      "It: 4800, Loss: 3.800e-04, Loss_res: 2.707e-05,  Loss_bcs: 2.083e-01, Loss_ut_ics: 3.639e-02,, Time: 27.00\n",
      "It: 4900, Loss: 3.861e-04, Loss_res: 2.576e-05,  Loss_bcs: 2.154e-01, Loss_ut_ics: 2.139e-02,, Time: 30.34\n",
      "It: 5000, Loss: 3.893e-04, Loss_res: 2.017e-05,  Loss_bcs: 2.198e-01, Loss_ut_ics: 1.503e-02,, Time: 33.82\n",
      "update_loss_res: 3.0393e-01\n",
      "update_loss_ics_u_t:3.6413e-01\n",
      "update_loss_bcs: 2.3319e+00\n",
      "adaptive_constant_res_val: 3.0393e-01\n",
      "adaptive_constant_ics_val:5.7052e-04\n",
      "adaptive_constant_bcs_val: 1.2823e-03\n",
      "max_grad_res: 8.629e-03\n",
      "mean_grad_ics: 1.567e-03\n",
      "mean_grad_bcs: 5.499e-04\n",
      "It: 5100, Loss: 2.587e-04, Loss_res: 2.736e-05,  Loss_bcs: 1.929e-01, Loss_ut_ics: 5.334e-03,, Time: 3.39\n",
      "It: 5200, Loss: 2.999e-04, Loss_res: 1.938e-05,  Loss_bcs: 2.276e-01, Loss_ut_ics: 3.681e-03,, Time: 6.73\n",
      "It: 5300, Loss: 2.644e-04, Loss_res: 6.540e-05,  Loss_bcs: 1.893e-01, Loss_ut_ics: 3.029e-03,, Time: 10.12\n",
      "It: 5400, Loss: 2.325e-04, Loss_res: 5.612e-05,  Loss_bcs: 1.668e-01, Loss_ut_ics: 2.902e-03,, Time: 13.52\n",
      "It: 5500, Loss: 2.785e-04, Loss_res: 9.494e-05,  Loss_bcs: 1.933e-01, Loss_ut_ics: 2.992e-03,, Time: 16.80\n",
      "It: 5600, Loss: 5.099e-04, Loss_res: 7.980e-04,  Loss_bcs: 2.074e-01, Loss_ut_ics: 2.582e-03,, Time: 20.16\n",
      "It: 5700, Loss: 2.564e-04, Loss_res: 2.358e-05,  Loss_bcs: 1.931e-01, Loss_ut_ics: 2.800e-03,, Time: 23.55\n",
      "It: 5800, Loss: 5.826e-04, Loss_res: 8.420e-04,  Loss_bcs: 2.537e-01, Loss_ut_ics: 2.316e-03,, Time: 26.86\n",
      "It: 5900, Loss: 4.361e-04, Loss_res: 5.449e-04,  Loss_bcs: 2.069e-01, Loss_ut_ics: 9.118e-03,, Time: 30.25\n",
      "It: 6000, Loss: 2.833e-04, Loss_res: 2.013e-05,  Loss_bcs: 2.144e-01, Loss_ut_ics: 4.045e-03,, Time: 33.62\n",
      "update_loss_res: 2.9796e-01\n",
      "update_loss_ics_u_t:3.1788e-01\n",
      "update_loss_bcs: 2.3842e+00\n",
      "adaptive_constant_res_val: 2.9796e-01\n",
      "adaptive_constant_ics_val:2.3147e-04\n",
      "adaptive_constant_bcs_val: 1.2732e-03\n",
      "max_grad_res: 7.360e-03\n",
      "mean_grad_ics: 7.282e-04\n",
      "mean_grad_bcs: 5.340e-04\n",
      "It: 6100, Loss: 2.318e-04, Loss_res: 4.297e-05,  Loss_bcs: 1.703e-01, Loss_ut_ics: 9.384e-03,, Time: 3.29\n",
      "It: 6200, Loss: 2.565e-04, Loss_res: 2.105e-05,  Loss_bcs: 1.954e-01, Loss_ut_ics: 6.144e-03,, Time: 6.70\n",
      "It: 6300, Loss: 2.416e-03, Loss_res: 7.223e-03,  Loss_bcs: 2.057e-01, Loss_ut_ics: 6.301e-03,, Time: 10.02\n",
      "It: 6400, Loss: 2.638e-04, Loss_res: 7.597e-05,  Loss_bcs: 1.872e-01, Loss_ut_ics: 1.258e-02,, Time: 13.36\n",
      "It: 6500, Loss: 2.724e-04, Loss_res: 3.299e-05,  Loss_bcs: 2.047e-01, Loss_ut_ics: 8.507e-03,, Time: 16.79\n",
      "It: 6600, Loss: 3.151e-04, Loss_res: 2.186e-04,  Loss_bcs: 1.940e-01, Loss_ut_ics: 1.320e-02,, Time: 20.15\n",
      "It: 6700, Loss: 2.794e-04, Loss_res: 5.097e-05,  Loss_bcs: 2.045e-01, Loss_ut_ics: 1.675e-02,, Time: 23.50\n",
      "It: 6800, Loss: 2.674e-04, Loss_res: 2.326e-05,  Loss_bcs: 2.020e-01, Loss_ut_ics: 1.410e-02,, Time: 26.91\n",
      "It: 6900, Loss: 3.340e-04, Loss_res: 1.571e-04,  Loss_bcs: 2.234e-01, Loss_ut_ics: 1.157e-02,, Time: 30.32\n",
      "It: 7000, Loss: 2.825e-04, Loss_res: 6.191e-05,  Loss_bcs: 2.037e-01, Loss_ut_ics: 2.040e-02,, Time: 33.72\n",
      "update_loss_res: 2.8633e-01\n",
      "update_loss_ics_u_t:3.3049e-01\n",
      "update_loss_bcs: 2.3832e+00\n",
      "adaptive_constant_res_val: 2.8633e-01\n",
      "adaptive_constant_ics_val:5.6222e-04\n",
      "adaptive_constant_bcs_val: 1.1089e-03\n",
      "max_grad_res: 3.103e-02\n",
      "mean_grad_ics: 1.701e-03\n",
      "mean_grad_bcs: 4.653e-04\n",
      "It: 7100, Loss: 2.241e-04, Loss_res: 1.800e-05,  Loss_bcs: 1.943e-01, Loss_ut_ics: 6.186e-03,, Time: 3.33\n",
      "It: 7200, Loss: 2.935e-04, Loss_res: 1.130e-05,  Loss_bcs: 2.599e-01, Loss_ut_ics: 3.758e-03,, Time: 6.72\n",
      "It: 7300, Loss: 3.138e-04, Loss_res: 1.051e-04,  Loss_bcs: 2.529e-01, Loss_ut_ics: 5.767e-03,, Time: 10.46\n",
      "It: 7400, Loss: 6.859e-04, Loss_res: 1.501e-03,  Loss_bcs: 2.287e-01, Loss_ut_ics: 4.579e-03,, Time: 14.17\n",
      "It: 7500, Loss: 2.312e-04, Loss_res: 3.453e-05,  Loss_bcs: 1.965e-01, Loss_ut_ics: 6.132e-03,, Time: 17.74\n",
      "It: 7600, Loss: 2.341e-04, Loss_res: 3.146e-05,  Loss_bcs: 2.000e-01, Loss_ut_ics: 5.767e-03,, Time: 21.28\n",
      "It: 7700, Loss: 3.818e-04, Loss_res: 5.800e-04,  Loss_bcs: 1.915e-01, Loss_ut_ics: 5.904e-03,, Time: 24.74\n",
      "It: 7800, Loss: 1.455e-03, Loss_res: 4.232e-03,  Loss_bcs: 2.163e-01, Loss_ut_ics: 5.689e-03,, Time: 28.13\n",
      "It: 7900, Loss: 2.411e-04, Loss_res: 7.637e-05,  Loss_bcs: 1.937e-01, Loss_ut_ics: 7.900e-03,, Time: 31.64\n",
      "It: 8000, Loss: 2.650e-04, Loss_res: 2.779e-05,  Loss_bcs: 2.283e-01, Loss_ut_ics: 7.012e-03,, Time: 35.32\n",
      "update_loss_res: 2.6886e-01\n",
      "update_loss_ics_u_t:2.8834e-01\n",
      "update_loss_bcs: 2.4428e+00\n",
      "adaptive_constant_res_val: 2.6886e-01\n",
      "adaptive_constant_ics_val:1.7931e-04\n",
      "adaptive_constant_bcs_val: 1.3817e-03\n",
      "max_grad_res: 9.413e-03\n",
      "mean_grad_ics: 6.219e-04\n",
      "mean_grad_bcs: 5.656e-04\n",
      "It: 8100, Loss: 3.304e-04, Loss_res: 8.894e-05,  Loss_bcs: 2.211e-01, Loss_ut_ics: 5.827e-03,, Time: 3.64\n",
      "It: 8200, Loss: 3.655e-04, Loss_res: 4.918e-05,  Loss_bcs: 2.518e-01, Loss_ut_ics: 2.389e-02,, Time: 7.13\n",
      "It: 8300, Loss: 3.005e-04, Loss_res: 1.114e-05,  Loss_bcs: 2.135e-01, Loss_ut_ics: 1.403e-02,, Time: 10.59\n",
      "It: 8400, Loss: 3.068e-04, Loss_res: 1.025e-04,  Loss_bcs: 2.001e-01, Loss_ut_ics: 1.565e-02,, Time: 14.03\n",
      "It: 8500, Loss: 6.139e-04, Loss_res: 9.951e-04,  Loss_bcs: 2.493e-01, Loss_ut_ics: 1.015e-02,, Time: 17.40\n",
      "It: 8600, Loss: 3.333e-04, Loss_res: 2.885e-05,  Loss_bcs: 2.319e-01, Loss_ut_ics: 2.879e-02,, Time: 20.90\n",
      "It: 8700, Loss: 3.142e-04, Loss_res: 1.090e-05,  Loss_bcs: 2.223e-01, Loss_ut_ics: 2.257e-02,, Time: 24.37\n",
      "It: 8800, Loss: 3.057e-04, Loss_res: 1.274e-05,  Loss_bcs: 2.163e-01, Loss_ut_ics: 1.885e-02,, Time: 27.76\n",
      "It: 8900, Loss: 3.333e-04, Loss_res: 2.901e-04,  Loss_bcs: 1.831e-01, Loss_ut_ics: 1.265e-02,, Time: 31.24\n",
      "It: 9000, Loss: 7.278e-04, Loss_res: 1.533e-03,  Loss_bcs: 2.262e-01, Loss_ut_ics: 1.660e-02,, Time: 34.68\n",
      "update_loss_res: 2.8438e-01\n",
      "update_loss_ics_u_t:3.1974e-01\n",
      "update_loss_bcs: 2.3959e+00\n",
      "adaptive_constant_res_val: 2.8438e-01\n",
      "adaptive_constant_ics_val:3.4926e-04\n",
      "adaptive_constant_bcs_val: 9.0658e-04\n",
      "max_grad_res: 1.606e-01\n",
      "mean_grad_ics: 1.092e-03\n",
      "mean_grad_bcs: 3.784e-04\n",
      "It: 9100, Loss: 2.197e-04, Loss_res: 6.563e-05,  Loss_bcs: 2.181e-01, Loss_ut_ics: 9.560e-03,, Time: 3.36\n",
      "It: 9200, Loss: 3.736e-04, Loss_res: 5.950e-04,  Loss_bcs: 2.206e-01, Loss_ut_ics: 1.240e-02,, Time: 6.82\n",
      "It: 9300, Loss: 2.451e-04, Loss_res: 1.020e-04,  Loss_bcs: 2.351e-01, Loss_ut_ics: 8.354e-03,, Time: 10.29\n",
      "It: 9400, Loss: 2.434e-04, Loss_res: 3.715e-05,  Loss_bcs: 2.534e-01, Loss_ut_ics: 8.908e-03,, Time: 13.79\n",
      "It: 9500, Loss: 2.232e-04, Loss_res: 1.792e-04,  Loss_bcs: 1.857e-01, Loss_ut_ics: 1.110e-02,, Time: 17.28\n",
      "It: 9600, Loss: 3.175e-04, Loss_res: 3.360e-04,  Loss_bcs: 2.409e-01, Loss_ut_ics: 1.026e-02,, Time: 20.68\n",
      "It: 9700, Loss: 3.717e-04, Loss_res: 5.033e-04,  Loss_bcs: 2.476e-01, Loss_ut_ics: 1.167e-02,, Time: 29.67\n",
      "It: 9800, Loss: 2.554e-04, Loss_res: 1.787e-05,  Loss_bcs: 2.717e-01, Loss_ut_ics: 1.138e-02,, Time: 33.74\n",
      "It: 9900, Loss: 3.880e-04, Loss_res: 6.342e-04,  Loss_bcs: 2.251e-01, Loss_ut_ics: 1.023e-02,, Time: 37.22\n",
      "It: 10000, Loss: 2.057e-04, Loss_res: 1.598e-05,  Loss_bcs: 2.173e-01, Loss_ut_ics: 1.191e-02,, Time: 40.22\n",
      "update_loss_res: 2.5466e-01\n",
      "update_loss_ics_u_t:2.8357e-01\n",
      "update_loss_bcs: 2.4618e+00\n",
      "adaptive_constant_res_val: 2.5466e-01\n",
      "adaptive_constant_ics_val:2.5859e-04\n",
      "adaptive_constant_bcs_val: 9.2096e-04\n",
      "max_grad_res: 8.784e-03\n",
      "mean_grad_ics: 9.119e-04\n",
      "mean_grad_bcs: 3.741e-04\n",
      "It: 10100, Loss: 1.954e-04, Loss_res: 1.007e-05,  Loss_bcs: 2.053e-01, Loss_ut_ics: 1.436e-02,, Time: 3.15\n",
      "It: 10200, Loss: 2.086e-04, Loss_res: 1.929e-05,  Loss_bcs: 2.188e-01, Loss_ut_ics: 8.473e-03,, Time: 6.54\n",
      "It: 10300, Loss: 2.997e-04, Loss_res: 3.479e-04,  Loss_bcs: 2.262e-01, Loss_ut_ics: 1.055e-02,, Time: 9.85\n",
      "It: 10400, Loss: 2.391e-04, Loss_res: 1.494e-05,  Loss_bcs: 2.527e-01, Loss_ut_ics: 9.895e-03,, Time: 13.05\n",
      "It: 10500, Loss: 2.060e-04, Loss_res: 3.256e-05,  Loss_bcs: 2.076e-01, Loss_ut_ics: 2.513e-02,, Time: 16.10\n",
      "It: 10600, Loss: 2.001e-04, Loss_res: 6.051e-06,  Loss_bcs: 2.094e-01, Loss_ut_ics: 2.214e-02,, Time: 19.31\n",
      "It: 10700, Loss: 2.137e-04, Loss_res: 1.649e-05,  Loss_bcs: 2.229e-01, Loss_ut_ics: 1.616e-02,, Time: 22.48\n",
      "It: 10800, Loss: 1.795e-04, Loss_res: 2.659e-05,  Loss_bcs: 1.848e-01, Loss_ut_ics: 9.850e-03,, Time: 25.85\n",
      "It: 10900, Loss: 2.584e-04, Loss_res: 9.473e-05,  Loss_bcs: 2.511e-01, Loss_ut_ics: 1.170e-02,, Time: 29.83\n",
      "It: 11000, Loss: 2.168e-04, Loss_res: 3.653e-05,  Loss_bcs: 2.210e-01, Loss_ut_ics: 1.532e-02,, Time: 33.06\n",
      "update_loss_res: 2.5906e-01\n",
      "update_loss_ics_u_t:2.9421e-01\n",
      "update_loss_bcs: 2.4467e+00\n",
      "adaptive_constant_res_val: 2.5906e-01\n",
      "adaptive_constant_ics_val:2.8467e-04\n",
      "adaptive_constant_bcs_val: 6.9492e-04\n",
      "max_grad_res: 2.285e-02\n",
      "mean_grad_ics: 9.676e-04\n",
      "mean_grad_bcs: 2.840e-04\n",
      "It: 11100, Loss: 1.570e-04, Loss_res: 1.220e-05,  Loss_bcs: 2.161e-01, Loss_ut_ics: 1.288e-02,, Time: 3.38\n",
      "It: 11200, Loss: 2.632e-04, Loss_res: 3.583e-04,  Loss_bcs: 2.413e-01, Loss_ut_ics: 9.659e-03,, Time: 6.66\n",
      "It: 11300, Loss: 2.135e-04, Loss_res: 2.564e-04,  Loss_bcs: 2.055e-01, Loss_ut_ics: 1.500e-02,, Time: 9.79\n",
      "It: 11400, Loss: 1.949e-04, Loss_res: 1.667e-05,  Loss_bcs: 2.697e-01, Loss_ut_ics: 1.131e-02,, Time: 12.86\n",
      "It: 11500, Loss: 1.726e-04, Loss_res: 9.379e-06,  Loss_bcs: 2.381e-01, Loss_ut_ics: 1.634e-02,, Time: 15.99\n",
      "It: 11600, Loss: 1.575e-04, Loss_res: 6.637e-06,  Loss_bcs: 2.191e-01, Loss_ut_ics: 1.248e-02,, Time: 19.05\n",
      "It: 11700, Loss: 1.535e-04, Loss_res: 2.310e-05,  Loss_bcs: 2.069e-01, Loss_ut_ics: 1.286e-02,, Time: 22.22\n",
      "It: 11800, Loss: 1.256e-04, Loss_res: 8.098e-06,  Loss_bcs: 1.708e-01, Loss_ut_ics: 1.712e-02,, Time: 25.28\n",
      "It: 11900, Loss: 1.445e-04, Loss_res: 5.146e-05,  Loss_bcs: 1.841e-01, Loss_ut_ics: 1.119e-02,, Time: 28.34\n",
      "It: 12000, Loss: 1.757e-04, Loss_res: 1.080e-05,  Loss_bcs: 2.438e-01, Loss_ut_ics: 1.242e-02,, Time: 31.44\n",
      "update_loss_res: 2.5529e-01\n",
      "update_loss_ics_u_t:2.9079e-01\n",
      "update_loss_bcs: 2.4539e+00\n",
      "adaptive_constant_res_val: 2.5529e-01\n",
      "adaptive_constant_ics_val:7.6191e-05\n",
      "adaptive_constant_bcs_val: 1.0794e-03\n",
      "max_grad_res: 4.734e-03\n",
      "mean_grad_ics: 2.620e-04\n",
      "mean_grad_bcs: 4.399e-04\n",
      "It: 12100, Loss: 2.497e-04, Loss_res: 1.750e-05,  Loss_bcs: 2.256e-01, Loss_ut_ics: 2.186e-02,, Time: 3.52\n",
      "It: 12200, Loss: 3.976e-04, Loss_res: 5.787e-04,  Loss_bcs: 2.297e-01, Loss_ut_ics: 2.572e-02,, Time: 6.94\n",
      "It: 12300, Loss: 2.695e-04, Loss_res: 5.303e-05,  Loss_bcs: 2.343e-01, Loss_ut_ics: 4.043e-02,, Time: 10.07\n",
      "It: 12400, Loss: 2.387e-04, Loss_res: 2.662e-05,  Loss_bcs: 2.126e-01, Loss_ut_ics: 3.199e-02,, Time: 13.15\n",
      "It: 12500, Loss: 2.772e-04, Loss_res: 8.030e-05,  Loss_bcs: 2.329e-01, Loss_ut_ics: 6.947e-02,, Time: 16.98\n",
      "It: 12600, Loss: 2.822e-04, Loss_res: 8.779e-06,  Loss_bcs: 2.553e-01, Loss_ut_ics: 5.738e-02,, Time: 20.22\n",
      "It: 12700, Loss: 2.465e-04, Loss_res: 8.787e-06,  Loss_bcs: 2.237e-01, Loss_ut_ics: 3.628e-02,, Time: 24.10\n",
      "It: 12800, Loss: 4.775e-04, Loss_res: 8.147e-04,  Loss_bcs: 2.479e-01, Loss_ut_ics: 2.566e-02,, Time: 27.66\n",
      "It: 12900, Loss: 2.064e-04, Loss_res: 1.612e-05,  Loss_bcs: 1.829e-01, Loss_ut_ics: 6.457e-02,, Time: 31.28\n",
      "It: 13000, Loss: 2.646e-04, Loss_res: 3.528e-05,  Loss_bcs: 2.342e-01, Loss_ut_ics: 3.710e-02,, Time: 34.63\n",
      "update_loss_res: 2.6139e-01\n",
      "update_loss_ics_u_t:3.9035e-01\n",
      "update_loss_bcs: 2.3483e+00\n",
      "adaptive_constant_res_val: 2.6139e-01\n",
      "adaptive_constant_ics_val:7.2959e-04\n",
      "adaptive_constant_bcs_val: 6.4149e-04\n",
      "max_grad_res: 1.329e-02\n",
      "mean_grad_ics: 1.869e-03\n",
      "mean_grad_bcs: 2.732e-04\n",
      "It: 13100, Loss: 1.817e-04, Loss_res: 5.393e-05,  Loss_bcs: 2.433e-01, Loss_ut_ics: 1.579e-02,, Time: 3.24\n",
      "It: 13200, Loss: 1.564e-04, Loss_res: 2.387e-05,  Loss_bcs: 2.191e-01, Loss_ut_ics: 1.309e-02,, Time: 6.35\n",
      "It: 13300, Loss: 1.943e-04, Loss_res: 5.288e-05,  Loss_bcs: 2.683e-01, Loss_ut_ics: 1.138e-02,, Time: 9.42\n",
      "It: 13400, Loss: 1.888e-04, Loss_res: 1.547e-04,  Loss_bcs: 2.165e-01, Loss_ut_ics: 1.305e-02,, Time: 12.55\n",
      "It: 13500, Loss: 1.833e-04, Loss_res: 7.357e-06,  Loss_bcs: 2.703e-01, Loss_ut_ics: 1.095e-02,, Time: 15.92\n",
      "It: 13600, Loss: 1.544e-04, Loss_res: 8.090e-06,  Loss_bcs: 2.262e-01, Loss_ut_ics: 9.853e-03,, Time: 19.04\n",
      "It: 13700, Loss: 1.880e-04, Loss_res: 6.504e-06,  Loss_bcs: 2.803e-01, Loss_ut_ics: 8.791e-03,, Time: 22.22\n",
      "It: 13800, Loss: 1.414e-04, Loss_res: 7.750e-06,  Loss_bcs: 2.071e-01, Loss_ut_ics: 8.883e-03,, Time: 25.21\n",
      "It: 13900, Loss: 1.326e-04, Loss_res: 1.989e-05,  Loss_bcs: 1.867e-01, Loss_ut_ics: 1.051e-02,, Time: 28.31\n",
      "It: 14000, Loss: 2.138e-04, Loss_res: 2.630e-04,  Loss_bcs: 2.148e-01, Loss_ut_ics: 1.002e-02,, Time: 31.33\n",
      "update_loss_res: 2.7256e-01\n",
      "update_loss_ics_u_t:2.9821e-01\n",
      "update_loss_bcs: 2.4292e+00\n",
      "adaptive_constant_res_val: 2.7256e-01\n",
      "adaptive_constant_ics_val:1.4308e-04\n",
      "adaptive_constant_bcs_val: 1.4963e-03\n",
      "max_grad_res: 5.216e-02\n",
      "mean_grad_ics: 4.798e-04\n",
      "mean_grad_bcs: 6.159e-04\n",
      "It: 14100, Loss: 4.039e-04, Loss_res: 2.460e-04,  Loss_bcs: 2.233e-01, Loss_ut_ics: 1.961e-02,, Time: 3.14\n",
      "It: 14200, Loss: 3.752e-04, Loss_res: 2.955e-04,  Loss_bcs: 1.893e-01, Loss_ut_ics: 7.916e-02,, Time: 6.37\n",
      "It: 14300, Loss: 3.640e-04, Loss_res: 6.762e-06,  Loss_bcs: 2.395e-01, Loss_ut_ics: 2.624e-02,, Time: 9.42\n",
      "It: 14400, Loss: 2.637e-04, Loss_res: 1.398e-05,  Loss_bcs: 1.719e-01, Loss_ut_ics: 1.799e-02,, Time: 12.54\n",
      "It: 14500, Loss: 4.844e-04, Loss_res: 5.317e-04,  Loss_bcs: 2.238e-01, Loss_ut_ics: 3.188e-02,, Time: 15.65\n",
      "It: 14600, Loss: 3.097e-04, Loss_res: 5.356e-05,  Loss_bcs: 1.955e-01, Loss_ut_ics: 1.777e-02,, Time: 18.85\n",
      "It: 14700, Loss: 3.617e-04, Loss_res: 4.560e-05,  Loss_bcs: 2.315e-01, Loss_ut_ics: 2.056e-02,, Time: 21.95\n",
      "It: 14800, Loss: 4.012e-04, Loss_res: 3.098e-04,  Loss_bcs: 2.097e-01, Loss_ut_ics: 2.109e-02,, Time: 25.02\n",
      "It: 14900, Loss: 4.108e-04, Loss_res: 2.560e-04,  Loss_bcs: 2.245e-01, Loss_ut_ics: 3.618e-02,, Time: 28.12\n",
      "It: 15000, Loss: 3.459e-04, Loss_res: 7.880e-05,  Loss_bcs: 2.139e-01, Loss_ut_ics: 3.065e-02,, Time: 31.16\n",
      "update_loss_res: 3.0675e-01\n",
      "update_loss_ics_u_t:4.0020e-01\n",
      "update_loss_bcs: 2.2931e+00\n",
      "adaptive_constant_res_val: 3.0675e-01\n",
      "adaptive_constant_ics_val:5.7454e-04\n",
      "adaptive_constant_bcs_val: 3.3314e-04\n",
      "max_grad_res: 4.396e-02\n",
      "mean_grad_ics: 1.436e-03\n",
      "mean_grad_bcs: 1.453e-04\n",
      "It: 15100, Loss: 7.590e-05, Loss_res: 1.386e-05,  Loss_bcs: 1.947e-01, Loss_ut_ics: 1.181e-02,, Time: 3.09\n",
      "It: 15200, Loss: 1.014e-04, Loss_res: 2.279e-05,  Loss_bcs: 2.635e-01, Loss_ut_ics: 1.151e-02,, Time: 6.25\n",
      "It: 15300, Loss: 8.074e-05, Loss_res: 2.419e-05,  Loss_bcs: 2.041e-01, Loss_ut_ics: 9.248e-03,, Time: 9.36\n",
      "It: 15400, Loss: 7.781e-05, Loss_res: 7.809e-06,  Loss_bcs: 2.113e-01, Loss_ut_ics: 8.714e-03,, Time: 12.50\n",
      "It: 15500, Loss: 7.759e-05, Loss_res: 2.207e-05,  Loss_bcs: 1.972e-01, Loss_ut_ics: 8.904e-03,, Time: 15.51\n",
      "It: 15600, Loss: 9.133e-05, Loss_res: 4.296e-05,  Loss_bcs: 2.235e-01, Loss_ut_ics: 6.432e-03,, Time: 18.62\n",
      "It: 15700, Loss: 8.937e-05, Loss_res: 6.487e-06,  Loss_bcs: 2.473e-01, Loss_ut_ics: 8.702e-03,, Time: 21.73\n",
      "It: 15800, Loss: 8.320e-05, Loss_res: 8.301e-06,  Loss_bcs: 2.288e-01, Loss_ut_ics: 7.705e-03,, Time: 24.79\n",
      "It: 15900, Loss: 1.379e-04, Loss_res: 1.895e-04,  Loss_bcs: 2.283e-01, Loss_ut_ics: 6.528e-03,, Time: 27.90\n",
      "It: 16000, Loss: 8.608e-05, Loss_res: 5.850e-05,  Loss_bcs: 1.943e-01, Loss_ut_ics: 5.928e-03,, Time: 30.92\n",
      "update_loss_res: 2.7922e-01\n",
      "update_loss_ics_u_t:2.9784e-01\n",
      "update_loss_bcs: 2.4229e+00\n",
      "adaptive_constant_res_val: 2.7922e-01\n",
      "adaptive_constant_ics_val:7.7582e-05\n",
      "adaptive_constant_bcs_val: 1.5888e-03\n",
      "max_grad_res: 1.244e-02\n",
      "mean_grad_ics: 2.605e-04\n",
      "mean_grad_bcs: 6.557e-04\n",
      "It: 16100, Loss: 3.795e-04, Loss_res: 1.710e-04,  Loss_bcs: 2.065e-01, Loss_ut_ics: 4.669e-02,, Time: 3.11\n",
      "It: 16200, Loss: 3.557e-04, Loss_res: 2.068e-05,  Loss_bcs: 2.188e-01, Loss_ut_ics: 2.940e-02,, Time: 6.31\n",
      "It: 16300, Loss: 3.999e-04, Loss_res: 3.144e-04,  Loss_bcs: 1.959e-01, Loss_ut_ics: 1.156e-02,, Time: 9.35\n",
      "It: 16400, Loss: 3.109e-04, Loss_res: 3.763e-05,  Loss_bcs: 1.881e-01, Loss_ut_ics: 1.898e-02,, Time: 12.47\n",
      "It: 16500, Loss: 3.454e-04, Loss_res: 7.145e-05,  Loss_bcs: 2.030e-01, Loss_ut_ics: 3.847e-02,, Time: 15.49\n",
      "It: 16600, Loss: 3.480e-04, Loss_res: 2.252e-04,  Loss_bcs: 1.780e-01, Loss_ut_ics: 2.916e-02,, Time: 18.60\n",
      "It: 16700, Loss: 3.202e-04, Loss_res: 7.928e-05,  Loss_bcs: 1.860e-01, Loss_ut_ics: 3.308e-02,, Time: 22.00\n",
      "It: 16800, Loss: 3.384e-04, Loss_res: 1.159e-04,  Loss_bcs: 1.910e-01, Loss_ut_ics: 3.241e-02,, Time: 25.38\n",
      "It: 16900, Loss: 3.716e-04, Loss_res: 2.257e-04,  Loss_bcs: 1.926e-01, Loss_ut_ics: 3.387e-02,, Time: 28.97\n",
      "It: 17000, Loss: 3.470e-04, Loss_res: 1.245e-04,  Loss_bcs: 1.951e-01, Loss_ut_ics: 2.993e-02,, Time: 32.41\n",
      "update_loss_res: 3.3191e-01\n",
      "update_loss_ics_u_t:4.5574e-01\n",
      "update_loss_bcs: 2.2124e+00\n",
      "adaptive_constant_res_val: 3.3191e-01\n",
      "adaptive_constant_ics_val:1.4762e-04\n",
      "adaptive_constant_bcs_val: 5.3171e-04\n",
      "max_grad_res: 4.935e-02\n",
      "mean_grad_ics: 3.239e-04\n",
      "mean_grad_bcs: 2.403e-04\n",
      "It: 17100, Loss: 1.000e-04, Loss_res: 6.375e-06,  Loss_bcs: 1.694e-01, Loss_ut_ics: 5.299e-02,, Time: 3.43\n",
      "It: 17200, Loss: 1.159e-04, Loss_res: 5.127e-06,  Loss_bcs: 2.083e-01, Loss_ut_ics: 2.354e-02,, Time: 7.15\n",
      "It: 17300, Loss: 1.091e-04, Loss_res: 3.357e-06,  Loss_bcs: 1.960e-01, Loss_ut_ics: 2.536e-02,, Time: 10.68\n",
      "It: 17400, Loss: 9.857e-05, Loss_res: 6.159e-06,  Loss_bcs: 1.766e-01, Loss_ut_ics: 1.768e-02,, Time: 14.34\n",
      "It: 17500, Loss: 1.100e-04, Loss_res: 5.871e-06,  Loss_bcs: 1.992e-01, Loss_ut_ics: 1.464e-02,, Time: 17.52\n",
      "It: 17600, Loss: 1.265e-04, Loss_res: 4.041e-06,  Loss_bcs: 2.308e-01, Loss_ut_ics: 1.614e-02,, Time: 20.65\n",
      "It: 17700, Loss: 1.104e-04, Loss_res: 4.145e-06,  Loss_bcs: 2.008e-01, Loss_ut_ics: 1.534e-02,, Time: 23.67\n",
      "It: 17800, Loss: 1.222e-04, Loss_res: 8.205e-06,  Loss_bcs: 2.202e-01, Loss_ut_ics: 1.610e-02,, Time: 27.03\n",
      "It: 17900, Loss: 1.089e-04, Loss_res: 1.475e-05,  Loss_bcs: 1.908e-01, Loss_ut_ics: 1.696e-02,, Time: 30.08\n",
      "It: 18000, Loss: 1.010e-04, Loss_res: 5.464e-06,  Loss_bcs: 1.819e-01, Loss_ut_ics: 1.631e-02,, Time: 33.49\n",
      "update_loss_res: 3.0306e-01\n",
      "update_loss_ics_u_t:3.5656e-01\n",
      "update_loss_bcs: 2.3404e+00\n",
      "adaptive_constant_res_val: 3.0306e-01\n",
      "adaptive_constant_ics_val:3.3734e-05\n",
      "adaptive_constant_bcs_val: 6.3783e-04\n",
      "max_grad_res: 4.300e-03\n",
      "mean_grad_ics: 9.461e-05\n",
      "mean_grad_bcs: 2.725e-04\n",
      "It: 18100, Loss: 1.314e-04, Loss_res: 8.530e-06,  Loss_bcs: 2.007e-01, Loss_ut_ics: 2.314e-02,, Time: 3.39\n",
      "It: 18200, Loss: 1.471e-04, Loss_res: 1.699e-05,  Loss_bcs: 2.215e-01, Loss_ut_ics: 2.046e-02,, Time: 6.59\n",
      "It: 18300, Loss: 1.184e-04, Loss_res: 8.796e-06,  Loss_bcs: 1.804e-01, Loss_ut_ics: 1.903e-02,, Time: 9.79\n",
      "It: 18400, Loss: 2.017e-04, Loss_res: 2.438e-04,  Loss_bcs: 1.991e-01, Loss_ut_ics: 2.504e-02,, Time: 12.93\n",
      "It: 18500, Loss: 1.461e-04, Loss_res: 4.242e-05,  Loss_bcs: 2.080e-01, Loss_ut_ics: 1.801e-02,, Time: 16.06\n",
      "It: 18600, Loss: 1.544e-04, Loss_res: 6.800e-05,  Loss_bcs: 2.084e-01, Loss_ut_ics: 2.789e-02,, Time: 19.35\n",
      "It: 18700, Loss: 1.277e-04, Loss_res: 1.007e-05,  Loss_bcs: 1.940e-01, Loss_ut_ics: 2.722e-02,, Time: 22.30\n",
      "It: 18800, Loss: 1.238e-04, Loss_res: 1.514e-05,  Loss_bcs: 1.855e-01, Loss_ut_ics: 2.556e-02,, Time: 25.33\n",
      "It: 18900, Loss: 1.326e-04, Loss_res: 2.072e-05,  Loss_bcs: 1.961e-01, Loss_ut_ics: 3.560e-02,, Time: 28.31\n",
      "It: 19000, Loss: 1.524e-04, Loss_res: 1.163e-04,  Loss_bcs: 1.815e-01, Loss_ut_ics: 4.134e-02,, Time: 31.32\n",
      "update_loss_res: 3.3452e-01\n",
      "update_loss_ics_u_t:4.5717e-01\n",
      "update_loss_bcs: 2.2083e+00\n",
      "adaptive_constant_res_val: 3.3452e-01\n",
      "adaptive_constant_ics_val:9.6726e-04\n",
      "adaptive_constant_bcs_val: 2.8085e-04\n",
      "max_grad_res: 5.966e-02\n",
      "mean_grad_ics: 2.116e-03\n",
      "mean_grad_bcs: 1.272e-04\n",
      "It: 19100, Loss: 6.315e-05, Loss_res: 7.922e-06,  Loss_bcs: 1.852e-01, Loss_ut_ics: 8.775e-03,, Time: 3.57\n",
      "It: 19200, Loss: 5.681e-05, Loss_res: 5.956e-06,  Loss_bcs: 1.756e-01, Loss_ut_ics: 5.691e-03,, Time: 6.78\n",
      "It: 19300, Loss: 6.849e-05, Loss_res: 2.825e-05,  Loss_bcs: 1.916e-01, Loss_ut_ics: 5.409e-03,, Time: 10.02\n",
      "It: 19400, Loss: 7.133e-05, Loss_res: 2.390e-05,  Loss_bcs: 2.101e-01, Loss_ut_ics: 4.476e-03,, Time: 13.41\n",
      "It: 19500, Loss: 5.466e-05, Loss_res: 1.263e-05,  Loss_bcs: 1.656e-01, Loss_ut_ics: 4.044e-03,, Time: 16.84\n",
      "It: 19600, Loss: 6.672e-05, Loss_res: 6.698e-06,  Loss_bcs: 2.154e-01, Loss_ut_ics: 4.130e-03,, Time: 20.24\n",
      "It: 19700, Loss: 8.135e-05, Loss_res: 6.776e-05,  Loss_bcs: 1.968e-01, Loss_ut_ics: 3.535e-03,, Time: 23.38\n",
      "It: 19800, Loss: 6.908e-05, Loss_res: 1.503e-05,  Loss_bcs: 2.162e-01, Loss_ut_ics: 3.433e-03,, Time: 26.42\n",
      "It: 19900, Loss: 7.994e-05, Loss_res: 6.302e-05,  Loss_bcs: 1.999e-01, Loss_ut_ics: 2.822e-03,, Time: 29.55\n",
      "It: 20000, Loss: 6.147e-05, Loss_res: 5.561e-06,  Loss_bcs: 2.041e-01, Loss_ut_ics: 2.367e-03,, Time: 32.61\n",
      "update_loss_res: 3.1337e-01\n",
      "update_loss_ics_u_t:3.2237e-01\n",
      "update_loss_bcs: 2.3643e+00\n",
      "adaptive_constant_res_val: 3.1337e-01\n",
      "adaptive_constant_ics_val:3.2715e-05\n",
      "adaptive_constant_bcs_val: 1.6639e-03\n",
      "max_grad_res: 2.416e-03\n",
      "mean_grad_ics: 1.015e-04\n",
      "mean_grad_bcs: 7.038e-04\n",
      "It: 20100, Loss: 3.570e-04, Loss_res: 1.217e-04,  Loss_bcs: 1.913e-01, Loss_ut_ics: 1.955e-02,, Time: 3.14\n",
      "It: 20200, Loss: 4.447e-04, Loss_res: 5.337e-04,  Loss_bcs: 1.661e-01, Loss_ut_ics: 3.366e-02,, Time: 6.25\n",
      "It: 20300, Loss: 3.712e-04, Loss_res: 1.112e-04,  Loss_bcs: 2.016e-01, Loss_ut_ics: 2.797e-02,, Time: 9.30\n",
      "It: 20400, Loss: 3.544e-04, Loss_res: 5.178e-06,  Loss_bcs: 2.113e-01, Loss_ut_ics: 3.417e-02,, Time: 12.45\n",
      "It: 20500, Loss: 3.113e-04, Loss_res: 3.251e-05,  Loss_bcs: 1.803e-01, Loss_ut_ics: 3.522e-02,, Time: 15.58\n",
      "It: 20600, Loss: 3.186e-04, Loss_res: 2.319e-05,  Loss_bcs: 1.862e-01, Loss_ut_ics: 4.569e-02,, Time: 19.24\n",
      "It: 20700, Loss: 3.620e-04, Loss_res: 1.128e-04,  Loss_bcs: 1.954e-01, Loss_ut_ics: 4.556e-02,, Time: 22.76\n",
      "It: 20800, Loss: 3.563e-04, Loss_res: 1.148e-04,  Loss_bcs: 1.916e-01, Loss_ut_ics: 4.645e-02,, Time: 26.35\n",
      "It: 20900, Loss: 3.429e-04, Loss_res: 2.758e-05,  Loss_bcs: 2.000e-01, Loss_ut_ics: 4.868e-02,, Time: 29.85\n",
      "It: 21000, Loss: 3.066e-04, Loss_res: 3.441e-05,  Loss_bcs: 1.766e-01, Loss_ut_ics: 6.109e-02,, Time: 33.54\n",
      "update_loss_res: 3.3852e-01\n",
      "update_loss_ics_u_t:5.7992e-01\n",
      "update_loss_bcs: 2.0816e+00\n",
      "adaptive_constant_res_val: 3.3852e-01\n",
      "adaptive_constant_ics_val:1.1944e-03\n",
      "adaptive_constant_bcs_val: 8.4459e-04\n",
      "max_grad_res: 2.113e-02\n",
      "mean_grad_ics: 2.060e-03\n",
      "mean_grad_bcs: 4.057e-04\n",
      "It: 21100, Loss: 1.679e-04, Loss_res: 7.064e-06,  Loss_bcs: 1.858e-01, Loss_ut_ics: 7.175e-03,, Time: 3.76\n",
      "It: 21200, Loss: 1.758e-04, Loss_res: 6.906e-06,  Loss_bcs: 1.955e-01, Loss_ut_ics: 6.972e-03,, Time: 7.19\n",
      "It: 21300, Loss: 1.639e-04, Loss_res: 6.973e-06,  Loss_bcs: 1.841e-01, Loss_ut_ics: 5.080e-03,, Time: 11.04\n",
      "It: 21400, Loss: 1.791e-04, Loss_res: 6.305e-06,  Loss_bcs: 2.040e-01, Loss_ut_ics: 3.908e-03,, Time: 14.22\n",
      "It: 21500, Loss: 1.472e-04, Loss_res: 6.444e-06,  Loss_bcs: 1.648e-01, Loss_ut_ics: 4.873e-03,, Time: 17.25\n",
      "It: 21600, Loss: 1.809e-04, Loss_res: 3.660e-05,  Loss_bcs: 1.938e-01, Loss_ut_ics: 4.026e-03,, Time: 20.47\n",
      "It: 21700, Loss: 2.072e-04, Loss_res: 8.951e-05,  Loss_bcs: 2.048e-01, Loss_ut_ics: 3.288e-03,, Time: 23.67\n",
      "It: 21800, Loss: 1.618e-04, Loss_res: 1.321e-05,  Loss_bcs: 1.813e-01, Loss_ut_ics: 3.502e-03,, Time: 26.96\n",
      "It: 21900, Loss: 1.645e-04, Loss_res: 1.859e-05,  Loss_bcs: 1.827e-01, Loss_ut_ics: 3.319e-03,, Time: 30.26\n",
      "It: 22000, Loss: 1.932e-04, Loss_res: 1.174e-04,  Loss_bcs: 1.776e-01, Loss_ut_ics: 2.963e-03,, Time: 33.33\n",
      "update_loss_res: 3.5212e-01\n",
      "update_loss_ics_u_t:3.6247e-01\n",
      "update_loss_bcs: 2.2854e+00\n",
      "adaptive_constant_res_val: 3.5212e-01\n",
      "adaptive_constant_ics_val:1.2634e-04\n",
      "adaptive_constant_bcs_val: 4.2423e-04\n",
      "max_grad_res: 5.662e-02\n",
      "mean_grad_ics: 3.485e-04\n",
      "mean_grad_bcs: 1.856e-04\n",
      "It: 22100, Loss: 9.540e-05, Loss_res: 3.515e-05,  Loss_bcs: 1.946e-01, Loss_ut_ics: 3.862e-03,, Time: 3.14\n",
      "It: 22200, Loss: 8.777e-05, Loss_res: 2.724e-05,  Loss_bcs: 1.832e-01, Loss_ut_ics: 3.748e-03,, Time: 6.21\n",
      "It: 22300, Loss: 8.725e-05, Loss_res: 5.450e-06,  Loss_bcs: 1.998e-01, Loss_ut_ics: 4.675e-03,, Time: 9.38\n",
      "It: 22400, Loss: 1.106e-04, Loss_res: 8.962e-05,  Loss_bcs: 1.853e-01, Loss_ut_ics: 3.843e-03,, Time: 12.50\n",
      "It: 22500, Loss: 8.948e-05, Loss_res: 2.448e-05,  Loss_bcs: 1.893e-01, Loss_ut_ics: 4.425e-03,, Time: 15.55\n",
      "It: 22600, Loss: 8.821e-05, Loss_res: 1.230e-05,  Loss_bcs: 1.963e-01, Loss_ut_ics: 4.650e-03,, Time: 18.65\n",
      "It: 22700, Loss: 1.294e-04, Loss_res: 1.355e-04,  Loss_bcs: 1.912e-01, Loss_ut_ics: 4.613e-03,, Time: 21.71\n",
      "It: 22800, Loss: 7.962e-05, Loss_res: 4.111e-05,  Loss_bcs: 1.520e-01, Loss_ut_ics: 5.147e-03,, Time: 25.00\n",
      "It: 22900, Loss: 1.313e-04, Loss_res: 1.146e-04,  Loss_bcs: 2.130e-01, Loss_ut_ics: 4.603e-03,, Time: 28.12\n",
      "It: 23000, Loss: 1.683e-04, Loss_res: 2.465e-04,  Loss_bcs: 1.908e-01, Loss_ut_ics: 4.731e-03,, Time: 31.18\n",
      "update_loss_res: 3.6416e-01\n",
      "update_loss_ics_u_t:3.8301e-01\n",
      "update_loss_bcs: 2.2528e+00\n",
      "adaptive_constant_res_val: 3.6416e-01\n",
      "adaptive_constant_ics_val:2.6006e-05\n",
      "adaptive_constant_bcs_val: 1.1205e-03\n",
      "max_grad_res: 8.330e-02\n",
      "mean_grad_ics: 6.790e-05\n",
      "mean_grad_bcs: 4.974e-04\n",
      "It: 23100, Loss: 1.580e-04, Loss_res: 5.045e-06,  Loss_bcs: 1.390e-01, Loss_ut_ics: 1.441e-02,, Time: 3.10\n",
      "It: 23200, Loss: 2.280e-04, Loss_res: 3.181e-05,  Loss_bcs: 1.927e-01, Loss_ut_ics: 1.515e-02,, Time: 6.08\n",
      "It: 23300, Loss: 1.765e-04, Loss_res: 1.565e-05,  Loss_bcs: 1.519e-01, Loss_ut_ics: 2.186e-02,, Time: 9.17\n",
      "It: 23400, Loss: 2.012e-04, Loss_res: 2.438e-05,  Loss_bcs: 1.709e-01, Loss_ut_ics: 3.111e-02,, Time: 12.25\n",
      "It: 23500, Loss: 2.283e-04, Loss_res: 4.166e-05,  Loss_bcs: 1.896e-01, Loss_ut_ics: 2.877e-02,, Time: 16.82\n",
      "It: 23600, Loss: 2.164e-04, Loss_res: 2.484e-05,  Loss_bcs: 1.843e-01, Loss_ut_ics: 3.516e-02,, Time: 19.95\n",
      "It: 23700, Loss: 2.404e-04, Loss_res: 1.546e-04,  Loss_bcs: 1.634e-01, Loss_ut_ics: 3.540e-02,, Time: 23.04\n",
      "It: 23800, Loss: 2.078e-04, Loss_res: 1.741e-05,  Loss_bcs: 1.789e-01, Loss_ut_ics: 3.834e-02,, Time: 26.04\n",
      "It: 23900, Loss: 2.188e-04, Loss_res: 6.361e-05,  Loss_bcs: 1.736e-01, Loss_ut_ics: 4.357e-02,, Time: 29.20\n",
      "It: 24000, Loss: 2.241e-04, Loss_res: 1.444e-05,  Loss_bcs: 1.940e-01, Loss_ut_ics: 5.606e-02,, Time: 32.21\n",
      "update_loss_res: 3.5835e-01\n",
      "update_loss_ics_u_t:5.6428e-01\n",
      "update_loss_bcs: 2.0774e+00\n",
      "adaptive_constant_res_val: 3.5835e-01\n",
      "adaptive_constant_ics_val:1.5733e-03\n",
      "adaptive_constant_bcs_val: 5.6445e-04\n",
      "max_grad_res: 2.672e-02\n",
      "mean_grad_ics: 2.788e-03\n",
      "mean_grad_bcs: 2.717e-04\n",
      "It: 24100, Loss: 1.256e-04, Loss_res: 1.651e-05,  Loss_bcs: 2.006e-01, Loss_ut_ics: 4.072e-03,, Time: 3.09\n",
      "It: 24200, Loss: 1.080e-04, Loss_res: 1.024e-05,  Loss_bcs: 1.769e-01, Loss_ut_ics: 2.895e-03,, Time: 6.17\n",
      "It: 24300, Loss: 1.098e-04, Loss_res: 7.647e-06,  Loss_bcs: 1.806e-01, Loss_ut_ics: 3.257e-03,, Time: 9.21\n",
      "It: 24400, Loss: 1.192e-04, Loss_res: 1.743e-05,  Loss_bcs: 1.909e-01, Loss_ut_ics: 3.322e-03,, Time: 12.35\n",
      "It: 24500, Loss: 1.115e-04, Loss_res: 1.120e-05,  Loss_bcs: 1.834e-01, Loss_ut_ics: 2.545e-03,, Time: 15.33\n",
      "It: 24600, Loss: 1.181e-04, Loss_res: 3.027e-05,  Loss_bcs: 1.814e-01, Loss_ut_ics: 3.099e-03,, Time: 18.43\n",
      "It: 24700, Loss: 1.266e-04, Loss_res: 4.771e-05,  Loss_bcs: 1.867e-01, Loss_ut_ics: 2.624e-03,, Time: 21.46\n",
      "It: 24800, Loss: 1.123e-04, Loss_res: 4.971e-06,  Loss_bcs: 1.891e-01, Loss_ut_ics: 2.397e-03,, Time: 24.49\n",
      "It: 24900, Loss: 1.076e-04, Loss_res: 7.911e-06,  Loss_bcs: 1.791e-01, Loss_ut_ics: 2.325e-03,, Time: 27.58\n",
      "It: 25000, Loss: 1.161e-04, Loss_res: 2.315e-05,  Loss_bcs: 1.816e-01, Loss_ut_ics: 3.376e-03,, Time: 30.60\n",
      "update_loss_res: 3.7007e-01\n",
      "update_loss_ics_u_t:3.7964e-01\n",
      "update_loss_bcs: 2.2503e+00\n",
      "adaptive_constant_res_val: 3.7007e-01\n",
      "adaptive_constant_ics_val:1.2753e-04\n",
      "adaptive_constant_bcs_val: 5.6840e-04\n",
      "max_grad_res: 1.725e-02\n",
      "mean_grad_ics: 3.359e-04\n",
      "mean_grad_bcs: 2.526e-04\n",
      "It: 25100, Loss: 1.059e-04, Loss_res: 2.302e-05,  Loss_bcs: 1.703e-01, Loss_ut_ics: 4.501e-03,, Time: 3.06\n",
      "It: 25200, Loss: 1.140e-04, Loss_res: 2.370e-05,  Loss_bcs: 1.843e-01, Loss_ut_ics: 3.606e-03,, Time: 6.10\n",
      "It: 25300, Loss: 9.329e-05, Loss_res: 2.946e-06,  Loss_bcs: 1.613e-01, Loss_ut_ics: 4.209e-03,, Time: 9.17\n",
      "It: 25400, Loss: 1.139e-04, Loss_res: 2.828e-05,  Loss_bcs: 1.808e-01, Loss_ut_ics: 5.049e-03,, Time: 12.25\n",
      "It: 25500, Loss: 1.014e-04, Loss_res: 9.568e-06,  Loss_bcs: 1.711e-01, Loss_ut_ics: 4.448e-03,, Time: 15.28\n",
      "It: 25600, Loss: 1.066e-04, Loss_res: 1.271e-05,  Loss_bcs: 1.778e-01, Loss_ut_ics: 6.448e-03,, Time: 18.36\n",
      "It: 25700, Loss: 1.018e-04, Loss_res: 3.787e-06,  Loss_bcs: 1.754e-01, Loss_ut_ics: 5.020e-03,, Time: 21.41\n",
      "It: 25800, Loss: 1.191e-04, Loss_res: 2.439e-05,  Loss_bcs: 1.925e-01, Loss_ut_ics: 5.090e-03,, Time: 24.49\n",
      "It: 25900, Loss: 1.041e-04, Loss_res: 1.048e-05,  Loss_bcs: 1.751e-01, Loss_ut_ics: 5.816e-03,, Time: 27.65\n",
      "It: 26000, Loss: 1.101e-04, Loss_res: 9.359e-06,  Loss_bcs: 1.865e-01, Loss_ut_ics: 5.039e-03,, Time: 30.77\n",
      "update_loss_res: 3.7528e-01\n",
      "update_loss_ics_u_t:3.9621e-01\n",
      "update_loss_bcs: 2.2285e+00\n",
      "adaptive_constant_res_val: 3.7528e-01\n",
      "adaptive_constant_ics_val:6.6434e-05\n",
      "adaptive_constant_bcs_val: 5.2229e-04\n",
      "max_grad_res: 1.291e-02\n",
      "mean_grad_ics: 1.677e-04\n",
      "mean_grad_bcs: 2.344e-04\n",
      "It: 26100, Loss: 9.626e-05, Loss_res: 1.474e-05,  Loss_bcs: 1.731e-01, Loss_ut_ics: 4.983e-03,, Time: 3.07\n",
      "It: 26200, Loss: 9.323e-05, Loss_res: 4.674e-06,  Loss_bcs: 1.742e-01, Loss_ut_ics: 7.204e-03,, Time: 6.05\n",
      "It: 26300, Loss: 8.429e-05, Loss_res: 8.727e-06,  Loss_bcs: 1.544e-01, Loss_ut_ics: 5.881e-03,, Time: 9.13\n",
      "It: 26400, Loss: 1.396e-04, Loss_res: 8.619e-05,  Loss_bcs: 2.046e-01, Loss_ut_ics: 5.908e-03,, Time: 12.26\n",
      "It: 26500, Loss: 1.042e-04, Loss_res: 4.166e-06,  Loss_bcs: 1.955e-01, Loss_ut_ics: 7.597e-03,, Time: 15.36\n",
      "It: 26600, Loss: 1.133e-04, Loss_res: 6.418e-05,  Loss_bcs: 1.698e-01, Loss_ut_ics: 8.368e-03,, Time: 18.51\n",
      "It: 26700, Loss: 1.039e-04, Loss_res: 5.253e-06,  Loss_bcs: 1.942e-01, Loss_ut_ics: 7.757e-03,, Time: 21.55\n",
      "It: 26800, Loss: 8.973e-05, Loss_res: 4.180e-06,  Loss_bcs: 1.677e-01, Loss_ut_ics: 8.616e-03,, Time: 24.66\n",
      "It: 26900, Loss: 1.013e-04, Loss_res: 1.710e-05,  Loss_bcs: 1.807e-01, Loss_ut_ics: 7.729e-03,, Time: 27.73\n",
      "It: 27000, Loss: 8.442e-05, Loss_res: 1.328e-05,  Loss_bcs: 1.511e-01, Loss_ut_ics: 7.591e-03,, Time: 30.77\n",
      "update_loss_res: 3.8166e-01\n",
      "update_loss_ics_u_t:4.1326e-01\n",
      "update_loss_bcs: 2.2051e+00\n",
      "adaptive_constant_res_val: 3.8166e-01\n",
      "adaptive_constant_ics_val:1.3311e-04\n",
      "adaptive_constant_bcs_val: 1.0656e-03\n",
      "max_grad_res: 1.423e-02\n",
      "mean_grad_ics: 3.221e-04\n",
      "mean_grad_bcs: 4.832e-04\n",
      "It: 27100, Loss: 2.178e-04, Loss_res: 5.634e-06,  Loss_bcs: 2.006e-01, Loss_ut_ics: 1.432e-02,, Time: 3.09\n",
      "It: 27200, Loss: 2.239e-04, Loss_res: 2.303e-05,  Loss_bcs: 2.010e-01, Loss_ut_ics: 7.524e-03,, Time: 6.09\n",
      "It: 27300, Loss: 1.824e-04, Loss_res: 2.189e-05,  Loss_bcs: 1.618e-01, Loss_ut_ics: 1.240e-02,, Time: 9.19\n",
      "It: 27400, Loss: 1.903e-04, Loss_res: 9.017e-06,  Loss_bcs: 1.740e-01, Loss_ut_ics: 1.078e-02,, Time: 12.27\n",
      "It: 27500, Loss: 2.080e-04, Loss_res: 8.193e-06,  Loss_bcs: 1.912e-01, Loss_ut_ics: 8.222e-03,, Time: 15.34\n",
      "It: 27600, Loss: 1.928e-04, Loss_res: 1.979e-05,  Loss_bcs: 1.727e-01, Loss_ut_ics: 9.375e-03,, Time: 18.41\n",
      "It: 27700, Loss: 1.911e-04, Loss_res: 4.228e-05,  Loss_bcs: 1.629e-01, Loss_ut_ics: 1.021e-02,, Time: 21.46\n",
      "It: 27800, Loss: 1.639e-04, Loss_res: 1.125e-05,  Loss_bcs: 1.486e-01, Loss_ut_ics: 9.648e-03,, Time: 24.58\n",
      "It: 27900, Loss: 1.932e-04, Loss_res: 5.139e-06,  Loss_bcs: 1.786e-01, Loss_ut_ics: 6.854e-03,, Time: 27.61\n",
      "It: 28000, Loss: 1.753e-04, Loss_res: 5.313e-06,  Loss_bcs: 1.613e-01, Loss_ut_ics: 1.006e-02,, Time: 30.79\n",
      "update_loss_res: 3.8108e-01\n",
      "update_loss_ics_u_t:4.2090e-01\n",
      "update_loss_bcs: 2.1980e+00\n",
      "adaptive_constant_res_val: 3.8108e-01\n",
      "adaptive_constant_ics_val:2.4879e-04\n",
      "adaptive_constant_bcs_val: 1.1534e-03\n",
      "max_grad_res: 4.354e-03\n",
      "mean_grad_ics: 5.911e-04\n",
      "mean_grad_bcs: 5.248e-04\n",
      "It: 28100, Loss: 2.272e-04, Loss_res: 7.880e-05,  Loss_bcs: 1.686e-01, Loss_ut_ics: 1.107e-02,, Time: 3.26\n",
      "It: 28200, Loss: 1.796e-04, Loss_res: 7.032e-06,  Loss_bcs: 1.514e-01, Loss_ut_ics: 9.455e-03,, Time: 6.35\n",
      "It: 28300, Loss: 1.817e-04, Loss_res: 1.595e-05,  Loss_bcs: 1.507e-01, Loss_ut_ics: 7.074e-03,, Time: 9.45\n",
      "It: 28400, Loss: 1.755e-04, Loss_res: 7.793e-06,  Loss_bcs: 1.481e-01, Loss_ut_ics: 6.862e-03,, Time: 12.47\n",
      "It: 28500, Loss: 1.907e-04, Loss_res: 3.314e-06,  Loss_bcs: 1.629e-01, Loss_ut_ics: 6.229e-03,, Time: 15.78\n",
      "It: 28600, Loss: 2.036e-04, Loss_res: 5.391e-06,  Loss_bcs: 1.733e-01, Loss_ut_ics: 6.614e-03,, Time: 19.04\n",
      "It: 28700, Loss: 2.092e-04, Loss_res: 2.263e-05,  Loss_bcs: 1.728e-01, Loss_ut_ics: 5.467e-03,, Time: 22.19\n",
      "It: 28800, Loss: 1.894e-04, Loss_res: 8.729e-06,  Loss_bcs: 1.598e-01, Loss_ut_ics: 6.857e-03,, Time: 25.66\n",
      "It: 28900, Loss: 2.043e-04, Loss_res: 6.696e-06,  Loss_bcs: 1.736e-01, Loss_ut_ics: 5.872e-03,, Time: 29.08\n",
      "It: 29000, Loss: 1.964e-04, Loss_res: 5.199e-06,  Loss_bcs: 1.668e-01, Loss_ut_ics: 8.063e-03,, Time: 32.15\n",
      "update_loss_res: 3.9435e-01\n",
      "update_loss_ics_u_t:4.2008e-01\n",
      "update_loss_bcs: 2.1856e+00\n",
      "adaptive_constant_res_val: 3.9435e-01\n",
      "adaptive_constant_ics_val:2.2469e-04\n",
      "adaptive_constant_bcs_val: 8.8527e-04\n",
      "max_grad_res: 6.960e-03\n",
      "mean_grad_ics: 5.349e-04\n",
      "mean_grad_bcs: 4.051e-04\n",
      "It: 29100, Loss: 1.593e-04, Loss_res: 1.708e-05,  Loss_bcs: 1.709e-01, Loss_ut_ics: 5.565e-03,, Time: 3.15\n",
      "It: 29200, Loss: 1.634e-04, Loss_res: 2.951e-05,  Loss_bcs: 1.697e-01, Loss_ut_ics: 6.755e-03,, Time: 6.25\n",
      "It: 29300, Loss: 1.429e-04, Loss_res: 2.010e-05,  Loss_bcs: 1.511e-01, Loss_ut_ics: 5.093e-03,, Time: 9.39\n",
      "It: 29400, Loss: 1.584e-04, Loss_res: 1.035e-05,  Loss_bcs: 1.731e-01, Loss_ut_ics: 4.839e-03,, Time: 12.47\n",
      "It: 29500, Loss: 1.611e-04, Loss_res: 1.143e-05,  Loss_bcs: 1.754e-01, Loss_ut_ics: 5.833e-03,, Time: 15.52\n",
      "It: 29600, Loss: 1.802e-04, Loss_res: 2.661e-05,  Loss_bcs: 1.901e-01, Loss_ut_ics: 6.158e-03,, Time: 18.59\n",
      "It: 29700, Loss: 1.560e-04, Loss_res: 9.313e-06,  Loss_bcs: 1.704e-01, Loss_ut_ics: 6.388e-03,, Time: 21.64\n",
      "It: 29800, Loss: 1.558e-04, Loss_res: 9.629e-06,  Loss_bcs: 1.705e-01, Loss_ut_ics: 4.782e-03,, Time: 24.99\n",
      "It: 29900, Loss: 1.645e-04, Loss_res: 1.453e-05,  Loss_bcs: 1.776e-01, Loss_ut_ics: 6.645e-03,, Time: 28.73\n",
      "It: 30000, Loss: 1.528e-04, Loss_res: 2.786e-05,  Loss_bcs: 1.590e-01, Loss_ut_ics: 4.604e-03,, Time: 31.88\n",
      "update_loss_res: 4.0055e-01\n",
      "update_loss_ics_u_t:4.2397e-01\n",
      "update_loss_bcs: 2.1755e+00\n",
      "adaptive_constant_res_val: 4.0055e-01\n",
      "adaptive_constant_ics_val:1.1130e-04\n",
      "adaptive_constant_bcs_val: 1.2700e-03\n",
      "max_grad_res: 3.854e-02\n",
      "mean_grad_ics: 2.625e-04\n",
      "mean_grad_bcs: 5.838e-04\n",
      "It: 30100, Loss: 1.959e-04, Loss_res: 7.477e-06,  Loss_bcs: 1.511e-01, Loss_ut_ics: 9.298e-03,, Time: 3.29\n",
      "It: 30200, Loss: 2.057e-04, Loss_res: 1.316e-05,  Loss_bcs: 1.568e-01, Loss_ut_ics: 1.120e-02,, Time: 6.54\n",
      "It: 30300, Loss: 2.247e-04, Loss_res: 1.484e-05,  Loss_bcs: 1.710e-01, Loss_ut_ics: 1.445e-02,, Time: 9.56\n",
      "It: 30400, Loss: 2.051e-04, Loss_res: 6.047e-05,  Loss_bcs: 1.414e-01, Loss_ut_ics: 1.148e-02,, Time: 12.73\n",
      "It: 30500, Loss: 2.216e-04, Loss_res: 3.562e-06,  Loss_bcs: 1.722e-01, Loss_ut_ics: 1.361e-02,, Time: 15.72\n",
      "It: 30600, Loss: 2.356e-04, Loss_res: 4.560e-05,  Loss_bcs: 1.703e-01, Loss_ut_ics: 9.991e-03,, Time: 18.78\n",
      "It: 30700, Loss: 2.126e-04, Loss_res: 1.982e-05,  Loss_bcs: 1.602e-01, Loss_ut_ics: 1.028e-02,, Time: 21.84\n",
      "It: 30800, Loss: 2.224e-04, Loss_res: 3.361e-06,  Loss_bcs: 1.731e-01, Loss_ut_ics: 9.996e-03,, Time: 24.85\n",
      "It: 30900, Loss: 2.188e-04, Loss_res: 3.282e-05,  Loss_bcs: 1.610e-01, Loss_ut_ics: 1.110e-02,, Time: 27.94\n",
      "It: 31000, Loss: 2.296e-04, Loss_res: 8.642e-06,  Loss_bcs: 1.770e-01, Loss_ut_ics: 1.209e-02,, Time: 31.31\n",
      "update_loss_res: 4.0212e-01\n",
      "update_loss_ics_u_t:4.5756e-01\n",
      "update_loss_bcs: 2.1403e+00\n",
      "adaptive_constant_res_val: 4.0212e-01\n",
      "adaptive_constant_ics_val:1.0478e-04\n",
      "adaptive_constant_bcs_val: 1.0302e-03\n",
      "max_grad_res: 2.037e-02\n",
      "mean_grad_ics: 2.290e-04\n",
      "mean_grad_bcs: 4.814e-04\n",
      "It: 31100, Loss: 2.137e-04, Loss_res: 1.321e-04,  Loss_bcs: 1.542e-01, Loss_ut_ics: 1.662e-02,, Time: 3.57\n",
      "It: 31200, Loss: 1.699e-04, Loss_res: 3.585e-06,  Loss_bcs: 1.625e-01, Loss_ut_ics: 9.527e-03,, Time: 7.60\n",
      "It: 31300, Loss: 1.891e-04, Loss_res: 9.695e-06,  Loss_bcs: 1.787e-01, Loss_ut_ics: 1.085e-02,, Time: 11.64\n",
      "It: 31400, Loss: 1.656e-04, Loss_res: 5.991e-06,  Loss_bcs: 1.574e-01, Loss_ut_ics: 9.902e-03,, Time: 15.70\n",
      "It: 31500, Loss: 1.701e-04, Loss_res: 9.845e-06,  Loss_bcs: 1.604e-01, Loss_ut_ics: 7.992e-03,, Time: 19.74\n",
      "It: 31600, Loss: 1.813e-04, Loss_res: 1.878e-05,  Loss_bcs: 1.677e-01, Loss_ut_ics: 9.816e-03,, Time: 23.73\n",
      "It: 31700, Loss: 1.918e-04, Loss_res: 8.613e-06,  Loss_bcs: 1.817e-01, Loss_ut_ics: 1.020e-02,, Time: 27.46\n",
      "It: 31800, Loss: 1.906e-04, Loss_res: 1.468e-05,  Loss_bcs: 1.781e-01, Loss_ut_ics: 1.144e-02,, Time: 32.39\n",
      "It: 31900, Loss: 2.165e-04, Loss_res: 1.451e-05,  Loss_bcs: 2.034e-01, Loss_ut_ics: 1.111e-02,, Time: 37.99\n",
      "It: 32000, Loss: 1.808e-04, Loss_res: 4.710e-06,  Loss_bcs: 1.722e-01, Loss_ut_ics: 1.428e-02,, Time: 41.91\n",
      "update_loss_res: 4.1182e-01\n",
      "update_loss_ics_u_t:4.6057e-01\n",
      "update_loss_bcs: 2.1276e+00\n",
      "adaptive_constant_res_val: 4.1182e-01\n",
      "adaptive_constant_ics_val:6.8085e-04\n",
      "adaptive_constant_bcs_val: 1.2230e-03\n",
      "max_grad_res: 9.082e-03\n",
      "mean_grad_ics: 1.478e-03\n",
      "mean_grad_bcs: 5.748e-04\n",
      "It: 32100, Loss: 2.012e-04, Loss_res: 1.873e-05,  Loss_bcs: 1.545e-01, Loss_ut_ics: 6.599e-03,, Time: 3.76\n",
      "It: 32200, Loss: 1.872e-04, Loss_res: 6.499e-06,  Loss_bcs: 1.478e-01, Loss_ut_ics: 5.542e-03,, Time: 9.73\n",
      "It: 32300, Loss: 1.974e-04, Loss_res: 2.572e-06,  Loss_bcs: 1.580e-01, Loss_ut_ics: 4.573e-03,, Time: 17.08\n",
      "It: 32400, Loss: 2.282e-04, Loss_res: 1.533e-05,  Loss_bcs: 1.795e-01, Loss_ut_ics: 3.558e-03,, Time: 24.46\n",
      "It: 32500, Loss: 2.075e-04, Loss_res: 2.025e-05,  Loss_bcs: 1.599e-01, Loss_ut_ics: 5.385e-03,, Time: 32.10\n",
      "It: 32600, Loss: 2.084e-04, Loss_res: 1.103e-05,  Loss_bcs: 1.639e-01, Loss_ut_ics: 4.961e-03,, Time: 39.61\n",
      "It: 32700, Loss: 2.130e-04, Loss_res: 2.298e-05,  Loss_bcs: 1.629e-01, Loss_ut_ics: 6.222e-03,, Time: 47.83\n",
      "It: 32800, Loss: 1.955e-04, Loss_res: 7.744e-06,  Loss_bcs: 1.537e-01, Loss_ut_ics: 6.267e-03,, Time: 55.49\n",
      "It: 32900, Loss: 1.948e-04, Loss_res: 1.059e-05,  Loss_bcs: 1.527e-01, Loss_ut_ics: 5.417e-03,, Time: 62.82\n",
      "It: 33000, Loss: 2.084e-04, Loss_res: 4.562e-06,  Loss_bcs: 1.659e-01, Loss_ut_ics: 5.243e-03,, Time: 68.26\n",
      "update_loss_res: 4.1124e-01\n",
      "update_loss_ics_u_t:4.3200e-01\n",
      "update_loss_bcs: 2.1568e+00\n",
      "adaptive_constant_res_val: 4.1124e-01\n",
      "adaptive_constant_ics_val:2.3969e-04\n",
      "adaptive_constant_bcs_val: 9.3460e-04\n",
      "max_grad_res: 3.796e-03\n",
      "mean_grad_ics: 5.548e-04\n",
      "mean_grad_bcs: 4.333e-04\n",
      "It: 33100, Loss: 1.451e-04, Loss_res: 1.569e-05,  Loss_bcs: 1.462e-01, Loss_ut_ics: 8.281e-03,, Time: 4.10\n",
      "It: 33200, Loss: 1.523e-04, Loss_res: 1.146e-05,  Loss_bcs: 1.565e-01, Loss_ut_ics: 5.707e-03,, Time: 10.43\n",
      "It: 33300, Loss: 1.350e-04, Loss_res: 1.402e-05,  Loss_bcs: 1.365e-01, Loss_ut_ics: 6.977e-03,, Time: 17.71\n",
      "It: 33400, Loss: 1.730e-04, Loss_res: 2.610e-05,  Loss_bcs: 1.723e-01, Loss_ut_ics: 5.188e-03,, Time: 25.01\n",
      "It: 33500, Loss: 1.459e-04, Loss_res: 2.717e-06,  Loss_bcs: 1.533e-01, Loss_ut_ics: 6.389e-03,, Time: 32.32\n",
      "It: 33600, Loss: 1.673e-04, Loss_res: 5.556e-06,  Loss_bcs: 1.753e-01, Loss_ut_ics: 5.050e-03,, Time: 39.74\n",
      "It: 33700, Loss: 1.586e-04, Loss_res: 1.056e-05,  Loss_bcs: 1.628e-01, Loss_ut_ics: 8.638e-03,, Time: 46.98\n",
      "It: 33800, Loss: 1.801e-04, Loss_res: 2.350e-05,  Loss_bcs: 1.810e-01, Loss_ut_ics: 5.201e-03,, Time: 53.95\n",
      "It: 33900, Loss: 1.480e-04, Loss_res: 2.975e-06,  Loss_bcs: 1.552e-01, Loss_ut_ics: 6.952e-03,, Time: 60.87\n",
      "It: 34000, Loss: 1.495e-04, Loss_res: 2.475e-06,  Loss_bcs: 1.577e-01, Loss_ut_ics: 4.551e-03,, Time: 67.89\n",
      "update_loss_res: 4.1808e-01\n",
      "update_loss_ics_u_t:4.4383e-01\n",
      "update_loss_bcs: 2.1381e+00\n",
      "adaptive_constant_res_val: 4.1808e-01\n",
      "adaptive_constant_ics_val:8.1998e-05\n",
      "adaptive_constant_bcs_val: 1.0521e-03\n",
      "max_grad_res: 4.757e-03\n",
      "mean_grad_ics: 1.847e-04\n",
      "mean_grad_bcs: 4.921e-04\n",
      "It: 34100, Loss: 1.640e-04, Loss_res: 3.542e-06,  Loss_bcs: 1.536e-01, Loss_ut_ics: 1.085e-02,, Time: 7.53\n",
      "It: 34200, Loss: 1.737e-04, Loss_res: 9.154e-06,  Loss_bcs: 1.608e-01, Loss_ut_ics: 8.011e-03,, Time: 15.15\n",
      "It: 34300, Loss: 1.812e-04, Loss_res: 1.423e-05,  Loss_bcs: 1.656e-01, Loss_ut_ics: 1.280e-02,, Time: 22.81\n",
      "It: 34400, Loss: 1.954e-04, Loss_res: 3.660e-06,  Loss_bcs: 1.836e-01, Loss_ut_ics: 8.885e-03,, Time: 30.22\n",
      "It: 34500, Loss: 1.752e-04, Loss_res: 3.492e-05,  Loss_bcs: 1.520e-01, Loss_ut_ics: 8.125e-03,, Time: 36.02\n",
      "It: 34600, Loss: 1.844e-04, Loss_res: 4.248e-06,  Loss_bcs: 1.727e-01, Loss_ut_ics: 1.234e-02,, Time: 39.60\n",
      "It: 34700, Loss: 1.601e-04, Loss_res: 7.390e-06,  Loss_bcs: 1.483e-01, Loss_ut_ics: 1.208e-02,, Time: 43.91\n",
      "It: 34800, Loss: 1.927e-04, Loss_res: 1.241e-05,  Loss_bcs: 1.772e-01, Loss_ut_ics: 1.339e-02,, Time: 47.91\n",
      "It: 34900, Loss: 1.685e-04, Loss_res: 7.117e-06,  Loss_bcs: 1.563e-01, Loss_ut_ics: 1.238e-02,, Time: 51.98\n",
      "It: 35000, Loss: 1.778e-04, Loss_res: 2.419e-06,  Loss_bcs: 1.674e-01, Loss_ut_ics: 9.134e-03,, Time: 55.96\n",
      "update_loss_res: 4.1623e-01\n",
      "update_loss_ics_u_t:4.6907e-01\n",
      "update_loss_bcs: 2.1147e+00\n",
      "adaptive_constant_res_val: 4.1623e-01\n",
      "adaptive_constant_ics_val:3.7081e-04\n",
      "adaptive_constant_bcs_val: 3.9933e-04\n",
      "max_grad_res: 2.651e-03\n",
      "mean_grad_ics: 7.905e-04\n",
      "mean_grad_bcs: 1.888e-04\n",
      "It: 35100, Loss: 7.301e-05, Loss_res: 3.218e-06,  Loss_bcs: 1.747e-01, Loss_ut_ics: 5.180e-03,, Time: 3.85\n",
      "It: 35200, Loss: 7.654e-05, Loss_res: 1.343e-05,  Loss_bcs: 1.721e-01, Loss_ut_ics: 6.018e-03,, Time: 13.45\n",
      "It: 35300, Loss: 6.712e-05, Loss_res: 3.774e-06,  Loss_bcs: 1.596e-01, Loss_ut_ics: 4.850e-03,, Time: 19.48\n",
      "It: 35400, Loss: 7.207e-05, Loss_res: 4.687e-06,  Loss_bcs: 1.708e-01, Loss_ut_ics: 5.140e-03,, Time: 22.59\n",
      "It: 35500, Loss: 6.661e-05, Loss_res: 5.587e-06,  Loss_bcs: 1.560e-01, Loss_ut_ics: 5.326e-03,, Time: 25.65\n",
      "It: 35600, Loss: 7.535e-05, Loss_res: 6.070e-06,  Loss_bcs: 1.779e-01, Loss_ut_ics: 4.805e-03,, Time: 28.72\n",
      "It: 35700, Loss: 6.462e-05, Loss_res: 3.311e-06,  Loss_bcs: 1.523e-01, Loss_ut_ics: 6.490e-03,, Time: 31.76\n",
      "It: 35800, Loss: 6.837e-05, Loss_res: 1.053e-05,  Loss_bcs: 1.557e-01, Loss_ut_ics: 4.934e-03,, Time: 34.84\n",
      "It: 35900, Loss: 7.439e-05, Loss_res: 4.956e-06,  Loss_bcs: 1.767e-01, Loss_ut_ics: 4.730e-03,, Time: 37.89\n",
      "It: 36000, Loss: 6.804e-05, Loss_res: 4.680e-06,  Loss_bcs: 1.611e-01, Loss_ut_ics: 4.726e-03,, Time: 40.98\n",
      "update_loss_res: 4.2769e-01\n",
      "update_loss_ics_u_t:4.5070e-01\n",
      "update_loss_bcs: 2.1216e+00\n",
      "adaptive_constant_res_val: 4.2769e-01\n",
      "adaptive_constant_ics_val:3.6510e-05\n",
      "adaptive_constant_bcs_val: 8.1224e-04\n",
      "max_grad_res: 1.415e-02\n",
      "mean_grad_ics: 8.101e-05\n",
      "mean_grad_bcs: 3.828e-04\n",
      "It: 36100, Loss: 1.301e-04, Loss_res: 3.246e-06,  Loss_bcs: 1.581e-01, Loss_ut_ics: 9.273e-03,, Time: 3.70\n",
      "It: 36200, Loss: 1.455e-04, Loss_res: 1.447e-05,  Loss_bcs: 1.711e-01, Loss_ut_ics: 7.952e-03,, Time: 12.14\n",
      "It: 36300, Loss: 1.434e-04, Loss_res: 3.797e-06,  Loss_bcs: 1.741e-01, Loss_ut_ics: 9.512e-03,, Time: 20.58\n",
      "It: 36400, Loss: 1.152e-04, Loss_res: 3.522e-06,  Loss_bcs: 1.392e-01, Loss_ut_ics: 1.808e-02,, Time: 26.53\n",
      "It: 36500, Loss: 1.272e-04, Loss_res: 4.269e-06,  Loss_bcs: 1.538e-01, Loss_ut_ics: 1.167e-02,, Time: 32.73\n",
      "It: 36600, Loss: 1.296e-04, Loss_res: 4.097e-06,  Loss_bcs: 1.566e-01, Loss_ut_ics: 1.647e-02,, Time: 36.18\n",
      "It: 36700, Loss: 1.747e-04, Loss_res: 8.079e-06,  Loss_bcs: 2.100e-01, Loss_ut_ics: 1.831e-02,, Time: 39.54\n",
      "It: 36800, Loss: 1.196e-04, Loss_res: 1.772e-06,  Loss_bcs: 1.455e-01, Loss_ut_ics: 1.822e-02,, Time: 42.90\n",
      "It: 36900, Loss: 1.380e-04, Loss_res: 3.120e-06,  Loss_bcs: 1.673e-01, Loss_ut_ics: 2.200e-02,, Time: 46.34\n",
      "It: 37000, Loss: 1.426e-04, Loss_res: 2.871e-05,  Loss_bcs: 1.594e-01, Loss_ut_ics: 2.331e-02,, Time: 49.69\n",
      "update_loss_res: 4.1876e-01\n",
      "update_loss_ics_u_t:5.0619e-01\n",
      "update_loss_bcs: 2.0751e+00\n",
      "adaptive_constant_res_val: 4.1876e-01\n",
      "adaptive_constant_ics_val:1.2011e-03\n",
      "adaptive_constant_bcs_val: 2.3684e-04\n",
      "max_grad_res: 3.164e-02\n",
      "mean_grad_ics: 2.373e-03\n",
      "mean_grad_bcs: 1.141e-04\n",
      "It: 37100, Loss: 4.890e-05, Loss_res: 4.261e-06,  Loss_bcs: 1.761e-01, Loss_ut_ics: 4.504e-03,, Time: 6.04\n",
      "It: 37200, Loss: 4.261e-05, Loss_res: 4.206e-06,  Loss_bcs: 1.442e-01, Loss_ut_ics: 5.584e-03,, Time: 9.40\n",
      "It: 37300, Loss: 4.522e-05, Loss_res: 1.716e-06,  Loss_bcs: 1.638e-01, Loss_ut_ics: 4.747e-03,, Time: 12.81\n",
      "It: 37400, Loss: 4.654e-05, Loss_res: 1.611e-06,  Loss_bcs: 1.622e-01, Loss_ut_ics: 6.208e-03,, Time: 16.15\n",
      "It: 37500, Loss: 4.664e-05, Loss_res: 2.359e-06,  Loss_bcs: 1.676e-01, Loss_ut_ics: 4.953e-03,, Time: 19.62\n",
      "It: 37600, Loss: 4.603e-05, Loss_res: 4.358e-06,  Loss_bcs: 1.642e-01, Loss_ut_ics: 4.435e-03,, Time: 22.99\n",
      "It: 37700, Loss: 4.913e-05, Loss_res: 1.814e-06,  Loss_bcs: 1.805e-01, Loss_ut_ics: 4.690e-03,, Time: 26.38\n",
      "It: 37800, Loss: 4.842e-05, Loss_res: 5.550e-06,  Loss_bcs: 1.748e-01, Loss_ut_ics: 3.918e-03,, Time: 29.72\n",
      "It: 37900, Loss: 4.448e-05, Loss_res: 1.793e-06,  Loss_bcs: 1.599e-01, Loss_ut_ics: 4.883e-03,, Time: 33.07\n",
      "It: 38000, Loss: 4.558e-05, Loss_res: 2.024e-06,  Loss_bcs: 1.653e-01, Loss_ut_ics: 4.654e-03,, Time: 36.49\n",
      "update_loss_res: 4.2299e-01\n",
      "update_loss_ics_u_t:4.4362e-01\n",
      "update_loss_bcs: 2.1334e+00\n",
      "adaptive_constant_res_val: 4.2299e-01\n",
      "adaptive_constant_ics_val:6.4745e-06\n",
      "adaptive_constant_bcs_val: 6.6406e-04\n",
      "max_grad_res: 4.525e-03\n",
      "mean_grad_ics: 1.459e-05\n",
      "mean_grad_bcs: 3.113e-04\n",
      "It: 38100, Loss: 1.216e-04, Loss_res: 3.854e-06,  Loss_bcs: 1.806e-01, Loss_ut_ics: 8.808e-03,, Time: 3.31\n",
      "It: 38200, Loss: 1.104e-04, Loss_res: 2.239e-06,  Loss_bcs: 1.647e-01, Loss_ut_ics: 1.029e-02,, Time: 6.72\n",
      "It: 38300, Loss: 1.107e-04, Loss_res: 2.773e-06,  Loss_bcs: 1.648e-01, Loss_ut_ics: 1.353e-02,, Time: 10.07\n",
      "It: 38400, Loss: 1.084e-04, Loss_res: 2.450e-06,  Loss_bcs: 1.615e-01, Loss_ut_ics: 1.895e-02,, Time: 13.39\n",
      "It: 38500, Loss: 1.238e-04, Loss_res: 3.254e-05,  Loss_bcs: 1.655e-01, Loss_ut_ics: 2.129e-02,, Time: 16.76\n",
      "It: 38600, Loss: 1.169e-04, Loss_res: 4.393e-06,  Loss_bcs: 1.730e-01, Loss_ut_ics: 2.760e-02,, Time: 20.08\n",
      "It: 38700, Loss: 9.745e-05, Loss_res: 2.341e-06,  Loss_bcs: 1.450e-01, Loss_ut_ics: 2.508e-02,, Time: 23.46\n",
      "It: 38800, Loss: 1.040e-04, Loss_res: 3.401e-06,  Loss_bcs: 1.542e-01, Loss_ut_ics: 2.968e-02,, Time: 26.80\n",
      "It: 38900, Loss: 1.131e-04, Loss_res: 2.937e-06,  Loss_bcs: 1.679e-01, Loss_ut_ics: 5.333e-02,, Time: 30.19\n",
      "It: 39000, Loss: 9.528e-05, Loss_res: 1.945e-06,  Loss_bcs: 1.419e-01, Loss_ut_ics: 3.617e-02,, Time: 33.48\n",
      "update_loss_res: 4.0722e-01\n",
      "update_loss_ics_u_t:6.2529e-01\n",
      "update_loss_bcs: 1.9675e+00\n",
      "adaptive_constant_res_val: 4.0722e-01\n",
      "adaptive_constant_ics_val:1.9500e-03\n",
      "adaptive_constant_bcs_val: 1.0507e-03\n",
      "max_grad_res: 2.712e-03\n",
      "mean_grad_ics: 3.119e-03\n",
      "mean_grad_bcs: 5.340e-04\n",
      "It: 39100, Loss: 2.029e-04, Loss_res: 1.153e-05,  Loss_bcs: 1.798e-01, Loss_ut_ics: 4.730e-03,, Time: 3.35\n",
      "It: 39200, Loss: 1.874e-04, Loss_res: 4.334e-06,  Loss_bcs: 1.682e-01, Loss_ut_ics: 4.530e-03,, Time: 6.71\n",
      "It: 39300, Loss: 2.031e-04, Loss_res: 1.767e-06,  Loss_bcs: 1.845e-01, Loss_ut_ics: 4.371e-03,, Time: 10.04\n",
      "It: 39400, Loss: 1.829e-04, Loss_res: 3.861e-06,  Loss_bcs: 1.593e-01, Loss_ut_ics: 7.158e-03,, Time: 13.50\n",
      "It: 39500, Loss: 1.855e-04, Loss_res: 2.142e-06,  Loss_bcs: 1.668e-01, Loss_ut_ics: 4.814e-03,, Time: 17.07\n",
      "It: 39600, Loss: 1.818e-04, Loss_res: 4.800e-06,  Loss_bcs: 1.633e-01, Loss_ut_ics: 4.219e-03,, Time: 20.38\n",
      "It: 39700, Loss: 1.648e-04, Loss_res: 3.757e-06,  Loss_bcs: 1.448e-01, Loss_ut_ics: 5.675e-03,, Time: 23.72\n",
      "It: 39800, Loss: 1.854e-04, Loss_res: 7.200e-06,  Loss_bcs: 1.661e-01, Loss_ut_ics: 4.074e-03,, Time: 27.08\n",
      "It: 39900, Loss: 1.870e-04, Loss_res: 2.372e-06,  Loss_bcs: 1.659e-01, Loss_ut_ics: 5.977e-03,, Time: 30.49\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000, Loss: 1.704e-04, Loss_res: 5.112e-06,  Loss_bcs: 1.501e-01, Loss_ut_ics: 5.462e-03,, Time: 38.30\n",
      "update_loss_res: 4.3021e-01\n",
      "update_loss_ics_u_t:4.5212e-01\n",
      "update_loss_bcs: 2.1177e+00\n",
      "adaptive_constant_res_val: 4.3021e-01\n",
      "adaptive_constant_ics_val:6.5899e-05\n",
      "adaptive_constant_bcs_val: 1.1100e-03\n",
      "max_grad_res: 1.627e-02\n",
      "mean_grad_ics: 1.458e-04\n",
      "mean_grad_bcs: 5.242e-04\n",
      "Relative L2 error_u: 6.74e-01\n",
      "Relative L2 error_r: 4.05e-03\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-24-2023_11-08-18-279267_M2\n",
      "Final loss total loss: 1.714497e-04\n",
      "Final loss loss_res: 6.719582e-06\n",
      "Final loss loss_bcs: 1.504912e-01\n",
      "Final loss loss_bc1: 2.609153e-02\n",
      "Final loss loss_bc2: 8.996237e-03\n",
      "Final loss loss_ics_u: 1.154035e-01\n",
      "Final loss loss_ics_u_t: 5.434678e-03\n",
      "average lambda_bc : 1.3258e+02\n",
      "average lambda_ic : 9.9782e+01\n",
      "average lambda_res : 1.0\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:1.4706e+03\n",
      "average of error_u_list:6.7400e-01\n",
      "average of error_r_list:4.0503e-03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define PINN model\n",
    "a = 0.5\n",
    "c = 2\n",
    "\n",
    "kernel_size = 300\n",
    "\n",
    "# Domain boundaries\n",
    "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
    "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
    "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
    "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
    "\n",
    "# Create boundary conditions samplers\n",
    "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
    "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
    "bcs_sampler = [bc1, bc2]\n",
    "\n",
    "# Create residual sampler\n",
    "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
    "\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 300\n",
    "\n",
    "\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'\n",
    "layers = [2, 500, 500, 500, 1]\n",
    "\n",
    "\n",
    "nn = 200\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "u_star = u(X_star, a,c)\n",
    "r_star = r(X_star, a, c)\n",
    "\n",
    "iterations = 1\n",
    "methods = [  \"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_r_list = []\n",
    "\n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "     # Create residual sampler\n",
    "\n",
    "        # [elapsed, error_u , model] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            # sess.run(init)\n",
    "\n",
    "            model = PINN(layers, operator ,  ics_sampler, bcs_sampler, res_sampler, c , mode , sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                print(\"full_batch method is used\")\n",
    "                model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                print(\"mini_batch method is used\")\n",
    "                model.trainmb(nIter, mbbatch_size)\n",
    "            else:\n",
    "                print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            r_pred = model.predict_r(X_star)\n",
    "            # Predictions\n",
    "\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "\n",
    " \n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "            model.plot_lambda()\n",
    "            # model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
    "            model.print(\"average lambda_bc : \" , np.average(model.adaptive_constant_bcs_log))\n",
    "            model.print(\"average lambda_ic : \" , np.average(model.adaptive_constant_ics_log))\n",
    "            model.print(\"average lambda_res : \" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "        # print('elapsed: {:.2e}'.format(elapsed))\n",
    "        # print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "        # print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_r_list.append(error_r)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list,error_r_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Relative L2 error_u: 6.74e-01\n",
      "Relative L2 error_r: 4.05e-03\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-24-2023_11-08-18-279267_M2\n",
      "Final loss total loss: 1.714497e-04\n",
      "Final loss loss_res: 6.719582e-06\n",
      "Final loss loss_bcs: 1.504912e-01\n",
      "Final loss loss_bc1: 2.609153e-02\n",
      "Final loss loss_bc2: 8.996237e-03\n",
      "Final loss loss_ics_u: 1.154035e-01\n",
      "Final loss loss_ics_u_t: 5.434678e-03\n",
      "average lambda_bc : 1.3258e+02\n",
      "average lambda_ic : 9.9782e+01\n",
      "average lambda_res : 1.0\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:1.4739e+03\n",
      "average of error_u_list:6.7400e-01\n",
      "average of error_r_list:4.0503e-03\n"
     ]
    }
   ],
   "source": [
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "r_pred = model.predict_r(X_star)\n",
    "# Predictions\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "\n",
    "\n",
    "model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "model.print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "model.plot_lambda()\n",
    "# model.plot_grad()\n",
    "model.save_NN()\n",
    "model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
    "model.print(\"average lambda_bc : \" , np.average(model.adaptive_constant_bcs_log))\n",
    "model.print(\"average lambda_ic : \" , np.average(model.adaptive_constant_ics_log))\n",
    "model.print(\"average lambda_res : \" , str(1.0))\n",
    "# sess.close()  \n",
    "\n",
    "# print('elapsed: {:.2e}'.format(elapsed))\n",
    "# print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "# print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "\n",
    "time_list.append(elapsed)\n",
    "error_u_list.append(error_u)\n",
    "error_r_list.append(error_r)\n",
    "\n",
    "model.print(\"\\n\\nMethod: \", mtd)\n",
    "model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "model.print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
    "\n",
    "result_dict[mtd] = [time_list ,error_u_list,error_r_list]\n",
    "# scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
