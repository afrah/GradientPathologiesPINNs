{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
    "import numpy as np\n",
    "import timeit\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "# Define the exact solution and its derivatives\n",
    "def u(x, a, c):\n",
    "    \"\"\"\n",
    "    :param x: x = (t, x)\n",
    "    \"\"\"\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    return np.sin(np.pi * x) * np.cos(c * np.pi * t) + a * np.sin(2 * c * np.pi* x) * np.cos(4 * c  * np.pi * t)\n",
    "\n",
    "def u_t(x,a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_t = -  c * np.pi * np.sin(np.pi * x) * np.sin(c * np.pi * t) -  a * 4 * c * np.pi * np.sin(2 * c * np.pi* x) * np.sin(4 * c * np.pi * t)\n",
    "    return u_t\n",
    "\n",
    "def u_tt(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_tt = -(c * np.pi)**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) - a * (4 * c * np.pi)**2 *  np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return u_tt\n",
    "\n",
    "def u_xx(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_xx = - np.pi**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) -  a * (2 * c * np.pi)** 2 * np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return  u_xx\n",
    "\n",
    "\n",
    "def r(x, a, c):\n",
    "    return u_tt(x, a, c) - c**2 * u_xx(x, a, c)\n",
    "\n",
    "def operator(u, t, x, c, sigma_t=1.0, sigma_x=1.0):\n",
    "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    residual = u_tt - c**2 * u_xx\n",
    "    return residual\n",
    "\n",
    "class PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess):\n",
    "        # Normalization \n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        self.sess = sess\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # weights\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "        self.adaptive_constant_ics_val = np.array(1.0)\n",
    "        self.lam_r_val = np.array(1.0)\n",
    "        self.rate = 0.9\n",
    "\n",
    "        # Wave constant\n",
    "        self.c = tf.constant(c, dtype=tf.float32)\n",
    "        \n",
    "        self.kernel_size = kernel_size # Size of the NTK matrix\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        \n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
    "        self.adaptive_constant_r_tf = tf.placeholder(tf.float32, shape=self.lam_r_val.shape)\n",
    "        \n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
    "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
    "        \n",
    "\n",
    "        # Boundary loss and Initial loss\n",
    "        self.loss_ics_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
    "        self.loss_ics_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
    "\n",
    "        self.loss_bcs = self.loss_ics_u + self.loss_bc1 + self.loss_bc2\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss =  self.loss_res + self.adaptive_constant_bcs_tf * self.loss_bcs + self.adaptive_constant_ics_tf * self.loss_ics_u_t \n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # # Generate dicts for gradients storage\n",
    "        # self.dict_gradients_res_layers = self.generate_grad_dict(self.layers)\n",
    "        # self.dict_gradients_bcs_layers = self.generate_grad_dict(self.layers)\n",
    "        # self.dict_gradients_ics_layers = self.generate_grad_dict(self.layers)\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_ics = []\n",
    "        self.grad_bcs = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "            self.grad_ics.append(tf.gradients(self.loss_ics_u_t, self.weights[i])[0])\n",
    "\n",
    "           \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        self.mean_grad_ics_list = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "            self.mean_grad_ics_list.append(tf.reduce_mean(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.mean_grad_ics = tf.reduce_mean(tf.stack(self.mean_grad_ics_list))\n",
    "        \n",
    "        self.adaptive_constant_bcs = self.max_grad_res / self.mean_grad_bcs\n",
    "        self.adaptive_constant_ics = self.max_grad_res / self.mean_grad_ics\n",
    "\n",
    "         # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        # Xavier initialization\n",
    "        def xavier_init(size):\n",
    "            in_dim = size[0]\n",
    "            out_dim = size[1]\n",
    "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "            return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H, layers, weights, biases):\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1),  self.layers, self.weights,   self.biases)\n",
    "        return u\n",
    "\n",
    "    # Forward pass for du/dt\n",
    "    def net_u_t(self, t, x):\n",
    "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
    "        return u_t\n",
    "\n",
    "    # Forward pass for the residual\n",
    "    def net_r(self, t, x):\n",
    "        u = self.net_u(t, x)\n",
    "        residual = self.operator(u, t, x,\n",
    "                                 self.c,\n",
    "                                 self.sigma_t,\n",
    "                                 self.sigma_x)\n",
    "        return residual\n",
    "    \n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "        # Trains the model by minimizing the MSE loss\n",
    "\n",
    "    def trainmb(self, nIter=10000, batch_size=128, log_NTK=False, update_lam=False):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size // 3)\n",
    "            X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], batch_size // 3)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size // 3)\n",
    "            \n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                       self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                       self.u_ics_tf: u_ics_batch,\n",
    "                       self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                        self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                       self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
    "                       self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val\n",
    "                       }#self.lam_r_val}\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 1000 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "                adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [self.adaptive_constant_bcs, self.adaptive_constant_ics  ], tf_dict)\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "\n",
    "                print('lambda_u: {:.3e}'.format(self.adaptive_constant_bcs_val))\n",
    "                print('lambda_ut: {:.3e}'.format(self.adaptive_constant_ics_val))\n",
    "                sys.stdout.flush()\n",
    "                start_time = timeit.default_timer()\n",
    "            \n",
    "    def train(self, nIter , bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, bcbatch_size)\n",
    "        X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        \n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                    self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                    self.u_ics_tf: u_ics_batch,\n",
    "                    self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                    self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                    self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
    "                    self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val\n",
    "                    }#self.lam_r_val}\n",
    "        \n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 1000 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "                adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [self.adaptive_constant_bcs, self.adaptive_constant_ics  ], tf_dict)\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "\n",
    "                print('lambda_u: {:.3e}'.format(self.adaptive_constant_bcs_val))\n",
    "                print('lambda_ut: {:.3e}'.format(self.adaptive_constant_ics_val))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                         \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "        # Evaluates predictions at test points\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "    \n",
    "   ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
    "def test_method(method , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "        # sess.run(init)\n",
    "\n",
    "        model = PINN(layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess)\n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "\n",
    "        if method ==\"full_batch\":\n",
    "            print(\"full_batch method is used\")\n",
    "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "        elif method ==\"mini_batch\":\n",
    "            print(\"mini_batch method is used\")\n",
    "            model.trainmb(nIter, mbbatch_size)\n",
    "        else:\n",
    "            print(\"unknown method!\")\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # Predictions\n",
    "        u_pred = model.predict_u(X_star)\n",
    "        r_pred = model.predict_r(X_star)\n",
    "        # Predictions\n",
    "\n",
    "        sess.close()   \n",
    "\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "\n",
    "    print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "\n",
    "    return [elapsed, error_u  ]\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_24636/3085221764.py:395: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_24636/3085221764.py:396: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_24636/3085221764.py:397: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_24636/3085221764.py:397: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_24636/3085221764.py:109: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 18:36:12.246565: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-09 18:36:12.275595: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-09 18:36:12.276120: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555a031f4f90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-09 18:36:12.276138: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-09 18:36:12.277674: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_24636/3085221764.py:157: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_24636/3085221764.py:159: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_24636/3085221764.py:200: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "mini_batch method is used\n",
      "It: 0, Loss: 1.501e+01, Loss_res: 7.232e+00,  Loss_bcs: 7.453e+00, Loss_ut_ics: 3.290e-01,, Time: 2.63\n",
      "lambda_u: 5.437e+00\n",
      "lambda_ut: 1.946e+01\n",
      "It: 1000, Loss: 1.007e+00, Loss_res: 1.823e-02,  Loss_bcs: 1.810e-01, Loss_ut_ics: 2.365e-04,, Time: 75.65\n",
      "lambda_u: 1.241e+01\n",
      "lambda_ut: 5.697e+01\n",
      "It: 2000, Loss: 2.868e+00, Loss_res: 6.030e-02,  Loss_bcs: 2.039e-01, Loss_ut_ics: 4.847e-03,, Time: 76.38\n",
      "lambda_u: 1.936e+01\n",
      "lambda_ut: 5.636e+01\n",
      "It: 3000, Loss: 2.810e+00, Loss_res: 5.056e-02,  Loss_bcs: 1.345e-01, Loss_ut_ics: 2.737e-03,, Time: 76.39\n",
      "lambda_u: 5.123e+01\n",
      "lambda_ut: 7.763e+01\n",
      "It: 4000, Loss: 7.641e+00, Loss_res: 1.980e-01,  Loss_bcs: 1.398e-01, Loss_ut_ics: 3.618e-03,, Time: 71.90\n",
      "lambda_u: 9.540e+01\n",
      "lambda_ut: 8.325e+01\n",
      "It: 5000, Loss: 1.256e+01, Loss_res: 4.063e-01,  Loss_bcs: 1.183e-01, Loss_ut_ics: 1.043e-02,, Time: 70.81\n",
      "lambda_u: 1.233e+02\n",
      "lambda_ut: 1.030e+02\n",
      "It: 6000, Loss: 1.179e+01, Loss_res: 3.283e-01,  Loss_bcs: 9.143e-02, Loss_ut_ics: 1.826e-03,, Time: 70.94\n",
      "lambda_u: 2.504e+02\n",
      "lambda_ut: 1.453e+02\n",
      "It: 7000, Loss: 1.755e+01, Loss_res: 4.657e-01,  Loss_bcs: 6.656e-02, Loss_ut_ics: 2.842e-03,, Time: 71.07\n",
      "lambda_u: 2.634e+02\n",
      "lambda_ut: 1.457e+02\n",
      "It: 8000, Loss: 1.146e+01, Loss_res: 4.056e-01,  Loss_bcs: 3.975e-02, Loss_ut_ics: 3.989e-03,, Time: 71.13\n",
      "lambda_u: 4.781e+02\n",
      "lambda_ut: 1.884e+02\n",
      "It: 9000, Loss: 1.937e+01, Loss_res: 9.211e-01,  Loss_bcs: 3.376e-02, Loss_ut_ics: 1.225e-02,, Time: 70.84\n",
      "lambda_u: 9.144e+02\n",
      "lambda_ut: 2.236e+02\n",
      "It: 10000, Loss: 1.843e+01, Loss_res: 1.844e+00,  Loss_bcs: 1.759e-02, Loss_ut_ics: 2.232e-03,, Time: 70.86\n",
      "lambda_u: 1.622e+03\n",
      "lambda_ut: 2.901e+02\n",
      "It: 11000, Loss: 1.899e+01, Loss_res: 2.458e+00,  Loss_bcs: 9.472e-03, Loss_ut_ics: 4.035e-03,, Time: 70.89\n",
      "lambda_u: 3.322e+03\n",
      "lambda_ut: 3.085e+02\n",
      "It: 12000, Loss: 2.002e+01, Loss_res: 4.528e+00,  Loss_bcs: 4.272e-03, Loss_ut_ics: 4.204e-03,, Time: 70.86\n",
      "lambda_u: 4.634e+03\n",
      "lambda_ut: 3.748e+02\n",
      "It: 13000, Loss: 1.900e+01, Loss_res: 4.381e+00,  Loss_bcs: 2.683e-03, Loss_ut_ics: 5.821e-03,, Time: 75.08\n",
      "lambda_u: 6.009e+03\n",
      "lambda_ut: 4.943e+02\n",
      "It: 14000, Loss: 1.609e+01, Loss_res: 5.984e+00,  Loss_bcs: 1.592e-03, Loss_ut_ics: 1.093e-03,, Time: 98.94\n",
      "lambda_u: 1.173e+04\n",
      "lambda_ut: 1.844e+03\n",
      "It: 15000, Loss: 3.006e+01, Loss_res: 9.033e+00,  Loss_bcs: 1.675e-03, Loss_ut_ics: 7.525e-04,, Time: 77.56\n",
      "lambda_u: 1.425e+04\n",
      "lambda_ut: 2.149e+03\n",
      "It: 16000, Loss: 2.463e+01, Loss_res: 8.885e+00,  Loss_bcs: 9.815e-04, Loss_ut_ics: 8.174e-04,, Time: 79.33\n",
      "lambda_u: 1.728e+04\n",
      "lambda_ut: 2.205e+03\n",
      "It: 17000, Loss: 2.200e+01, Loss_res: 7.967e+00,  Loss_bcs: 6.802e-04, Loss_ut_ics: 1.031e-03,, Time: 72.76\n",
      "lambda_u: 2.088e+04\n",
      "lambda_ut: 2.130e+03\n",
      "It: 18000, Loss: 2.829e+01, Loss_res: 8.890e+00,  Loss_bcs: 5.104e-04, Loss_ut_ics: 4.104e-03,, Time: 78.20\n",
      "lambda_u: 2.239e+04\n",
      "lambda_ut: 1.970e+03\n",
      "It: 19000, Loss: 1.662e+01, Loss_res: 8.491e+00,  Loss_bcs: 2.632e-04, Loss_ut_ics: 1.134e-03,, Time: 70.15\n",
      "lambda_u: 4.106e+04\n",
      "lambda_ut: 2.291e+03\n",
      "It: 20000, Loss: 1.821e+01, Loss_res: 1.007e+01,  Loss_bcs: 1.798e-04, Loss_ut_ics: 3.305e-04,, Time: 33.20\n",
      "lambda_u: 4.729e+04\n",
      "lambda_ut: 2.617e+03\n",
      "It: 21000, Loss: 1.609e+01, Loss_res: 8.143e+00,  Loss_bcs: 1.464e-04, Loss_ut_ics: 3.911e-04,, Time: 33.80\n",
      "lambda_u: 4.759e+04\n",
      "lambda_ut: 2.512e+03\n",
      "It: 22000, Loss: 1.405e+01, Loss_res: 7.209e+00,  Loss_bcs: 9.779e-05, Loss_ut_ics: 8.724e-04,, Time: 33.05\n",
      "lambda_u: 5.499e+04\n",
      "lambda_ut: 2.371e+03\n",
      "It: 23000, Loss: 2.655e+01, Loss_res: 6.173e+00,  Loss_bcs: 7.102e-05, Loss_ut_ics: 6.946e-03,, Time: 34.16\n",
      "lambda_u: 5.884e+04\n",
      "lambda_ut: 2.178e+03\n",
      "It: 24000, Loss: 2.290e+01, Loss_res: 6.363e+00,  Loss_bcs: 7.512e-05, Loss_ut_ics: 5.561e-03,, Time: 35.22\n",
      "lambda_u: 6.101e+04\n",
      "lambda_ut: 1.986e+03\n",
      "It: 25000, Loss: 9.018e+00, Loss_res: 4.815e+00,  Loss_bcs: 5.941e-05, Loss_ut_ics: 2.915e-04,, Time: 32.82\n",
      "lambda_u: 6.393e+04\n",
      "lambda_ut: 1.975e+03\n",
      "It: 26000, Loss: 7.715e+00, Loss_res: 3.898e+00,  Loss_bcs: 5.728e-05, Loss_ut_ics: 7.865e-05,, Time: 34.92\n",
      "lambda_u: 6.905e+04\n",
      "lambda_ut: 3.217e+03\n",
      "It: 27000, Loss: 7.491e+00, Loss_res: 4.240e+00,  Loss_bcs: 4.275e-05, Loss_ut_ics: 9.306e-05,, Time: 35.60\n",
      "lambda_u: 7.276e+04\n",
      "lambda_ut: 3.527e+03\n",
      "It: 28000, Loss: 6.165e+00, Loss_res: 3.003e+00,  Loss_bcs: 3.716e-05, Loss_ut_ics: 1.302e-04,, Time: 34.07\n",
      "lambda_u: 7.568e+04\n",
      "lambda_ut: 3.418e+03\n",
      "It: 29000, Loss: 7.718e+00, Loss_res: 4.518e+00,  Loss_bcs: 3.646e-05, Loss_ut_ics: 1.290e-04,, Time: 34.80\n",
      "lambda_u: 9.045e+04\n",
      "lambda_ut: 3.381e+03\n",
      "It: 30000, Loss: 7.655e+00, Loss_res: 3.500e+00,  Loss_bcs: 3.603e-05, Loss_ut_ics: 2.653e-04,, Time: 34.25\n",
      "lambda_u: 8.931e+04\n",
      "lambda_ut: 3.313e+03\n",
      "It: 31000, Loss: 6.653e+00, Loss_res: 3.364e+00,  Loss_bcs: 2.871e-05, Loss_ut_ics: 2.187e-04,, Time: 35.43\n",
      "lambda_u: 8.795e+04\n",
      "lambda_ut: 3.109e+03\n",
      "It: 32000, Loss: 5.208e+00, Loss_res: 2.748e+00,  Loss_bcs: 2.580e-05, Loss_ut_ics: 6.150e-05,, Time: 32.68\n",
      "lambda_u: 8.727e+04\n",
      "lambda_ut: 3.231e+03\n",
      "It: 33000, Loss: 5.348e+00, Loss_res: 3.139e+00,  Loss_bcs: 2.154e-05, Loss_ut_ics: 1.020e-04,, Time: 32.45\n",
      "lambda_u: 9.027e+04\n",
      "lambda_ut: 3.267e+03\n",
      "It: 34000, Loss: 6.462e+00, Loss_res: 3.660e+00,  Loss_bcs: 2.822e-05, Loss_ut_ics: 7.825e-05,, Time: 32.48\n",
      "lambda_u: 9.874e+04\n",
      "lambda_ut: 3.779e+03\n",
      "It: 35000, Loss: 6.017e+00, Loss_res: 3.267e+00,  Loss_bcs: 2.349e-05, Loss_ut_ics: 1.139e-04,, Time: 32.46\n",
      "lambda_u: 1.022e+05\n",
      "lambda_ut: 3.723e+03\n",
      "It: 36000, Loss: 5.465e+00, Loss_res: 2.850e+00,  Loss_bcs: 2.031e-05, Loss_ut_ics: 1.449e-04,, Time: 32.36\n",
      "lambda_u: 1.098e+05\n",
      "lambda_ut: 3.511e+03\n",
      "It: 37000, Loss: 5.509e+00, Loss_res: 2.618e+00,  Loss_bcs: 2.144e-05, Loss_ut_ics: 1.529e-04,, Time: 32.50\n",
      "lambda_u: 1.253e+05\n",
      "lambda_ut: 3.353e+03\n",
      "It: 38000, Loss: 5.482e+00, Loss_res: 3.088e+00,  Loss_bcs: 1.654e-05, Loss_ut_ics: 9.576e-05,, Time: 32.33\n",
      "lambda_u: 1.372e+05\n",
      "lambda_ut: 3.404e+03\n",
      "It: 39000, Loss: 6.284e+00, Loss_res: 3.275e+00,  Loss_bcs: 1.984e-05, Loss_ut_ics: 8.436e-05,, Time: 32.48\n",
      "lambda_u: 1.368e+05\n",
      "lambda_ut: 3.448e+03\n",
      "elapsed: 2.14e+03\n",
      "Relative L2 error_u: 2.88e-02\n",
      "elapsed: 2.14e+03\n",
      "Relative L2 error_u: 2.88e-02\n",
      "\n",
      "\n",
      "Method:  mini_batch\n",
      "\n",
      "average of time_list: 2136.311792373657\n",
      "average of error_u_list: 0.028776093426972148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define PINN model\n",
    "a = 0.5\n",
    "c = 2\n",
    "\n",
    "kernel_size = 300\n",
    "\n",
    "# Domain boundaries\n",
    "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
    "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
    "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
    "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
    "\n",
    "# Create boundary conditions samplers\n",
    "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
    "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
    "bcs_sampler = [bc1, bc2]\n",
    "\n",
    "# Create residual sampler\n",
    "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
    "\n",
    "\n",
    "\n",
    "nIter =40000\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 300\n",
    "\n",
    "\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'\n",
    "layers = [2, 500, 500, 500, 1]\n",
    "\n",
    "\n",
    "nn = 200\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "u_star = u(X_star, a,c)\n",
    "r_star = r(X_star, a, c)\n",
    "\n",
    "iterations = 1\n",
    "methods = [  \"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "        # Create residual sampler\n",
    "\n",
    "        [elapsed, error_u] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
    "\n",
    "\n",
    "        print('elapsed: {:.2e}'.format(elapsed))\n",
    "        print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "\n",
    "    print(\"\\n\\nMethod: \", mtd)\n",
    "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    # print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(\"./1DWave_database/1Dwave_\"+mtd+\"_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
