{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
    "import numpy as np\n",
    "import timeit\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "# Define the exact solution and its derivatives\n",
    "def u(x, a, c):\n",
    "    \"\"\"\n",
    "    :param x: x = (t, x)\n",
    "    \"\"\"\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    return np.sin(np.pi * x) * np.cos(c * np.pi * t) + a * np.sin(2 * c * np.pi* x) * np.cos(4 * c  * np.pi * t)\n",
    "\n",
    "def u_t(x,a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_t = -  c * np.pi * np.sin(np.pi * x) * np.sin(c * np.pi * t) -  a * 4 * c * np.pi * np.sin(2 * c * np.pi* x) * np.sin(4 * c * np.pi * t)\n",
    "    return u_t\n",
    "\n",
    "def u_tt(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_tt = -(c * np.pi)**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) - a * (4 * c * np.pi)**2 *  np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return u_tt\n",
    "\n",
    "def u_xx(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_xx = - np.pi**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) -  a * (2 * c * np.pi)** 2 * np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return  u_xx\n",
    "\n",
    "\n",
    "def r(x, a, c):\n",
    "    return u_tt(x, a, c) - c**2 * u_xx(x, a, c)\n",
    "\n",
    "def operator(u, t, x, c, sigma_t=1.0, sigma_x=1.0):\n",
    "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    residual = u_tt - c**2 * u_xx\n",
    "    return residual\n",
    "\n",
    "class PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, c , mode ,  sess):\n",
    "        # Normalization \n",
    "\n",
    "\n",
    "\n",
    "        self.update = False\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        self.sess = sess\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        self.T = 1.0\n",
    "        # weights\n",
    "        self.adaptive_constant_bcs1_val = np.array(1.0)\n",
    "        self.adaptive_constant_bcs2_val = np.array(1.0)\n",
    "        self.adaptive_constant_ics_val = np.array(1.0)\n",
    "        self.adaptive_constant_res_val = np.array(1.0)\n",
    "        self.rate = 0.9\n",
    "\n",
    "        # Wave constant\n",
    "        self.c = tf.constant(c, dtype=tf.float32)\n",
    "        \n",
    "        # self.kernel_size = kernel_size # Size of the NTK matrix\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        \n",
    "        self.adaptive_constant_bcs1_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs1_val.shape)\n",
    "        self.adaptive_constant_bcs2_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs2_val.shape)\n",
    "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
    "        self.adaptive_constant_res_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_val.shape)\n",
    "        \n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
    "        self.f_pred = self.net_u(self.t_r_tf, self.x_r_tf)\n",
    "        \n",
    "\n",
    "        # Boundary loss and Initial loss\n",
    "        self.loss_ics_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
    "        self.loss_ics_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
    "        self.loss_ics_us = self.loss_ics_u_t\n",
    "        self.loss_bc1 =   self.loss_ics_u +  tf.reduce_mean(tf.square(self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
    "\n",
    "        self.loss_u = tf.reduce_mean(tf.square(self.f_pred - self.u_r_tf))\n",
    "\n",
    "        # self.loss_bcs = self.loss_ics_u + self.loss_bc1 + self.loss_bc2\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = 1.0* tf.reduce_mean(tf.square(self.loss_u))\n",
    "\n",
    "        # Total loss\n",
    "        # self.loss =  self.loss_res +  self.loss_bc1 +  self.loss_bc2 +  ( self.loss_ics_us)\n",
    "        self.loss = self.adaptive_constant_res_tf * self.loss_res  + self.adaptive_constant_bcs1_tf  * self.loss_bc1 + self.adaptive_constant_bcs2_tf  * self.loss_bc2 +  self.adaptive_constant_ics_tf * ( self.loss_ics_us) #-( self.adaptive_constant_res_tf * self.adaptive_constant_bcs1_tf  * self.adaptive_constant_bcs2_tf *  self.adaptive_constant_ics_tf)\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step , var_list = [self.weights+self.biases])\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res, self.loss_bc1 , self.loss_bc2 , self.loss_ics_us , self.loss_ics_u, self.loss_ics_u_t] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\"  , \"loss_bc1\", \"loss_bc2\", \"loss_ics_us\", \"loss_ics_u\", \"loss_ics_u_t\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "         # # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs1_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs2_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_ics_layers = self.generate_grad_dict()\n",
    "        \n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_ics = []\n",
    "        self.grad_bcs1 = []\n",
    "        self.grad_bcs2 = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs1.append(tf.gradients(self.loss_bc1, self.weights[i])[0])\n",
    "            self.grad_bcs2.append(tf.gradients(self.loss_bc2, self.weights[i])[0])\n",
    "            self.grad_ics.append(tf.gradients(self.loss_ics_us, self.weights[i])[0])\n",
    "          \n",
    "        self.max_grad_res_list = []\n",
    "        self.max_grad_bcs1_list = []\n",
    "        self.max_grad_bcs2_list = []\n",
    "        self.max_grad_ics_list = []\n",
    "\n",
    "        self.min_grad_res_list = []\n",
    "        self.min_grad_bcs1_list = []\n",
    "        self.min_grad_bcs2_list = []\n",
    "        self.min_grad_ics_list = []\n",
    "    \n",
    "        self.adaptive_constant_bcs1_log = []\n",
    "        self.adaptive_constant_bcs2_log = []\n",
    "        self.adaptive_constant_ics_log = []\n",
    "        self.adaptive_constant_res_log = []\n",
    "\n",
    "        self.mean_adaptive_constant_res_log = []\n",
    "        self.mean_adaptive_constant_bcs1_log = []\n",
    "        self.mean_adaptive_constant_bcs2_log = []\n",
    "        self.mean_adaptive_constant_ics_log = []\n",
    "\n",
    "        for i in range(1 , len(self.layers) - 2):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.max_grad_bcs1_list.append(tf.reduce_mean(tf.abs(self.grad_bcs1[i])))\n",
    "            self.max_grad_bcs2_list.append(tf.reduce_mean(tf.abs(self.grad_bcs2[i])))\n",
    "            self.max_grad_ics_list.append(tf.reduce_mean(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "            self.min_grad_res_list.append(tf.reduce_min(tf.abs(self.grad_res[i]))) \n",
    "            self.min_grad_bcs1_list.append(tf.reduce_min(tf.abs(self.grad_bcs1[i])))\n",
    "            self.min_grad_bcs2_list.append(tf.reduce_min(tf.abs(self.grad_bcs2[i])))\n",
    "            self.min_grad_ics_list.append(tf.reduce_min(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.max_grad_bcs1 = tf.reduce_mean(tf.stack(self.max_grad_bcs1_list))\n",
    "        self.max_grad_bcs2 = tf.reduce_mean(tf.stack(self.max_grad_bcs2_list))\n",
    "        self.max_grad_ics = tf.reduce_mean(tf.stack(self.max_grad_ics_list))\n",
    "        \n",
    "        \n",
    "        self.min_grad_res = tf.reduce_min(tf.stack(self.min_grad_res_list))\n",
    "        self.min_grad_bcs1 = tf.reduce_min(tf.stack(self.min_grad_bcs1_list))\n",
    "        self.min_grad_bcs2 = tf.reduce_min(tf.stack(self.min_grad_bcs2_list))\n",
    "        self.min_grad_ics = tf.reduce_min(tf.stack(self.min_grad_ics_list))\n",
    "        \n",
    "        self.adaptive_constant_bcs1 = self.max_grad_bcs1\n",
    "        self.adaptive_constant_bcs2 = self.max_grad_bcs2\n",
    "        self.adaptive_constant_ics = self.max_grad_ics\n",
    "        self.adaptive_constant_res = self.max_grad_res \n",
    "\n",
    "         # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        # Xavier initialization\n",
    "        def xavier_init(size):\n",
    "            in_dim = size[0]\n",
    "            out_dim = size[1]\n",
    "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "            return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H, layers, weights, biases):\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1),  self.layers, self.weights,  self.biases)\n",
    "        return u\n",
    "\n",
    "    # Forward pass for du/dt\n",
    "    def net_u_t(self, t, x):\n",
    "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
    "        return u_t\n",
    "\n",
    "    # Forward pass for the residual\n",
    "    def net_r(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1),  self.layers, self.weights,  self.biases)\n",
    "        residual = self.operator(u, t, x, self.c, self.sigma_t, self.sigma_x)\n",
    "        return residual\n",
    "    \n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "        # Trains the model by minimizing the MSE loss\n",
    "\n",
    "\n",
    "    def lambda_balance(self  , term  ):\n",
    "                histoy_mean =  np.mean(self.loss_history[term])\n",
    "                m = 4 #len(self.loss_list)\n",
    "                num = np.exp( np.std(self.loss_history[term][-100::]) / (np.mean(self.loss_history[term][-100::]) + 1e-12) )#/(self.T * histoy_mean)) np.exp( )\n",
    "                denum = 0 \n",
    "                self.loss_list = [ \"loss_res\"  , \"loss_bc1\", \"loss_bc2\", \"loss_ics_us\"] \n",
    "\n",
    "                for  key in self.loss_list:\n",
    "                    denum +=  np.exp(  np.std(self.loss_history[key][-100::]) / (np.mean(self.loss_history[key][-100::]) + 1e-12) )# /(self.T * histoy_mean))  np.exp(self.loss_history[key][-1] )\n",
    "                return m * (num / denum)\n",
    "    \n",
    "    def trainmb(self, nIter, batch_size, a , c):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(1 , nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size // 3)\n",
    "            X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], batch_size // 3)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size // 3)\n",
    "            \n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "            _star = u(X_res_batch, a,c)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                       self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                       self.u_ics_tf: u_ics_batch,\n",
    "                       self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                        self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                       self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.u_r_tf: _star,\n",
    "                       self.adaptive_constant_bcs1_tf: self.adaptive_constant_bcs1_val,\n",
    "                       self.adaptive_constant_bcs2_tf: self.adaptive_constant_bcs2_val,\n",
    "                       self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
    "                       self.adaptive_constant_res_tf: self.adaptive_constant_res_val\n",
    "                       }#self.lam_r_val}\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "            \n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                # loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                # loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                # loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                # loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "                [loss ,  loss_res, loss_bc1 , loss_bc2 , loss_ics_us , loss_ics_u, loss_ics_u_t]   =  batch_losses \n",
    "                self.print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs1: %.3e,  Loss_bcs2: %.3e,  loss_ics_u: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss, loss_res, loss_bc1, loss_bc2, loss_ics_u ,  loss_ics_u_t, elapsed))\n",
    "                \n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "                max_grad_bcs1, max_grad_bcs2, max_grad_ics, max_grad_res = self.sess.run( [self.max_grad_bcs1, self.max_grad_bcs2,  self.max_grad_ics,  self.max_grad_res  ], tf_dict)\n",
    "                min_grad_bcs1, min_grad_bcs2, min_grad_ics, min_grad_res = self.sess.run( [self.min_grad_bcs1, self.min_grad_bcs2,  self.min_grad_ics,  self.min_grad_res  ], tf_dict)\n",
    "\n",
    "                # Print adaptive weights during training\n",
    "\n",
    "                max_grad = np.max( [max_grad_bcs1, max_grad_bcs2, max_grad_ics, max_grad_res])\n",
    "                min_grad = np.min( [min_grad_bcs1, min_grad_bcs2, min_grad_ics, min_grad_res])\n",
    "\n",
    "                ratio =  (max_grad/min_grad + 1e-8 )\n",
    "                update_res = self.lambda_balance( \"loss_res\"  )\n",
    "                update_ics_u = self.lambda_balance( \"loss_ics_us\"  )\n",
    "                update_bcs1 = self.lambda_balance( \"loss_bc1\"  )\n",
    "                update_bcs2 = self.lambda_balance( \"loss_bc2\"  )\n",
    "                \n",
    "                self.adaptive_constant_res_val =  max_grad_res  #/ (adaptive_constant_res_val + 1e-12) #( 1.0 - self.rate) + self.rate * self.adaptive_constant_res_val\n",
    "                self.adaptive_constant_ics_val =  max_grad_res/ max_grad_ics  #/ (adaptive_constant_ics_val + 1e-12)#( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs1_val = max_grad_res/ max_grad_bcs1  #/ (adaptive_constant_bcs1_val + 1e-12) #( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "                self.adaptive_constant_bcs2_val = max_grad_res / max_grad_bcs2  #/ (adaptive_constant_bcs2_val + 1e-12) #( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                self.print(\"minmax ratio : \"  ,ratio )\n",
    "                self.print('max_grad_bcs1: {:.3e} ,max_grad_bcs2: {:.3e} ,max_grad_ics: {:.3e} ,max_grad_res: {:.3e}  '.format( max_grad_bcs1 , max_grad_bcs2 ,max_grad_ics , max_grad_res ))\n",
    "                self.print('min_grad_bcs1: {:.3e} ,min_grad_bcs2: {:.3e} ,min_grad_ics: {:.3e} ,min_grad_res: {:.3e}  '.format( min_grad_bcs1 , min_grad_bcs2 ,min_grad_ics , min_grad_res ))\n",
    "\n",
    "                self.print('update_res: {:.3e}'.format( update_res))\n",
    "                self.print('update_ics_u_t: {:.3e}'.format( update_ics_u))\n",
    "                self.print('update_bcs1: {:.3e}'.format( update_bcs1))\n",
    "                self.print('update_bcs2: {:.3e}'.format( update_bcs2))\n",
    "                \n",
    "                self.print('adaptive_constant_res_val: {:.3e}'.format( self.adaptive_constant_res_val))\n",
    "                self.print('adaptive_constant_ics_val: {:.3e}'.format( self.adaptive_constant_ics_val))\n",
    "                self.print('adaptive_constant_bcs1_val: {:.3e}'.format( self.adaptive_constant_bcs1_val))\n",
    "                self.print('adaptive_constant_bcs2_val: {:.3e}'.format( self.adaptive_constant_bcs2_val))\n",
    "                \n",
    "                self.adaptive_constant_res_log.append(self.adaptive_constant_res_val)\n",
    "                self.adaptive_constant_bcs1_log.append(self.adaptive_constant_bcs1_val)\n",
    "                self.adaptive_constant_bcs2_log.append(self.adaptive_constant_bcs2_val)\n",
    "                self.adaptive_constant_ics_log.append(self.adaptive_constant_ics_val)\n",
    "\n",
    "                max_grad_res , mean_grad_bcs1, mean_grad_bcs2, mean_grad_ics = self.sess.run( [ self.max_grad_res , self.max_grad_bcs1, self.max_grad_bcs2, self.max_grad_ics  ], tf_dict)\n",
    "\n",
    "                self.mean_adaptive_constant_res_log.append( max_grad_res)\n",
    "                self.mean_adaptive_constant_bcs1_log.append( mean_grad_bcs1)\n",
    "                self.mean_adaptive_constant_bcs2_log.append( mean_grad_bcs2)\n",
    "                self.mean_adaptive_constant_ics_log.append( mean_grad_ics)\n",
    "\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                start_time = timeit.default_timer()\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "    def train(self, nIter , bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, bcbatch_size)\n",
    "        X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        \n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                    self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                    self.u_ics_tf: u_ics_batch,\n",
    "                    self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                    self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                    self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs1_val,\n",
    "                    self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val\n",
    "                    }#self.lam_r_val}\n",
    "        \n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 1000 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "                adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [self.adaptive_constant_bcs1, self.adaptive_constant_ics  ], tf_dict)\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs1_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs1_val\n",
    "\n",
    "\n",
    "                print('lambda_u: {:.3e}'.format(self.adaptive_constant_bcs1_val))\n",
    "                print('lambda_ut: {:.3e}'.format(self.adaptive_constant_ics_val))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                         \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "        # Evaluates predictions at test points\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "    \n",
    "   ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   \n",
    "     ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1  , grad_bc2 ,grad_ics  = self.sess.run([ self.grad_res[i],self.grad_bcs1[i],self.grad_bcs2[i],self.grad_ics[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs1_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "            self.dict_gradients_bcs2_layers['layer_' + str(i + 1)].append(grad_bc2.flatten())\n",
    "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs1_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc2 = self.dict_gradients_bcs2_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_ics = self.dict_gradients_ics_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "            sns.distplot(gradients_bc2, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc2}}$')\n",
    "            sns.distplot(gradients_ics, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{ics}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=4)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        #plt.show()\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , t , x , X_star , u_star , u_pred , r_star , r_pred):\n",
    "        \n",
    "        U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "        r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "        R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(18, 9))\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.pcolor(t, x, U_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$x_1$')\n",
    "        plt.ylabel('$x_2$')\n",
    "        plt.title('Exact u(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Predicted u(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Absolute error')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.pcolor(t, x, r_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Exact r(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.pcolor(t, x, R_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Predicted r(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Absolute error')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fontsize = 17\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([16,8])\n",
    "        ax.semilogy(self.mean_adaptive_constant_bcs1_log, label=r'$\\bar{\\nabla_\\theta {u_{bc1}}}$' , color = 'tab:green')\n",
    "        ax.semilogy(self.mean_adaptive_constant_bcs2_log, label=r'$\\bar{\\nabla_\\theta {u_{bc2}}}$' , color = 'tab:brown')\n",
    "        ax.semilogy(self.mean_adaptive_constant_ics_log, label=r'$\\bar{\\nabla_\\theta {u_{ics}}}$' , color = 'tab:blue')\n",
    "        ax.semilogy(self.mean_adaptive_constant_res_log, label=r'$Max{\\nabla_\\theta {u_{phy}}}$' , color = 'tab:red')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
    "        ax.tick_params(labelsize=fontsize)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
    "\n",
    "        ax2 = ax.twinx() \n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_size_inches([15,8])\n",
    "    \n",
    "        ax2.semilogy(self.adaptive_constant_bcs1_log, label=r'$\\bar{\\lambda_{bc1}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
    "        ax2.semilogy(self.adaptive_constant_bcs2_log, label=r'$\\bar{\\lambda_{bc2}}$'  ,  linestyle='dashed' , color = 'tab:brown') \n",
    "        ax2.semilogy(self.adaptive_constant_ics_log, label=r'$\\bar{\\lambda_{ics}}$' , linestyle='dashed'  , color = 'tab:blue')\n",
    "        ax2.semilogy(self.adaptive_constant_res_log, label=r'$\\bar{\\lambda_{phy}}$' ,  linestyle='dashed' , color = 'tab:red')\n",
    "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
    "        ax2.tick_params(labelsize=fontsize)\n",
    "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   #  \n",
    "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
    "def test_method(method , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "        # sess.run(init)\n",
    "\n",
    "        model = PINN(layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess)\n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "\n",
    "        if method ==\"full_batch\":\n",
    "            print(\"full_batch method is used\")\n",
    "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "        elif method ==\"mini_batch\":\n",
    "            print(\"mini_batch method is used\")\n",
    "            model.trainmb(nIter, mbbatch_size)\n",
    "        else:\n",
    "            print(\"unknown method!\")\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # Predictions\n",
    "        u_pred = model.predict_u(X_star)\n",
    "        r_pred = model.predict_r(X_star)\n",
    "        # Predictions\n",
    "\n",
    "        sess.close()   \n",
    "\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "\n",
    "    print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "\n",
    "    return [elapsed, error_u  ]\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18757/3933714513.py:65: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18757/3933714513.py:66: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18757/3933714513.py:67: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18757/3933714513.py:67: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18757/3166399342.py:127: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18757/3166399342.py:181: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_18757/3166399342.py:183: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 00:43:21.478464: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-26 00:43:21.506291: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-26 00:43:21.507057: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5626c6989290 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-26 00:43:21.507078: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-26 00:43:21.510817: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_18757/3166399342.py:262: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "mini_batch method is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, Loss: 5.356e-01, Loss_res: 1.343e-01,  Loss_bcs1: 3.596e-01,  Loss_bcs2: 4.740e-03,  loss_ics_u: 3.289e-01, Loss_ut_ics: 3.700e-02,, Time: 13.52\n",
      "minmax ratio : 1.4607e+08\n",
      "max_grad_bcs1: 1.827e-03 ,max_grad_bcs2: 5.165e-04 ,max_grad_ics: 1.136e-03 ,max_grad_res: 7.510e-03  \n",
      "min_grad_bcs1: 1.064e-09 ,min_grad_bcs2: 5.141e-11 ,min_grad_ics: 8.945e-10 ,min_grad_res: 3.311e-10  \n",
      "update_res: 3.571e-01\n",
      "update_ics_u_t: 2.573e+00\n",
      "update_bcs1: 1.095e-01\n",
      "update_bcs2: 9.607e-01\n",
      "adaptive_constant_res_val: 7.510e-03\n",
      "adaptive_constant_ics_val: 6.611e+00\n",
      "adaptive_constant_bcs1_val: 4.112e+00\n",
      "adaptive_constant_bcs2_val: 1.454e+01\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, Loss: 5.942e-01, Loss_res: 3.839e-01,  Loss_bcs1: 1.383e-01,  Loss_bcs2: 4.687e-04,  loss_ics_u: 1.368e-01, Loss_ut_ics: 2.375e-03,, Time: 10.05\n",
      "minmax ratio : 5.3260e+09\n",
      "max_grad_bcs1: 2.502e-04 ,max_grad_bcs2: 4.029e-05 ,max_grad_ics: 1.154e-04 ,max_grad_res: 7.750e-02  \n",
      "min_grad_bcs1: 1.455e-11 ,min_grad_bcs2: 2.252e-11 ,min_grad_ics: 4.340e-11 ,min_grad_res: 6.276e-11  \n",
      "update_res: 2.548e-01\n",
      "update_ics_u_t: 5.995e-01\n",
      "update_bcs1: 2.822e-01\n",
      "update_bcs2: 2.863e+00\n",
      "adaptive_constant_res_val: 7.750e-02\n",
      "adaptive_constant_ics_val: 6.718e+02\n",
      "adaptive_constant_bcs1_val: 3.098e+02\n",
      "adaptive_constant_bcs2_val: 1.924e+03\n",
      "It: 300, Loss: 5.726e+01, Loss_res: 1.807e-01,  Loss_bcs1: 1.734e-01,  Loss_bcs2: 4.853e-04,  loss_ics_u: 1.572e-01, Loss_ut_ics: 3.844e-03,, Time: 2.22\n",
      "minmax ratio : 2.6723e+10\n",
      "max_grad_bcs1: 5.940e-04 ,max_grad_bcs2: 3.479e-05 ,max_grad_ics: 9.413e-05 ,max_grad_res: 2.836e-02  \n",
      "min_grad_bcs1: 1.795e-10 ,min_grad_bcs2: 1.061e-12 ,min_grad_ics: 2.001e-11 ,min_grad_res: 1.149e-11  \n",
      "update_res: 1.782e-01\n",
      "update_ics_u_t: 1.065e+00\n",
      "update_bcs1: 1.847e-01\n",
      "update_bcs2: 2.572e+00\n",
      "adaptive_constant_res_val: 2.836e-02\n",
      "adaptive_constant_ics_val: 3.012e+02\n",
      "adaptive_constant_bcs1_val: 4.774e+01\n",
      "adaptive_constant_bcs2_val: 8.151e+02\n",
      "It: 400, Loss: 6.181e+00, Loss_res: 2.402e-01,  Loss_bcs1: 1.201e-01,  Loss_bcs2: 4.618e-05,  loss_ics_u: 1.117e-01, Loss_ut_ics: 1.340e-03,, Time: 2.22\n",
      "minmax ratio : 6.7658e+09\n",
      "max_grad_bcs1: 2.332e-04 ,max_grad_bcs2: 9.356e-06 ,max_grad_ics: 2.161e-05 ,max_grad_res: 2.435e-02  \n",
      "min_grad_bcs1: 1.164e-10 ,min_grad_bcs2: 3.652e-12 ,min_grad_ics: 3.599e-12 ,min_grad_res: 4.109e-10  \n",
      "update_res: 8.807e-01\n",
      "update_ics_u_t: 1.009e+00\n",
      "update_bcs1: 8.318e-01\n",
      "update_bcs2: 1.278e+00\n",
      "adaptive_constant_res_val: 2.435e-02\n",
      "adaptive_constant_ics_val: 1.126e+03\n",
      "adaptive_constant_bcs1_val: 1.044e+02\n",
      "adaptive_constant_bcs2_val: 2.602e+03\n",
      "It: 500, Loss: 1.212e+01, Loss_res: 2.612e-01,  Loss_bcs1: 1.072e-01,  Loss_bcs2: 2.371e-05,  loss_ics_u: 1.018e-01, Loss_ut_ics: 7.688e-04,, Time: 2.17\n",
      "minmax ratio : 1.1441e+10\n",
      "max_grad_bcs1: 2.286e-04 ,max_grad_bcs2: 9.032e-06 ,max_grad_ics: 6.339e-05 ,max_grad_res: 2.769e-02  \n",
      "min_grad_bcs1: 8.496e-11 ,min_grad_bcs2: 2.421e-12 ,min_grad_ics: 1.121e-11 ,min_grad_res: 3.170e-10  \n",
      "update_res: 8.874e-01\n",
      "update_ics_u_t: 9.874e-01\n",
      "update_bcs1: 8.544e-01\n",
      "update_bcs2: 1.271e+00\n",
      "adaptive_constant_res_val: 2.769e-02\n",
      "adaptive_constant_ics_val: 4.369e+02\n",
      "adaptive_constant_bcs1_val: 1.212e+02\n",
      "adaptive_constant_bcs2_val: 3.066e+03\n",
      "It: 600, Loss: 1.279e+01, Loss_res: 3.228e-01,  Loss_bcs1: 1.023e-01,  Loss_bcs2: 1.545e-05,  loss_ics_u: 9.603e-02, Loss_ut_ics: 7.644e-04,, Time: 2.23\n",
      "minmax ratio : 5.5984e+09\n",
      "max_grad_bcs1: 1.497e-04 ,max_grad_bcs2: 2.532e-05 ,max_grad_ics: 7.095e-05 ,max_grad_res: 4.996e-02  \n",
      "min_grad_bcs1: 1.989e-10 ,min_grad_bcs2: 8.924e-12 ,min_grad_ics: 3.634e-11 ,min_grad_res: 9.225e-10  \n",
      "update_res: 9.118e-01\n",
      "update_ics_u_t: 9.720e-01\n",
      "update_bcs1: 8.619e-01\n",
      "update_bcs2: 1.254e+00\n",
      "adaptive_constant_res_val: 4.996e-02\n",
      "adaptive_constant_ics_val: 7.042e+02\n",
      "adaptive_constant_bcs1_val: 3.337e+02\n",
      "adaptive_constant_bcs2_val: 1.973e+03\n",
      "It: 700, Loss: 3.943e+01, Loss_res: 3.938e-01,  Loss_bcs1: 1.024e-01,  Loss_bcs2: 1.837e-04,  loss_ics_u: 9.626e-02, Loss_ut_ics: 6.910e-03,, Time: 2.17\n",
      "minmax ratio : 1.4182e+09\n",
      "max_grad_bcs1: 3.217e-04 ,max_grad_bcs2: 7.692e-05 ,max_grad_ics: 1.766e-04 ,max_grad_res: 4.094e-02  \n",
      "min_grad_bcs1: 1.673e-10 ,min_grad_bcs2: 2.887e-11 ,min_grad_ics: 8.117e-11 ,min_grad_res: 1.937e-10  \n",
      "update_res: 7.298e-01\n",
      "update_ics_u_t: 1.097e+00\n",
      "update_bcs1: 6.881e-01\n",
      "update_bcs2: 1.485e+00\n",
      "adaptive_constant_res_val: 4.094e-02\n",
      "adaptive_constant_ics_val: 2.319e+02\n",
      "adaptive_constant_bcs1_val: 1.273e+02\n",
      "adaptive_constant_bcs2_val: 5.322e+02\n",
      "It: 800, Loss: 1.089e+01, Loss_res: 3.394e-01,  Loss_bcs1: 8.200e-02,  Loss_bcs2: 4.916e-04,  loss_ics_u: 7.638e-02, Loss_ut_ics: 7.755e-04,, Time: 2.28\n",
      "minmax ratio : 8.4867e+09\n",
      "max_grad_bcs1: 1.311e-04 ,max_grad_bcs2: 7.101e-05 ,max_grad_ics: 1.495e-05 ,max_grad_res: 5.601e-02  \n",
      "min_grad_bcs1: 1.501e-11 ,min_grad_bcs2: 6.599e-12 ,min_grad_ics: 2.694e-11 ,min_grad_res: 1.789e-10  \n",
      "update_res: 7.650e-01\n",
      "update_ics_u_t: 1.101e+00\n",
      "update_bcs1: 7.226e-01\n",
      "update_bcs2: 1.411e+00\n",
      "adaptive_constant_res_val: 5.601e-02\n",
      "adaptive_constant_ics_val: 3.747e+03\n",
      "adaptive_constant_bcs1_val: 4.271e+02\n",
      "adaptive_constant_bcs2_val: 7.887e+02\n",
      "It: 900, Loss: 2.855e+01, Loss_res: 6.612e-01,  Loss_bcs1: 6.495e-02,  Loss_bcs2: 3.568e-04,  loss_ics_u: 6.025e-02, Loss_ut_ics: 1.299e-04,, Time: 2.28\n",
      "minmax ratio : 1.8219e+12\n",
      "max_grad_bcs1: 2.930e-04 ,max_grad_bcs2: 5.958e-05 ,max_grad_ics: 3.130e-06 ,max_grad_res: 9.435e-02  \n",
      "min_grad_bcs1: 1.819e-12 ,min_grad_bcs2: 6.479e-12 ,min_grad_ics: 5.178e-14 ,min_grad_res: 8.122e-10  \n",
      "update_res: 8.203e-02\n",
      "update_ics_u_t: 3.194e+00\n",
      "update_bcs1: 1.241e-01\n",
      "update_bcs2: 5.996e-01\n",
      "adaptive_constant_res_val: 9.435e-02\n",
      "adaptive_constant_ics_val: 3.015e+04\n",
      "adaptive_constant_bcs1_val: 3.220e+02\n",
      "adaptive_constant_bcs2_val: 1.583e+03\n",
      "It: 1000, Loss: 2.498e+01, Loss_res: 7.879e-01,  Loss_bcs1: 7.602e-02,  Loss_bcs2: 4.225e-05,  loss_ics_u: 7.405e-02, Loss_ut_ics: 1.206e-05,, Time: 2.44\n",
      "minmax ratio : 1.6157e+11\n",
      "max_grad_bcs1: 1.026e-04 ,max_grad_bcs2: 4.651e-05 ,max_grad_ics: 3.544e-06 ,max_grad_res: 1.148e-01  \n",
      "min_grad_bcs1: 5.821e-11 ,min_grad_bcs2: 3.214e-11 ,min_grad_ics: 7.104e-13 ,min_grad_res: 1.579e-09  \n",
      "update_res: 5.365e-01\n",
      "update_ics_u_t: 1.743e+00\n",
      "update_bcs1: 5.212e-01\n",
      "update_bcs2: 1.199e+00\n",
      "adaptive_constant_res_val: 1.148e-01\n",
      "adaptive_constant_ics_val: 3.239e+04\n",
      "adaptive_constant_bcs1_val: 1.119e+03\n",
      "adaptive_constant_bcs2_val: 2.468e+03\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, Loss: 6.379e+01, Loss_res: 5.873e-01,  Loss_bcs1: 5.338e-02,  Loss_bcs2: 5.512e-05,  loss_ics_u: 5.268e-02, Loss_ut_ics: 1.189e-04,, Time: 10.25\n",
      "minmax ratio : 8.2674e+10\n",
      "max_grad_bcs1: 2.304e-04 ,max_grad_bcs2: 9.598e-05 ,max_grad_ics: 2.046e-06 ,max_grad_res: 1.047e-01  \n",
      "min_grad_bcs1: 3.485e-10 ,min_grad_bcs2: 6.641e-12 ,min_grad_ics: 1.267e-12 ,min_grad_res: 2.402e-10  \n",
      "update_res: 5.669e-01\n",
      "update_ics_u_t: 1.385e+00\n",
      "update_bcs1: 5.675e-01\n",
      "update_bcs2: 1.481e+00\n",
      "adaptive_constant_res_val: 1.047e-01\n",
      "adaptive_constant_ics_val: 5.119e+04\n",
      "adaptive_constant_bcs1_val: 4.545e+02\n",
      "adaptive_constant_bcs2_val: 1.091e+03\n",
      "It: 1200, Loss: 2.916e+01, Loss_res: 5.906e-01,  Loss_bcs1: 6.378e-02,  Loss_bcs2: 5.304e-06,  loss_ics_u: 6.163e-02, Loss_ut_ics: 2.143e-06,, Time: 2.15\n",
      "minmax ratio : 3.0391e+13\n",
      "max_grad_bcs1: 1.252e-04 ,max_grad_bcs2: 1.845e-05 ,max_grad_ics: 9.571e-07 ,max_grad_res: 1.195e-01  \n",
      "min_grad_bcs1: 1.819e-12 ,min_grad_bcs2: 4.470e-13 ,min_grad_ics: 3.932e-15 ,min_grad_res: 1.834e-10  \n",
      "update_res: 3.844e-01\n",
      "update_ics_u_t: 1.828e+00\n",
      "update_bcs1: 3.735e-01\n",
      "update_bcs2: 1.414e+00\n",
      "adaptive_constant_res_val: 1.195e-01\n",
      "adaptive_constant_ics_val: 1.249e+05\n",
      "adaptive_constant_bcs1_val: 9.545e+02\n",
      "adaptive_constant_bcs2_val: 6.478e+03\n",
      "It: 1300, Loss: 5.911e+01, Loss_res: 6.999e-01,  Loss_bcs1: 6.117e-02,  Loss_bcs2: 6.250e-05,  loss_ics_u: 6.060e-02, Loss_ut_ics: 1.850e-06,, Time: 2.17\n",
      "minmax ratio : 1.2913e+16\n",
      "max_grad_bcs1: 2.486e-04 ,max_grad_bcs2: 4.113e-05 ,max_grad_ics: 1.087e-07 ,max_grad_res: 1.378e-01  \n",
      "min_grad_bcs1: 1.087e-11 ,min_grad_bcs2: 2.852e-14 ,min_grad_ics: 1.067e-17 ,min_grad_res: 4.454e-12  \n",
      "update_res: 2.351e-01\n",
      "update_ics_u_t: 2.751e+00\n",
      "update_bcs1: 2.299e-01\n",
      "update_bcs2: 7.840e-01\n",
      "adaptive_constant_res_val: 1.378e-01\n",
      "adaptive_constant_ics_val: 1.268e+06\n",
      "adaptive_constant_bcs1_val: 5.541e+02\n",
      "adaptive_constant_bcs2_val: 3.349e+03\n",
      "It: 1400, Loss: 1.678e+02, Loss_res: 2.805e-01,  Loss_bcs1: 2.990e-01,  Loss_bcs2: 2.912e-05,  loss_ics_u: 2.638e-01, Loss_ut_ics: 1.586e-06,, Time: 2.27\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:391: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 4.648e-04 ,max_grad_bcs2: 3.129e-06 ,max_grad_ics: 1.401e-07 ,max_grad_res: 3.494e-02  \n",
      "min_grad_bcs1: 1.051e-09 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 8.024e-14 ,min_grad_res: 2.636e-10  \n",
      "update_res: 1.545e-01\n",
      "update_ics_u_t: 2.609e+00\n",
      "update_bcs1: 1.433e-01\n",
      "update_bcs2: 1.093e+00\n",
      "adaptive_constant_res_val: 3.494e-02\n",
      "adaptive_constant_ics_val: 2.495e+05\n",
      "adaptive_constant_bcs1_val: 7.517e+01\n",
      "adaptive_constant_bcs2_val: 1.117e+04\n",
      "It: 1500, Loss: 1.375e+01, Loss_res: 3.022e-01,  Loss_bcs1: 1.813e-01,  Loss_bcs2: 1.416e-07,  loss_ics_u: 1.512e-01, Loss_ut_ics: 4.148e-07,, Time: 2.16\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.978e-04 ,max_grad_bcs2: 4.258e-07 ,max_grad_ics: 3.480e-08 ,max_grad_res: 2.634e-02  \n",
      "min_grad_bcs1: 2.528e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.556e-14 ,min_grad_res: 1.164e-10  \n",
      "update_res: 4.090e-01\n",
      "update_ics_u_t: 7.037e-01\n",
      "update_bcs1: 3.922e-01\n",
      "update_bcs2: 2.495e+00\n",
      "adaptive_constant_res_val: 2.634e-02\n",
      "adaptive_constant_ics_val: 7.569e+05\n",
      "adaptive_constant_bcs1_val: 8.845e+01\n",
      "adaptive_constant_bcs2_val: 6.186e+04\n",
      "It: 1600, Loss: 1.841e+01, Loss_res: 4.100e-01,  Loss_bcs1: 2.049e-01,  Loss_bcs2: 2.175e-08,  loss_ics_u: 1.786e-01, Loss_ut_ics: 3.572e-07,, Time: 2.18\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.945e-04 ,max_grad_bcs2: 2.107e-07 ,max_grad_ics: 3.870e-08 ,max_grad_res: 3.945e-02  \n",
      "min_grad_bcs1: 1.601e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.082e-14 ,min_grad_res: 4.366e-11  \n",
      "update_res: 6.325e-01\n",
      "update_ics_u_t: 7.144e-01\n",
      "update_bcs1: 6.171e-01\n",
      "update_bcs2: 2.036e+00\n",
      "adaptive_constant_res_val: 3.945e-02\n",
      "adaptive_constant_ics_val: 1.019e+06\n",
      "adaptive_constant_bcs1_val: 9.999e+01\n",
      "adaptive_constant_bcs2_val: 1.872e+05\n",
      "It: 1700, Loss: 1.313e+01, Loss_res: 4.548e-01,  Loss_bcs1: 1.283e-01,  Loss_bcs2: 1.104e-08,  loss_ics_u: 1.030e-01, Loss_ut_ics: 2.777e-07,, Time: 2.11\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.894e-04 ,max_grad_bcs2: 2.545e-07 ,max_grad_ics: 2.294e-08 ,max_grad_res: 4.537e-02  \n",
      "min_grad_bcs1: 2.210e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 5.329e-15 ,min_grad_res: 3.893e-10  \n",
      "update_res: 7.137e-01\n",
      "update_ics_u_t: 7.963e-01\n",
      "update_bcs1: 6.804e-01\n",
      "update_bcs2: 1.810e+00\n",
      "adaptive_constant_res_val: 4.537e-02\n",
      "adaptive_constant_ics_val: 1.978e+06\n",
      "adaptive_constant_bcs1_val: 2.396e+02\n",
      "adaptive_constant_bcs2_val: 1.783e+05\n",
      "It: 1800, Loss: 3.188e+01, Loss_res: 5.353e-01,  Loss_bcs1: 1.315e-01,  Loss_bcs2: 5.752e-07,  loss_ics_u: 1.089e-01, Loss_ut_ics: 1.298e-07,, Time: 2.34\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.520e-04 ,max_grad_bcs2: 6.501e-07 ,max_grad_ics: 1.730e-08 ,max_grad_res: 4.572e-02  \n",
      "min_grad_bcs1: 3.474e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.151e-16 ,min_grad_res: 3.711e-10  \n",
      "update_res: 7.116e-01\n",
      "update_ics_u_t: 7.684e-01\n",
      "update_bcs1: 6.941e-01\n",
      "update_bcs2: 1.826e+00\n",
      "adaptive_constant_res_val: 4.572e-02\n",
      "adaptive_constant_ics_val: 2.643e+06\n",
      "adaptive_constant_bcs1_val: 3.008e+02\n",
      "adaptive_constant_bcs2_val: 7.032e+04\n",
      "It: 1900, Loss: 2.624e+01, Loss_res: 6.941e-01,  Loss_bcs1: 8.615e-02,  Loss_bcs2: 1.493e-07,  loss_ics_u: 6.669e-02, Loss_ut_ics: 1.047e-07,, Time: 2.25\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.143e-04 ,max_grad_bcs2: 5.687e-07 ,max_grad_ics: 2.889e-08 ,max_grad_res: 5.936e-02  \n",
      "min_grad_bcs1: 8.367e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 3.282e-15 ,min_grad_res: 8.731e-11  \n",
      "update_res: 7.146e-01\n",
      "update_ics_u_t: 7.388e-01\n",
      "update_bcs1: 6.903e-01\n",
      "update_bcs2: 1.856e+00\n",
      "adaptive_constant_res_val: 5.936e-02\n",
      "adaptive_constant_ics_val: 2.055e+06\n",
      "adaptive_constant_bcs1_val: 5.195e+02\n",
      "adaptive_constant_bcs2_val: 1.044e+05\n",
      "It: 2000, Loss: 5.795e+01, Loss_res: 8.984e-01,  Loss_bcs1: 1.110e-01,  Loss_bcs2: 6.274e-07,  loss_ics_u: 9.527e-02, Loss_ut_ics: 7.896e-08,, Time: 2.18\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 7.071e-05 ,max_grad_bcs2: 4.257e-07 ,max_grad_ics: 1.451e-08 ,max_grad_res: 7.725e-02  \n",
      "min_grad_bcs1: 1.908e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 6.658e-16 ,min_grad_res: 3.691e-09  \n",
      "update_res: 7.479e-01\n",
      "update_ics_u_t: 7.629e-01\n",
      "update_bcs1: 7.202e-01\n",
      "update_bcs2: 1.769e+00\n",
      "adaptive_constant_res_val: 7.725e-02\n",
      "adaptive_constant_ics_val: 5.324e+06\n",
      "adaptive_constant_bcs1_val: 1.092e+03\n",
      "adaptive_constant_bcs2_val: 1.815e+05\n",
      "It: 2100, Loss: 8.898e+01, Loss_res: 6.637e-01,  Loss_bcs1: 8.105e-02,  Loss_bcs2: 7.491e-07,  loss_ics_u: 6.884e-02, Loss_ut_ics: 4.480e-08,, Time: 2.24\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 4.760e-05 ,max_grad_bcs2: 2.298e-07 ,max_grad_ics: 5.833e-09 ,max_grad_res: 5.398e-02  \n",
      "min_grad_bcs1: 2.263e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.249e-15 ,min_grad_res: 4.366e-10  \n",
      "update_res: 2.465e-01\n",
      "update_ics_u_t: 6.697e-01\n",
      "update_bcs1: 2.483e-01\n",
      "update_bcs2: 2.835e+00\n",
      "adaptive_constant_res_val: 5.398e-02\n",
      "adaptive_constant_ics_val: 9.253e+06\n",
      "adaptive_constant_bcs1_val: 1.134e+03\n",
      "adaptive_constant_bcs2_val: 2.349e+05\n",
      "It: 2200, Loss: 1.029e+02, Loss_res: 6.857e-01,  Loss_bcs1: 9.013e-02,  Loss_bcs2: 1.560e-06,  loss_ics_u: 7.479e-02, Loss_ut_ics: 2.750e-08,, Time: 2.18\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.347e-05 ,max_grad_bcs2: 5.672e-07 ,max_grad_ics: 1.890e-08 ,max_grad_res: 4.605e-02  \n",
      "min_grad_bcs1: 1.048e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 3.109e-15 ,min_grad_res: 5.621e-10  \n",
      "update_res: 2.154e-01\n",
      "update_ics_u_t: 8.377e-01\n",
      "update_bcs1: 2.136e-01\n",
      "update_bcs2: 2.733e+00\n",
      "adaptive_constant_res_val: 4.605e-02\n",
      "adaptive_constant_ics_val: 2.437e+06\n",
      "adaptive_constant_bcs1_val: 1.376e+03\n",
      "adaptive_constant_bcs2_val: 8.119e+04\n",
      "It: 2300, Loss: 9.675e+01, Loss_res: 7.436e-01,  Loss_bcs1: 7.010e-02,  Loss_bcs2: 8.989e-07,  loss_ics_u: 5.803e-02, Loss_ut_ics: 7.820e-08,, Time: 2.25\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 4.146e-05 ,max_grad_bcs2: 1.505e-07 ,max_grad_ics: 1.279e-08 ,max_grad_res: 5.954e-02  \n",
      "min_grad_bcs1: 1.902e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 6.247e-15 ,min_grad_res: 8.004e-11  \n",
      "update_res: 6.789e-01\n",
      "update_ics_u_t: 7.726e-01\n",
      "update_bcs1: 6.831e-01\n",
      "update_bcs2: 1.865e+00\n",
      "adaptive_constant_res_val: 5.954e-02\n",
      "adaptive_constant_ics_val: 4.657e+06\n",
      "adaptive_constant_bcs1_val: 1.436e+03\n",
      "adaptive_constant_bcs2_val: 3.956e+05\n",
      "It: 2400, Loss: 8.759e+01, Loss_res: 6.707e-01,  Loss_bcs1: 6.086e-02,  Loss_bcs2: 4.431e-08,  loss_ics_u: 4.911e-02, Loss_ut_ics: 2.872e-08,, Time: 2.17\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.428e-05 ,max_grad_bcs2: 2.422e-07 ,max_grad_ics: 1.146e-08 ,max_grad_res: 5.441e-02  \n",
      "min_grad_bcs1: 2.061e-13 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.935e-16 ,min_grad_res: 7.669e-10  \n",
      "update_res: 1.837e-01\n",
      "update_ics_u_t: 1.170e+00\n",
      "update_bcs1: 1.957e-01\n",
      "update_bcs2: 2.450e+00\n",
      "adaptive_constant_res_val: 5.441e-02\n",
      "adaptive_constant_ics_val: 4.749e+06\n",
      "adaptive_constant_bcs1_val: 1.587e+03\n",
      "adaptive_constant_bcs2_val: 2.247e+05\n",
      "It: 2500, Loss: 1.102e+02, Loss_res: 9.908e-01,  Loss_bcs1: 6.927e-02,  Loss_bcs2: 1.418e-07,  loss_ics_u: 6.010e-02, Loss_ut_ics: 4.050e-08,, Time: 2.29\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.462e-05 ,max_grad_bcs2: 1.536e-08 ,max_grad_ics: 6.318e-09 ,max_grad_res: 6.646e-02  \n",
      "min_grad_bcs1: 5.354e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.403e-15 ,min_grad_res: 2.242e-09  \n",
      "update_res: 7.086e-01\n",
      "update_ics_u_t: 7.525e-01\n",
      "update_bcs1: 6.999e-01\n",
      "update_bcs2: 1.839e+00\n",
      "adaptive_constant_res_val: 6.646e-02\n",
      "adaptive_constant_ics_val: 1.052e+07\n",
      "adaptive_constant_bcs1_val: 1.920e+03\n",
      "adaptive_constant_bcs2_val: 4.327e+06\n",
      "It: 2600, Loss: 5.038e+02, Loss_res: 3.087e-01,  Loss_bcs1: 2.616e-01,  Loss_bcs2: 1.622e-07,  loss_ics_u: 2.505e-01, Loss_ut_ics: 8.921e-08,, Time: 2.13\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.959e-04 ,max_grad_bcs2: 5.237e-08 ,max_grad_ics: 1.464e-08 ,max_grad_res: 1.980e-02  \n",
      "min_grad_bcs1: 9.797e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.065e-14 ,min_grad_res: 2.183e-10  \n",
      "update_res: 3.401e-01\n",
      "update_ics_u_t: 1.035e+00\n",
      "update_bcs1: 1.787e-01\n",
      "update_bcs2: 2.446e+00\n",
      "adaptive_constant_res_val: 1.980e-02\n",
      "adaptive_constant_ics_val: 1.353e+06\n",
      "adaptive_constant_bcs1_val: 1.011e+02\n",
      "adaptive_constant_bcs2_val: 3.781e+05\n",
      "It: 2700, Loss: 2.277e+01, Loss_res: 3.141e-01,  Loss_bcs1: 2.247e-01,  Loss_bcs2: 2.655e-09,  loss_ics_u: 2.137e-01, Loss_ut_ics: 3.817e-08,, Time: 2.19\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.455e-04 ,max_grad_bcs2: 6.754e-10 ,max_grad_ics: 1.072e-08 ,max_grad_res: 1.770e-02  \n",
      "min_grad_bcs1: 6.352e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.232e-16 ,min_grad_res: 3.638e-12  \n",
      "update_res: 2.907e-01\n",
      "update_ics_u_t: 3.650e-01\n",
      "update_bcs1: 2.836e-01\n",
      "update_bcs2: 3.061e+00\n",
      "adaptive_constant_res_val: 1.770e-02\n",
      "adaptive_constant_ics_val: 1.652e+06\n",
      "adaptive_constant_bcs1_val: 1.216e+02\n",
      "adaptive_constant_bcs2_val: 2.621e+07\n",
      "It: 2800, Loss: 8.227e+01, Loss_res: 2.587e-01,  Loss_bcs1: 3.253e-01,  Loss_bcs2: 1.626e-06,  loss_ics_u: 3.253e-01, Loss_ut_ics: 4.224e-08,, Time: 2.16\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.399e-04 ,max_grad_bcs2: 6.829e-09 ,max_grad_ics: 4.495e-09 ,max_grad_res: 1.606e-02  \n",
      "min_grad_bcs1: 1.027e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.984e-16 ,min_grad_res: 2.965e-10  \n",
      "update_res: 2.259e-01\n",
      "update_ics_u_t: 3.616e-01\n",
      "update_bcs1: 2.212e-01\n",
      "update_bcs2: 3.191e+00\n",
      "adaptive_constant_res_val: 1.606e-02\n",
      "adaptive_constant_ics_val: 3.573e+06\n",
      "adaptive_constant_bcs1_val: 1.148e+02\n",
      "adaptive_constant_bcs2_val: 2.352e+06\n",
      "It: 2900, Loss: 3.760e+01, Loss_res: 2.499e-01,  Loss_bcs1: 3.262e-01,  Loss_bcs2: 1.948e-09,  loss_ics_u: 3.262e-01, Loss_ut_ics: 3.995e-08,, Time: 2.15\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.540e-04 ,max_grad_bcs2: 1.730e-10 ,max_grad_ics: 7.165e-09 ,max_grad_res: 1.698e-02  \n",
      "min_grad_bcs1: 1.826e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.637e-15 ,min_grad_res: 3.569e-10  \n",
      "update_res: 2.855e-01\n",
      "update_ics_u_t: 3.234e-01\n",
      "update_bcs1: 2.789e-01\n",
      "update_bcs2: 3.112e+00\n",
      "adaptive_constant_res_val: 1.698e-02\n",
      "adaptive_constant_ics_val: 2.369e+06\n",
      "adaptive_constant_bcs1_val: 1.102e+02\n",
      "adaptive_constant_bcs2_val: 9.816e+07\n",
      "It: 3000, Loss: 4.898e+01, Loss_res: 3.537e-01,  Loss_bcs1: 3.353e-01,  Loss_bcs2: 1.216e-07,  loss_ics_u: 3.352e-01, Loss_ut_ics: 3.798e-08,, Time: 2.18\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.426e-04 ,max_grad_bcs2: 5.129e-09 ,max_grad_ics: 4.127e-09 ,max_grad_res: 2.321e-02  \n",
      "min_grad_bcs1: 2.260e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 7.355e-16 ,min_grad_res: 4.895e-10  \n",
      "update_res: 2.211e-01\n",
      "update_ics_u_t: 4.375e-01\n",
      "update_bcs1: 2.198e-01\n",
      "update_bcs2: 3.122e+00\n",
      "adaptive_constant_res_val: 2.321e-02\n",
      "adaptive_constant_ics_val: 5.625e+06\n",
      "adaptive_constant_bcs1_val: 1.627e+02\n",
      "adaptive_constant_bcs2_val: 4.525e+06\n",
      "It: 3100, Loss: 4.501e+01, Loss_res: 3.094e-01,  Loss_bcs1: 2.761e-01,  Loss_bcs2: 2.887e-09,  loss_ics_u: 2.761e-01, Loss_ut_ics: 9.662e-09,, Time: 2.14\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 9.889e-05 ,max_grad_bcs2: 1.253e-10 ,max_grad_ics: 3.773e-09 ,max_grad_res: 2.225e-02  \n",
      "min_grad_bcs1: 1.838e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.227e-15 ,min_grad_res: 9.486e-10  \n",
      "update_res: 2.967e-01\n",
      "update_ics_u_t: 3.900e-01\n",
      "update_bcs1: 2.915e-01\n",
      "update_bcs2: 3.022e+00\n",
      "adaptive_constant_res_val: 2.225e-02\n",
      "adaptive_constant_ics_val: 5.899e+06\n",
      "adaptive_constant_bcs1_val: 2.250e+02\n",
      "adaptive_constant_bcs2_val: 1.776e+08\n",
      "It: 3200, Loss: 6.853e+01, Loss_res: 3.495e-01,  Loss_bcs1: 3.042e-01,  Loss_bcs2: 7.939e-13,  loss_ics_u: 3.042e-01, Loss_ut_ics: 1.082e-08,, Time: 2.19\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.246e-04 ,max_grad_bcs2: 1.907e-12 ,max_grad_ics: 3.582e-09 ,max_grad_res: 2.851e-02  \n",
      "min_grad_bcs1: 1.941e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 5.707e-16 ,min_grad_res: 3.203e-10  \n",
      "update_res: 2.823e-01\n",
      "update_ics_u_t: 3.429e-01\n",
      "update_bcs1: 2.615e-01\n",
      "update_bcs2: 3.113e+00\n",
      "adaptive_constant_res_val: 2.851e-02\n",
      "adaptive_constant_ics_val: 7.959e+06\n",
      "adaptive_constant_bcs1_val: 2.289e+02\n",
      "adaptive_constant_bcs2_val: 1.495e+10\n",
      "It: 3300, Loss: 4.985e+05, Loss_res: 1.261e-01,  Loss_bcs1: 4.733e-01,  Loss_bcs2: 3.334e-05,  loss_ics_u: 4.732e-01, Loss_ut_ics: 3.086e-06,, Time: 2.12\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.009e-04 ,max_grad_bcs2: 4.794e-11 ,max_grad_ics: 2.318e-07 ,max_grad_res: 5.526e-04  \n",
      "min_grad_bcs1: 4.744e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 3.279e-14 ,min_grad_res: 1.075e-12  \n",
      "update_res: 5.793e-01\n",
      "update_ics_u_t: 8.416e-01\n",
      "update_bcs1: 1.837e-01\n",
      "update_bcs2: 2.395e+00\n",
      "adaptive_constant_res_val: 5.526e-04\n",
      "adaptive_constant_ics_val: 2.384e+03\n",
      "adaptive_constant_bcs1_val: 5.479e+00\n",
      "adaptive_constant_bcs2_val: 1.153e+07\n",
      "It: 3400, Loss: 2.098e+03, Loss_res: 1.422e-01,  Loss_bcs1: 6.567e-01,  Loss_bcs2: 1.817e-04,  loss_ics_u: 6.566e-01, Loss_ut_ics: 2.320e-06,, Time: 2.27\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.031e-04 ,max_grad_bcs2: 1.904e-10 ,max_grad_ics: 2.913e-07 ,max_grad_res: 1.637e-03  \n",
      "min_grad_bcs1: 1.279e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.860e-13 ,min_grad_res: 1.978e-11  \n",
      "update_res: 8.732e-01\n",
      "update_ics_u_t: 1.242e+00\n",
      "update_bcs1: 8.304e-01\n",
      "update_bcs2: 1.055e+00\n",
      "adaptive_constant_res_val: 1.637e-03\n",
      "adaptive_constant_ics_val: 5.618e+03\n",
      "adaptive_constant_bcs1_val: 1.588e+01\n",
      "adaptive_constant_bcs2_val: 8.595e+06\n",
      "It: 3500, Loss: 8.329e+02, Loss_res: 9.303e-02,  Loss_bcs1: 6.036e-01,  Loss_bcs2: 9.579e-05,  loss_ics_u: 6.036e-01, Loss_ut_ics: 3.797e-06,, Time: 2.16\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.132e-04 ,max_grad_bcs2: 1.360e-10 ,max_grad_ics: 4.296e-07 ,max_grad_res: 1.013e-03  \n",
      "min_grad_bcs1: 7.518e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 7.312e-13 ,min_grad_res: 7.788e-11  \n",
      "update_res: 8.939e-01\n",
      "update_ics_u_t: 1.281e+00\n",
      "update_bcs1: 8.746e-01\n",
      "update_bcs2: 9.504e-01\n",
      "adaptive_constant_res_val: 1.013e-03\n",
      "adaptive_constant_ics_val: 2.358e+03\n",
      "adaptive_constant_bcs1_val: 8.948e+00\n",
      "adaptive_constant_bcs2_val: 7.449e+06\n",
      "It: 3600, Loss: 4.106e+02, Loss_res: 1.009e-01,  Loss_bcs1: 5.722e-01,  Loss_bcs2: 5.443e-05,  loss_ics_u: 5.722e-01, Loss_ut_ics: 5.818e-06,, Time: 2.14\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.187e-04 ,max_grad_bcs2: 1.044e-10 ,max_grad_ics: 7.348e-07 ,max_grad_res: 1.188e-03  \n",
      "min_grad_bcs1: 1.654e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.255e-12 ,min_grad_res: 2.228e-11  \n",
      "update_res: 8.971e-01\n",
      "update_ics_u_t: 1.284e+00\n",
      "update_bcs1: 8.809e-01\n",
      "update_bcs2: 9.382e-01\n",
      "adaptive_constant_res_val: 1.188e-03\n",
      "adaptive_constant_ics_val: 1.616e+03\n",
      "adaptive_constant_bcs1_val: 5.431e+00\n",
      "adaptive_constant_bcs2_val: 1.137e+07\n",
      "It: 3700, Loss: 2.644e+02, Loss_res: 1.480e-01,  Loss_bcs1: 7.089e-01,  Loss_bcs2: 2.290e-05,  loss_ics_u: 7.089e-01, Loss_ut_ics: 3.517e-06,, Time: 2.19\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.401e-04 ,max_grad_bcs2: 6.644e-11 ,max_grad_ics: 2.719e-07 ,max_grad_res: 2.893e-03  \n",
      "min_grad_bcs1: 4.494e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 4.308e-14 ,min_grad_res: 9.344e-13  \n",
      "update_res: 8.521e-01\n",
      "update_ics_u_t: 1.344e+00\n",
      "update_bcs1: 8.370e-01\n",
      "update_bcs2: 9.665e-01\n",
      "adaptive_constant_res_val: 2.893e-03\n",
      "adaptive_constant_ics_val: 1.064e+04\n",
      "adaptive_constant_bcs1_val: 2.065e+01\n",
      "adaptive_constant_bcs2_val: 4.354e+07\n",
      "It: 3800, Loss: 3.270e+01, Loss_res: 1.061e-01,  Loss_bcs1: 5.332e-01,  Loss_bcs2: 4.978e-07,  loss_ics_u: 5.332e-01, Loss_ut_ics: 1.771e-06,, Time: 2.21\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.387e-04 ,max_grad_bcs2: 9.811e-12 ,max_grad_ics: 2.467e-07 ,max_grad_res: 1.157e-03  \n",
      "min_grad_bcs1: 3.521e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.882e-14 ,min_grad_res: 2.068e-13  \n",
      "update_res: 7.034e-01\n",
      "update_ics_u_t: 1.015e+00\n",
      "update_bcs1: 6.802e-01\n",
      "update_bcs2: 1.601e+00\n",
      "adaptive_constant_res_val: 1.157e-03\n",
      "adaptive_constant_ics_val: 4.689e+03\n",
      "adaptive_constant_bcs1_val: 8.340e+00\n",
      "adaptive_constant_bcs2_val: 1.179e+08\n",
      "It: 3900, Loss: 4.389e+00, Loss_res: 1.378e-01,  Loss_bcs1: 5.242e-01,  Loss_bcs2: 8.951e-12,  loss_ics_u: 5.242e-01, Loss_ut_ics: 3.364e-06,, Time: 2.15\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.444e-04 ,max_grad_bcs2: 3.846e-14 ,max_grad_ics: 4.734e-07 ,max_grad_res: 2.990e-03  \n",
      "min_grad_bcs1: 2.907e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 7.236e-14 ,min_grad_res: 1.563e-12  \n",
      "update_res: 3.853e-01\n",
      "update_ics_u_t: 5.575e-01\n",
      "update_bcs1: 3.657e-01\n",
      "update_bcs2: 2.692e+00\n",
      "adaptive_constant_res_val: 2.990e-03\n",
      "adaptive_constant_ics_val: 6.317e+03\n",
      "adaptive_constant_bcs1_val: 2.071e+01\n",
      "adaptive_constant_bcs2_val: 7.776e+10\n",
      "It: 4000, Loss: 1.216e+01, Loss_res: 1.127e-01,  Loss_bcs1: 5.859e-01,  Loss_bcs2: 2.239e-13,  loss_ics_u: 5.859e-01, Loss_ut_ics: 1.604e-06,, Time: 2.15\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.240e-04 ,max_grad_bcs2: 1.309e-15 ,max_grad_ics: 1.325e-07 ,max_grad_res: 9.746e-04  \n",
      "min_grad_bcs1: 6.839e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.039e-14 ,min_grad_res: 7.250e-12  \n",
      "update_res: 4.819e-01\n",
      "update_ics_u_t: 7.052e-01\n",
      "update_bcs1: 4.577e-01\n",
      "update_bcs2: 2.355e+00\n",
      "adaptive_constant_res_val: 9.746e-04\n",
      "adaptive_constant_ics_val: 7.358e+03\n",
      "adaptive_constant_bcs1_val: 7.857e+00\n",
      "adaptive_constant_bcs2_val: 7.445e+11\n",
      "It: 4100, Loss: 2.951e+02, Loss_res: 1.135e-01,  Loss_bcs1: 6.479e-01,  Loss_bcs2: 2.824e-10,  loss_ics_u: 6.479e-01, Loss_ut_ics: 1.084e-02,, Time: 2.11\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.171e-04 ,max_grad_bcs2: 1.658e-11 ,max_grad_ics: 7.139e-05 ,max_grad_res: 1.253e-03  \n",
      "min_grad_bcs1: 1.247e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.390e-11 ,min_grad_res: 1.558e-11  \n",
      "update_res: 4.270e-01\n",
      "update_ics_u_t: 2.951e-01\n",
      "update_bcs1: 2.557e-01\n",
      "update_bcs2: 3.022e+00\n",
      "adaptive_constant_res_val: 1.253e-03\n",
      "adaptive_constant_ics_val: 1.756e+01\n",
      "adaptive_constant_bcs1_val: 5.772e+00\n",
      "adaptive_constant_bcs2_val: 7.560e+07\n",
      "It: 4200, Loss: 1.090e+05, Loss_res: 1.433e-01,  Loss_bcs1: 6.001e-01,  Loss_bcs2: 1.442e-03,  loss_ics_u: 5.983e-01, Loss_ut_ics: 7.251e-03,, Time: 2.19\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.301e-04 ,max_grad_bcs2: 1.578e-10 ,max_grad_ics: 9.103e-05 ,max_grad_res: 3.181e-03  \n",
      "min_grad_bcs1: 6.352e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.464e-10 ,min_grad_res: 1.076e-11  \n",
      "update_res: 9.100e-01\n",
      "update_ics_u_t: 1.148e+00\n",
      "update_bcs1: 8.788e-01\n",
      "update_bcs2: 1.063e+00\n",
      "adaptive_constant_res_val: 3.181e-03\n",
      "adaptive_constant_ics_val: 3.495e+01\n",
      "adaptive_constant_bcs1_val: 9.639e+00\n",
      "adaptive_constant_bcs2_val: 2.016e+07\n",
      "It: 4300, Loss: 2.798e+04, Loss_res: 1.216e-01,  Loss_bcs1: 5.810e-01,  Loss_bcs2: 1.388e-03,  loss_ics_u: 5.792e-01, Loss_ut_ics: 8.538e-03,, Time: 2.13\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.461e-04 ,max_grad_bcs2: 1.553e-10 ,max_grad_ics: 1.205e-04 ,max_grad_res: 1.585e-03  \n",
      "min_grad_bcs1: 9.316e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 5.593e-11 ,min_grad_res: 1.796e-11  \n",
      "update_res: 9.799e-01\n",
      "update_ics_u_t: 1.192e+00\n",
      "update_bcs1: 9.555e-01\n",
      "update_bcs2: 8.722e-01\n",
      "adaptive_constant_res_val: 1.585e-03\n",
      "adaptive_constant_ics_val: 1.315e+01\n",
      "adaptive_constant_bcs1_val: 6.440e+00\n",
      "adaptive_constant_bcs2_val: 1.021e+07\n",
      "It: 4400, Loss: 1.392e+04, Loss_res: 1.128e-01,  Loss_bcs1: 4.737e-01,  Loss_bcs2: 1.364e-03,  loss_ics_u: 4.720e-01, Loss_ut_ics: 6.951e-03,, Time: 2.31\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 4.017e-04 ,max_grad_bcs2: 1.551e-10 ,max_grad_ics: 1.224e-04 ,max_grad_res: 1.430e-03  \n",
      "min_grad_bcs1: 3.107e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.517e-11 ,min_grad_res: 1.989e-11  \n",
      "update_res: 9.675e-01\n",
      "update_ics_u_t: 1.231e+00\n",
      "update_bcs1: 9.446e-01\n",
      "update_bcs2: 8.570e-01\n",
      "adaptive_constant_res_val: 1.430e-03\n",
      "adaptive_constant_ics_val: 1.168e+01\n",
      "adaptive_constant_bcs1_val: 3.558e+00\n",
      "adaptive_constant_bcs2_val: 9.214e+06\n",
      "It: 4500, Loss: 1.238e+04, Loss_res: 1.463e-01,  Loss_bcs1: 5.266e-01,  Loss_bcs2: 1.343e-03,  loss_ics_u: 5.249e-01, Loss_ut_ics: 1.220e-02,, Time: 2.27\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.844e-04 ,max_grad_bcs2: 1.533e-10 ,max_grad_ics: 1.278e-04 ,max_grad_res: 1.449e-03  \n",
      "min_grad_bcs1: 1.897e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.260e-10 ,min_grad_res: 9.038e-12  \n",
      "update_res: 9.851e-01\n",
      "update_ics_u_t: 1.180e+00\n",
      "update_bcs1: 9.584e-01\n",
      "update_bcs2: 8.764e-01\n",
      "adaptive_constant_res_val: 1.449e-03\n",
      "adaptive_constant_ics_val: 1.134e+01\n",
      "adaptive_constant_bcs1_val: 5.095e+00\n",
      "adaptive_constant_bcs2_val: 9.450e+06\n",
      "It: 4600, Loss: 1.250e+04, Loss_res: 1.180e-01,  Loss_bcs1: 4.879e-01,  Loss_bcs2: 1.322e-03,  loss_ics_u: 4.863e-01, Loss_ut_ics: 1.024e-02,, Time: 2.22\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.729e-04 ,max_grad_bcs2: 1.510e-10 ,max_grad_ics: 8.390e-05 ,max_grad_res: 3.131e-03  \n",
      "min_grad_bcs1: 3.347e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 7.276e-12 ,min_grad_res: 2.574e-11  \n",
      "update_res: 9.735e-01\n",
      "update_ics_u_t: 1.234e+00\n",
      "update_bcs1: 9.384e-01\n",
      "update_bcs2: 8.541e-01\n",
      "adaptive_constant_res_val: 3.131e-03\n",
      "adaptive_constant_ics_val: 3.732e+01\n",
      "adaptive_constant_bcs1_val: 1.147e+01\n",
      "adaptive_constant_bcs2_val: 2.074e+07\n",
      "It: 4700, Loss: 2.649e+04, Loss_res: 9.132e-02,  Loss_bcs1: 5.637e-01,  Loss_bcs2: 1.277e-03,  loss_ics_u: 5.621e-01, Loss_ut_ics: 1.084e-02,, Time: 2.18\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.253e-04 ,max_grad_bcs2: 1.490e-10 ,max_grad_ics: 1.018e-04 ,max_grad_res: 2.146e-03  \n",
      "min_grad_bcs1: 1.475e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.105e-10 ,min_grad_res: 5.821e-12  \n",
      "update_res: 9.690e-01\n",
      "update_ics_u_t: 1.191e+00\n",
      "update_bcs1: 9.652e-01\n",
      "update_bcs2: 8.752e-01\n",
      "adaptive_constant_res_val: 2.146e-03\n",
      "adaptive_constant_ics_val: 2.107e+01\n",
      "adaptive_constant_bcs1_val: 9.526e+00\n",
      "adaptive_constant_bcs2_val: 1.441e+07\n",
      "It: 4800, Loss: 1.790e+04, Loss_res: 1.101e-01,  Loss_bcs1: 6.648e-01,  Loss_bcs2: 1.242e-03,  loss_ics_u: 6.632e-01, Loss_ut_ics: 8.507e-03,, Time: 2.16\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.605e-04 ,max_grad_bcs2: 1.485e-10 ,max_grad_ics: 5.655e-05 ,max_grad_res: 2.298e-03  \n",
      "min_grad_bcs1: 1.390e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.175e-11 ,min_grad_res: 3.723e-12  \n",
      "update_res: 9.962e-01\n",
      "update_ics_u_t: 1.181e+00\n",
      "update_bcs1: 9.621e-01\n",
      "update_bcs2: 8.604e-01\n",
      "adaptive_constant_res_val: 2.298e-03\n",
      "adaptive_constant_ics_val: 4.065e+01\n",
      "adaptive_constant_bcs1_val: 1.432e+01\n",
      "adaptive_constant_bcs2_val: 1.548e+07\n",
      "It: 4900, Loss: 1.869e+04, Loss_res: 1.047e-01,  Loss_bcs1: 5.660e-01,  Loss_bcs2: 1.206e-03,  loss_ics_u: 5.645e-01, Loss_ut_ics: 5.625e-03,, Time: 2.17\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.255e-04 ,max_grad_bcs2: 1.456e-10 ,max_grad_ics: 7.350e-05 ,max_grad_res: 5.332e-04  \n",
      "min_grad_bcs1: 9.367e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.819e-12 ,min_grad_res: 2.160e-12  \n",
      "update_res: 9.609e-01\n",
      "update_ics_u_t: 1.262e+00\n",
      "update_bcs1: 9.335e-01\n",
      "update_bcs2: 8.436e-01\n",
      "adaptive_constant_res_val: 5.332e-04\n",
      "adaptive_constant_ics_val: 7.254e+00\n",
      "adaptive_constant_bcs1_val: 2.364e+00\n",
      "adaptive_constant_bcs2_val: 3.663e+06\n",
      "It: 5000, Loss: 4.380e+03, Loss_res: 1.260e-01,  Loss_bcs1: 6.683e-01,  Loss_bcs2: 1.195e-03,  loss_ics_u: 6.668e-01, Loss_ut_ics: 1.273e-02,, Time: 2.14\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.060e-04 ,max_grad_bcs2: 1.455e-10 ,max_grad_ics: 1.231e-04 ,max_grad_res: 1.608e-03  \n",
      "min_grad_bcs1: 7.223e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 3.911e-11 ,min_grad_res: 2.585e-11  \n",
      "update_res: 9.869e-01\n",
      "update_ics_u_t: 1.208e+00\n",
      "update_bcs1: 9.436e-01\n",
      "update_bcs2: 8.618e-01\n",
      "adaptive_constant_res_val: 1.608e-03\n",
      "adaptive_constant_ics_val: 1.306e+01\n",
      "adaptive_constant_bcs1_val: 5.255e+00\n",
      "adaptive_constant_bcs2_val: 1.105e+07\n",
      "It: 5100, Loss: 1.293e+04, Loss_res: 1.332e-01,  Loss_bcs1: 5.375e-01,  Loss_bcs2: 1.170e-03,  loss_ics_u: 5.360e-01, Loss_ut_ics: 8.056e-03,, Time: 2.19\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.377e-04 ,max_grad_bcs2: 1.433e-10 ,max_grad_ics: 5.021e-05 ,max_grad_res: 1.457e-03  \n",
      "min_grad_bcs1: 7.887e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 6.620e-11 ,min_grad_res: 1.751e-11  \n",
      "update_res: 9.632e-01\n",
      "update_ics_u_t: 1.230e+00\n",
      "update_bcs1: 9.478e-01\n",
      "update_bcs2: 8.586e-01\n",
      "adaptive_constant_res_val: 1.457e-03\n",
      "adaptive_constant_ics_val: 2.902e+01\n",
      "adaptive_constant_bcs1_val: 1.059e+01\n",
      "adaptive_constant_bcs2_val: 1.017e+07\n",
      "It: 5200, Loss: 1.165e+04, Loss_res: 1.196e-01,  Loss_bcs1: 5.297e-01,  Loss_bcs2: 1.145e-03,  loss_ics_u: 5.282e-01, Loss_ut_ics: 9.107e-03,, Time: 2.16\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.391e-04 ,max_grad_bcs2: 1.427e-10 ,max_grad_ics: 9.476e-05 ,max_grad_res: 2.044e-03  \n",
      "min_grad_bcs1: 2.373e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 9.614e-11 ,min_grad_res: 5.912e-12  \n",
      "update_res: 9.681e-01\n",
      "update_ics_u_t: 1.239e+00\n",
      "update_bcs1: 9.409e-01\n",
      "update_bcs2: 8.517e-01\n",
      "adaptive_constant_res_val: 2.044e-03\n",
      "adaptive_constant_ics_val: 2.157e+01\n",
      "adaptive_constant_bcs1_val: 8.548e+00\n",
      "adaptive_constant_bcs2_val: 1.432e+07\n",
      "It: 5300, Loss: 1.590e+04, Loss_res: 1.125e-01,  Loss_bcs1: 5.788e-01,  Loss_bcs2: 1.110e-03,  loss_ics_u: 5.773e-01, Loss_ut_ics: 5.802e-03,, Time: 2.24\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.324e-04 ,max_grad_bcs2: 1.400e-10 ,max_grad_ics: 6.243e-05 ,max_grad_res: 1.299e-03  \n",
      "min_grad_bcs1: 2.534e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.768e-10 ,min_grad_res: 9.610e-12  \n",
      "update_res: 9.670e-01\n",
      "update_ics_u_t: 1.220e+00\n",
      "update_bcs1: 9.544e-01\n",
      "update_bcs2: 8.581e-01\n",
      "adaptive_constant_res_val: 1.299e-03\n",
      "adaptive_constant_ics_val: 2.080e+01\n",
      "adaptive_constant_bcs1_val: 5.587e+00\n",
      "adaptive_constant_bcs2_val: 9.278e+06\n",
      "It: 5400, Loss: 1.007e+04, Loss_res: 1.088e-01,  Loss_bcs1: 6.137e-01,  Loss_bcs2: 1.085e-03,  loss_ics_u: 6.123e-01, Loss_ut_ics: 8.872e-03,, Time: 2.17\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.366e-04 ,max_grad_bcs2: 1.377e-10 ,max_grad_ics: 8.054e-05 ,max_grad_res: 6.170e-04  \n",
      "min_grad_bcs1: 5.159e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 6.571e-11 ,min_grad_res: 1.023e-12  \n",
      "update_res: 9.745e-01\n",
      "update_ics_u_t: 1.194e+00\n",
      "update_bcs1: 9.622e-01\n",
      "update_bcs2: 8.698e-01\n",
      "adaptive_constant_res_val: 6.170e-04\n",
      "adaptive_constant_ics_val: 7.661e+00\n",
      "adaptive_constant_bcs1_val: 2.607e+00\n",
      "adaptive_constant_bcs2_val: 4.480e+06\n",
      "It: 5500, Loss: 4.806e+03, Loss_res: 1.088e-01,  Loss_bcs1: 5.833e-01,  Loss_bcs2: 1.072e-03,  loss_ics_u: 5.819e-01, Loss_ut_ics: 9.962e-03,, Time: 2.15\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.906e-04 ,max_grad_bcs2: 1.362e-10 ,max_grad_ics: 1.395e-04 ,max_grad_res: 1.334e-03  \n",
      "min_grad_bcs1: 4.223e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 5.002e-11 ,min_grad_res: 2.274e-13  \n",
      "update_res: 9.957e-01\n",
      "update_ics_u_t: 1.199e+00\n",
      "update_bcs1: 9.444e-01\n",
      "update_bcs2: 8.610e-01\n",
      "adaptive_constant_res_val: 1.334e-03\n",
      "adaptive_constant_ics_val: 9.563e+00\n",
      "adaptive_constant_bcs1_val: 4.589e+00\n",
      "adaptive_constant_bcs2_val: 9.788e+06\n",
      "It: 5600, Loss: 1.026e+04, Loss_res: 1.180e-01,  Loss_bcs1: 5.987e-01,  Loss_bcs2: 1.048e-03,  loss_ics_u: 5.973e-01, Loss_ut_ics: 5.772e-03,, Time: 2.21\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.872e-04 ,max_grad_bcs2: 1.349e-10 ,max_grad_ics: 3.558e-05 ,max_grad_res: 2.460e-03  \n",
      "min_grad_bcs1: 8.514e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 4.827e-11 ,min_grad_res: 6.321e-11  \n",
      "update_res: 9.759e-01\n",
      "update_ics_u_t: 1.206e+00\n",
      "update_bcs1: 9.575e-01\n",
      "update_bcs2: 8.607e-01\n",
      "adaptive_constant_res_val: 2.460e-03\n",
      "adaptive_constant_ics_val: 6.914e+01\n",
      "adaptive_constant_bcs1_val: 1.314e+01\n",
      "adaptive_constant_bcs2_val: 1.824e+07\n",
      "It: 5700, Loss: 1.827e+04, Loss_res: 1.203e-01,  Loss_bcs1: 5.639e-01,  Loss_bcs2: 1.001e-03,  loss_ics_u: 5.626e-01, Loss_ut_ics: 8.318e-03,, Time: 2.19\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.963e-04 ,max_grad_bcs2: 1.325e-10 ,max_grad_ics: 9.790e-05 ,max_grad_res: 1.384e-03  \n",
      "min_grad_bcs1: 1.858e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.446e-10 ,min_grad_res: 6.480e-12  \n",
      "update_res: 9.609e-01\n",
      "update_ics_u_t: 1.237e+00\n",
      "update_bcs1: 9.363e-01\n",
      "update_bcs2: 8.659e-01\n",
      "adaptive_constant_res_val: 1.384e-03\n",
      "adaptive_constant_ics_val: 1.414e+01\n",
      "adaptive_constant_bcs1_val: 4.670e+00\n",
      "adaptive_constant_bcs2_val: 1.044e+07\n",
      "It: 5800, Loss: 1.014e+04, Loss_res: 1.023e-01,  Loss_bcs1: 4.329e-01,  Loss_bcs2: 9.711e-04,  loss_ics_u: 4.317e-01, Loss_ut_ics: 9.417e-03,, Time: 2.16\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.857e-04 ,max_grad_bcs2: 1.308e-10 ,max_grad_ics: 1.022e-04 ,max_grad_res: 1.460e-03  \n",
      "min_grad_bcs1: 5.365e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 5.457e-12 ,min_grad_res: 3.846e-12  \n",
      "update_res: 9.867e-01\n",
      "update_ics_u_t: 1.196e+00\n",
      "update_bcs1: 9.550e-01\n",
      "update_bcs2: 8.625e-01\n",
      "adaptive_constant_res_val: 1.460e-03\n",
      "adaptive_constant_ics_val: 1.429e+01\n",
      "adaptive_constant_bcs1_val: 7.866e+00\n",
      "adaptive_constant_bcs2_val: 1.117e+07\n",
      "It: 5900, Loss: 1.051e+04, Loss_res: 1.229e-01,  Loss_bcs1: 4.728e-01,  Loss_bcs2: 9.411e-04,  loss_ics_u: 4.716e-01, Loss_ut_ics: 5.651e-03,, Time: 2.14\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.259e-04 ,max_grad_bcs2: 1.281e-10 ,max_grad_ics: 5.970e-05 ,max_grad_res: 1.159e-03  \n",
      "min_grad_bcs1: 3.850e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.910e-11 ,min_grad_res: 2.359e-12  \n",
      "update_res: 9.762e-01\n",
      "update_ics_u_t: 1.209e+00\n",
      "update_bcs1: 9.560e-01\n",
      "update_bcs2: 8.588e-01\n",
      "adaptive_constant_res_val: 1.159e-03\n",
      "adaptive_constant_ics_val: 1.941e+01\n",
      "adaptive_constant_bcs1_val: 5.130e+00\n",
      "adaptive_constant_bcs2_val: 9.045e+06\n",
      "It: 6000, Loss: 8.286e+03, Loss_res: 9.356e-02,  Loss_bcs1: 5.108e-01,  Loss_bcs2: 9.158e-04,  loss_ics_u: 5.096e-01, Loss_ut_ics: 5.195e-03,, Time: 2.27\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.628e-04 ,max_grad_bcs2: 1.285e-10 ,max_grad_ics: 7.776e-05 ,max_grad_res: 1.263e-03  \n",
      "min_grad_bcs1: 1.536e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 3.365e-11 ,min_grad_res: 7.049e-12  \n",
      "update_res: 9.962e-01\n",
      "update_ics_u_t: 1.181e+00\n",
      "update_bcs1: 9.578e-01\n",
      "update_bcs2: 8.647e-01\n",
      "adaptive_constant_res_val: 1.263e-03\n",
      "adaptive_constant_ics_val: 1.624e+01\n",
      "adaptive_constant_bcs1_val: 7.758e+00\n",
      "adaptive_constant_bcs2_val: 9.831e+06\n",
      "It: 6100, Loss: 8.743e+03, Loss_res: 1.753e-01,  Loss_bcs1: 5.793e-01,  Loss_bcs2: 8.888e-04,  loss_ics_u: 5.782e-01, Loss_ut_ics: 6.528e-03,, Time: 2.13\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.180e-04 ,max_grad_bcs2: 1.241e-10 ,max_grad_ics: 5.786e-05 ,max_grad_res: 3.215e-03  \n",
      "min_grad_bcs1: 1.605e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.967e-11 ,min_grad_res: 5.798e-12  \n",
      "update_res: 9.821e-01\n",
      "update_ics_u_t: 1.198e+00\n",
      "update_bcs1: 9.549e-01\n",
      "update_bcs2: 8.648e-01\n",
      "adaptive_constant_res_val: 3.215e-03\n",
      "adaptive_constant_ics_val: 5.556e+01\n",
      "adaptive_constant_bcs1_val: 2.725e+01\n",
      "adaptive_constant_bcs2_val: 2.590e+07\n",
      "It: 6200, Loss: 2.132e+04, Loss_res: 1.585e-01,  Loss_bcs1: 5.180e-01,  Loss_bcs2: 8.225e-04,  loss_ics_u: 5.169e-01, Loss_ut_ics: 1.076e-02,, Time: 2.24\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.973e-04 ,max_grad_bcs2: 1.209e-10 ,max_grad_ics: 8.942e-05 ,max_grad_res: 1.960e-03  \n",
      "min_grad_bcs1: 1.665e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 4.056e-10 ,min_grad_res: 4.178e-12  \n",
      "update_res: 9.703e-01\n",
      "update_ics_u_t: 1.216e+00\n",
      "update_bcs1: 9.461e-01\n",
      "update_bcs2: 8.681e-01\n",
      "adaptive_constant_res_val: 1.960e-03\n",
      "adaptive_constant_ics_val: 2.192e+01\n",
      "adaptive_constant_bcs1_val: 9.932e+00\n",
      "adaptive_constant_bcs2_val: 1.621e+07\n",
      "It: 6300, Loss: 1.260e+04, Loss_res: 1.550e-01,  Loss_bcs1: 6.053e-01,  Loss_bcs2: 7.773e-04,  loss_ics_u: 6.042e-01, Loss_ut_ics: 6.476e-03,, Time: 2.20\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.487e-04 ,max_grad_bcs2: 1.174e-10 ,max_grad_ics: 4.600e-05 ,max_grad_res: 2.269e-03  \n",
      "min_grad_bcs1: 9.157e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 7.912e-12 ,min_grad_res: 8.100e-13  \n",
      "update_res: 9.762e-01\n",
      "update_ics_u_t: 1.209e+00\n",
      "update_bcs1: 9.473e-01\n",
      "update_bcs2: 8.679e-01\n",
      "adaptive_constant_res_val: 2.269e-03\n",
      "adaptive_constant_ics_val: 4.931e+01\n",
      "adaptive_constant_bcs1_val: 1.525e+01\n",
      "adaptive_constant_bcs2_val: 1.933e+07\n",
      "It: 6400, Loss: 1.408e+04, Loss_res: 1.395e-01,  Loss_bcs1: 5.956e-01,  Loss_bcs2: 7.281e-04,  loss_ics_u: 5.946e-01, Loss_ut_ics: 5.927e-03,, Time: 2.18\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.342e-04 ,max_grad_bcs2: 1.143e-10 ,max_grad_ics: 6.214e-05 ,max_grad_res: 1.709e-03  \n",
      "min_grad_bcs1: 2.203e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 6.366e-11 ,min_grad_res: 3.865e-12  \n",
      "update_res: 9.793e-01\n",
      "update_ics_u_t: 1.172e+00\n",
      "update_bcs1: 9.710e-01\n",
      "update_bcs2: 8.773e-01\n",
      "adaptive_constant_res_val: 1.709e-03\n",
      "adaptive_constant_ics_val: 2.750e+01\n",
      "adaptive_constant_bcs1_val: 1.273e+01\n",
      "adaptive_constant_bcs2_val: 1.495e+07\n",
      "It: 6500, Loss: 1.031e+04, Loss_res: 1.167e-01,  Loss_bcs1: 5.328e-01,  Loss_bcs2: 6.888e-04,  loss_ics_u: 5.319e-01, Loss_ut_ics: 9.347e-03,, Time: 2.19\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.235e-04 ,max_grad_bcs2: 1.109e-10 ,max_grad_ics: 6.957e-05 ,max_grad_res: 1.134e-03  \n",
      "min_grad_bcs1: 2.703e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.008e-12 ,min_grad_res: 9.819e-12  \n",
      "update_res: 9.988e-01\n",
      "update_ics_u_t: 1.183e+00\n",
      "update_bcs1: 9.443e-01\n",
      "update_bcs2: 8.737e-01\n",
      "adaptive_constant_res_val: 1.134e-03\n",
      "adaptive_constant_ics_val: 1.631e+01\n",
      "adaptive_constant_bcs1_val: 5.077e+00\n",
      "adaptive_constant_bcs2_val: 1.023e+07\n",
      "It: 6600, Loss: 6.772e+03, Loss_res: 1.234e-01,  Loss_bcs1: 6.174e-01,  Loss_bcs2: 6.618e-04,  loss_ics_u: 6.165e-01, Loss_ut_ics: 7.807e-03,, Time: 2.14\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.058e-04 ,max_grad_bcs2: 1.085e-10 ,max_grad_ics: 9.724e-05 ,max_grad_res: 2.540e-03  \n",
      "min_grad_bcs1: 3.479e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.619e-10 ,min_grad_res: 2.785e-12  \n",
      "update_res: 9.786e-01\n",
      "update_ics_u_t: 1.188e+00\n",
      "update_bcs1: 9.578e-01\n",
      "update_bcs2: 8.754e-01\n",
      "adaptive_constant_res_val: 2.540e-03\n",
      "adaptive_constant_ics_val: 2.612e+01\n",
      "adaptive_constant_bcs1_val: 8.306e+00\n",
      "adaptive_constant_bcs2_val: 2.342e+07\n",
      "It: 6700, Loss: 1.422e+04, Loss_res: 1.159e-01,  Loss_bcs1: 5.683e-01,  Loss_bcs2: 6.071e-04,  loss_ics_u: 5.675e-01, Loss_ut_ics: 6.750e-03,, Time: 2.22\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.842e-04 ,max_grad_bcs2: 1.031e-10 ,max_grad_ics: 6.453e-05 ,max_grad_res: 5.442e-03  \n",
      "min_grad_bcs1: 5.298e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.672e-12 ,min_grad_res: 5.258e-12  \n",
      "update_res: 9.779e-01\n",
      "update_ics_u_t: 1.193e+00\n",
      "update_bcs1: 9.516e-01\n",
      "update_bcs2: 8.771e-01\n",
      "adaptive_constant_res_val: 5.442e-03\n",
      "adaptive_constant_ics_val: 8.434e+01\n",
      "adaptive_constant_bcs1_val: 1.915e+01\n",
      "adaptive_constant_bcs2_val: 5.280e+07\n",
      "It: 6800, Loss: 2.616e+04, Loss_res: 1.449e-01,  Loss_bcs1: 6.262e-01,  Loss_bcs2: 4.953e-04,  loss_ics_u: 6.255e-01, Loss_ut_ics: 4.692e-03,, Time: 2.15\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.370e-04 ,max_grad_bcs2: 9.307e-11 ,max_grad_ics: 5.164e-05 ,max_grad_res: 2.118e-03  \n",
      "min_grad_bcs1: 4.471e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 3.991e-11 ,min_grad_res: 3.951e-11  \n",
      "update_res: 9.775e-01\n",
      "update_ics_u_t: 1.188e+00\n",
      "update_bcs1: 9.302e-01\n",
      "update_bcs2: 9.047e-01\n",
      "adaptive_constant_res_val: 2.118e-03\n",
      "adaptive_constant_ics_val: 4.101e+01\n",
      "adaptive_constant_bcs1_val: 8.937e+00\n",
      "adaptive_constant_bcs2_val: 2.275e+07\n",
      "It: 6900, Loss: 1.011e+04, Loss_res: 1.405e-01,  Loss_bcs1: 5.965e-01,  Loss_bcs2: 4.442e-04,  loss_ics_u: 5.959e-01, Loss_ut_ics: 5.629e-03,, Time: 2.19\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.134e-04 ,max_grad_bcs2: 8.859e-11 ,max_grad_ics: 6.466e-05 ,max_grad_res: 7.069e-04  \n",
      "min_grad_bcs1: 1.713e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 3.229e-11 ,min_grad_res: 1.164e-11  \n",
      "update_res: 9.635e-01\n",
      "update_ics_u_t: 1.233e+00\n",
      "update_bcs1: 9.342e-01\n",
      "update_bcs2: 8.689e-01\n",
      "adaptive_constant_res_val: 7.069e-04\n",
      "adaptive_constant_ics_val: 1.093e+01\n",
      "adaptive_constant_bcs1_val: 2.255e+00\n",
      "adaptive_constant_bcs2_val: 7.979e+06\n",
      "It: 7000, Loss: 3.402e+03, Loss_res: 8.511e-02,  Loss_bcs1: 5.274e-01,  Loss_bcs2: 4.262e-04,  loss_ics_u: 5.268e-01, Loss_ut_ics: 1.303e-02,, Time: 2.19\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.450e-04 ,max_grad_bcs2: 8.669e-11 ,max_grad_ics: 7.289e-05 ,max_grad_res: 1.398e-03  \n",
      "min_grad_bcs1: 4.163e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 9.422e-12 ,min_grad_res: 1.046e-11  \n",
      "update_res: 9.903e-01\n",
      "update_ics_u_t: 1.209e+00\n",
      "update_bcs1: 9.394e-01\n",
      "update_bcs2: 8.618e-01\n",
      "adaptive_constant_res_val: 1.398e-03\n",
      "adaptive_constant_ics_val: 1.917e+01\n",
      "adaptive_constant_bcs1_val: 5.706e+00\n",
      "adaptive_constant_bcs2_val: 1.612e+07\n",
      "It: 7100, Loss: 6.410e+03, Loss_res: 1.216e-01,  Loss_bcs1: 5.787e-01,  Loss_bcs2: 3.973e-04,  loss_ics_u: 5.781e-01, Loss_ut_ics: 8.712e-03,, Time: 2.16\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.857e-04 ,max_grad_bcs2: 8.398e-11 ,max_grad_ics: 8.383e-05 ,max_grad_res: 2.492e-03  \n",
      "min_grad_bcs1: 2.934e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.029e-11 ,min_grad_res: 2.910e-11  \n",
      "update_res: 9.836e-01\n",
      "update_ics_u_t: 1.183e+00\n",
      "update_bcs1: 9.556e-01\n",
      "update_bcs2: 8.783e-01\n",
      "adaptive_constant_res_val: 2.492e-03\n",
      "adaptive_constant_ics_val: 2.972e+01\n",
      "adaptive_constant_bcs1_val: 6.460e+00\n",
      "adaptive_constant_bcs2_val: 2.967e+07\n",
      "It: 7200, Loss: 1.030e+04, Loss_res: 1.137e-01,  Loss_bcs1: 5.041e-01,  Loss_bcs2: 3.470e-04,  loss_ics_u: 5.036e-01, Loss_ut_ics: 7.343e-03,, Time: 2.23\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.821e-04 ,max_grad_bcs2: 7.795e-11 ,max_grad_ics: 5.461e-05 ,max_grad_res: 1.164e-03  \n",
      "min_grad_bcs1: 2.105e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 6.054e-11 ,min_grad_res: 4.955e-13  \n",
      "update_res: 9.557e-01\n",
      "update_ics_u_t: 1.235e+00\n",
      "update_bcs1: 9.305e-01\n",
      "update_bcs2: 8.788e-01\n",
      "adaptive_constant_res_val: 1.164e-03\n",
      "adaptive_constant_ics_val: 2.132e+01\n",
      "adaptive_constant_bcs1_val: 6.391e+00\n",
      "adaptive_constant_bcs2_val: 1.493e+07\n",
      "It: 7300, Loss: 4.781e+03, Loss_res: 1.251e-01,  Loss_bcs1: 5.386e-01,  Loss_bcs2: 3.199e-04,  loss_ics_u: 5.381e-01, Loss_ut_ics: 7.834e-03,, Time: 2.31\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.450e-04 ,max_grad_bcs2: 7.520e-11 ,max_grad_ics: 5.747e-05 ,max_grad_res: 2.377e-03  \n",
      "min_grad_bcs1: 6.097e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 7.270e-12 ,min_grad_res: 3.005e-13  \n",
      "update_res: 9.717e-01\n",
      "update_ics_u_t: 1.203e+00\n",
      "update_bcs1: 9.458e-01\n",
      "update_bcs2: 8.799e-01\n",
      "adaptive_constant_res_val: 2.377e-03\n",
      "adaptive_constant_ics_val: 4.137e+01\n",
      "adaptive_constant_bcs1_val: 9.704e+00\n",
      "adaptive_constant_bcs2_val: 3.162e+07\n",
      "It: 7400, Loss: 8.665e+03, Loss_res: 1.222e-01,  Loss_bcs1: 5.006e-01,  Loss_bcs2: 2.739e-04,  loss_ics_u: 5.001e-01, Loss_ut_ics: 6.253e-03,, Time: 2.27\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.140e-04 ,max_grad_bcs2: 7.013e-11 ,max_grad_ics: 7.131e-05 ,max_grad_res: 1.066e-03  \n",
      "min_grad_bcs1: 2.788e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 4.600e-11 ,min_grad_res: 6.223e-12  \n",
      "update_res: 9.883e-01\n",
      "update_ics_u_t: 1.175e+00\n",
      "update_bcs1: 9.421e-01\n",
      "update_bcs2: 8.941e-01\n",
      "adaptive_constant_res_val: 1.066e-03\n",
      "adaptive_constant_ics_val: 1.495e+01\n",
      "adaptive_constant_bcs1_val: 4.981e+00\n",
      "adaptive_constant_bcs2_val: 1.520e+07\n",
      "It: 7500, Loss: 3.806e+03, Loss_res: 1.276e-01,  Loss_bcs1: 5.983e-01,  Loss_bcs2: 2.502e-04,  loss_ics_u: 5.979e-01, Loss_ut_ics: 1.012e-02,, Time: 2.13\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.304e-04 ,max_grad_bcs2: 6.707e-11 ,max_grad_ics: 9.865e-05 ,max_grad_res: 2.033e-03  \n",
      "min_grad_bcs1: 1.176e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.046e-11 ,min_grad_res: 7.958e-13  \n",
      "update_res: 9.791e-01\n",
      "update_ics_u_t: 1.216e+00\n",
      "update_bcs1: 9.367e-01\n",
      "update_bcs2: 8.686e-01\n",
      "adaptive_constant_res_val: 2.033e-03\n",
      "adaptive_constant_ics_val: 2.061e+01\n",
      "adaptive_constant_bcs1_val: 6.155e+00\n",
      "adaptive_constant_bcs2_val: 3.032e+07\n",
      "It: 7600, Loss: 6.461e+03, Loss_res: 1.199e-01,  Loss_bcs1: 6.054e-01,  Loss_bcs2: 2.130e-04,  loss_ics_u: 6.050e-01, Loss_ut_ics: 9.028e-03,, Time: 2.21\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.301e-04 ,max_grad_bcs2: 6.137e-11 ,max_grad_ics: 8.925e-05 ,max_grad_res: 7.259e-04  \n",
      "min_grad_bcs1: 1.560e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 9.180e-12 ,min_grad_res: 3.319e-11  \n",
      "update_res: 9.749e-01\n",
      "update_ics_u_t: 1.211e+00\n",
      "update_bcs1: 9.303e-01\n",
      "update_bcs2: 8.839e-01\n",
      "adaptive_constant_res_val: 7.259e-04\n",
      "adaptive_constant_ics_val: 8.133e+00\n",
      "adaptive_constant_bcs1_val: 3.154e+00\n",
      "adaptive_constant_bcs2_val: 1.183e+07\n",
      "It: 7700, Loss: 2.331e+03, Loss_res: 1.198e-01,  Loss_bcs1: 5.538e-01,  Loss_bcs2: 1.969e-04,  loss_ics_u: 5.535e-01, Loss_ut_ics: 9.403e-03,, Time: 2.17\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.148e-04 ,max_grad_bcs2: 5.882e-11 ,max_grad_ics: 6.949e-05 ,max_grad_res: 2.269e-03  \n",
      "min_grad_bcs1: 3.138e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 7.013e-12 ,min_grad_res: 2.274e-12  \n",
      "update_res: 1.002e+00\n",
      "update_ics_u_t: 1.179e+00\n",
      "update_bcs1: 9.452e-01\n",
      "update_bcs2: 8.736e-01\n",
      "adaptive_constant_res_val: 2.269e-03\n",
      "adaptive_constant_ics_val: 3.265e+01\n",
      "adaptive_constant_bcs1_val: 7.208e+00\n",
      "adaptive_constant_bcs2_val: 3.858e+07\n",
      "It: 7800, Loss: 6.118e+03, Loss_res: 1.387e-01,  Loss_bcs1: 5.440e-01,  Loss_bcs2: 1.585e-04,  loss_ics_u: 5.437e-01, Loss_ut_ics: 1.395e-02,, Time: 2.20\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.606e-04 ,max_grad_bcs2: 5.256e-11 ,max_grad_ics: 8.843e-05 ,max_grad_res: 3.685e-03  \n",
      "min_grad_bcs1: 3.344e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 4.351e-11 ,min_grad_res: 2.424e-11  \n",
      "update_res: 9.806e-01\n",
      "update_ics_u_t: 1.171e+00\n",
      "update_bcs1: 9.393e-01\n",
      "update_bcs2: 9.088e-01\n",
      "adaptive_constant_res_val: 3.685e-03\n",
      "adaptive_constant_ics_val: 4.167e+01\n",
      "adaptive_constant_bcs1_val: 2.295e+01\n",
      "adaptive_constant_bcs2_val: 7.011e+07\n",
      "It: 7900, Loss: 7.267e+03, Loss_res: 1.320e-01,  Loss_bcs1: 5.033e-01,  Loss_bcs2: 1.035e-04,  loss_ics_u: 5.031e-01, Loss_ut_ics: 1.832e-02,, Time: 2.18\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.633e-04 ,max_grad_bcs2: 4.287e-11 ,max_grad_ics: 9.656e-05 ,max_grad_res: 9.324e-04  \n",
      "min_grad_bcs1: 4.560e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 6.548e-11 ,min_grad_res: 9.436e-12  \n",
      "update_res: 9.567e-01\n",
      "update_ics_u_t: 1.192e+00\n",
      "update_bcs1: 9.130e-01\n",
      "update_bcs2: 9.383e-01\n",
      "adaptive_constant_res_val: 9.324e-04\n",
      "adaptive_constant_ics_val: 9.656e+00\n",
      "adaptive_constant_bcs1_val: 3.541e+00\n",
      "adaptive_constant_bcs2_val: 2.175e+07\n",
      "It: 8000, Loss: 1.890e+03, Loss_res: 1.005e-01,  Loss_bcs1: 5.213e-01,  Loss_bcs2: 8.683e-05,  loss_ics_u: 5.211e-01, Loss_ut_ics: 4.063e-03,, Time: 2.17\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.914e-04 ,max_grad_bcs2: 3.897e-11 ,max_grad_ics: 6.830e-05 ,max_grad_res: 2.095e-03  \n",
      "min_grad_bcs1: 9.227e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 3.539e-11 ,min_grad_res: 7.738e-13  \n",
      "update_res: 9.872e-01\n",
      "update_ics_u_t: 1.194e+00\n",
      "update_bcs1: 9.335e-01\n",
      "update_bcs2: 8.852e-01\n",
      "adaptive_constant_res_val: 2.095e-03\n",
      "adaptive_constant_ics_val: 3.067e+01\n",
      "adaptive_constant_bcs1_val: 7.189e+00\n",
      "adaptive_constant_bcs2_val: 5.375e+07\n",
      "It: 8100, Loss: 3.305e+03, Loss_res: 1.215e-01,  Loss_bcs1: 4.545e-01,  Loss_bcs2: 6.142e-05,  loss_ics_u: 4.544e-01, Loss_ut_ics: 8.224e-03,, Time: 2.23\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.169e-04 ,max_grad_bcs2: 3.313e-11 ,max_grad_ics: 8.168e-05 ,max_grad_res: 7.753e-04  \n",
      "min_grad_bcs1: 2.423e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.609e-11 ,min_grad_res: 1.182e-11  \n",
      "update_res: 9.756e-01\n",
      "update_ics_u_t: 1.173e+00\n",
      "update_bcs1: 9.255e-01\n",
      "update_bcs2: 9.258e-01\n",
      "adaptive_constant_res_val: 7.753e-04\n",
      "adaptive_constant_ics_val: 9.492e+00\n",
      "adaptive_constant_bcs1_val: 3.575e+00\n",
      "adaptive_constant_bcs2_val: 2.340e+07\n",
      "It: 8200, Loss: 1.192e+03, Loss_res: 1.303e-01,  Loss_bcs1: 5.541e-01,  Loss_bcs2: 5.085e-05,  loss_ics_u: 5.539e-01, Loss_ut_ics: 5.162e-03,, Time: 2.22\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.546e-04 ,max_grad_bcs2: 3.005e-11 ,max_grad_ics: 4.734e-05 ,max_grad_res: 4.750e-03  \n",
      "min_grad_bcs1: 5.328e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.410e-11 ,min_grad_res: 3.745e-12  \n",
      "update_res: 9.741e-01\n",
      "update_ics_u_t: 1.173e+00\n",
      "update_bcs1: 9.469e-01\n",
      "update_bcs2: 9.056e-01\n",
      "adaptive_constant_res_val: 4.750e-03\n",
      "adaptive_constant_ics_val: 1.003e+02\n",
      "adaptive_constant_bcs1_val: 3.072e+01\n",
      "adaptive_constant_bcs2_val: 1.580e+08\n",
      "It: 8300, Loss: 2.660e+03, Loss_res: 1.126e-01,  Loss_bcs1: 6.288e-01,  Loss_bcs2: 1.671e-05,  loss_ics_u: 6.287e-01, Loss_ut_ics: 6.296e-03,, Time: 2.23\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.668e-04 ,max_grad_bcs2: 1.719e-11 ,max_grad_ics: 6.941e-05 ,max_grad_res: 9.164e-04  \n",
      "min_grad_bcs1: 1.281e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.546e-11 ,min_grad_res: 1.982e-11  \n",
      "update_res: 9.113e-01\n",
      "update_ics_u_t: 1.107e+00\n",
      "update_bcs1: 8.762e-01\n",
      "update_bcs2: 1.106e+00\n",
      "adaptive_constant_res_val: 9.164e-04\n",
      "adaptive_constant_ics_val: 1.320e+01\n",
      "adaptive_constant_bcs1_val: 3.435e+00\n",
      "adaptive_constant_bcs2_val: 5.333e+07\n",
      "It: 8400, Loss: 5.399e+02, Loss_res: 1.421e-01,  Loss_bcs1: 5.654e-01,  Loss_bcs2: 1.009e-05,  loss_ics_u: 5.654e-01, Loss_ut_ics: 1.017e-02,, Time: 2.20\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 3.782e-04 ,max_grad_bcs2: 1.336e-11 ,max_grad_ics: 1.133e-04 ,max_grad_res: 3.364e-03  \n",
      "min_grad_bcs1: 2.877e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 4.968e-11 ,min_grad_res: 9.722e-12  \n",
      "update_res: 9.614e-01\n",
      "update_ics_u_t: 1.138e+00\n",
      "update_bcs1: 9.347e-01\n",
      "update_bcs2: 9.660e-01\n",
      "adaptive_constant_res_val: 3.364e-03\n",
      "adaptive_constant_ics_val: 2.968e+01\n",
      "adaptive_constant_bcs1_val: 8.894e+00\n",
      "adaptive_constant_bcs2_val: 2.517e+08\n",
      "It: 8500, Loss: 3.439e+02, Loss_res: 1.284e-01,  Loss_bcs1: 6.187e-01,  Loss_bcs2: 1.344e-06,  loss_ics_u: 6.186e-01, Loss_ut_ics: 5.951e-03,, Time: 2.15\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.758e-04 ,max_grad_bcs2: 4.821e-12 ,max_grad_ics: 7.281e-05 ,max_grad_res: 1.899e-03  \n",
      "min_grad_bcs1: 2.024e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 3.081e-11 ,min_grad_res: 8.664e-12  \n",
      "update_res: 8.453e-01\n",
      "update_ics_u_t: 1.044e+00\n",
      "update_bcs1: 8.080e-01\n",
      "update_bcs2: 1.303e+00\n",
      "adaptive_constant_res_val: 1.899e-03\n",
      "adaptive_constant_ics_val: 2.609e+01\n",
      "adaptive_constant_bcs1_val: 6.887e+00\n",
      "adaptive_constant_bcs2_val: 3.939e+08\n",
      "It: 8600, Loss: 1.636e+01, Loss_res: 1.194e-01,  Loss_bcs1: 5.553e-01,  Loss_bcs2: 3.140e-08,  loss_ics_u: 5.553e-01, Loss_ut_ics: 6.244e-03,, Time: 2.33\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.778e-04 ,max_grad_bcs2: 7.295e-13 ,max_grad_ics: 4.480e-05 ,max_grad_res: 4.266e-03  \n",
      "min_grad_bcs1: 1.682e-10 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.393e-11 ,min_grad_res: 1.082e-10  \n",
      "update_res: 7.348e-01\n",
      "update_ics_u_t: 8.836e-01\n",
      "update_bcs1: 7.068e-01\n",
      "update_bcs2: 1.675e+00\n",
      "adaptive_constant_res_val: 4.266e-03\n",
      "adaptive_constant_ics_val: 9.522e+01\n",
      "adaptive_constant_bcs1_val: 2.400e+01\n",
      "adaptive_constant_bcs2_val: 5.849e+09\n",
      "It: 8700, Loss: 1.510e+01, Loss_res: 1.170e-01,  Loss_bcs1: 5.831e-01,  Loss_bcs2: 2.415e-14,  loss_ics_u: 5.831e-01, Loss_ut_ics: 1.159e-02,, Time: 2.46\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.378e-04 ,max_grad_bcs2: 1.174e-15 ,max_grad_ics: 7.155e-05 ,max_grad_res: 2.198e-03  \n",
      "min_grad_bcs1: 1.094e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 8.384e-11 ,min_grad_res: 6.657e-11  \n",
      "update_res: 2.179e-01\n",
      "update_ics_u_t: 2.550e-01\n",
      "update_bcs1: 2.075e-01\n",
      "update_bcs2: 3.320e+00\n",
      "adaptive_constant_res_val: 2.198e-03\n",
      "adaptive_constant_ics_val: 3.073e+01\n",
      "adaptive_constant_bcs1_val: 1.596e+01\n",
      "adaptive_constant_bcs2_val: 1.872e+12\n",
      "It: 8800, Loss: 2.357e+06, Loss_res: 1.276e-01,  Loss_bcs1: 6.032e-01,  Loss_bcs2: 1.259e-06,  loss_ics_u: 6.032e-01, Loss_ut_ics: 2.253e-02,, Time: 2.81\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.531e-04 ,max_grad_bcs2: 5.135e-13 ,max_grad_ics: 3.235e-04 ,max_grad_res: 2.567e-03  \n",
      "min_grad_bcs1: 5.974e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.019e-10 ,min_grad_res: 3.729e-11  \n",
      "update_res: 2.318e-01\n",
      "update_ics_u_t: 2.849e-01\n",
      "update_bcs1: 2.163e-01\n",
      "update_bcs2: 3.267e+00\n",
      "adaptive_constant_res_val: 2.567e-03\n",
      "adaptive_constant_ics_val: 7.936e+00\n",
      "adaptive_constant_bcs1_val: 1.014e+01\n",
      "adaptive_constant_bcs2_val: 4.999e+09\n",
      "It: 8900, Loss: 2.232e+04, Loss_res: 1.400e-01,  Loss_bcs1: 5.884e-01,  Loss_bcs2: 4.464e-06,  loss_ics_u: 5.884e-01, Loss_ut_ics: 1.777e-02,, Time: 3.36\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.788e-04 ,max_grad_bcs2: 8.131e-13 ,max_grad_ics: 1.859e-04 ,max_grad_res: 1.189e-03  \n",
      "min_grad_bcs1: 4.110e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 2.156e-10 ,min_grad_res: 2.285e-11  \n",
      "update_res: 7.390e-01\n",
      "update_ics_u_t: 9.213e-01\n",
      "update_bcs1: 7.211e-01\n",
      "update_bcs2: 1.619e+00\n",
      "adaptive_constant_res_val: 1.189e-03\n",
      "adaptive_constant_ics_val: 6.394e+00\n",
      "adaptive_constant_bcs1_val: 4.264e+00\n",
      "adaptive_constant_bcs2_val: 1.462e+09\n",
      "It: 9000, Loss: 6.937e+02, Loss_res: 1.487e-01,  Loss_bcs1: 5.803e-01,  Loss_bcs2: 4.727e-07,  loss_ics_u: 5.803e-01, Loss_ut_ics: 2.541e-02,, Time: 3.15\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 2.442e-04 ,max_grad_bcs2: 2.723e-13 ,max_grad_ics: 3.216e-04 ,max_grad_res: 1.959e-03  \n",
      "min_grad_bcs1: 2.112e-11 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 6.094e-11 ,min_grad_res: 5.315e-12  \n",
      "update_res: 8.264e-01\n",
      "update_ics_u_t: 1.008e+00\n",
      "update_bcs1: 7.938e-01\n",
      "update_bcs2: 1.372e+00\n",
      "adaptive_constant_res_val: 1.959e-03\n",
      "adaptive_constant_ics_val: 6.093e+00\n",
      "adaptive_constant_bcs1_val: 8.025e+00\n",
      "adaptive_constant_bcs2_val: 7.195e+09\n",
      "It: 9100, Loss: 4.348e+00, Loss_res: 1.111e-01,  Loss_bcs1: 5.288e-01,  Loss_bcs2: 6.853e-12,  loss_ics_u: 5.288e-01, Loss_ut_ics: 9.014e-03,, Time: 2.42\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.391e-04 ,max_grad_bcs2: 1.037e-15 ,max_grad_ics: 1.102e-04 ,max_grad_res: 1.668e-03  \n",
      "min_grad_bcs1: 5.962e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 4.115e-11 ,min_grad_res: 1.247e-11  \n",
      "update_res: 5.078e-01\n",
      "update_ics_u_t: 6.444e-01\n",
      "update_bcs1: 4.958e-01\n",
      "update_bcs2: 2.352e+00\n",
      "adaptive_constant_res_val: 1.668e-03\n",
      "adaptive_constant_ics_val: 1.514e+01\n",
      "adaptive_constant_bcs1_val: 1.199e+01\n",
      "adaptive_constant_bcs2_val: 1.609e+12\n",
      "It: 9200, Loss: 7.108e+00, Loss_res: 1.086e-01,  Loss_bcs1: 5.803e-01,  Loss_bcs2: 2.408e-15,  loss_ics_u: 5.803e-01, Loss_ut_ics: 9.528e-03,, Time: 2.41\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.199e-04 ,max_grad_bcs2: 1.986e-18 ,max_grad_ics: 1.141e-04 ,max_grad_res: 1.799e-03  \n",
      "min_grad_bcs1: 9.026e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 1.134e-10 ,min_grad_res: 5.139e-11  \n",
      "update_res: 7.939e-01\n",
      "update_ics_u_t: 9.789e-01\n",
      "update_bcs1: 7.628e-01\n",
      "update_bcs2: 1.464e+00\n",
      "adaptive_constant_res_val: 1.799e-03\n",
      "adaptive_constant_ics_val: 1.576e+01\n",
      "adaptive_constant_bcs1_val: 1.500e+01\n",
      "adaptive_constant_bcs2_val: 9.059e+14\n",
      "It: 9300, Loss: 1.052e+10, Loss_res: 9.916e-02,  Loss_bcs1: 6.421e-01,  Loss_bcs2: 1.161e-05,  loss_ics_u: 6.421e-01, Loss_ut_ics: 7.616e-03,, Time: 2.55\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:400: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "minmax ratio : inf\n",
      "max_grad_bcs1: 1.076e-04 ,max_grad_bcs2: 0.000e+00 ,max_grad_ics: 9.244e-05 ,max_grad_res: 1.325e-03  \n",
      "min_grad_bcs1: 6.025e-12 ,min_grad_bcs2: 0.000e+00 ,min_grad_ics: 7.276e-12 ,min_grad_res: 3.013e-12  \n",
      "update_res: 2.532e-01\n",
      "update_ics_u_t: 4.283e-01\n",
      "update_bcs1: 2.251e-01\n",
      "update_bcs2: 3.093e+00\n",
      "adaptive_constant_res_val: 1.325e-03\n",
      "adaptive_constant_ics_val: 1.433e+01\n",
      "adaptive_constant_bcs1_val: 1.231e+01\n",
      "adaptive_constant_bcs2_val: inf\n",
      "It: 9400, Loss: nan, Loss_res: nan,  Loss_bcs1: nan,  Loss_bcs2: nan,  loss_ics_u: nan, Loss_ut_ics: nan,, Time: 2.56\n",
      "minmax ratio : nanmax_grad_bcs1: nan ,max_grad_bcs2: nan ,max_grad_ics: nan ,max_grad_res: -inf  min_grad_bcs1: inf ,min_grad_bcs2: inf ,min_grad_ics: inf ,min_grad_res: inf  update_res: nanupdate_ics_u_t: nanupdate_bcs1: nanupdate_bcs2: nanadaptive_constant_res_val: -infadaptive_constant_ics_val: nanadaptive_constant_bcs1_val: nanadaptive_constant_bcs2_val: nanIt: 9500, Loss: nan, Loss_res: nan,  Loss_bcs1: nan,  Loss_bcs2: nan,  loss_ics_u: nan, Loss_ut_ics: nan,, Time: 2.77minmax ratio : nanmax_grad_bcs1: nan ,max_grad_bcs2: nan ,max_grad_ics: nan ,max_grad_res: -inf  min_grad_bcs1: inf ,min_grad_bcs2: inf ,min_grad_ics: inf ,min_grad_res: inf  update_res: nanupdate_ics_u_t: nanupdate_bcs1: nanupdate_bcs2: nanadaptive_constant_res_val: -infadaptive_constant_ics_val: nanadaptive_constant_bcs1_val: nanadaptive_constant_bcs2_val: nanIt: 9600, Loss: nan, Loss_res: nan,  Loss_bcs1: nan,  Loss_bcs2: nan,  loss_ics_u: nan, Loss_ut_ics: nan,, Time: 2.16minmax ratio : nanmax_grad_bcs1: nan ,max_grad_bcs2: nan ,max_grad_ics: nan ,max_grad_res: -inf  min_grad_bcs1: inf ,min_grad_bcs2: inf ,min_grad_ics: inf ,min_grad_res: inf  update_res: nanupdate_ics_u_t: nanupdate_bcs1: nanupdate_bcs2: nanadaptive_constant_res_val: -infadaptive_constant_ics_val: nanadaptive_constant_bcs1_val: nanadaptive_constant_bcs2_val: nanIt: 9700, Loss: nan, Loss_res: nan,  Loss_bcs1: nan,  Loss_bcs2: nan,  loss_ics_u: nan, Loss_ut_ics: nan,, Time: 2.46minmax ratio : nanmax_grad_bcs1: nan ,max_grad_bcs2: nan ,max_grad_ics: nan ,max_grad_res: -inf  min_grad_bcs1: inf ,min_grad_bcs2: inf ,min_grad_ics: inf ,min_grad_res: inf  update_res: nanupdate_ics_u_t: nanupdate_bcs1: nanupdate_bcs2: nanadaptive_constant_res_val: -infadaptive_constant_ics_val: nanadaptive_constant_bcs1_val: nanadaptive_constant_bcs2_val: nanIt: 9800, Loss: nan, Loss_res: nan,  Loss_bcs1: nan,  Loss_bcs2: nan,  loss_ics_u: nan, Loss_ut_ics: nan,, Time: 2.19minmax ratio : nanmax_grad_bcs1: nan ,max_grad_bcs2: nan ,max_grad_ics: nan ,max_grad_res: -inf  min_grad_bcs1: inf ,min_grad_bcs2: inf ,min_grad_ics: inf ,min_grad_res: inf  update_res: nanupdate_ics_u_t: nanupdate_bcs1: nanupdate_bcs2: nanadaptive_constant_res_val: -infadaptive_constant_ics_val: nanadaptive_constant_bcs1_val: nanadaptive_constant_bcs2_val: nanIt: 9900, Loss: nan, Loss_res: nan,  Loss_bcs1: nan,  Loss_bcs2: nan,  loss_ics_u: nan, Loss_ut_ics: nan,, Time: 2.24minmax ratio : nanmax_grad_bcs1: nan ,max_grad_bcs2: nan ,max_grad_ics: nan ,max_grad_res: -inf  min_grad_bcs1: inf ,min_grad_bcs2: inf ,min_grad_ics: inf ,min_grad_res: inf  update_res: nanupdate_ics_u_t: nanupdate_bcs1: nanupdate_bcs2: nanadaptive_constant_res_val: -infadaptive_constant_ics_val: nanadaptive_constant_bcs1_val: nanadaptive_constant_bcs2_val: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18757/3933714513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmtd\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"mini_batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mini_batch method is used\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainmb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmbbatch_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown method!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18757/3166399342.py\u001b[0m in \u001b[0;36mtrainmb\u001b[0;34m(self, nIter, batch_size, a, c)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# Run the Tensorflow session to minimize the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbatch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_tensor_list\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_batch_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define PINN model\n",
    "a = 0.5\n",
    "c = 2\n",
    "\n",
    "kernel_size = 300\n",
    "\n",
    "# Domain boundaries\n",
    "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
    "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
    "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
    "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
    "\n",
    "# Create boundary conditions samplers\n",
    "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
    "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
    "bcs_sampler = [bc1, bc2]\n",
    "\n",
    "# Create residual sampler\n",
    "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
    "\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 300\n",
    "\n",
    "\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'\n",
    "layers = [2, 500, 500, 500, 1]\n",
    "\n",
    "\n",
    "nn = 200\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "u_star = u(X_star, a,c)\n",
    "r_star = r(X_star, a, c)\n",
    "\n",
    "iterations = 1\n",
    "methods = [  \"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_r_list = []\n",
    "\n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "     # Create residual sampler\n",
    "\n",
    "        # [elapsed, error_u , model] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            # sess.run(init)\n",
    "\n",
    "            model = PINN(layers, operator ,  ics_sampler, bcs_sampler, res_sampler, c , mode , sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                print(\"full_batch method is used\")\n",
    "                model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                print(\"mini_batch method is used\")\n",
    "                model.trainmb(nIter, mbbatch_size , a , c)\n",
    "            else:\n",
    "                print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            r_pred = model.predict_r(X_star)\n",
    "            # Predictions\n",
    "\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
    "\n",
    "            model.print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "            model.plot_lambda()\n",
    "            # model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
    "            model.print(\"average lambda_bc : \" , np.average(model.adaptive_constant_bcs1_log))\n",
    "            model.print(\"average lambda_ic : \" , np.average(model.adaptive_constant_ics_log))\n",
    "            model.print(\"average lambda_res : \" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "        # print('elapsed: {:.2e}'.format(elapsed))\n",
    "        # print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "        # print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_r_list.append(error_r)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list,error_r_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if __name__ == '__main__':\n",
      "elapsed: 3.52e+02\n",
      "Relative L2 error_u: 1.57e+00\n",
      "Relative L2 error_r: inf\n",
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-25-2023_21-15-31-383584_M2\n",
      "Final loss total loss: 4.097462e-05\n",
      "Final loss loss_res: 3.035861e-07\n",
      "Final loss loss_bc1: 2.308045e-05\n",
      "Final loss loss_bc2: 9.876904e-06\n",
      "Final loss loss_ics_us: 8.017262e-06\n",
      "Final loss loss_ics_u: 1.726670e-05\n",
      "Final loss loss_ics_u_t: 8.017262e-06\n",
      "average lambda_bc : 2.5003e-01\n",
      "average lambda_ic : 3.9388e-01\n",
      "average lambda_res : 1.0\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:3.5179e+02\n",
      "average of error_u_list:1.5745e+00\n",
      "average of error_r_list:inf\n"
     ]
    }
   ],
   "source": [
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "r_pred = model.predict_r(X_star)\n",
    "# Predictions\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
    "\n",
    "model.print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "model.print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "model.plot_lambda()\n",
    "# model.plot_grad()\n",
    "model.save_NN()\n",
    "model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
    "model.print(\"average lambda_bc : \" , np.average(model.adaptive_constant_bcs1_log))\n",
    "model.print(\"average lambda_ic : \" , np.average(model.adaptive_constant_ics_log))\n",
    "model.print(\"average lambda_res : \" , str(1.0))\n",
    "# sess.close()  \n",
    "\n",
    "# print('elapsed: {:.2e}'.format(elapsed))\n",
    "# print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "# print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "\n",
    "time_list.append(elapsed)\n",
    "error_u_list.append(error_u)\n",
    "error_r_list.append(error_r)\n",
    "\n",
    "model.print(\"\\n\\nMethod: \", mtd)\n",
    "model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "model.print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
    "\n",
    "result_dict[mtd] = [time_list ,error_u_list,error_r_list]\n",
    "# scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
