{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import griddata\n",
    "# from Klein_Gordon_model_tf import Sampler, Klein_Gordon\n",
    "import timeit\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "\n",
    "def u(x):\n",
    "    \"\"\"\n",
    "    :param x: x = (t, x)\n",
    "    \"\"\"\n",
    "    return x[:, 1:2] * np.cos(5 * np.pi * x[:, 0:1]) + (x[:, 0:1] * x[:, 1:2])**3\n",
    "\n",
    "def u_tt(x):\n",
    "    return - 25 * np.pi**2 * x[:, 1:2] * np.cos(5 * np.pi * x[:, 0:1]) + 6 * x[:,0:1] * x[:,1:2]**3\n",
    "\n",
    "def u_xx(x):\n",
    "    return np.zeros((x.shape[0], 1)) +  6 * x[:,1:2] * x[:,0:1]**3\n",
    "\n",
    "def f(x, alpha, beta, gamma, k):\n",
    "    return u_tt(x) + alpha * u_xx(x) + beta * u(x) + gamma * u(x)**k\n",
    "\n",
    "def operator(u, t, x, alpha, beta, gamma, k,  sigma_t=1.0, sigma_x=1.0):\n",
    "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    residual = u_tt + alpha * u_xx + beta * u + gamma * u**k\n",
    "    return residual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "class Klein_Gordon:\n",
    "    # Initialize the class\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, model, stiff_ratio):\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Klein_Gordon constant\n",
    "        self.alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "        self.beta = tf.constant(beta, dtype=tf.float32)\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "        self.k = tf.constant(k, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = model\n",
    "\n",
    "        # Record stiff ratio\n",
    "        self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive re-weighting constant\n",
    "        self.rate = 0.9\n",
    "        self.adaptive_constant_ics_val = np.array(1.0)\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        if model in ['M3', 'M4']:\n",
    "            # Initialize encoder weights and biases\n",
    "            self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "            self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
    "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
    "\n",
    "        # Boundary loss and Initial loss\n",
    "        self.loss_ic_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
    "        self.loss_ic_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred - self.u_bc1_tf))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred - self.u_bc2_tf))\n",
    "\n",
    "        self.loss_bcs = self.adaptive_constant_bcs_tf * (self.loss_bc1 + self.loss_bc2)\n",
    "        self.loss_ics = self.adaptive_constant_ics_tf * (self.loss_ic_u + self.loss_ic_u_t)\n",
    "        self.loss_u = self.loss_bcs + self.loss_ics\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred - self.r_tf))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_u\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                        1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict(self.layers)\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict(self.layers)\n",
    "        self.dict_gradients_ics_layers = self.generate_grad_dict(self.layers)\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_ics = []\n",
    "        self.grad_bcs = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "            self.grad_ics.append(tf.gradients(self.loss_ics, self.weights[i])[0])\n",
    "\n",
    "        # Store the adaptive constant\n",
    "        self.adaptive_constant_ics_log = []\n",
    "        self.adaptive_constant_bcs_log = []\n",
    "\n",
    "        # Compute the adaptive constant\n",
    "        self.adaptive_constant_ics_list = []\n",
    "        self.adaptive_constant_bcs_list = []\n",
    "        \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        self.mean_grad_ics_list = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "            self.mean_grad_ics_list.append(tf.reduce_mean(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.mean_grad_ics = tf.reduce_mean(tf.stack(self.mean_grad_ics_list))\n",
    "        \n",
    "        self.adaptive_constant_bcs = self.max_grad_res / self.mean_grad_bcs\n",
    "        self.adaptive_constant_ics = self.max_grad_res / self.mean_grad_ics\n",
    "\n",
    "        # Stiff Ratio\n",
    "        if self.stiff_ratio:\n",
    "            self.Hessian, self.Hessian_ics, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "            self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "            self.eigenvalues_ics, _ = tf.linalg.eigh(self.Hessian_ics)\n",
    "            self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "            self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "            self.eigenvalue_log = []\n",
    "            self.eigenvalue_ics_log = []\n",
    "            self.eigenvalue_bcs_log = []\n",
    "            self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_ics_value , grad_bcs_value, grad_res_value= self.sess.run([self.grad_ics[i],\n",
    "                                                                            self.grad_bcs[i],\n",
    "                                                                            self.grad_res[i]],\n",
    "                                                                            feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_ics(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_ics, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,\n",
    "                                                   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,\n",
    "                                          self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P),\n",
    "                      dtype='float32')\n",
    "        H_ics = tf.map_fn(self.get_Hv_ics, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "\n",
    "        return H, H_ics, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1))\n",
    "        return u\n",
    "\n",
    "    def net_u_t(self, t, x):\n",
    "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
    "        return u_t\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, t, x):\n",
    "        u = self.net_u(t, x)\n",
    "        residual = self.operator(u, t, x,\n",
    "                                 self.alpha, self.beta, self.gamma, self.k,\n",
    "                                 self.sigma_t, self.sigma_x)\n",
    "        return residual\n",
    "\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter , bcBatch_size , fBatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # Fetch boundary mini-batches\n",
    "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, bcBatch_size)\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcBatch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], bcBatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, fBatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                    self.u_ics_tf: u_ics_batch,\n",
    "                    self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.r_tf: f_res_batch,\n",
    "                    self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val}\n",
    "\n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_u, self.loss_res], tf_dict)\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     # Compute the adaptive constant\n",
    "                #     adaptive_constant_ics_val, adaptive_constant_bcs_val = self.sess.run( [self.adaptive_constant_ics, self.adaptive_constant_bcs],  tf_dict)\n",
    "                #     # Print adaptive weights during training\n",
    "                #     self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                    # self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                # # Store loss and adaptive weights\n",
    "                # self.loss_u_log.append(loss_u_value)\n",
    "                # self.loss_r_log.append(loss_r_value)\n",
    "\n",
    "                # self.adaptive_constant_ics_log.append(self.adaptive_constant_ics_val)\n",
    "                # self.adaptive_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' % (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "                # print(\"constant_ics_val: {:.3f}, constant_bcs_val: {:.3f}\".format( self.adaptive_constant_ics_val, self.adaptive_constant_bcs_val))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_ics, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                                         self.eigenvalues_ics,\n",
    "            #                                                                                         self.eigenvalues_bcs,\n",
    "            #                                                                                         self.eigenvalues_res], tf_dict)\n",
    "            #         self.eigenvalue_log.append(eigenvalues)\n",
    "            #         self.eigenvalue_ics_log.append(eigenvalues_bcs)\n",
    "            #         self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "            #         self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size)\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                       self.u_ics_tf: u_ics_batch,\n",
    "                       self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val}\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_u, self.loss_res], tf_dict)\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                # if self.model in ['M2', 'M4']:\n",
    "                #     # Compute the adaptive constant\n",
    "                #     adaptive_constant_ics_val, adaptive_constant_bcs_val = self.sess.run( [self.adaptive_constant_ics, self.adaptive_constant_bcs],  tf_dict)\n",
    "                #     # Print adaptive weights during training\n",
    "                #     self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                    # self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                # # Store loss and adaptive weights\n",
    "                # self.loss_u_log.append(loss_u_value)\n",
    "                # self.loss_r_log.append(loss_r_value)\n",
    "\n",
    "                # self.adaptive_constant_ics_log.append(self.adaptive_constant_ics_val)\n",
    "                # self.adaptive_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' % (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "                # print(\"constant_ics_val: {:.3f}, constant_bcs_val: {:.3f}\".format( self.adaptive_constant_ics_val, self.adaptive_constant_bcs_val))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # # Compute the eigenvalues of the Hessian of losses\n",
    "            # if self.stiff_ratio:\n",
    "            #     if it % 1000 == 0:\n",
    "            #         print(\"Eigenvalues information stored ...\")\n",
    "            #         eigenvalues, eigenvalues_ics, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "            #                                                                                         self.eigenvalues_ics,\n",
    "            #                                                                                         self.eigenvalues_bcs,\n",
    "            #                                                                                         self.eigenvalues_res], tf_dict)\n",
    "            #         self.eigenvalue_log.append(eigenvalues)\n",
    "            #         self.eigenvalue_ics_log.append(eigenvalues_bcs)\n",
    "            #         self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "            #         self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_method(mtd , layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma ,k ,mode , stiff_ratio ,  X_star , u_star , f_star  , nIter , batch_size  , bcbatch_size , ubatch_size )\n",
    "\n",
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma ,k ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter , batch_size  , bcbatch_size , ubatch_size ):\n",
    "\n",
    "\n",
    "    model = Klein_Gordon(layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter , bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_25932/4209295966.py:252: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_25932/4209295966.py:58: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_25932/4209295966.py:58: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_25932/4209295966.py:61: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 03:54:16.876123: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 03:54:16.902554: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-11-23 03:54:16.903025: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559b1daec760 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-23 03:54:16.903039: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-11-23 03:54:16.934640: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_25932/4209295966.py:111: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_25932/4209295966.py:114: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /tmp/ipykernel_25932/4209295966.py:119: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_25932/4209295966.py:174: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "It: 0, Loss: 9.449e+03, Loss_u: 1.350e+00, Loss_r: 9.447e+03, Time: 2.66\n",
      "It: 10, Loss: 1.009e+04, Loss_u: 3.435e+00, Loss_r: 1.009e+04, Time: 0.12\n",
      "It: 20, Loss: 7.903e+03, Loss_u: 4.452e+00, Loss_r: 7.899e+03, Time: 0.14\n",
      "It: 30, Loss: 7.809e+03, Loss_u: 2.067e+00, Loss_r: 7.807e+03, Time: 0.12\n",
      "It: 40, Loss: 8.079e+03, Loss_u: 3.033e+00, Loss_r: 8.076e+03, Time: 0.14\n",
      "It: 50, Loss: 6.593e+03, Loss_u: 4.523e+00, Loss_r: 6.589e+03, Time: 0.12\n",
      "It: 60, Loss: 6.142e+03, Loss_u: 1.153e+01, Loss_r: 6.131e+03, Time: 0.14\n",
      "It: 70, Loss: 6.036e+03, Loss_u: 1.767e+01, Loss_r: 6.018e+03, Time: 0.12\n",
      "It: 80, Loss: 4.792e+03, Loss_u: 2.678e+01, Loss_r: 4.766e+03, Time: 0.14\n",
      "It: 90, Loss: 3.376e+03, Loss_u: 3.406e+01, Loss_r: 3.342e+03, Time: 0.12\n",
      "It: 100, Loss: 2.343e+03, Loss_u: 3.825e+01, Loss_r: 2.304e+03, Time: 0.16\n",
      "It: 110, Loss: 2.331e+03, Loss_u: 4.154e+01, Loss_r: 2.290e+03, Time: 0.13\n",
      "It: 120, Loss: 2.507e+03, Loss_u: 4.351e+01, Loss_r: 2.464e+03, Time: 0.15\n",
      "It: 130, Loss: 2.219e+03, Loss_u: 3.838e+01, Loss_r: 2.181e+03, Time: 0.39\n",
      "It: 140, Loss: 2.047e+03, Loss_u: 3.475e+01, Loss_r: 2.012e+03, Time: 0.17\n",
      "It: 150, Loss: 2.538e+03, Loss_u: 3.937e+01, Loss_r: 2.498e+03, Time: 0.16\n",
      "It: 160, Loss: 1.623e+03, Loss_u: 3.635e+01, Loss_r: 1.587e+03, Time: 0.17\n",
      "It: 170, Loss: 1.470e+03, Loss_u: 3.733e+01, Loss_r: 1.433e+03, Time: 0.15\n",
      "It: 180, Loss: 1.955e+03, Loss_u: 3.623e+01, Loss_r: 1.918e+03, Time: 0.15\n",
      "It: 190, Loss: 1.751e+03, Loss_u: 3.693e+01, Loss_r: 1.714e+03, Time: 0.17\n",
      "It: 200, Loss: 1.606e+03, Loss_u: 3.508e+01, Loss_r: 1.571e+03, Time: 0.13\n",
      "It: 210, Loss: 1.435e+03, Loss_u: 3.774e+01, Loss_r: 1.397e+03, Time: 0.15\n",
      "It: 220, Loss: 1.009e+03, Loss_u: 3.375e+01, Loss_r: 9.752e+02, Time: 0.14\n",
      "It: 230, Loss: 1.190e+03, Loss_u: 3.482e+01, Loss_r: 1.155e+03, Time: 0.14\n",
      "It: 240, Loss: 1.585e+03, Loss_u: 3.871e+01, Loss_r: 1.546e+03, Time: 0.13\n",
      "It: 250, Loss: 1.333e+03, Loss_u: 3.892e+01, Loss_r: 1.294e+03, Time: 0.14\n",
      "It: 260, Loss: 1.110e+03, Loss_u: 3.524e+01, Loss_r: 1.075e+03, Time: 0.13\n",
      "It: 270, Loss: 5.655e+02, Loss_u: 3.647e+01, Loss_r: 5.290e+02, Time: 0.13\n",
      "It: 280, Loss: 8.891e+02, Loss_u: 4.392e+01, Loss_r: 8.452e+02, Time: 0.15\n",
      "It: 290, Loss: 1.032e+03, Loss_u: 4.697e+01, Loss_r: 9.853e+02, Time: 0.15\n",
      "It: 300, Loss: 8.747e+02, Loss_u: 5.090e+01, Loss_r: 8.238e+02, Time: 0.15\n",
      "It: 310, Loss: 7.826e+02, Loss_u: 5.807e+01, Loss_r: 7.246e+02, Time: 0.16\n",
      "It: 320, Loss: 8.255e+02, Loss_u: 6.091e+01, Loss_r: 7.646e+02, Time: 0.15\n",
      "It: 330, Loss: 6.267e+02, Loss_u: 6.415e+01, Loss_r: 5.626e+02, Time: 0.15\n",
      "It: 340, Loss: 4.796e+02, Loss_u: 7.509e+01, Loss_r: 4.045e+02, Time: 0.13\n",
      "It: 350, Loss: 2.725e+02, Loss_u: 8.124e+01, Loss_r: 1.913e+02, Time: 0.15\n",
      "It: 360, Loss: 2.874e+02, Loss_u: 7.626e+01, Loss_r: 2.111e+02, Time: 0.16\n",
      "It: 370, Loss: 2.344e+02, Loss_u: 7.782e+01, Loss_r: 1.566e+02, Time: 0.14\n",
      "It: 380, Loss: 2.578e+02, Loss_u: 8.336e+01, Loss_r: 1.745e+02, Time: 0.12\n",
      "It: 390, Loss: 3.597e+02, Loss_u: 8.671e+01, Loss_r: 2.729e+02, Time: 0.14\n",
      "It: 400, Loss: 2.023e+02, Loss_u: 8.261e+01, Loss_r: 1.197e+02, Time: 0.13\n",
      "It: 410, Loss: 2.032e+02, Loss_u: 7.765e+01, Loss_r: 1.256e+02, Time: 0.13\n",
      "It: 420, Loss: 2.762e+02, Loss_u: 8.944e+01, Loss_r: 1.867e+02, Time: 0.15\n",
      "It: 430, Loss: 2.116e+02, Loss_u: 8.061e+01, Loss_r: 1.310e+02, Time: 0.13\n",
      "It: 440, Loss: 1.807e+02, Loss_u: 8.963e+01, Loss_r: 9.112e+01, Time: 0.15\n",
      "It: 450, Loss: 1.934e+02, Loss_u: 8.969e+01, Loss_r: 1.037e+02, Time: 0.14\n",
      "It: 460, Loss: 2.441e+02, Loss_u: 8.229e+01, Loss_r: 1.618e+02, Time: 0.13\n",
      "It: 470, Loss: 2.470e+02, Loss_u: 8.189e+01, Loss_r: 1.651e+02, Time: 0.12\n",
      "It: 480, Loss: 1.792e+02, Loss_u: 8.469e+01, Loss_r: 9.450e+01, Time: 0.13\n",
      "It: 490, Loss: 1.580e+02, Loss_u: 8.574e+01, Loss_r: 7.227e+01, Time: 0.14\n",
      "It: 500, Loss: 1.537e+02, Loss_u: 8.156e+01, Loss_r: 7.212e+01, Time: 0.13\n",
      "It: 510, Loss: 1.826e+02, Loss_u: 8.290e+01, Loss_r: 9.966e+01, Time: 0.17\n",
      "It: 520, Loss: 1.601e+02, Loss_u: 7.858e+01, Loss_r: 8.154e+01, Time: 0.14\n",
      "It: 530, Loss: 1.658e+02, Loss_u: 9.159e+01, Loss_r: 7.417e+01, Time: 0.15\n",
      "It: 540, Loss: 1.430e+02, Loss_u: 8.011e+01, Loss_r: 6.291e+01, Time: 0.14\n",
      "It: 550, Loss: 1.844e+02, Loss_u: 8.491e+01, Loss_r: 9.946e+01, Time: 0.17\n",
      "It: 560, Loss: 1.432e+02, Loss_u: 8.384e+01, Loss_r: 5.934e+01, Time: 0.15\n",
      "It: 570, Loss: 1.338e+02, Loss_u: 7.847e+01, Loss_r: 5.536e+01, Time: 0.14\n",
      "It: 580, Loss: 1.496e+02, Loss_u: 7.848e+01, Loss_r: 7.115e+01, Time: 0.13\n",
      "It: 590, Loss: 2.952e+02, Loss_u: 8.602e+01, Loss_r: 2.092e+02, Time: 0.13\n",
      "It: 600, Loss: 1.266e+02, Loss_u: 7.752e+01, Loss_r: 4.909e+01, Time: 0.12\n",
      "It: 610, Loss: 1.424e+02, Loss_u: 8.350e+01, Loss_r: 5.890e+01, Time: 0.11\n",
      "It: 620, Loss: 1.631e+02, Loss_u: 7.859e+01, Loss_r: 8.448e+01, Time: 0.15\n",
      "It: 630, Loss: 1.768e+02, Loss_u: 7.706e+01, Loss_r: 9.969e+01, Time: 0.11\n",
      "It: 640, Loss: 1.371e+02, Loss_u: 7.571e+01, Loss_r: 6.137e+01, Time: 0.13\n",
      "It: 650, Loss: 1.277e+02, Loss_u: 7.597e+01, Loss_r: 5.176e+01, Time: 0.12\n",
      "It: 660, Loss: 1.290e+02, Loss_u: 8.315e+01, Loss_r: 4.588e+01, Time: 0.12\n",
      "It: 670, Loss: 1.515e+02, Loss_u: 7.272e+01, Loss_r: 7.877e+01, Time: 0.12\n",
      "It: 680, Loss: 1.304e+02, Loss_u: 7.759e+01, Loss_r: 5.280e+01, Time: 0.14\n",
      "It: 690, Loss: 1.635e+02, Loss_u: 6.602e+01, Loss_r: 9.748e+01, Time: 0.17\n",
      "It: 700, Loss: 1.776e+02, Loss_u: 8.439e+01, Loss_r: 9.318e+01, Time: 0.14\n",
      "It: 710, Loss: 1.192e+02, Loss_u: 6.859e+01, Loss_r: 5.063e+01, Time: 0.14\n",
      "It: 720, Loss: 1.359e+02, Loss_u: 6.752e+01, Loss_r: 6.839e+01, Time: 0.16\n",
      "It: 730, Loss: 1.337e+02, Loss_u: 7.030e+01, Loss_r: 6.336e+01, Time: 0.15\n",
      "It: 740, Loss: 1.378e+02, Loss_u: 6.981e+01, Loss_r: 6.795e+01, Time: 0.16\n",
      "It: 750, Loss: 1.138e+02, Loss_u: 6.127e+01, Loss_r: 5.251e+01, Time: 0.16\n",
      "It: 760, Loss: 1.665e+02, Loss_u: 6.604e+01, Loss_r: 1.005e+02, Time: 0.16\n",
      "It: 770, Loss: 1.471e+02, Loss_u: 6.831e+01, Loss_r: 7.880e+01, Time: 0.13\n",
      "It: 780, Loss: 1.922e+02, Loss_u: 6.132e+01, Loss_r: 1.309e+02, Time: 0.12\n",
      "It: 790, Loss: 1.576e+02, Loss_u: 7.201e+01, Loss_r: 8.556e+01, Time: 0.13\n",
      "It: 800, Loss: 1.398e+02, Loss_u: 6.213e+01, Loss_r: 7.765e+01, Time: 0.14\n",
      "It: 810, Loss: 8.917e+01, Loss_u: 5.760e+01, Loss_r: 3.157e+01, Time: 0.13\n",
      "It: 820, Loss: 1.326e+02, Loss_u: 6.259e+01, Loss_r: 7.004e+01, Time: 0.15\n",
      "It: 830, Loss: 1.093e+02, Loss_u: 6.494e+01, Loss_r: 4.432e+01, Time: 0.15\n",
      "It: 840, Loss: 1.178e+02, Loss_u: 5.543e+01, Loss_r: 6.241e+01, Time: 0.14\n",
      "It: 850, Loss: 1.369e+02, Loss_u: 5.877e+01, Loss_r: 7.810e+01, Time: 0.14\n",
      "It: 860, Loss: 1.037e+02, Loss_u: 6.102e+01, Loss_r: 4.268e+01, Time: 0.13\n",
      "It: 870, Loss: 9.829e+01, Loss_u: 5.947e+01, Loss_r: 3.882e+01, Time: 0.13\n",
      "It: 880, Loss: 9.381e+01, Loss_u: 5.523e+01, Loss_r: 3.858e+01, Time: 0.12\n",
      "It: 890, Loss: 9.880e+01, Loss_u: 5.584e+01, Loss_r: 4.296e+01, Time: 0.15\n",
      "It: 900, Loss: 1.091e+02, Loss_u: 6.000e+01, Loss_r: 4.911e+01, Time: 0.14\n",
      "It: 910, Loss: 1.356e+02, Loss_u: 5.769e+01, Loss_r: 7.791e+01, Time: 0.17\n",
      "It: 920, Loss: 8.377e+01, Loss_u: 5.679e+01, Loss_r: 2.698e+01, Time: 0.12\n",
      "It: 930, Loss: 1.021e+02, Loss_u: 5.452e+01, Loss_r: 4.759e+01, Time: 0.16\n",
      "It: 940, Loss: 9.424e+01, Loss_u: 5.794e+01, Loss_r: 3.630e+01, Time: 0.12\n",
      "It: 950, Loss: 1.068e+02, Loss_u: 5.974e+01, Loss_r: 4.711e+01, Time: 0.14\n",
      "It: 960, Loss: 1.410e+02, Loss_u: 5.266e+01, Loss_r: 8.831e+01, Time: 0.15\n",
      "It: 970, Loss: 8.360e+01, Loss_u: 5.358e+01, Loss_r: 3.002e+01, Time: 0.19\n",
      "It: 980, Loss: 8.148e+01, Loss_u: 5.179e+01, Loss_r: 2.969e+01, Time: 0.12\n",
      "It: 990, Loss: 8.362e+01, Loss_u: 5.361e+01, Loss_r: 3.000e+01, Time: 0.13\n",
      "It: 1000, Loss: 1.073e+02, Loss_u: 5.655e+01, Loss_r: 5.075e+01, Time: 0.12\n",
      "Relative L2 error_u: 7.00e+00\n",
      "Relative L2 error_f: 7.17e-02\n",
      "Epoch:  2\n",
      "It: 0, Loss: 9.199e+03, Loss_u: 1.125e+00, Loss_r: 9.198e+03, Time: 2.92\n",
      "It: 10, Loss: 9.201e+03, Loss_u: 1.022e+00, Loss_r: 9.200e+03, Time: 0.12\n",
      "It: 20, Loss: 7.252e+03, Loss_u: 1.393e+00, Loss_r: 7.251e+03, Time: 0.14\n",
      "It: 30, Loss: 8.603e+03, Loss_u: 2.047e+00, Loss_r: 8.601e+03, Time: 0.15\n",
      "It: 40, Loss: 6.767e+03, Loss_u: 5.222e+00, Loss_r: 6.762e+03, Time: 0.14\n",
      "It: 50, Loss: 6.301e+03, Loss_u: 1.192e+01, Loss_r: 6.289e+03, Time: 0.13\n",
      "It: 60, Loss: 5.628e+03, Loss_u: 2.619e+01, Loss_r: 5.601e+03, Time: 0.14\n",
      "It: 70, Loss: 4.574e+03, Loss_u: 3.765e+01, Loss_r: 4.537e+03, Time: 0.13\n",
      "It: 80, Loss: 3.406e+03, Loss_u: 4.382e+01, Loss_r: 3.362e+03, Time: 0.14\n",
      "It: 90, Loss: 2.829e+03, Loss_u: 4.448e+01, Loss_r: 2.784e+03, Time: 0.13\n",
      "It: 100, Loss: 3.383e+03, Loss_u: 4.130e+01, Loss_r: 3.342e+03, Time: 0.14\n",
      "It: 110, Loss: 2.937e+03, Loss_u: 4.088e+01, Loss_r: 2.896e+03, Time: 0.13\n",
      "It: 120, Loss: 2.495e+03, Loss_u: 3.924e+01, Loss_r: 2.456e+03, Time: 0.13\n",
      "It: 130, Loss: 2.494e+03, Loss_u: 4.079e+01, Loss_r: 2.453e+03, Time: 0.13\n",
      "It: 140, Loss: 2.062e+03, Loss_u: 3.912e+01, Loss_r: 2.023e+03, Time: 0.13\n",
      "It: 150, Loss: 2.461e+03, Loss_u: 3.605e+01, Loss_r: 2.425e+03, Time: 0.12\n",
      "It: 160, Loss: 2.693e+03, Loss_u: 4.091e+01, Loss_r: 2.653e+03, Time: 0.14\n",
      "It: 170, Loss: 2.086e+03, Loss_u: 3.677e+01, Loss_r: 2.049e+03, Time: 0.15\n",
      "It: 180, Loss: 2.324e+03, Loss_u: 3.580e+01, Loss_r: 2.288e+03, Time: 0.15\n",
      "It: 190, Loss: 1.488e+03, Loss_u: 3.824e+01, Loss_r: 1.449e+03, Time: 0.13\n",
      "It: 200, Loss: 1.280e+03, Loss_u: 3.646e+01, Loss_r: 1.244e+03, Time: 0.15\n",
      "It: 210, Loss: 1.668e+03, Loss_u: 3.917e+01, Loss_r: 1.629e+03, Time: 0.15\n",
      "It: 220, Loss: 1.172e+03, Loss_u: 4.456e+01, Loss_r: 1.127e+03, Time: 0.15\n",
      "It: 230, Loss: 1.268e+03, Loss_u: 3.579e+01, Loss_r: 1.232e+03, Time: 0.17\n",
      "It: 240, Loss: 1.393e+03, Loss_u: 4.747e+01, Loss_r: 1.345e+03, Time: 0.16\n",
      "It: 250, Loss: 1.446e+03, Loss_u: 4.530e+01, Loss_r: 1.400e+03, Time: 0.13\n",
      "It: 260, Loss: 1.216e+03, Loss_u: 4.703e+01, Loss_r: 1.169e+03, Time: 0.14\n",
      "It: 270, Loss: 1.315e+03, Loss_u: 4.487e+01, Loss_r: 1.270e+03, Time: 0.14\n",
      "It: 280, Loss: 7.070e+02, Loss_u: 5.873e+01, Loss_r: 6.483e+02, Time: 0.13\n",
      "It: 290, Loss: 1.148e+03, Loss_u: 5.398e+01, Loss_r: 1.094e+03, Time: 0.13\n",
      "It: 300, Loss: 6.746e+02, Loss_u: 5.798e+01, Loss_r: 6.166e+02, Time: 0.13\n",
      "It: 310, Loss: 8.804e+02, Loss_u: 6.582e+01, Loss_r: 8.146e+02, Time: 0.20\n",
      "It: 320, Loss: 7.282e+02, Loss_u: 6.829e+01, Loss_r: 6.599e+02, Time: 0.11\n",
      "It: 330, Loss: 4.491e+02, Loss_u: 7.725e+01, Loss_r: 3.718e+02, Time: 0.11\n",
      "It: 340, Loss: 7.501e+02, Loss_u: 8.836e+01, Loss_r: 6.617e+02, Time: 0.12\n",
      "It: 350, Loss: 6.135e+02, Loss_u: 7.353e+01, Loss_r: 5.400e+02, Time: 0.11\n",
      "It: 360, Loss: 5.487e+02, Loss_u: 8.678e+01, Loss_r: 4.619e+02, Time: 0.13\n",
      "It: 370, Loss: 3.800e+02, Loss_u: 8.243e+01, Loss_r: 2.975e+02, Time: 0.14\n",
      "It: 380, Loss: 5.471e+02, Loss_u: 9.817e+01, Loss_r: 4.490e+02, Time: 0.15\n",
      "It: 390, Loss: 2.683e+02, Loss_u: 9.245e+01, Loss_r: 1.759e+02, Time: 0.13\n",
      "It: 400, Loss: 3.228e+02, Loss_u: 9.541e+01, Loss_r: 2.274e+02, Time: 0.14\n",
      "It: 410, Loss: 3.340e+02, Loss_u: 1.084e+02, Loss_r: 2.256e+02, Time: 0.15\n",
      "It: 420, Loss: 3.002e+02, Loss_u: 9.362e+01, Loss_r: 2.065e+02, Time: 0.13\n",
      "It: 430, Loss: 3.020e+02, Loss_u: 1.053e+02, Loss_r: 1.967e+02, Time: 0.16\n",
      "It: 440, Loss: 3.105e+02, Loss_u: 1.071e+02, Loss_r: 2.035e+02, Time: 0.15\n",
      "It: 450, Loss: 3.034e+02, Loss_u: 1.043e+02, Loss_r: 1.992e+02, Time: 0.14\n",
      "It: 460, Loss: 2.826e+02, Loss_u: 1.077e+02, Loss_r: 1.749e+02, Time: 0.13\n",
      "It: 470, Loss: 2.873e+02, Loss_u: 1.031e+02, Loss_r: 1.843e+02, Time: 0.13\n",
      "It: 480, Loss: 2.516e+02, Loss_u: 1.028e+02, Loss_r: 1.488e+02, Time: 0.13\n",
      "It: 490, Loss: 2.334e+02, Loss_u: 1.032e+02, Loss_r: 1.303e+02, Time: 0.13\n",
      "It: 500, Loss: 3.022e+02, Loss_u: 8.636e+01, Loss_r: 2.158e+02, Time: 0.15\n",
      "It: 510, Loss: 2.799e+02, Loss_u: 9.259e+01, Loss_r: 1.873e+02, Time: 0.14\n",
      "It: 520, Loss: 1.974e+02, Loss_u: 1.031e+02, Loss_r: 9.429e+01, Time: 0.13\n",
      "It: 530, Loss: 1.833e+02, Loss_u: 9.459e+01, Loss_r: 8.869e+01, Time: 0.14\n",
      "It: 540, Loss: 2.729e+02, Loss_u: 8.605e+01, Loss_r: 1.868e+02, Time: 0.13\n",
      "It: 550, Loss: 2.187e+02, Loss_u: 9.856e+01, Loss_r: 1.201e+02, Time: 0.15\n",
      "It: 560, Loss: 1.679e+02, Loss_u: 9.684e+01, Loss_r: 7.106e+01, Time: 0.13\n",
      "It: 570, Loss: 2.441e+02, Loss_u: 9.552e+01, Loss_r: 1.485e+02, Time: 0.15\n",
      "It: 580, Loss: 2.595e+02, Loss_u: 8.365e+01, Loss_r: 1.758e+02, Time: 0.14\n",
      "It: 590, Loss: 1.794e+02, Loss_u: 9.248e+01, Loss_r: 8.693e+01, Time: 0.14\n",
      "It: 600, Loss: 2.522e+02, Loss_u: 9.714e+01, Loss_r: 1.551e+02, Time: 0.14\n",
      "It: 610, Loss: 2.363e+02, Loss_u: 9.052e+01, Loss_r: 1.458e+02, Time: 0.15\n",
      "It: 620, Loss: 1.936e+02, Loss_u: 8.144e+01, Loss_r: 1.121e+02, Time: 0.12\n",
      "It: 630, Loss: 2.185e+02, Loss_u: 9.063e+01, Loss_r: 1.278e+02, Time: 0.15\n",
      "It: 640, Loss: 1.742e+02, Loss_u: 9.415e+01, Loss_r: 8.001e+01, Time: 0.15\n",
      "It: 650, Loss: 1.523e+02, Loss_u: 8.533e+01, Loss_r: 6.693e+01, Time: 0.14\n",
      "It: 660, Loss: 1.789e+02, Loss_u: 8.607e+01, Loss_r: 9.287e+01, Time: 0.13\n",
      "It: 670, Loss: 1.920e+02, Loss_u: 7.776e+01, Loss_r: 1.142e+02, Time: 0.14\n",
      "It: 680, Loss: 1.862e+02, Loss_u: 9.110e+01, Loss_r: 9.506e+01, Time: 0.14\n",
      "It: 690, Loss: 2.323e+02, Loss_u: 8.145e+01, Loss_r: 1.508e+02, Time: 0.13\n",
      "It: 700, Loss: 1.475e+02, Loss_u: 8.059e+01, Loss_r: 6.690e+01, Time: 0.13\n",
      "It: 710, Loss: 1.608e+02, Loss_u: 8.632e+01, Loss_r: 7.447e+01, Time: 0.15\n",
      "It: 720, Loss: 1.685e+02, Loss_u: 8.546e+01, Loss_r: 8.301e+01, Time: 0.13\n",
      "It: 730, Loss: 1.599e+02, Loss_u: 8.479e+01, Loss_r: 7.510e+01, Time: 0.11\n",
      "It: 740, Loss: 1.633e+02, Loss_u: 7.772e+01, Loss_r: 8.558e+01, Time: 0.13\n",
      "It: 750, Loss: 1.912e+02, Loss_u: 8.335e+01, Loss_r: 1.079e+02, Time: 0.13\n",
      "It: 760, Loss: 1.433e+02, Loss_u: 7.341e+01, Loss_r: 6.990e+01, Time: 0.13\n",
      "It: 770, Loss: 1.411e+02, Loss_u: 7.829e+01, Loss_r: 6.282e+01, Time: 0.11\n",
      "It: 780, Loss: 1.849e+02, Loss_u: 7.512e+01, Loss_r: 1.098e+02, Time: 0.13\n",
      "It: 790, Loss: 1.501e+02, Loss_u: 8.040e+01, Loss_r: 6.975e+01, Time: 0.21\n",
      "It: 800, Loss: 1.308e+02, Loss_u: 7.904e+01, Loss_r: 5.175e+01, Time: 0.13\n",
      "It: 810, Loss: 1.796e+02, Loss_u: 7.447e+01, Loss_r: 1.051e+02, Time: 0.15\n",
      "It: 820, Loss: 1.377e+02, Loss_u: 7.225e+01, Loss_r: 6.542e+01, Time: 0.15\n",
      "It: 830, Loss: 1.281e+02, Loss_u: 7.182e+01, Loss_r: 5.631e+01, Time: 0.14\n",
      "It: 840, Loss: 1.595e+02, Loss_u: 6.600e+01, Loss_r: 9.347e+01, Time: 0.16\n",
      "It: 850, Loss: 1.671e+02, Loss_u: 8.151e+01, Loss_r: 8.555e+01, Time: 0.15\n",
      "It: 860, Loss: 1.583e+02, Loss_u: 6.622e+01, Loss_r: 9.211e+01, Time: 0.11\n",
      "It: 870, Loss: 1.056e+02, Loss_u: 6.061e+01, Loss_r: 4.502e+01, Time: 0.12\n",
      "It: 880, Loss: 1.780e+02, Loss_u: 7.703e+01, Loss_r: 1.010e+02, Time: 0.13\n",
      "It: 890, Loss: 1.325e+02, Loss_u: 6.440e+01, Loss_r: 6.807e+01, Time: 0.12\n",
      "It: 900, Loss: 1.579e+02, Loss_u: 5.887e+01, Loss_r: 9.899e+01, Time: 0.14\n",
      "It: 910, Loss: 1.341e+02, Loss_u: 6.371e+01, Loss_r: 7.039e+01, Time: 0.13\n",
      "It: 920, Loss: 1.793e+02, Loss_u: 6.235e+01, Loss_r: 1.169e+02, Time: 0.12\n",
      "It: 930, Loss: 1.317e+02, Loss_u: 5.870e+01, Loss_r: 7.302e+01, Time: 0.12\n",
      "It: 940, Loss: 1.145e+02, Loss_u: 6.112e+01, Loss_r: 5.339e+01, Time: 0.12\n",
      "It: 950, Loss: 9.868e+01, Loss_u: 5.870e+01, Loss_r: 3.998e+01, Time: 0.13\n",
      "It: 960, Loss: 1.214e+02, Loss_u: 5.931e+01, Loss_r: 6.210e+01, Time: 0.13\n",
      "It: 970, Loss: 8.966e+01, Loss_u: 5.408e+01, Loss_r: 3.558e+01, Time: 0.13\n",
      "It: 980, Loss: 9.804e+01, Loss_u: 5.640e+01, Loss_r: 4.165e+01, Time: 0.12\n",
      "It: 990, Loss: 1.279e+02, Loss_u: 5.436e+01, Loss_r: 7.356e+01, Time: 0.15\n",
      "It: 1000, Loss: 1.319e+02, Loss_u: 5.563e+01, Loss_r: 7.622e+01, Time: 0.13\n",
      "Relative L2 error_u: 7.11e+00\n",
      "Relative L2 error_f: 8.93e-02\n",
      "\n",
      "average of time_list: 19.13122868537903\n",
      "average of error_u_list: 7.054819976886802\n",
      "average of error_f_list: 0.08053132322320912\n",
      "Method:  full_batch\n",
      "Epoch:  1\n",
      "It: 0, Loss: 9.916e+03, Loss_u: 2.015e+00, Loss_r: 9.914e+03, Time: 2.86\n",
      "It: 10, Loss: 9.839e+03, Loss_u: 1.317e+01, Loss_r: 9.826e+03, Time: 0.48\n",
      "It: 20, Loss: 9.072e+03, Loss_u: 1.983e+01, Loss_r: 9.052e+03, Time: 0.52\n",
      "It: 30, Loss: 6.921e+03, Loss_u: 1.374e+01, Loss_r: 6.908e+03, Time: 0.55\n",
      "It: 40, Loss: 5.515e+03, Loss_u: 2.697e+01, Loss_r: 5.488e+03, Time: 0.49\n",
      "It: 50, Loss: 4.248e+03, Loss_u: 3.012e+01, Loss_r: 4.217e+03, Time: 0.49\n",
      "It: 60, Loss: 3.381e+03, Loss_u: 3.102e+01, Loss_r: 3.350e+03, Time: 0.49\n",
      "It: 70, Loss: 2.647e+03, Loss_u: 3.192e+01, Loss_r: 2.615e+03, Time: 0.52\n",
      "It: 80, Loss: 2.241e+03, Loss_u: 3.688e+01, Loss_r: 2.204e+03, Time: 0.52\n",
      "It: 90, Loss: 1.919e+03, Loss_u: 3.513e+01, Loss_r: 1.884e+03, Time: 0.53\n",
      "It: 100, Loss: 1.646e+03, Loss_u: 3.487e+01, Loss_r: 1.611e+03, Time: 0.48\n",
      "It: 110, Loss: 1.399e+03, Loss_u: 3.470e+01, Loss_r: 1.364e+03, Time: 0.51\n",
      "It: 120, Loss: 1.114e+03, Loss_u: 3.743e+01, Loss_r: 1.076e+03, Time: 0.49\n",
      "It: 130, Loss: 7.715e+02, Loss_u: 4.409e+01, Loss_r: 7.274e+02, Time: 0.51\n",
      "It: 140, Loss: 4.713e+02, Loss_u: 6.148e+01, Loss_r: 4.098e+02, Time: 0.53\n",
      "It: 150, Loss: 3.093e+02, Loss_u: 8.045e+01, Loss_r: 2.288e+02, Time: 0.54\n",
      "It: 160, Loss: 2.380e+02, Loss_u: 9.048e+01, Loss_r: 1.475e+02, Time: 0.50\n",
      "It: 170, Loss: 2.085e+02, Loss_u: 9.035e+01, Loss_r: 1.181e+02, Time: 0.49\n",
      "It: 180, Loss: 1.881e+02, Loss_u: 9.651e+01, Loss_r: 9.158e+01, Time: 0.52\n",
      "It: 190, Loss: 1.722e+02, Loss_u: 9.015e+01, Loss_r: 8.200e+01, Time: 0.51\n",
      "It: 200, Loss: 1.600e+02, Loss_u: 8.740e+01, Loss_r: 7.265e+01, Time: 0.54\n",
      "It: 210, Loss: 1.499e+02, Loss_u: 8.485e+01, Loss_r: 6.506e+01, Time: 0.54\n",
      "It: 220, Loss: 1.405e+02, Loss_u: 8.152e+01, Loss_r: 5.896e+01, Time: 0.49\n",
      "It: 230, Loss: 1.318e+02, Loss_u: 7.802e+01, Loss_r: 5.376e+01, Time: 0.47\n",
      "It: 240, Loss: 1.235e+02, Loss_u: 7.456e+01, Loss_r: 4.899e+01, Time: 0.50\n",
      "It: 250, Loss: 1.158e+02, Loss_u: 7.128e+01, Loss_r: 4.450e+01, Time: 0.57\n",
      "It: 260, Loss: 1.084e+02, Loss_u: 6.780e+01, Loss_r: 4.065e+01, Time: 0.59\n",
      "It: 270, Loss: 1.016e+02, Loss_u: 6.434e+01, Loss_r: 3.722e+01, Time: 0.49\n",
      "It: 280, Loss: 9.520e+01, Loss_u: 6.105e+01, Loss_r: 3.415e+01, Time: 0.50\n",
      "It: 290, Loss: 9.529e+01, Loss_u: 5.870e+01, Loss_r: 3.659e+01, Time: 0.53\n",
      "It: 300, Loss: 8.609e+01, Loss_u: 5.421e+01, Loss_r: 3.188e+01, Time: 0.53\n",
      "It: 310, Loss: 7.936e+01, Loss_u: 5.081e+01, Loss_r: 2.854e+01, Time: 0.59\n",
      "It: 320, Loss: 7.307e+01, Loss_u: 4.773e+01, Loss_r: 2.534e+01, Time: 0.49\n",
      "It: 330, Loss: 6.770e+01, Loss_u: 4.455e+01, Loss_r: 2.315e+01, Time: 0.55\n",
      "It: 340, Loss: 6.269e+01, Loss_u: 4.152e+01, Loss_r: 2.116e+01, Time: 0.50\n",
      "It: 350, Loss: 5.794e+01, Loss_u: 3.880e+01, Loss_r: 1.915e+01, Time: 0.54\n",
      "It: 360, Loss: 5.362e+01, Loss_u: 3.621e+01, Loss_r: 1.741e+01, Time: 0.55\n",
      "It: 370, Loss: 4.988e+01, Loss_u: 3.405e+01, Loss_r: 1.583e+01, Time: 0.55\n",
      "It: 380, Loss: 4.705e+01, Loss_u: 3.212e+01, Loss_r: 1.494e+01, Time: 0.51\n",
      "It: 390, Loss: 5.071e+01, Loss_u: 3.054e+01, Loss_r: 2.017e+01, Time: 0.53\n",
      "It: 400, Loss: 4.348e+01, Loss_u: 2.960e+01, Loss_r: 1.388e+01, Time: 0.56\n",
      "It: 410, Loss: 4.015e+01, Loss_u: 2.894e+01, Loss_r: 1.121e+01, Time: 0.58\n",
      "It: 420, Loss: 3.882e+01, Loss_u: 2.840e+01, Loss_r: 1.042e+01, Time: 0.54\n",
      "It: 430, Loss: 3.733e+01, Loss_u: 2.791e+01, Loss_r: 9.423e+00, Time: 0.53\n",
      "It: 440, Loss: 3.619e+01, Loss_u: 2.752e+01, Loss_r: 8.674e+00, Time: 0.54\n",
      "It: 450, Loss: 3.523e+01, Loss_u: 2.724e+01, Loss_r: 7.985e+00, Time: 0.51\n",
      "It: 460, Loss: 3.442e+01, Loss_u: 2.700e+01, Loss_r: 7.422e+00, Time: 0.56\n",
      "It: 470, Loss: 3.373e+01, Loss_u: 2.679e+01, Loss_r: 6.942e+00, Time: 0.54\n",
      "It: 480, Loss: 3.358e+01, Loss_u: 2.660e+01, Loss_r: 6.974e+00, Time: 0.51\n",
      "It: 490, Loss: 3.325e+01, Loss_u: 2.646e+01, Loss_r: 6.800e+00, Time: 0.53\n",
      "It: 500, Loss: 3.401e+01, Loss_u: 2.630e+01, Loss_r: 7.708e+00, Time: 0.57\n",
      "It: 510, Loss: 3.746e+01, Loss_u: 2.609e+01, Loss_r: 1.137e+01, Time: 0.57\n",
      "It: 520, Loss: 3.207e+01, Loss_u: 2.596e+01, Loss_r: 6.112e+00, Time: 0.56\n",
      "It: 530, Loss: 3.090e+01, Loss_u: 2.576e+01, Loss_r: 5.138e+00, Time: 0.53\n",
      "It: 540, Loss: 3.066e+01, Loss_u: 2.557e+01, Loss_r: 5.090e+00, Time: 0.52\n",
      "It: 550, Loss: 3.019e+01, Loss_u: 2.541e+01, Loss_r: 4.780e+00, Time: 0.54\n",
      "It: 560, Loss: 2.977e+01, Loss_u: 2.523e+01, Loss_r: 4.535e+00, Time: 0.53\n",
      "It: 570, Loss: 2.941e+01, Loss_u: 2.505e+01, Loss_r: 4.356e+00, Time: 0.53\n",
      "It: 580, Loss: 2.907e+01, Loss_u: 2.487e+01, Loss_r: 4.202e+00, Time: 0.53\n",
      "It: 590, Loss: 2.876e+01, Loss_u: 2.468e+01, Loss_r: 4.086e+00, Time: 0.51\n",
      "It: 600, Loss: 3.234e+01, Loss_u: 2.455e+01, Loss_r: 7.791e+00, Time: 0.52\n",
      "It: 610, Loss: 3.451e+01, Loss_u: 2.431e+01, Loss_r: 1.020e+01, Time: 0.52\n",
      "It: 620, Loss: 3.022e+01, Loss_u: 2.413e+01, Loss_r: 6.092e+00, Time: 0.58\n",
      "It: 630, Loss: 2.852e+01, Loss_u: 2.385e+01, Loss_r: 4.672e+00, Time: 0.57\n",
      "It: 640, Loss: 2.757e+01, Loss_u: 2.371e+01, Loss_r: 3.863e+00, Time: 0.52\n",
      "It: 650, Loss: 2.707e+01, Loss_u: 2.345e+01, Loss_r: 3.620e+00, Time: 0.51\n",
      "It: 660, Loss: 2.669e+01, Loss_u: 2.326e+01, Loss_r: 3.427e+00, Time: 0.50\n",
      "It: 670, Loss: 2.633e+01, Loss_u: 2.302e+01, Loss_r: 3.310e+00, Time: 0.55\n",
      "It: 680, Loss: 2.603e+01, Loss_u: 2.278e+01, Loss_r: 3.253e+00, Time: 0.56\n",
      "It: 690, Loss: 2.588e+01, Loss_u: 2.249e+01, Loss_r: 3.388e+00, Time: 0.53\n",
      "It: 700, Loss: 3.407e+01, Loss_u: 2.213e+01, Loss_r: 1.194e+01, Time: 0.51\n",
      "It: 710, Loss: 2.590e+01, Loss_u: 2.211e+01, Loss_r: 3.791e+00, Time: 0.53\n",
      "It: 720, Loss: 2.575e+01, Loss_u: 2.184e+01, Loss_r: 3.911e+00, Time: 0.56\n",
      "It: 730, Loss: 2.457e+01, Loss_u: 2.151e+01, Loss_r: 3.060e+00, Time: 0.59\n",
      "It: 740, Loss: 2.417e+01, Loss_u: 2.112e+01, Loss_r: 3.050e+00, Time: 0.55\n",
      "It: 750, Loss: 2.368e+01, Loss_u: 2.083e+01, Loss_r: 2.848e+00, Time: 0.52\n",
      "It: 760, Loss: 2.331e+01, Loss_u: 2.053e+01, Loss_r: 2.782e+00, Time: 0.51\n",
      "It: 770, Loss: 2.315e+01, Loss_u: 2.017e+01, Loss_r: 2.982e+00, Time: 0.52\n",
      "It: 780, Loss: 3.670e+01, Loss_u: 2.004e+01, Loss_r: 1.665e+01, Time: 0.56\n",
      "It: 790, Loss: 2.404e+01, Loss_u: 1.929e+01, Loss_r: 4.745e+00, Time: 0.59\n",
      "It: 800, Loss: 2.201e+01, Loss_u: 1.914e+01, Loss_r: 2.869e+00, Time: 0.51\n",
      "It: 810, Loss: 2.138e+01, Loss_u: 1.858e+01, Loss_r: 2.804e+00, Time: 0.54\n",
      "It: 820, Loss: 2.098e+01, Loss_u: 1.822e+01, Loss_r: 2.758e+00, Time: 0.53\n",
      "It: 830, Loss: 2.037e+01, Loss_u: 1.767e+01, Loss_r: 2.692e+00, Time: 0.55\n",
      "It: 840, Loss: 1.981e+01, Loss_u: 1.712e+01, Loss_r: 2.695e+00, Time: 0.56\n",
      "It: 850, Loss: 1.953e+01, Loss_u: 1.646e+01, Loss_r: 3.071e+00, Time: 0.51\n",
      "It: 860, Loss: 2.448e+01, Loss_u: 1.557e+01, Loss_r: 8.913e+00, Time: 0.49\n",
      "It: 870, Loss: 1.960e+01, Loss_u: 1.556e+01, Loss_r: 4.041e+00, Time: 0.54\n",
      "It: 880, Loss: 1.733e+01, Loss_u: 1.442e+01, Loss_r: 2.911e+00, Time: 0.56\n",
      "It: 890, Loss: 1.678e+01, Loss_u: 1.364e+01, Loss_r: 3.139e+00, Time: 0.57\n",
      "It: 900, Loss: 1.638e+01, Loss_u: 1.280e+01, Loss_r: 3.580e+00, Time: 0.54\n",
      "It: 910, Loss: 2.038e+01, Loss_u: 1.182e+01, Loss_r: 8.564e+00, Time: 0.53\n",
      "It: 920, Loss: 1.571e+01, Loss_u: 1.095e+01, Loss_r: 4.765e+00, Time: 0.53\n",
      "It: 930, Loss: 1.391e+01, Loss_u: 1.013e+01, Loss_r: 3.779e+00, Time: 0.55\n",
      "It: 940, Loss: 1.216e+01, Loss_u: 8.953e+00, Loss_r: 3.207e+00, Time: 0.56\n",
      "It: 950, Loss: 1.075e+01, Loss_u: 7.883e+00, Loss_r: 2.868e+00, Time: 0.58\n",
      "It: 960, Loss: 1.011e+01, Loss_u: 6.977e+00, Loss_r: 3.135e+00, Time: 0.50\n",
      "It: 970, Loss: 1.638e+01, Loss_u: 6.678e+00, Loss_r: 9.705e+00, Time: 0.56\n",
      "It: 980, Loss: 8.053e+00, Loss_u: 5.140e+00, Loss_r: 2.913e+00, Time: 0.53\n",
      "It: 990, Loss: 7.724e+00, Loss_u: 4.099e+00, Loss_r: 3.625e+00, Time: 0.59\n",
      "It: 1000, Loss: 1.063e+01, Loss_u: 3.425e+00, Loss_r: 7.204e+00, Time: 0.58\n",
      "Relative L2 error_u: 2.09e+00\n",
      "Relative L2 error_f: 2.85e-02\n",
      "Epoch:  2\n",
      "It: 0, Loss: 9.859e+03, Loss_u: 9.915e-01, Loss_r: 9.858e+03, Time: 2.96\n",
      "It: 10, Loss: 9.615e+03, Loss_u: 1.273e+00, Loss_r: 9.613e+03, Time: 0.54\n",
      "It: 20, Loss: 7.890e+03, Loss_u: 2.617e+00, Loss_r: 7.888e+03, Time: 0.50\n",
      "It: 30, Loss: 7.114e+03, Loss_u: 2.144e+00, Loss_r: 7.112e+03, Time: 0.53\n",
      "It: 40, Loss: 6.146e+03, Loss_u: 9.228e+00, Loss_r: 6.136e+03, Time: 0.55\n",
      "It: 50, Loss: 4.875e+03, Loss_u: 2.512e+01, Loss_r: 4.850e+03, Time: 0.55\n",
      "It: 60, Loss: 3.335e+03, Loss_u: 4.245e+01, Loss_r: 3.292e+03, Time: 0.48\n",
      "It: 70, Loss: 2.568e+03, Loss_u: 4.913e+01, Loss_r: 2.519e+03, Time: 0.49\n",
      "It: 80, Loss: 2.149e+03, Loss_u: 4.077e+01, Loss_r: 2.108e+03, Time: 0.48\n",
      "It: 90, Loss: 1.875e+03, Loss_u: 4.036e+01, Loss_r: 1.835e+03, Time: 0.50\n",
      "It: 100, Loss: 1.653e+03, Loss_u: 3.730e+01, Loss_r: 1.616e+03, Time: 0.53\n",
      "It: 110, Loss: 1.476e+03, Loss_u: 3.777e+01, Loss_r: 1.438e+03, Time: 0.51\n",
      "It: 120, Loss: 1.236e+03, Loss_u: 4.066e+01, Loss_r: 1.195e+03, Time: 0.48\n",
      "It: 130, Loss: 9.077e+02, Loss_u: 4.679e+01, Loss_r: 8.609e+02, Time: 0.50\n",
      "It: 140, Loss: 5.529e+02, Loss_u: 6.427e+01, Loss_r: 4.886e+02, Time: 0.57\n",
      "It: 150, Loss: 3.217e+02, Loss_u: 8.424e+01, Loss_r: 2.375e+02, Time: 0.56\n",
      "It: 160, Loss: 2.431e+02, Loss_u: 9.268e+01, Loss_r: 1.504e+02, Time: 0.58\n",
      "It: 170, Loss: 2.004e+02, Loss_u: 9.634e+01, Loss_r: 1.040e+02, Time: 0.55\n",
      "It: 180, Loss: 1.803e+02, Loss_u: 9.274e+01, Loss_r: 8.761e+01, Time: 0.52\n",
      "It: 190, Loss: 1.672e+02, Loss_u: 9.112e+01, Loss_r: 7.608e+01, Time: 0.49\n",
      "It: 200, Loss: 1.571e+02, Loss_u: 8.912e+01, Loss_r: 6.796e+01, Time: 0.54\n",
      "It: 210, Loss: 1.488e+02, Loss_u: 8.695e+01, Loss_r: 6.181e+01, Time: 0.57\n",
      "It: 220, Loss: 1.417e+02, Loss_u: 8.449e+01, Loss_r: 5.724e+01, Time: 0.58\n",
      "It: 230, Loss: 1.357e+02, Loss_u: 8.197e+01, Loss_r: 5.368e+01, Time: 0.51\n",
      "It: 240, Loss: 1.303e+02, Loss_u: 7.937e+01, Loss_r: 5.094e+01, Time: 0.58\n",
      "It: 250, Loss: 1.255e+02, Loss_u: 7.694e+01, Loss_r: 4.861e+01, Time: 0.53\n",
      "It: 260, Loss: 1.217e+02, Loss_u: 7.508e+01, Loss_r: 4.659e+01, Time: 0.58\n",
      "It: 270, Loss: 1.187e+02, Loss_u: 7.125e+01, Loss_r: 4.747e+01, Time: 0.58\n",
      "It: 280, Loss: 1.153e+02, Loss_u: 6.931e+01, Loss_r: 4.601e+01, Time: 0.52\n",
      "It: 290, Loss: 1.113e+02, Loss_u: 6.790e+01, Loss_r: 4.340e+01, Time: 0.53\n",
      "It: 300, Loss: 1.076e+02, Loss_u: 6.664e+01, Loss_r: 4.097e+01, Time: 0.53\n",
      "It: 310, Loss: 1.046e+02, Loss_u: 6.524e+01, Loss_r: 3.932e+01, Time: 0.55\n",
      "It: 320, Loss: 1.017e+02, Loss_u: 6.374e+01, Loss_r: 3.796e+01, Time: 0.57\n",
      "It: 330, Loss: 9.891e+01, Loss_u: 6.230e+01, Loss_r: 3.661e+01, Time: 0.50\n",
      "It: 340, Loss: 9.617e+01, Loss_u: 6.098e+01, Loss_r: 3.519e+01, Time: 0.56\n",
      "It: 350, Loss: 9.350e+01, Loss_u: 5.960e+01, Loss_r: 3.390e+01, Time: 0.51\n",
      "It: 360, Loss: 9.086e+01, Loss_u: 5.815e+01, Loss_r: 3.271e+01, Time: 0.57\n",
      "It: 370, Loss: 8.825e+01, Loss_u: 5.673e+01, Loss_r: 3.152e+01, Time: 0.55\n",
      "It: 380, Loss: 8.567e+01, Loss_u: 5.524e+01, Loss_r: 3.043e+01, Time: 0.50\n",
      "It: 390, Loss: 8.848e+01, Loss_u: 5.239e+01, Loss_r: 3.609e+01, Time: 0.53\n",
      "It: 400, Loss: 8.369e+01, Loss_u: 5.352e+01, Loss_r: 3.017e+01, Time: 0.50\n",
      "It: 410, Loss: 7.943e+01, Loss_u: 5.132e+01, Loss_r: 2.811e+01, Time: 0.59\n",
      "It: 420, Loss: 7.636e+01, Loss_u: 4.964e+01, Loss_r: 2.672e+01, Time: 0.56\n",
      "It: 430, Loss: 7.343e+01, Loss_u: 4.802e+01, Loss_r: 2.541e+01, Time: 0.52\n",
      "It: 440, Loss: 7.064e+01, Loss_u: 4.629e+01, Loss_r: 2.435e+01, Time: 0.54\n",
      "It: 450, Loss: 6.801e+01, Loss_u: 4.453e+01, Loss_r: 2.347e+01, Time: 0.55\n",
      "It: 460, Loss: 6.533e+01, Loss_u: 4.294e+01, Loss_r: 2.239e+01, Time: 0.55\n",
      "It: 470, Loss: 6.268e+01, Loss_u: 4.114e+01, Loss_r: 2.154e+01, Time: 0.56\n",
      "It: 480, Loss: 6.005e+01, Loss_u: 3.947e+01, Loss_r: 2.058e+01, Time: 0.56\n",
      "It: 490, Loss: 5.752e+01, Loss_u: 3.792e+01, Loss_r: 1.960e+01, Time: 0.55\n",
      "It: 500, Loss: 5.847e+01, Loss_u: 3.711e+01, Loss_r: 2.136e+01, Time: 0.54\n",
      "It: 510, Loss: 5.743e+01, Loss_u: 3.420e+01, Loss_r: 2.323e+01, Time: 0.57\n",
      "It: 520, Loss: 5.280e+01, Loss_u: 3.330e+01, Loss_r: 1.950e+01, Time: 0.64\n",
      "It: 530, Loss: 4.914e+01, Loss_u: 3.232e+01, Loss_r: 1.682e+01, Time: 0.57\n",
      "It: 540, Loss: 4.671e+01, Loss_u: 3.138e+01, Loss_r: 1.533e+01, Time: 0.53\n",
      "It: 550, Loss: 4.484e+01, Loss_u: 3.052e+01, Loss_r: 1.432e+01, Time: 0.61\n",
      "It: 560, Loss: 4.314e+01, Loss_u: 2.965e+01, Loss_r: 1.349e+01, Time: 0.53\n",
      "It: 570, Loss: 4.151e+01, Loss_u: 2.879e+01, Loss_r: 1.272e+01, Time: 0.56\n",
      "It: 580, Loss: 4.004e+01, Loss_u: 2.812e+01, Loss_r: 1.192e+01, Time: 0.57\n",
      "It: 590, Loss: 3.871e+01, Loss_u: 2.752e+01, Loss_r: 1.119e+01, Time: 0.52\n",
      "It: 600, Loss: 3.754e+01, Loss_u: 2.702e+01, Loss_r: 1.053e+01, Time: 0.53\n",
      "It: 610, Loss: 3.932e+01, Loss_u: 2.664e+01, Loss_r: 1.268e+01, Time: 0.51\n",
      "It: 620, Loss: 3.953e+01, Loss_u: 2.639e+01, Loss_r: 1.314e+01, Time: 0.61\n",
      "It: 630, Loss: 3.488e+01, Loss_u: 2.589e+01, Loss_r: 8.983e+00, Time: 0.59\n",
      "It: 640, Loss: 3.445e+01, Loss_u: 2.567e+01, Loss_r: 8.779e+00, Time: 0.57\n",
      "It: 650, Loss: 3.340e+01, Loss_u: 2.546e+01, Loss_r: 7.934e+00, Time: 0.59\n",
      "It: 660, Loss: 3.275e+01, Loss_u: 2.529e+01, Loss_r: 7.462e+00, Time: 0.52\n",
      "It: 670, Loss: 3.221e+01, Loss_u: 2.513e+01, Loss_r: 7.083e+00, Time: 0.58\n",
      "It: 680, Loss: 3.182e+01, Loss_u: 2.499e+01, Loss_r: 6.828e+00, Time: 0.57\n",
      "It: 690, Loss: 3.303e+01, Loss_u: 2.491e+01, Loss_r: 8.119e+00, Time: 0.64\n",
      "It: 700, Loss: 3.125e+01, Loss_u: 2.475e+01, Loss_r: 6.499e+00, Time: 0.56\n",
      "It: 710, Loss: 3.081e+01, Loss_u: 2.461e+01, Loss_r: 6.198e+00, Time: 0.59\n",
      "It: 720, Loss: 3.041e+01, Loss_u: 2.446e+01, Loss_r: 5.952e+00, Time: 0.61\n",
      "It: 730, Loss: 3.001e+01, Loss_u: 2.437e+01, Loss_r: 5.646e+00, Time: 0.58\n",
      "It: 740, Loss: 2.966e+01, Loss_u: 2.421e+01, Loss_r: 5.443e+00, Time: 0.53\n",
      "It: 750, Loss: 3.081e+01, Loss_u: 2.410e+01, Loss_r: 6.710e+00, Time: 0.56\n",
      "It: 760, Loss: 3.071e+01, Loss_u: 2.405e+01, Loss_r: 6.662e+00, Time: 0.56\n",
      "It: 770, Loss: 2.895e+01, Loss_u: 2.377e+01, Loss_r: 5.174e+00, Time: 0.58\n",
      "It: 780, Loss: 2.896e+01, Loss_u: 2.362e+01, Loss_r: 5.347e+00, Time: 0.58\n",
      "It: 790, Loss: 2.838e+01, Loss_u: 2.349e+01, Loss_r: 4.891e+00, Time: 0.52\n",
      "It: 800, Loss: 2.818e+01, Loss_u: 2.336e+01, Loss_r: 4.820e+00, Time: 0.52\n",
      "It: 810, Loss: 3.086e+01, Loss_u: 2.338e+01, Loss_r: 7.489e+00, Time: 0.57\n",
      "It: 820, Loss: 2.778e+01, Loss_u: 2.295e+01, Loss_r: 4.826e+00, Time: 0.59\n",
      "It: 830, Loss: 2.772e+01, Loss_u: 2.285e+01, Loss_r: 4.874e+00, Time: 0.58\n",
      "It: 840, Loss: 2.716e+01, Loss_u: 2.255e+01, Loss_r: 4.608e+00, Time: 0.50\n",
      "It: 850, Loss: 2.674e+01, Loss_u: 2.243e+01, Loss_r: 4.313e+00, Time: 0.53\n",
      "It: 860, Loss: 2.664e+01, Loss_u: 2.219e+01, Loss_r: 4.445e+00, Time: 0.53\n",
      "It: 870, Loss: 3.267e+01, Loss_u: 2.202e+01, Loss_r: 1.065e+01, Time: 0.56\n",
      "It: 880, Loss: 2.825e+01, Loss_u: 2.180e+01, Loss_r: 6.450e+00, Time: 0.58\n",
      "It: 890, Loss: 2.560e+01, Loss_u: 2.151e+01, Loss_r: 4.091e+00, Time: 0.51\n",
      "It: 900, Loss: 2.565e+01, Loss_u: 2.124e+01, Loss_r: 4.408e+00, Time: 0.52\n",
      "It: 910, Loss: 2.690e+01, Loss_u: 2.092e+01, Loss_r: 5.977e+00, Time: 0.62\n",
      "It: 920, Loss: 2.528e+01, Loss_u: 2.071e+01, Loss_r: 4.573e+00, Time: 0.58\n",
      "It: 930, Loss: 2.486e+01, Loss_u: 2.062e+01, Loss_r: 4.241e+00, Time: 0.60\n",
      "It: 940, Loss: 2.436e+01, Loss_u: 2.018e+01, Loss_r: 4.182e+00, Time: 0.55\n",
      "It: 950, Loss: 2.377e+01, Loss_u: 1.999e+01, Loss_r: 3.783e+00, Time: 0.52\n",
      "It: 960, Loss: 2.345e+01, Loss_u: 1.968e+01, Loss_r: 3.769e+00, Time: 0.59\n",
      "It: 970, Loss: 2.311e+01, Loss_u: 1.936e+01, Loss_r: 3.750e+00, Time: 0.56\n",
      "It: 980, Loss: 2.383e+01, Loss_u: 1.913e+01, Loss_r: 4.701e+00, Time: 0.57\n",
      "It: 990, Loss: 2.762e+01, Loss_u: 1.897e+01, Loss_r: 8.647e+00, Time: 0.55\n",
      "It: 1000, Loss: 2.324e+01, Loss_u: 1.808e+01, Loss_r: 5.160e+00, Time: 0.53\n",
      "Relative L2 error_u: 5.06e+00\n",
      "Relative L2 error_f: 2.58e-02\n",
      "\n",
      "average of time_list: 61.601646065711975\n",
      "average of error_u_list: 3.576910454041215\n",
      "average of error_f_list: 0.027176298214993587\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nIter =1001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters of equations\n",
    "alpha = -1.0\n",
    "beta = 0.0\n",
    "gamma = 1.0\n",
    "k = 3\n",
    "# Domain boundaries\n",
    "ics_coords = np.array([[0.0, 0.0], [0.0, 1.0]])\n",
    "bc1_coords = np.array([[0.0, 0.0], [1.0, 0.0]])\n",
    "bc2_coords = np.array([[0.0, 1.0], [1.0, 1.0]])\n",
    "dom_coords = np.array([[0.0, 0.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Define model\n",
    "layers = [2, 50, 50, 50, 50, 50, 1]\n",
    "mode = 'M1'          # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False  # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "\n",
    "iterations = 2\n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "        # Create initial conditions samplers\n",
    "        ics_sampler = Sampler(2, ics_coords, lambda x: u(x), name='Initial Condition 1')\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x), name='Dirichlet BC2')\n",
    "        bcs_sampler = [bc1, bc2]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, alpha, beta, gamma, k), name='Forcing')\n",
    "        bcs_sampler = [bc1, bc2]\n",
    "\n",
    "        [elapsed, error_u , error_f] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma ,k ,mode , stiff_ratio ,  X_star , u_star , f_star  , nIter , batch_size  , bcbatch_size , ubatch_size )\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_f_list.append(error_f)\n",
    "\n",
    "    # print(\"\\n\\nMethod: \", mtd)\n",
    "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    print(\"average of error_f_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,  error_f_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(\"./Klein_Gordon_model_tf_dataset/Klein_Gordon_model_\"+mode+\"_result_mb\"+str(bcbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### database is a vailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters of equations\n",
    "alpha = -1.0\n",
    "beta = 0.0\n",
    "gamma = 1.0\n",
    "k = 3\n",
    "\n",
    "# Domain boundaries\n",
    "ics_coords = np.array([[0.0, 0.0],\n",
    "                        [0.0, 1.0]])\n",
    "bc1_coords = np.array([[0.0, 0.0],\n",
    "                        [1.0, 0.0]])\n",
    "bc2_coords = np.array([[0.0, 1.0],\n",
    "                        [1.0, 1.0]])\n",
    "dom_coords = np.array([[0.0, 0.0],\n",
    "                        [1.0, 1.0]])\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = Sampler(2, ics_coords, lambda x: u(x), name='Initial Condition 1')\n",
    "\n",
    "# Create boundary conditions samplers\n",
    "bc1 = Sampler(2, bc1_coords, lambda x: u(x), name='Dirichlet BC1')\n",
    "bc2 = Sampler(2, bc2_coords, lambda x: u(x), name='Dirichlet BC2')\n",
    "bcs_sampler = [bc1, bc2]\n",
    "\n",
    "# Create residual sampler\n",
    "res_sampler = Sampler(2, dom_coords, lambda x: f(x, alpha, beta, gamma, k), name='Forcing')\n",
    "\n",
    "# Define model\n",
    "layers = [2, 50, 50, 50, 50, 50, 1]\n",
    "mode = 'M1'          # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False  # Log the eigenvalues of Hessian of losses\n",
    "model = Klein_Gordon(layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, mode, stiff_ratio)\n",
    "\n",
    "# Train model\n",
    "model.train(nIter=40001, batch_size=128)\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "loss_r = model.loss_r_log\n",
    "loss_u = model.loss_u_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_r, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_u, label='$\\mathcal{L}_{u}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant_ics = model.adaptive_constant_ics_log\n",
    "adaptive_constant_bcs = model.adaptive_constant_bcs_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant_ics, label='$\\lambda_{u_0}$')\n",
    "ax.plot(adaptive_constant_bcs, label='$\\lambda_{u_b}$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_ics = model.dict_gradients_ics_layers\n",
    "data_gradients_bcs = model.dict_gradients_bcs_layers\n",
    "data_gradients_res = model.dict_gradients_res_layers\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 8))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(2, 3, cnt)\n",
    "    gradients_ics = data_gradients_ics['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    sns.distplot(gradients_ics, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_0}\\mathcal{L}_{u_0}$')\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([0, 500])\n",
    "    ax.get_legend().remove()\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.3, 0.01),\n",
    "                borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Eigenvalues of Hessian of losses if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = model.eigenvalue_log\n",
    "    eigenvalues_ics_list = model.eigenvalue_ics_log\n",
    "    eigenvalues_bcs_list = model.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = model.eigenvalue_res_log\n",
    "\n",
    "    eigenvalues_ics = eigenvalues_ics_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_ics, label='$\\mathcal{L}_{u_0}$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
