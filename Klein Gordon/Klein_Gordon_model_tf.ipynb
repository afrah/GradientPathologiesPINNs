{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import griddata\n",
    "from Klein_Gordon_model_tf import Sampler, Klein_Gordon\n",
    "import timeit\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "class Klein_Gordon:\n",
    "    # Initialize the class\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, model, stiff_ratio):\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Klein_Gordon constant\n",
    "        self.alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "        self.beta = tf.constant(beta, dtype=tf.float32)\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "        self.k = tf.constant(k, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = model\n",
    "\n",
    "        # Record stiff ratio\n",
    "        self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive re-weighting constant\n",
    "        self.rate = 0.9\n",
    "        self.adaptive_constant_ics_val = np.array(1.0)\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        if model in ['M3', 'M4']:\n",
    "            # Initialize encoder weights and biases\n",
    "            self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "            self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
    "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
    "\n",
    "        # Boundary loss and Initial loss\n",
    "        self.loss_ic_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
    "        self.loss_ic_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred - self.u_bc1_tf))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred - self.u_bc2_tf))\n",
    "\n",
    "        self.loss_bcs = self.adaptive_constant_bcs_tf * (self.loss_bc1 + self.loss_bc2)\n",
    "        self.loss_ics = self.adaptive_constant_ics_tf * (self.loss_ic_u + self.loss_ic_u_t)\n",
    "        self.loss_u = self.loss_bcs + self.loss_ics\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred - self.r_tf))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_u\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                        1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict(self.layers)\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict(self.layers)\n",
    "        self.dict_gradients_ics_layers = self.generate_grad_dict(self.layers)\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_ics = []\n",
    "        self.grad_bcs = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "            self.grad_ics.append(tf.gradients(self.loss_ics, self.weights[i])[0])\n",
    "\n",
    "        # Store the adaptive constant\n",
    "        self.adaptive_constant_ics_log = []\n",
    "        self.adaptive_constant_bcs_log = []\n",
    "\n",
    "        # Compute the adaptive constant\n",
    "        self.adaptive_constant_ics_list = []\n",
    "        self.adaptive_constant_bcs_list = []\n",
    "        \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        self.mean_grad_ics_list = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "            self.mean_grad_ics_list.append(tf.reduce_mean(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.mean_grad_ics = tf.reduce_mean(tf.stack(self.mean_grad_ics_list))\n",
    "        \n",
    "        self.adaptive_constant_bcs = self.max_grad_res / self.mean_grad_bcs\n",
    "        self.adaptive_constant_ics = self.max_grad_res / self.mean_grad_ics\n",
    "\n",
    "        # Stiff Ratio\n",
    "        if self.stiff_ratio:\n",
    "            self.Hessian, self.Hessian_ics, self.Hessian_bcs, self.Hessian_res = self.get_H_op()\n",
    "            self.eigenvalues, _ = tf.linalg.eigh(self.Hessian)\n",
    "            self.eigenvalues_ics, _ = tf.linalg.eigh(self.Hessian_ics)\n",
    "            self.eigenvalues_bcs, _ = tf.linalg.eigh(self.Hessian_bcs)\n",
    "            self.eigenvalues_res, _ = tf.linalg.eigh(self.Hessian_res)\n",
    "\n",
    "            self.eigenvalue_log = []\n",
    "            self.eigenvalue_ics_log = []\n",
    "            self.eigenvalue_bcs_log = []\n",
    "            self.eigenvalue_res_log = []\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_ics_value , grad_bcs_value, grad_res_value= self.sess.run([self.grad_ics[i],\n",
    "                                                                            self.grad_bcs[i],\n",
    "                                                                            self.grad_res[i]],\n",
    "                                                                            feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_ics(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_ics, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,\n",
    "                                                   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,\n",
    "                                          self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P),\n",
    "                      dtype='float32')\n",
    "        H_ics = tf.map_fn(self.get_Hv_ics, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "\n",
    "        return H, H_ics, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1))\n",
    "        return u\n",
    "\n",
    "    def net_u_t(self, t, x):\n",
    "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
    "        return u_t\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, t, x):\n",
    "        u = self.net_u(t, x)\n",
    "        residual = self.operator(u, t, x,\n",
    "                                 self.alpha, self.beta, self.gamma, self.k,\n",
    "                                 self.sigma_t, self.sigma_x)\n",
    "        return residual\n",
    "\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter=10000, batch_size=128):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size)\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                       self.u_ics_tf: u_ics_batch,\n",
    "                       self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val}\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_u, self.loss_res], tf_dict)\n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                if self.model in ['M2', 'M4']:\n",
    "                    # Compute the adaptive constant\n",
    "                    adaptive_constant_ics_val, adaptive_constant_bcs_val = self.sess.run(\n",
    "                        [self.adaptive_constant_ics,\n",
    "                         self.adaptive_constant_bcs],\n",
    "                        tf_dict)\n",
    "                    # Print adaptive weights during training\n",
    "                    self.adaptive_constant_ics_val = adaptive_constant_ics_val * (\n",
    "                            1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                    self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * (\n",
    "                            1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                # Store loss and adaptive weights\n",
    "                self.loss_u_log.append(loss_u_value)\n",
    "                self.loss_r_log.append(loss_r_value)\n",
    "\n",
    "                self.adaptive_constant_ics_log.append(self.adaptive_constant_ics_val)\n",
    "                self.adaptive_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' %\n",
    "                      (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "                print(\"constant_ics_val: {:.3f}, constant_bcs_val: {:.3f}\".format(\n",
    "                    self.adaptive_constant_ics_val,\n",
    "                    self.adaptive_constant_bcs_val))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            # Compute the eigenvalues of the Hessian of losses\n",
    "            if self.stiff_ratio:\n",
    "                if it % 1000 == 0:\n",
    "                    print(\"Eigenvalues information stored ...\")\n",
    "                    eigenvalues, eigenvalues_ics, eigenvalues_bcs, eigenvalues_res = self.sess.run([self.eigenvalues,\n",
    "                                                                                                    self.eigenvalues_ics,\n",
    "                                                                                                    self.eigenvalues_bcs,\n",
    "                                                                                                    self.eigenvalues_res], tf_dict)\n",
    "                    self.eigenvalue_log.append(eigenvalues)\n",
    "                    self.eigenvalue_ics_log.append(eigenvalues_bcs)\n",
    "                    self.eigenvalue_bcs_log.append(eigenvalues_bcs)\n",
    "                    self.eigenvalue_res_log.append(eigenvalues_res)\n",
    "\n",
    "            # Store gradients\n",
    "            if it % 10000 == 0:\n",
    "                self.save_gradients(tf_dict)\n",
    "                print(\"Gradients information stored ...\")\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
