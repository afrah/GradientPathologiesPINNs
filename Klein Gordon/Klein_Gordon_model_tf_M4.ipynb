{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import griddata\n",
    "# from Klein_Gordon_model_tf import Sampler, Klein_Gordon\n",
    "import timeit\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "def u(x):\n",
    "    \"\"\"\n",
    "    :param x: x = (t, x)\n",
    "    \"\"\"\n",
    "    return x[:, 1:2] * np.cos(5 * np.pi * x[:, 0:1]) + (x[:, 0:1] * x[:, 1:2])**3\n",
    "\n",
    "def u_tt(x):\n",
    "    return - 25 * np.pi**2 * x[:, 1:2] * np.cos(5 * np.pi * x[:, 0:1]) + 6 * x[:,0:1] * x[:,1:2]**3\n",
    "\n",
    "def u_xx(x):\n",
    "    return np.zeros((x.shape[0], 1)) +  6 * x[:,1:2] * x[:,0:1]**3\n",
    "\n",
    "def f(x, alpha, beta, gamma, k):\n",
    "    return u_tt(x) + alpha * u_xx(x) + beta * u(x) + gamma * u(x)**k\n",
    "\n",
    "def operator(u, t, x, alpha, beta, gamma, k,  sigma_t=1.0, sigma_x=1.0):\n",
    "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    residual = u_tt + alpha * u_xx + beta * u + gamma * u**k\n",
    "    return residual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "class Klein_Gordon:\n",
    "    # Initialize the class\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, model, stiff_ratio):\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Klein_Gordon constant\n",
    "        self.alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "        self.beta = tf.constant(beta, dtype=tf.float32)\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "        self.k = tf.constant(k, dtype=tf.float32)\n",
    "\n",
    "        # Mode\n",
    "        self.model = model\n",
    "\n",
    "        # Record stiff ratio\n",
    "        self.stiff_ratio = stiff_ratio\n",
    "\n",
    "        # Adaptive re-weighting constant\n",
    "        self.rate = 0.9\n",
    "        self.adaptive_constant_ics_val = np.array(10.0)\n",
    "        self.adaptive_constant_ics1_val = np.array(10.0)\n",
    "        self.adaptive_constant_ics2_val = np.array(10.0)\n",
    "        self.adaptive_constant_bcs_val = np.array(2.0)\n",
    "        self.adaptive_constant_bcs1_val = np.array(2.0)\n",
    "        self.adaptive_constant_bcs2_val = np.array(2.0)\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        if model in ['M3', 'M4']:\n",
    "            # Initialize encoder weights and biases\n",
    "            self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "            self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        # self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
    "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics1_val.shape)\n",
    "        # self.adaptive_constant_ics2_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics2_val.shape)\n",
    "        # self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs1_val.shape)\n",
    "        # self.adaptive_constant_bcs2_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs2_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_t_ics_pred =  tf.gradients(self.u_ics_pred, self.t_ics_tf)[0] / self.sigma_t # self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
    "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
    "\n",
    "        # Boundary loss and Initial loss\n",
    "        self.loss_ic_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
    "        self.loss_ic_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred - self.u_bc1_tf))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred - self.u_bc2_tf))\n",
    "\n",
    "        \n",
    "        self.loss_u = self.adaptive_constant_bcs_tf * ( self.loss_bc1 +  self.loss_bc2 ) +  self.adaptive_constant_ics_tf * (self.loss_ic_u + self.loss_ic_u_t)\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred - self.r_tf))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_u\n",
    "\n",
    "        self.count = 1\n",
    "        self.loss_bcs_avg = 0\n",
    "        self.loss_ics_avg = 0\n",
    "        \n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,  1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict(self.layers)\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict(self.layers)\n",
    "        self.dict_gradients_ics_layers = self.generate_grad_dict(self.layers)\n",
    "\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Create dictionary to store gradients\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "\n",
    "    # Save gradients\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_ics_value , grad_bcs_value, grad_res_value= self.sess.run([self.grad_ics[i],\n",
    "                                                                            self.grad_bcs[i],\n",
    "                                                                            self.grad_res[i]],\n",
    "                                                                            feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_res and loss_bcs\n",
    "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Compute the Hessian\n",
    "    def flatten(self, vectors):\n",
    "        return tf.concat([tf.reshape(v, [-1]) for v in vectors], axis=0)\n",
    "\n",
    "    def get_Hv(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_ics(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_ics, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_bcs(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_bcs, self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod, self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_Hv_res(self, v):\n",
    "        loss_gradients = self.flatten(tf.gradients(self.loss_res,\n",
    "                                                   self.weights))\n",
    "        vprod = tf.math.multiply(loss_gradients,\n",
    "                                 tf.stop_gradient(v))\n",
    "        Hv_op = self.flatten(tf.gradients(vprod,\n",
    "                                          self.weights))\n",
    "        return Hv_op\n",
    "\n",
    "    def get_H_op(self):\n",
    "        self.P = self.flatten(self.weights).get_shape().as_list()[0]\n",
    "        H = tf.map_fn(self.get_Hv, tf.eye(self.P, self.P),\n",
    "                      dtype='float32')\n",
    "        H_ics = tf.map_fn(self.get_Hv_ics, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "        H_bcs = tf.map_fn(self.get_Hv_bcs, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "        H_res = tf.map_fn(self.get_Hv_res, tf.eye(self.P, self.P),\n",
    "                          dtype='float32')\n",
    "\n",
    "        return H, H_ics, H_bcs, H_res\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1))\n",
    "        return u\n",
    "\n",
    "    def net_u_t(self, t, x):\n",
    "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
    "        return u_t\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, t, x):\n",
    "        u = self.net_u(t, x)\n",
    "        residual = self.operator(u, t, x,\n",
    "                                 self.alpha, self.beta, self.gamma, self.k,\n",
    "                                 self.sigma_t, self.sigma_x)\n",
    "        return residual\n",
    "\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "###########################################################################################################\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter , bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        batch_size = bcbatch_size\n",
    "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size)\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "        X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "\n",
    "        batch_size = ubatch_size\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                    self.u_ics_tf: u_ics_batch,\n",
    "                    self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch,\n",
    "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.u_bc2_tf: u_bc2_batch,\n",
    "                    self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.r_tf: f_res_batch,\n",
    "                    self.adaptive_constant_ics1_tf: self.adaptive_constant_ics1_val,\n",
    "                    self.adaptive_constant_ics2_tf: self.adaptive_constant_ics2_val,\n",
    "                    self.adaptive_constant_bcs1_tf: self.adaptive_constant_bcs1_val,\n",
    "                    self.adaptive_constant_bcs2_tf: self.adaptive_constant_bcs2_val\n",
    "                    }\n",
    "\n",
    "        for it in range(1, nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bc1 , loss_bc2, loss_ic_u, loss_ic_u_t,loss_u_value, loss_r_value = self.sess.run([self.loss_bc1, self.loss_bc2 , self.loss_ic_u , self.loss_ic_u_t, self.loss_u, self.loss_res], tf_dict)\n",
    "\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, loss_bcs1: %.3e, loss_bcs2: %.3e, loss_ic_u: %.3e, loss_ic_u_t: %.3e, Time: %.2f' %  (it, \n",
    "                                                                                                                                                           loss_value, loss_u_value, loss_r_value,\n",
    "                                                                                                                                                            loss_bc1,loss_bc2,\n",
    "                                                                                                                                                             loss_ic_u,loss_ic_u_t, elapsed))\n",
    "            \n",
    "\n",
    "                # Compute and Print adaptive weights during training\n",
    "                # Compute the adaptive constant\n",
    "                # Print adaptive weights during training\n",
    "                alpha = 10000\n",
    "                self.adaptive_constant_ics1_val = alpha * loss_ic_u # adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_ics2_val = alpha * loss_ic_u_t # adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs1_val = alpha *  loss_bc1 #adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "                self.adaptive_constant_bcs2_val = alpha *  loss_bc2 #adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                # # Store loss and adaptive weights\n",
    "                # self.loss_u_log.append(loss_u_value)\n",
    "                # self.loss_r_log.append(loss_r_value)\n",
    "\n",
    "                # self.adaptive_constant_ics_log.append(self.adaptive_constant_ics_val)\n",
    "                # self.adaptive_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "\n",
    "                print(\"adaptive_constant_ics1_val: {:.3f}\".format(self.adaptive_constant_ics1_val))\n",
    "                print(\"adaptive_constant_ics2_val: {:.3f}\".format(self.adaptive_constant_ics2_val))\n",
    "                print(\"adaptive_constant_bcs1_val: {:.3f}\".format(self.adaptive_constant_bcs1_val))\n",
    "                print(\"adaptive_constant_bcs2_val: {:.3f}\".format(self.adaptive_constant_bcs2_val))\n",
    "                    \n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter=10000, batch_size=128):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(1, nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size)\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, u_bc2_batch = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, f_res_batch = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1], self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                       self.u_ics_tf: u_ics_batch,\n",
    "                       self.t_bc1_tf: X_bc1_batch[:, 0:1], self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch,\n",
    "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.u_bc2_tf: u_bc2_batch,\n",
    "                       self.t_r_tf: X_res_batch[:, 0:1], self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.r_tf: f_res_batch,\n",
    "                       self.adaptive_constant_ics1_tf: self.adaptive_constant_ics1_val,\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss , loss_bc1 , loss_bc2, loss_ic_u, loss_ic_u_t,loss_u_value, loss_r_value = self.sess.run([ self.loss , self.loss_bc1,self.loss_bc2 , self.loss_ic_u,self.loss_ic_u_t,self.loss_u, self.loss_res], tf_dict)\n",
    "\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, loss_bcs1: %.3e, loss_bcs2: %.3e, loss_ic_u: %.3e, loss_ic_u_t: %.3e, Time: %.2f' %  (it, \n",
    "                                                                                                                                                           loss, loss_u_value, loss_r_value,\n",
    "                                                                                                                                                            loss_bc1,loss_bc2,\n",
    "                                                                                                                                                             loss_ic_u,loss_ic_u_t, elapsed))\n",
    "        \n",
    "\n",
    "                alpha = 10000\n",
    "                loss_bcs_avg +=  alpha * ()\n",
    "                loss_ics_avg +=  alpha * ()\n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                # Compute the adaptive constant\n",
    "                # Print adaptive weights during training\n",
    "                if it % 1000 == 0 :\n",
    "                    self.adaptive_constant_ics1_val = alpha * loss_ic_u # adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                    # self.adaptive_constant_ics2_val = alpha * loss_ic_u_t # adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                    self.adaptive_constant_bcs_val = alpha *  (loss_bc1 +  loss_bc2 ) #adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "                    # self.adaptive_constant_bcs2_val = alpha *  loss_bc2 #adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                    # # Store loss and adaptive weights\n",
    "                    # self.loss_u_log.append(loss_u_value)\n",
    "                    # self.loss_r_log.append(loss_r_value)\n",
    "\n",
    "                    print(\"adaptive_constant_ics1_val: {:.3f}\".format(self.adaptive_constant_ics1_val))\n",
    "                    print(\"adaptive_constant_ics2_val: {:.3f}\".format(self.adaptive_constant_bcs_val))\n",
    "                    # print(\"adaptive_constant_bcs1_val: {:.3f}\".format(self.adaptive_constant_bcs1_val))\n",
    "                    # print(\"adaptive_constant_bcs2_val: {:.3f}\".format(self.adaptive_constant_bcs2_val))\n",
    "                \n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "###########################################################################################################\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_method(method , layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma ,k ,mode , stiff_ratio ,\n",
    "                  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size):\n",
    "\n",
    "\n",
    "    model = Klein_Gordon(layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, mode, stiff_ratio)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  ,bcbatch_size , ubatch_size )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, batch_size=mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    u_pred = model.predict_u(X_star)\n",
    "    f_pred = model.predict_r(X_star)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    error_f = np.linalg.norm(f_star - f_pred, 2) / np.linalg.norm(f_star, 2)\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_f: {:.2e}'.format(error_f))\n",
    "\n",
    "    return [elapsed, error_u , error_f]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_17955/1848962430.py:259: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_17955/1848962430.py:62: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_17955/1848962430.py:62: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_17955/1848962430.py:65: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 01:32:29.157004: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 01:32:29.178556: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-11-23 01:32:29.178967: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5599ef3054c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-23 01:32:29.178981: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-11-23 01:32:29.179426: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_17955/1848962430.py:119: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_17955/1848962430.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /tmp/ipykernel_17955/1848962430.py:126: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_17955/1848962430.py:181: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "It: 100, Loss: 6.814e+03, Loss_u: 2.394e+01, Loss_r: 6.790e+03, loss_bcs1: 1.252e-01, loss_bcs2: 7.095e-01, loss_ic_u: 2.702e-01, loss_ic_u_t: 1.957e+00, Time: 23.57\n",
      "adaptive_constant_ics1_val: 2702.146\n",
      "adaptive_constant_ics2_val: 19571.908\n",
      "adaptive_constant_bcs1_val: 1251.574\n",
      "adaptive_constant_bcs2_val: 7095.349\n",
      "It: 200, Loss: 8.274e+03, Loss_u: 1.832e+03, Loss_r: 6.442e+03, loss_bcs1: 1.309e-02, loss_bcs2: 2.262e-01, loss_ic_u: 6.348e-02, loss_ic_u_t: 2.015e-03, Time: 5.97\n",
      "adaptive_constant_ics1_val: 634.839\n",
      "adaptive_constant_ics2_val: 20.145\n",
      "adaptive_constant_bcs1_val: 130.860\n",
      "adaptive_constant_bcs2_val: 2261.872\n",
      "It: 300, Loss: 1.952e+03, Loss_u: 1.232e+02, Loss_r: 1.829e+03, loss_bcs1: 2.110e-02, loss_bcs2: 2.981e-02, loss_ic_u: 4.083e-02, loss_ic_u_t: 1.348e+00, Time: 6.15\n",
      "adaptive_constant_ics1_val: 408.304\n",
      "adaptive_constant_ics2_val: 13476.628\n",
      "adaptive_constant_bcs1_val: 210.998\n",
      "adaptive_constant_bcs2_val: 298.068\n",
      "It: 400, Loss: 1.687e+03, Loss_u: 8.731e+01, Loss_r: 1.599e+03, loss_bcs1: 2.791e-02, loss_bcs2: 5.024e-02, loss_ic_u: 1.504e-01, loss_ic_u_t: 3.752e-04, Time: 6.21\n",
      "adaptive_constant_ics1_val: 1503.615\n",
      "adaptive_constant_ics2_val: 3.752\n",
      "adaptive_constant_bcs1_val: 279.149\n",
      "adaptive_constant_bcs2_val: 502.376\n",
      "It: 500, Loss: 7.555e+02, Loss_u: 5.766e+01, Loss_r: 6.978e+02, loss_bcs1: 2.553e-02, loss_bcs2: 1.512e-02, loss_ic_u: 2.250e-02, loss_ic_u_t: 2.427e+00, Time: 6.43\n",
      "adaptive_constant_ics1_val: 224.972\n",
      "adaptive_constant_ics2_val: 24267.440\n",
      "adaptive_constant_bcs1_val: 255.315\n",
      "adaptive_constant_bcs2_val: 151.248\n",
      "It: 600, Loss: 2.096e+03, Loss_u: 8.159e+01, Loss_r: 2.015e+03, loss_bcs1: 1.606e-02, loss_bcs2: 3.337e-01, loss_ic_u: 8.132e-02, loss_ic_u_t: 3.598e-04, Time: 6.27\n",
      "adaptive_constant_ics1_val: 813.182\n",
      "adaptive_constant_ics2_val: 3.598\n",
      "adaptive_constant_bcs1_val: 160.578\n",
      "adaptive_constant_bcs2_val: 3336.842\n",
      "It: 700, Loss: 6.292e+02, Loss_u: 9.683e+01, Loss_r: 5.324e+02, loss_bcs1: 3.044e-02, loss_bcs2: 1.622e-02, loss_ic_u: 3.883e-02, loss_ic_u_t: 1.736e+00, Time: 6.12\n",
      "adaptive_constant_ics1_val: 388.282\n",
      "adaptive_constant_ics2_val: 17356.548\n",
      "adaptive_constant_bcs1_val: 304.392\n",
      "adaptive_constant_bcs2_val: 162.198\n",
      "It: 800, Loss: 1.115e+03, Loss_u: 9.491e+01, Loss_r: 1.020e+03, loss_bcs1: 1.778e-02, loss_bcs2: 3.232e-01, loss_ic_u: 7.802e-02, loss_ic_u_t: 3.909e-04, Time: 6.24\n",
      "adaptive_constant_ics1_val: 780.204\n",
      "adaptive_constant_ics2_val: 3.909\n",
      "adaptive_constant_bcs1_val: 177.848\n",
      "adaptive_constant_bcs2_val: 3231.730\n",
      "It: 900, Loss: 6.567e+02, Loss_u: 6.160e+01, Loss_r: 5.951e+02, loss_bcs1: 2.449e-02, loss_bcs2: 8.401e-03, loss_ic_u: 3.061e-02, loss_ic_u_t: 1.589e+00, Time: 6.39\n",
      "adaptive_constant_ics1_val: 306.092\n",
      "adaptive_constant_ics2_val: 15891.910\n",
      "adaptive_constant_bcs1_val: 244.858\n",
      "adaptive_constant_bcs2_val: 84.010\n",
      "It: 1000, Loss: 9.680e+02, Loss_u: 6.050e+01, Loss_r: 9.075e+02, loss_bcs1: 1.346e-02, loss_bcs2: 3.254e-01, loss_ic_u: 6.099e-02, loss_ic_u_t: 7.043e-04, Time: 6.20\n",
      "adaptive_constant_ics1_val: 609.926\n",
      "adaptive_constant_ics2_val: 7.043\n",
      "adaptive_constant_bcs1_val: 134.615\n",
      "adaptive_constant_bcs2_val: 3254.361\n",
      "It: 1100, Loss: 6.104e+02, Loss_u: 5.430e+01, Loss_r: 5.561e+02, loss_bcs1: 2.226e-02, loss_bcs2: 7.619e-03, loss_ic_u: 2.607e-02, loss_ic_u_t: 1.506e+00, Time: 6.31\n",
      "adaptive_constant_ics1_val: 260.701\n",
      "adaptive_constant_ics2_val: 15062.979\n",
      "adaptive_constant_bcs1_val: 222.644\n",
      "adaptive_constant_bcs2_val: 76.192\n",
      "It: 1200, Loss: 7.361e+02, Loss_u: 5.130e+01, Loss_r: 6.848e+02, loss_bcs1: 9.299e-03, loss_bcs2: 3.636e-01, loss_ic_u: 5.576e-02, loss_ic_u_t: 4.637e-04, Time: 6.26\n",
      "adaptive_constant_ics1_val: 557.607\n",
      "adaptive_constant_ics2_val: 4.637\n",
      "adaptive_constant_bcs1_val: 92.986\n",
      "adaptive_constant_bcs2_val: 3636.207\n",
      "It: 1300, Loss: 3.991e+02, Loss_u: 3.629e+01, Loss_r: 3.628e+02, loss_bcs1: 1.733e-02, loss_bcs2: 5.345e-03, loss_ic_u: 1.682e-02, loss_ic_u_t: 1.265e+00, Time: 6.17\n",
      "adaptive_constant_ics1_val: 168.199\n",
      "adaptive_constant_ics2_val: 12646.861\n",
      "adaptive_constant_bcs1_val: 173.279\n",
      "adaptive_constant_bcs2_val: 53.452\n",
      "It: 1400, Loss: 5.253e+02, Loss_u: 3.194e+01, Loss_r: 4.934e+02, loss_bcs1: 8.489e-03, loss_bcs2: 2.672e-01, loss_ic_u: 4.530e-02, loss_ic_u_t: 6.776e-04, Time: 5.75\n",
      "adaptive_constant_ics1_val: 453.025\n",
      "adaptive_constant_ics2_val: 6.776\n",
      "adaptive_constant_bcs1_val: 84.886\n",
      "adaptive_constant_bcs2_val: 2671.555\n",
      "It: 1500, Loss: 1.940e+02, Loss_u: 2.788e+01, Loss_r: 1.661e+02, loss_bcs1: 1.184e-02, loss_bcs2: 4.773e-03, loss_ic_u: 1.759e-02, loss_ic_u_t: 9.085e-01, Time: 6.10\n",
      "adaptive_constant_ics1_val: 175.900\n",
      "adaptive_constant_ics2_val: 9085.178\n",
      "adaptive_constant_bcs1_val: 118.434\n",
      "adaptive_constant_bcs2_val: 47.728\n",
      "It: 1600, Loss: 4.053e+02, Loss_u: 1.935e+01, Loss_r: 3.859e+02, loss_bcs1: 7.068e-03, loss_bcs2: 1.603e-01, loss_ic_u: 4.256e-02, loss_ic_u_t: 3.714e-04, Time: 5.92\n",
      "adaptive_constant_ics1_val: 425.573\n",
      "adaptive_constant_ics2_val: 3.714\n",
      "adaptive_constant_bcs1_val: 70.684\n",
      "adaptive_constant_bcs2_val: 1603.493\n",
      "It: 1700, Loss: 3.727e+02, Loss_u: 1.746e+01, Loss_r: 3.553e+02, loss_bcs1: 1.242e-02, loss_bcs2: 3.746e-03, loss_ic_u: 1.665e-02, loss_ic_u_t: 9.394e-01, Time: 5.80\n",
      "adaptive_constant_ics1_val: 166.503\n",
      "adaptive_constant_ics2_val: 9394.439\n",
      "adaptive_constant_bcs1_val: 124.196\n",
      "adaptive_constant_bcs2_val: 37.458\n",
      "It: 1800, Loss: 2.809e+02, Loss_u: 1.133e+01, Loss_r: 2.695e+02, loss_bcs1: 6.277e-03, loss_bcs2: 9.090e-02, loss_ic_u: 3.091e-02, loss_ic_u_t: 2.124e-04, Time: 5.89\n",
      "adaptive_constant_ics1_val: 309.137\n",
      "adaptive_constant_ics2_val: 2.124\n",
      "adaptive_constant_bcs1_val: 62.767\n",
      "adaptive_constant_bcs2_val: 909.000\n",
      "It: 1900, Loss: 2.340e+02, Loss_u: 9.821e+00, Loss_r: 2.242e+02, loss_bcs1: 9.961e-03, loss_bcs2: 3.452e-03, loss_ic_u: 1.516e-02, loss_ic_u_t: 6.457e-01, Time: 5.60\n",
      "adaptive_constant_ics1_val: 151.629\n",
      "adaptive_constant_ics2_val: 6456.680\n",
      "adaptive_constant_bcs1_val: 99.610\n",
      "adaptive_constant_bcs2_val: 34.515\n",
      "It: 2000, Loss: 2.154e+02, Loss_u: 1.209e+01, Loss_r: 2.033e+02, loss_bcs1: 7.606e-03, loss_bcs2: 5.883e-02, loss_ic_u: 2.728e-02, loss_ic_u_t: 7.994e-04, Time: 5.79\n",
      "adaptive_constant_ics1_val: 272.771\n",
      "adaptive_constant_ics2_val: 7.994\n",
      "adaptive_constant_bcs1_val: 76.065\n",
      "adaptive_constant_bcs2_val: 588.265\n",
      "It: 2100, Loss: 1.133e+02, Loss_u: 1.057e+01, Loss_r: 1.028e+02, loss_bcs1: 1.175e-02, loss_bcs2: 2.729e-03, loss_ic_u: 1.273e-02, loss_ic_u_t: 5.751e-01, Time: 5.82\n",
      "adaptive_constant_ics1_val: 127.341\n",
      "adaptive_constant_ics2_val: 5750.951\n",
      "adaptive_constant_bcs1_val: 117.525\n",
      "adaptive_constant_bcs2_val: 27.288\n",
      "It: 2200, Loss: 1.189e+02, Loss_u: 1.050e+01, Loss_r: 1.084e+02, loss_bcs1: 7.808e-03, loss_bcs2: 4.098e-02, loss_ic_u: 2.277e-02, loss_ic_u_t: 9.679e-04, Time: 5.88\n",
      "adaptive_constant_ics1_val: 227.657\n",
      "adaptive_constant_ics2_val: 9.679\n",
      "adaptive_constant_bcs1_val: 78.076\n",
      "adaptive_constant_bcs2_val: 409.849\n",
      "It: 2300, Loss: 2.506e+02, Loss_u: 8.731e+00, Loss_r: 2.419e+02, loss_bcs1: 8.878e-03, loss_bcs2: 2.451e-03, loss_ic_u: 1.281e-02, loss_ic_u_t: 4.254e-01, Time: 6.19\n",
      "adaptive_constant_ics1_val: 128.058\n",
      "adaptive_constant_ics2_val: 4254.164\n",
      "adaptive_constant_bcs1_val: 88.780\n",
      "adaptive_constant_bcs2_val: 24.506\n",
      "It: 2400, Loss: 3.007e+02, Loss_u: 8.303e+00, Loss_r: 2.924e+02, loss_bcs1: 9.198e-03, loss_bcs2: 2.202e-02, loss_ic_u: 1.474e-02, loss_ic_u_t: 1.189e-03, Time: 5.87\n",
      "adaptive_constant_ics1_val: 147.358\n",
      "adaptive_constant_ics2_val: 11.894\n",
      "adaptive_constant_bcs1_val: 91.977\n",
      "adaptive_constant_bcs2_val: 220.199\n",
      "It: 2500, Loss: 3.535e+01, Loss_u: 6.938e+00, Loss_r: 2.842e+01, loss_bcs1: 7.524e-03, loss_bcs2: 1.787e-03, loss_ic_u: 1.047e-02, loss_ic_u_t: 3.623e-01, Time: 6.31\n",
      "adaptive_constant_ics1_val: 104.721\n",
      "adaptive_constant_ics2_val: 3623.248\n",
      "adaptive_constant_bcs1_val: 75.244\n",
      "adaptive_constant_bcs2_val: 17.871\n",
      "It: 2600, Loss: 1.635e+02, Loss_u: 4.159e+00, Loss_r: 1.593e+02, loss_bcs1: 8.123e-03, loss_bcs2: 1.298e-02, loss_ic_u: 1.423e-02, loss_ic_u_t: 5.039e-04, Time: 5.95\n",
      "adaptive_constant_ics1_val: 142.284\n",
      "adaptive_constant_ics2_val: 5.039\n",
      "adaptive_constant_bcs1_val: 81.231\n",
      "adaptive_constant_bcs2_val: 129.846\n",
      "It: 2700, Loss: 1.271e+02, Loss_u: 4.835e+00, Loss_r: 1.223e+02, loss_bcs1: 9.835e-03, loss_bcs2: 3.174e-03, loss_ic_u: 7.417e-03, loss_ic_u_t: 5.097e-01, Time: 6.14\n",
      "adaptive_constant_ics1_val: 74.172\n",
      "adaptive_constant_ics2_val: 5097.238\n",
      "adaptive_constant_bcs1_val: 98.354\n",
      "adaptive_constant_bcs2_val: 31.743\n",
      "It: 2800, Loss: 1.285e+02, Loss_u: 3.926e+00, Loss_r: 1.245e+02, loss_bcs1: 7.261e-03, loss_bcs2: 1.557e-02, loss_ic_u: 1.083e-02, loss_ic_u_t: 3.756e-04, Time: 6.01\n",
      "adaptive_constant_ics1_val: 108.327\n",
      "adaptive_constant_ics2_val: 3.756\n",
      "adaptive_constant_bcs1_val: 72.607\n",
      "adaptive_constant_bcs2_val: 155.694\n",
      "It: 2900, Loss: 6.923e+01, Loss_u: 2.865e+00, Loss_r: 6.637e+01, loss_bcs1: 7.579e-03, loss_bcs2: 4.177e-03, loss_ic_u: 1.129e-02, loss_ic_u_t: 1.175e-01, Time: 6.46\n",
      "adaptive_constant_ics1_val: 112.940\n",
      "adaptive_constant_ics2_val: 1174.842\n",
      "adaptive_constant_bcs1_val: 75.795\n",
      "adaptive_constant_bcs2_val: 41.771\n",
      "It: 3000, Loss: 6.125e+01, Loss_u: 3.732e+00, Loss_r: 5.752e+01, loss_bcs1: 7.551e-03, loss_bcs2: 8.801e-03, loss_ic_u: 1.242e-02, loss_ic_u_t: 1.183e-03, Time: 6.18\n",
      "adaptive_constant_ics1_val: 124.151\n",
      "adaptive_constant_ics2_val: 11.831\n",
      "adaptive_constant_bcs1_val: 75.512\n",
      "adaptive_constant_bcs2_val: 88.011\n",
      "It: 3100, Loss: 3.864e+01, Loss_u: 3.492e+00, Loss_r: 3.515e+01, loss_bcs1: 7.871e-03, loss_bcs2: 5.002e-03, loss_ic_u: 1.227e-02, loss_ic_u_t: 7.895e-02, Time: 5.86\n",
      "adaptive_constant_ics1_val: 122.705\n",
      "adaptive_constant_ics2_val: 789.521\n",
      "adaptive_constant_bcs1_val: 78.712\n",
      "adaptive_constant_bcs2_val: 50.018\n",
      "It: 3200, Loss: 4.155e+01, Loss_u: 3.723e+00, Loss_r: 3.783e+01, loss_bcs1: 7.180e-03, loss_bcs2: 1.119e-02, loss_ic_u: 1.310e-02, loss_ic_u_t: 1.255e-03, Time: 6.25\n",
      "adaptive_constant_ics1_val: 130.959\n",
      "adaptive_constant_ics2_val: 12.554\n",
      "adaptive_constant_bcs1_val: 71.797\n",
      "adaptive_constant_bcs2_val: 111.908\n",
      "It: 3300, Loss: 2.918e+01, Loss_u: 3.843e+00, Loss_r: 2.534e+01, loss_bcs1: 7.685e-03, loss_bcs2: 5.714e-03, loss_ic_u: 1.223e-02, loss_ic_u_t: 8.363e-02, Time: 6.26\n",
      "adaptive_constant_ics1_val: 122.338\n",
      "adaptive_constant_ics2_val: 836.304\n",
      "adaptive_constant_bcs1_val: 76.845\n",
      "adaptive_constant_bcs2_val: 57.137\n",
      "It: 3400, Loss: 4.114e+01, Loss_u: 3.865e+00, Loss_r: 3.727e+01, loss_bcs1: 1.014e-02, loss_bcs2: 8.675e-03, loss_ic_u: 1.151e-02, loss_ic_u_t: 1.413e-03, Time: 6.16\n",
      "adaptive_constant_ics1_val: 115.115\n",
      "adaptive_constant_ics2_val: 14.130\n",
      "adaptive_constant_bcs1_val: 101.407\n",
      "adaptive_constant_bcs2_val: 86.755\n",
      "It: 3500, Loss: 4.239e+01, Loss_u: 3.518e+00, Loss_r: 3.887e+01, loss_bcs1: 7.477e-03, loss_bcs2: 8.493e-03, loss_ic_u: 1.357e-02, loss_ic_u_t: 3.258e-02, Time: 6.08\n",
      "adaptive_constant_ics1_val: 135.706\n",
      "adaptive_constant_ics2_val: 325.787\n",
      "adaptive_constant_bcs1_val: 74.770\n",
      "adaptive_constant_bcs2_val: 84.929\n",
      "It: 3600, Loss: 1.876e+01, Loss_u: 3.795e+00, Loss_r: 1.496e+01, loss_bcs1: 6.900e-03, loss_bcs2: 9.584e-03, loss_ic_u: 1.566e-02, loss_ic_u_t: 1.047e-03, Time: 5.97\n",
      "adaptive_constant_ics1_val: 156.563\n",
      "adaptive_constant_ics2_val: 10.466\n",
      "adaptive_constant_bcs1_val: 69.000\n",
      "adaptive_constant_bcs2_val: 95.839\n",
      "It: 3700, Loss: 2.077e+01, Loss_u: 3.765e+00, Loss_r: 1.701e+01, loss_bcs1: 8.698e-03, loss_bcs2: 6.794e-03, loss_ic_u: 1.413e-02, loss_ic_u_t: 2.876e-02, Time: 5.94\n",
      "adaptive_constant_ics1_val: 141.344\n",
      "adaptive_constant_ics2_val: 287.573\n",
      "adaptive_constant_bcs1_val: 86.982\n",
      "adaptive_constant_bcs2_val: 67.936\n",
      "It: 3800, Loss: 1.145e+01, Loss_u: 3.739e+00, Loss_r: 7.711e+00, loss_bcs1: 8.704e-03, loss_bcs2: 7.289e-03, loss_ic_u: 1.575e-02, loss_ic_u_t: 9.054e-04, Time: 5.90\n",
      "adaptive_constant_ics1_val: 157.480\n",
      "adaptive_constant_ics2_val: 9.054\n",
      "adaptive_constant_bcs1_val: 87.044\n",
      "adaptive_constant_bcs2_val: 72.894\n",
      "It: 3900, Loss: 1.540e+01, Loss_u: 3.606e+00, Loss_r: 1.180e+01, loss_bcs1: 8.239e-03, loss_bcs2: 7.919e-03, loss_ic_u: 1.365e-02, loss_ic_u_t: 1.792e-02, Time: 6.04\n",
      "adaptive_constant_ics1_val: 136.502\n",
      "adaptive_constant_ics2_val: 179.196\n",
      "adaptive_constant_bcs1_val: 82.393\n",
      "adaptive_constant_bcs2_val: 79.193\n",
      "It: 4000, Loss: 7.689e+00, Loss_u: 3.328e+00, Loss_r: 4.361e+00, loss_bcs1: 6.463e-03, loss_bcs2: 5.673e-03, loss_ic_u: 1.434e-02, loss_ic_u_t: 2.172e-03, Time: 6.28\n",
      "adaptive_constant_ics1_val: 143.391\n",
      "adaptive_constant_ics2_val: 21.721\n",
      "adaptive_constant_bcs1_val: 64.627\n",
      "adaptive_constant_bcs2_val: 56.729\n",
      "It: 4100, Loss: 1.147e+01, Loss_u: 2.915e+00, Loss_r: 8.553e+00, loss_bcs1: 8.261e-03, loss_bcs2: 7.005e-03, loss_ic_u: 1.255e-02, loss_ic_u_t: 8.440e-03, Time: 6.03\n",
      "adaptive_constant_ics1_val: 125.535\n",
      "adaptive_constant_ics2_val: 84.395\n",
      "adaptive_constant_bcs1_val: 82.613\n",
      "adaptive_constant_bcs2_val: 70.047\n",
      "It: 4200, Loss: 1.612e+01, Loss_u: 2.774e+00, Loss_r: 1.334e+01, loss_bcs1: 7.490e-03, loss_bcs2: 6.977e-03, loss_ic_u: 1.228e-02, loss_ic_u_t: 1.490e-03, Time: 6.22\n",
      "adaptive_constant_ics1_val: 122.760\n",
      "adaptive_constant_ics2_val: 14.896\n",
      "adaptive_constant_bcs1_val: 74.898\n",
      "adaptive_constant_bcs2_val: 69.771\n",
      "It: 4300, Loss: 8.363e+00, Loss_u: 2.814e+00, Loss_r: 5.549e+00, loss_bcs1: 7.543e-03, loss_bcs2: 5.569e-03, loss_ic_u: 1.293e-02, loss_ic_u_t: 1.835e-02, Time: 5.87\n",
      "adaptive_constant_ics1_val: 129.272\n",
      "adaptive_constant_ics2_val: 183.520\n",
      "adaptive_constant_bcs1_val: 75.431\n",
      "adaptive_constant_bcs2_val: 55.689\n",
      "It: 4400, Loss: 7.020e+00, Loss_u: 2.689e+00, Loss_r: 4.332e+00, loss_bcs1: 7.074e-03, loss_bcs2: 5.818e-03, loss_ic_u: 1.276e-02, loss_ic_u_t: 9.855e-04, Time: 6.04\n",
      "adaptive_constant_ics1_val: 127.641\n",
      "adaptive_constant_ics2_val: 9.855\n",
      "adaptive_constant_bcs1_val: 70.741\n",
      "adaptive_constant_bcs2_val: 58.182\n",
      "It: 4500, Loss: 8.777e+00, Loss_u: 2.321e+00, Loss_r: 6.456e+00, loss_bcs1: 6.670e-03, loss_bcs2: 6.683e-03, loss_ic_u: 1.091e-02, loss_ic_u_t: 6.801e-03, Time: 5.85\n",
      "adaptive_constant_ics1_val: 109.149\n",
      "adaptive_constant_ics2_val: 68.008\n",
      "adaptive_constant_bcs1_val: 66.702\n",
      "adaptive_constant_bcs2_val: 66.832\n",
      "It: 4600, Loss: 5.998e+00, Loss_u: 2.052e+00, Loss_r: 3.946e+00, loss_bcs1: 6.146e-03, loss_bcs2: 4.501e-03, loss_ic_u: 1.126e-02, loss_ic_u_t: 1.644e-03, Time: 6.07\n",
      "adaptive_constant_ics1_val: 112.614\n",
      "adaptive_constant_ics2_val: 16.435\n",
      "adaptive_constant_bcs1_val: 61.465\n",
      "adaptive_constant_bcs2_val: 45.008\n",
      "It: 4700, Loss: 5.814e+00, Loss_u: 1.782e+00, Loss_r: 4.033e+00, loss_bcs1: 5.853e-03, loss_bcs2: 5.398e-03, loss_ic_u: 9.910e-03, loss_ic_u_t: 3.826e-03, Time: 6.10\n",
      "adaptive_constant_ics1_val: 99.097\n",
      "adaptive_constant_ics2_val: 38.257\n",
      "adaptive_constant_bcs1_val: 58.525\n",
      "adaptive_constant_bcs2_val: 53.984\n",
      "It: 4800, Loss: 6.988e+00, Loss_u: 1.915e+00, Loss_r: 5.073e+00, loss_bcs1: 6.765e-03, loss_bcs2: 6.470e-03, loss_ic_u: 1.098e-02, loss_ic_u_t: 2.152e-03, Time: 5.89\n",
      "adaptive_constant_ics1_val: 109.759\n",
      "adaptive_constant_ics2_val: 21.521\n",
      "adaptive_constant_bcs1_val: 67.653\n",
      "adaptive_constant_bcs2_val: 64.698\n",
      "It: 4900, Loss: 7.271e+00, Loss_u: 1.699e+00, Loss_r: 5.572e+00, loss_bcs1: 4.863e-03, loss_bcs2: 4.121e-03, loss_ic_u: 9.183e-03, loss_ic_u_t: 4.432e-03, Time: 6.46\n",
      "adaptive_constant_ics1_val: 91.834\n",
      "adaptive_constant_ics2_val: 44.324\n",
      "adaptive_constant_bcs1_val: 48.629\n",
      "adaptive_constant_bcs2_val: 41.215\n",
      "It: 5000, Loss: 4.994e+00, Loss_u: 1.542e+00, Loss_r: 3.452e+00, loss_bcs1: 5.650e-03, loss_bcs2: 5.758e-03, loss_ic_u: 9.995e-03, loss_ic_u_t: 2.531e-03, Time: 6.78\n",
      "adaptive_constant_ics1_val: 99.954\n",
      "adaptive_constant_ics2_val: 25.314\n",
      "adaptive_constant_bcs1_val: 56.495\n",
      "adaptive_constant_bcs2_val: 57.584\n",
      "It: 5100, Loss: 5.948e+00, Loss_u: 1.589e+00, Loss_r: 4.359e+00, loss_bcs1: 4.720e-03, loss_bcs2: 3.571e-03, loss_ic_u: 9.574e-03, loss_ic_u_t: 6.307e-03, Time: 6.28\n",
      "adaptive_constant_ics1_val: 95.742\n",
      "adaptive_constant_ics2_val: 63.073\n",
      "adaptive_constant_bcs1_val: 47.204\n",
      "adaptive_constant_bcs2_val: 35.712\n",
      "It: 5200, Loss: 5.955e+00, Loss_u: 1.376e+00, Loss_r: 4.579e+00, loss_bcs1: 5.131e-03, loss_bcs2: 5.268e-03, loss_ic_u: 8.648e-03, loss_ic_u_t: 1.870e-03, Time: 6.19\n",
      "adaptive_constant_ics1_val: 86.476\n",
      "adaptive_constant_ics2_val: 18.699\n",
      "adaptive_constant_bcs1_val: 51.309\n",
      "adaptive_constant_bcs2_val: 52.685\n",
      "It: 5300, Loss: 3.845e+00, Loss_u: 1.176e+00, Loss_r: 2.669e+00, loss_bcs1: 4.798e-03, loss_bcs2: 3.855e-03, loss_ic_u: 7.847e-03, loss_ic_u_t: 2.576e-03, Time: 6.61\n",
      "adaptive_constant_ics1_val: 78.467\n",
      "adaptive_constant_ics2_val: 25.756\n",
      "adaptive_constant_bcs1_val: 47.976\n",
      "adaptive_constant_bcs2_val: 38.552\n",
      "It: 5400, Loss: 3.388e+00, Loss_u: 1.103e+00, Loss_r: 2.285e+00, loss_bcs1: 4.383e-03, loss_bcs2: 4.351e-03, loss_ic_u: 8.522e-03, loss_ic_u_t: 2.202e-03, Time: 5.87\n",
      "adaptive_constant_ics1_val: 85.224\n",
      "adaptive_constant_ics2_val: 22.015\n",
      "adaptive_constant_bcs1_val: 43.832\n",
      "adaptive_constant_bcs2_val: 43.507\n",
      "It: 5500, Loss: 4.423e+00, Loss_u: 1.021e+00, Loss_r: 3.402e+00, loss_bcs1: 3.581e-03, loss_bcs2: 3.610e-03, loss_ic_u: 7.606e-03, loss_ic_u_t: 2.676e-03, Time: 6.25\n",
      "adaptive_constant_ics1_val: 76.064\n",
      "adaptive_constant_ics2_val: 26.760\n",
      "adaptive_constant_bcs1_val: 35.806\n",
      "adaptive_constant_bcs2_val: 36.098\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17955/3173765156.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mbcs_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_u\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0merror_f\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmtd\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mics_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbcs_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstiff_ratio\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0mX_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mu_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mf_star\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnIter\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmbbatch_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbcbatch_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mubatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17955/281325136.py\u001b[0m in \u001b[0;36mtest_method\u001b[0;34m(method, layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma, k, mode, stiff_ratio, X_star, u_star, f_star, nIter, mbbatch_size, bcbatch_size, ubatch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnIter\u001b[0m  \u001b[0;34m,\u001b[0m\u001b[0mbcbatch_size\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mubatch_size\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"mini_batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainmb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmbbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown method!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17955/1848962430.py\u001b[0m in \u001b[0;36mtrainmb\u001b[0;34m(self, nIter, batch_size)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Run the Tensorflow session to minimize the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# Print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################################################################################################################\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "# Parameters of equations\n",
    "alpha = -1.0\n",
    "beta = 0.0\n",
    "gamma = 1.0\n",
    "k = 3\n",
    "# Domain boundaries\n",
    "ics_coords = np.array([[0.0, 0.0], [0.0, 1.0]])\n",
    "bc1_coords = np.array([[0.0, 0.0], [1.0, 0.0]])\n",
    "bc2_coords = np.array([[0.0, 1.0], [1.0, 1.0]])\n",
    "dom_coords = np.array([[0.0, 0.0], [1.0, 1.0]])\n",
    "\n",
    "\n",
    "# Define model\n",
    "layers = [2, 50, 50, 50, 50, 50, 1]\n",
    "mode = 'M4'          # Method: 'M1', 'M2', 'M3', 'M4'\n",
    "stiff_ratio = False  # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_f_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "        # Create initial conditions samplers\n",
    "        ics_sampler = Sampler(2, ics_coords, lambda x: u(x), name='Initial Condition 1')\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: u(x), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: u(x), name='Dirichlet BC2')\n",
    "        bcs_sampler = [bc1, bc2]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x, alpha, beta, gamma, k), name='Forcing')\n",
    "        bcs_sampler = [bc1, bc2]\n",
    "\n",
    "        [elapsed, error_u , error_f] = test_method(mtd , layers, operator, ics_sampler, bcs_sampler, res_sampler, alpha, beta, gamma ,k ,mode , stiff_ratio ,  X_star , u_star , f_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "        print('Relative L2 error_u: {:.2e}'.format(error_f))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_f_list.append(error_f)\n",
    "\n",
    "    # print(\"\\n\\nMethod: \", mtd)\n",
    "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    print(\"average of error_f_list:\" , sum(error_f_list) / len(error_f_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,  error_f_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(\"Klein Gordon_Dataset/Klein_Gordon_model2_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.io\n",
    "\n",
    "mode = 'M4'\n",
    "mbbatch_size = 128\n",
    "ubatch_size = 5000\n",
    "bcbatch_size = 500\n",
    "iterations = 40000\n",
    "\n",
    "time_list = []\n",
    "error_u_list = []\n",
    "error_v_list = []\n",
    "error_p_list = []\n",
    "    \n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "##Mini Batch\n",
    "time_list = [1141.30 , ]\n",
    "error_u_list = [0.0184 , 0.]\n",
    "error_f_list = [0.0009 , 0.]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "\n",
    "##Full Batch\n",
    "time_list = [4978.18,  ]\n",
    "error_u_list = [ 0.0058,0. ]\n",
    "error_f_list = [0.0003 , 0.]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "\n",
    "scipy.io.savemat(\"./dataset/NS_model_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test data\n",
    "nn = 100\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "# Exact solution\n",
    "u_star = u(X_star)\n",
    "f_star = f(X_star, alpha, beta, gamma, k)\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "# Test data\n",
    "U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "F_star = griddata(X_star, f_star.flatten(), (t, x), method='cubic')\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(t, x, U_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Exact u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Predicted u(x)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss\n",
    "loss_r = model.loss_r_log\n",
    "loss_u = model.loss_u_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_r, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_u, label='$\\mathcal{L}_{u}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss\n",
    "loss_r = model.loss_r_log\n",
    "loss_u = model.loss_u_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_r, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_u, label='$\\mathcal{L}_{u}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant_ics = model.adaptive_constant_ics_log\n",
    "adaptive_constant_bcs = model.adaptive_constant_bcs_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant_ics, label='$\\lambda_{u_0}$')\n",
    "ax.plot(adaptive_constant_bcs, label='$\\lambda_{u_b}$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adaptive Constant\n",
    "adaptive_constant_ics = model.adaptive_constant_ics_log\n",
    "adaptive_constant_bcs = model.adaptive_constant_bcs_log\n",
    "\n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant_ics, label='$\\lambda_{u_0}$')\n",
    "ax.plot(adaptive_constant_bcs, label='$\\lambda_{u_b}$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Gradients at the end of training\n",
    "data_gradients_ics = model.dict_gradients_ics_layers\n",
    "data_gradients_bcs = model.dict_gradients_bcs_layers\n",
    "data_gradients_res = model.dict_gradients_res_layers\n",
    "\n",
    "num_hidden_layers = len(layers) - 1\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 8))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(2, 3, cnt)\n",
    "    gradients_ics = data_gradients_ics['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "\n",
    "    sns.distplot(gradients_ics, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_0}\\mathcal{L}_{u_0}$')\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\lambda_{u_b} \\mathcal{L}_{u_b}$')\n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([0, 500])\n",
    "    # ax.get_legend().remove()\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.3, 0.01),\n",
    "                borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Eigenvalues of Hessian of losses if applicable\n",
    "if stiff_ratio:\n",
    "    eigenvalues_list = model.eigenvalue_log\n",
    "    eigenvalues_ics_list = model.eigenvalue_ics_log\n",
    "    eigenvalues_bcs_list = model.eigenvalue_bcs_log\n",
    "    eigenvalues_res_list = model.eigenvalue_res_log\n",
    "\n",
    "    eigenvalues_ics = eigenvalues_ics_list[-1]\n",
    "    eigenvalues_bcs = eigenvalues_bcs_list[-1]\n",
    "    eigenvalues_res = eigenvalues_res_list[-1]\n",
    "\n",
    "    fig_5 = plt.figure(5)\n",
    "    ax = fig_5.add_subplot(1, 1, 1)\n",
    "    ax.plot(eigenvalues_ics, label='$\\mathcal{L}_{u_0}$')\n",
    "    ax.plot(eigenvalues_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "    ax.plot(eigenvalues_res, label='$\\mathcal{L}_r$')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('eigenvalue')\n",
    "    ax.set_yscale('symlog')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
