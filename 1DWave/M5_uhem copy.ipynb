{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
    "import numpy as np\n",
    "import timeit\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "# Define the exact solution and its derivatives\n",
    "def u(x, a, c):\n",
    "    \"\"\"\n",
    "    :param x: x = (t, x)\n",
    "    \"\"\"\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    return np.sin(np.pi * x) * np.cos(c * np.pi * t) + a * np.sin(2 * c * np.pi* x) * np.cos(4 * c  * np.pi * t)\n",
    "\n",
    "def u_t(x,a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_t = -  c * np.pi * np.sin(np.pi * x) * np.sin(c * np.pi * t) -  a * 4 * c * np.pi * np.sin(2 * c * np.pi* x) * np.sin(4 * c * np.pi * t)\n",
    "    return u_t\n",
    "\n",
    "def u_tt(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_tt = -(c * np.pi)**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) - a * (4 * c * np.pi)**2 *  np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return u_tt\n",
    "\n",
    "def u_xx(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_xx = - np.pi**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) -  a * (2 * c * np.pi)** 2 * np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return  u_xx\n",
    "\n",
    "\n",
    "def r(x, a, c):\n",
    "    return u_tt(x, a, c) - c**2 * u_xx(x, a, c)\n",
    "\n",
    "def operator(u, t, x, c, sigma_t=1.0, sigma_x=1.0):\n",
    "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    residual = u_tt - c**2 * u_xx\n",
    "    return residual\n",
    "\n",
    "class PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, c , mode ,  sess):\n",
    "        # Normalization \n",
    "\n",
    "\n",
    "\n",
    "        self.update = False\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        self.sess = sess\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        self.T = 1.0\n",
    "        # weights\n",
    "        self.adaptive_constant_bcs1_val = np.array(1.0)\n",
    "        self.adaptive_constant_bcs2_val = np.array(1.0)\n",
    "        self.adaptive_constant_ics_val = np.array(1.0)\n",
    "        self.adaptive_constant_res_val = np.array(1.0)\n",
    "        self.rate = 0.9\n",
    "\n",
    "        # Wave constant\n",
    "        self.c = tf.constant(c, dtype=tf.float32)\n",
    "        \n",
    "        # self.kernel_size = kernel_size # Size of the NTK matrix\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        \n",
    "        self.adaptive_constant_bcs1_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs1_val.shape)\n",
    "        self.adaptive_constant_bcs2_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs2_val.shape)\n",
    "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
    "        self.adaptive_constant_res_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_val.shape)\n",
    "        \n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
    "        self.r_pred = self.net_r(self.t_u_tf, self.x_u_tf)\n",
    "\n",
    "        self.f_pred = self.net_u(self.t_r_tf, self.x_r_tf)\n",
    "        \n",
    "\n",
    "        # Boundary loss and Initial loss\n",
    "        self.loss_ics_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
    "        self.loss_ics_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
    "        self.loss_ics_us = self.loss_ics_u_t\n",
    "        self.loss_bc1 =   self.loss_ics_u +  tf.reduce_mean(tf.square(self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
    "\n",
    "        self.loss_u = tf.reduce_mean(tf.square(self.f_pred - self.u_r_tf))\n",
    "\n",
    "        # self.loss_bcs = self.loss_ics_u + self.loss_bc1 + self.loss_bc2\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = 1.0* tf.reduce_mean(tf.square(self.loss_u))\n",
    "\n",
    "        # Total loss\n",
    "        # self.loss =  self.loss_res +  self.loss_bc1 +  self.loss_bc2 +  ( self.loss_ics_us)\n",
    "        self.loss = self.adaptive_constant_res_tf * self.loss_res  + self.adaptive_constant_bcs1_tf  * self.loss_bc1 + self.adaptive_constant_bcs2_tf  * self.loss_bc2 +  self.adaptive_constant_ics_tf * ( self.loss_ics_us) #-( self.adaptive_constant_res_tf * self.adaptive_constant_bcs1_tf  * self.adaptive_constant_bcs2_tf *  self.adaptive_constant_ics_tf)\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step , var_list = [self.weights+self.biases])\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res, self.loss_bc1 , self.loss_bc2 , self.loss_ics_us , self.loss_ics_u, self.loss_ics_u_t] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\"  , \"loss_bc1\", \"loss_bc2\", \"loss_ics_us\", \"loss_ics_u\", \"loss_ics_u_t\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "         # # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs1_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs2_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_ics_layers = self.generate_grad_dict()\n",
    "        \n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_ics = []\n",
    "        self.grad_bcs1 = []\n",
    "        self.grad_bcs2 = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs1.append(tf.gradients(self.loss_bc1, self.weights[i])[0])\n",
    "            self.grad_bcs2.append(tf.gradients(self.loss_bc2, self.weights[i])[0])\n",
    "            self.grad_ics.append(tf.gradients(self.loss_ics_us, self.weights[i])[0])\n",
    "          \n",
    "        self.max_grad_res_list = []\n",
    "        self.max_grad_bcs1_list = []\n",
    "        self.max_grad_bcs2_list = []\n",
    "        self.max_grad_ics_list = []\n",
    "\n",
    "        self.min_grad_res_list = []\n",
    "        self.min_grad_bcs1_list = []\n",
    "        self.min_grad_bcs2_list = []\n",
    "        self.min_grad_ics_list = []\n",
    "    \n",
    "        self.adaptive_constant_bcs1_log = []\n",
    "        self.adaptive_constant_bcs2_log = []\n",
    "        self.adaptive_constant_ics_log = []\n",
    "        self.adaptive_constant_res_log = []\n",
    "\n",
    "        self.mean_adaptive_constant_res_log = []\n",
    "        self.mean_adaptive_constant_bcs1_log = []\n",
    "        self.mean_adaptive_constant_bcs2_log = []\n",
    "        self.mean_adaptive_constant_ics_log = []\n",
    "\n",
    "        for i in range(1 , len(self.layers) - 2):\n",
    "            self.max_grad_res_list.append(tf.reduce_max(tf.abs(self.grad_res[i]))) \n",
    "            self.max_grad_bcs1_list.append(tf.reduce_mean(tf.abs(self.grad_bcs1[i])))\n",
    "            self.max_grad_bcs2_list.append(tf.reduce_mean(tf.abs(self.grad_bcs2[i])))\n",
    "            self.max_grad_ics_list.append(tf.reduce_mean(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "            self.min_grad_res_list.append(tf.reduce_min(tf.abs(self.grad_res[i]))) \n",
    "            self.min_grad_bcs1_list.append(tf.reduce_min(tf.abs(self.grad_bcs1[i])))\n",
    "            self.min_grad_bcs2_list.append(tf.reduce_min(tf.abs(self.grad_bcs2[i])))\n",
    "            self.min_grad_ics_list.append(tf.reduce_min(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_max(tf.stack(self.max_grad_res_list))\n",
    "        self.max_grad_bcs1 = tf.reduce_mean(tf.stack(self.max_grad_bcs1_list))\n",
    "        self.max_grad_bcs2 = tf.reduce_mean(tf.stack(self.max_grad_bcs2_list))\n",
    "        self.max_grad_ics = tf.reduce_mean(tf.stack(self.max_grad_ics_list))\n",
    "        \n",
    "        \n",
    "        self.min_grad_res = tf.reduce_min(tf.stack(self.min_grad_res_list))\n",
    "        self.min_grad_bcs1 = tf.reduce_min(tf.stack(self.min_grad_bcs1_list))\n",
    "        self.min_grad_bcs2 = tf.reduce_min(tf.stack(self.min_grad_bcs2_list))\n",
    "        self.min_grad_ics = tf.reduce_min(tf.stack(self.min_grad_ics_list))\n",
    "        \n",
    "        self.adaptive_constant_bcs1 = self.max_grad_bcs1\n",
    "        self.adaptive_constant_bcs2 = self.max_grad_bcs2\n",
    "        self.adaptive_constant_ics = self.max_grad_ics\n",
    "        self.adaptive_constant_res = self.max_grad_res \n",
    "\n",
    "         # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        # Xavier initialization\n",
    "        def xavier_init(size):\n",
    "            in_dim = size[0]\n",
    "            out_dim = size[1]\n",
    "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "            return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H, layers, weights, biases):\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1),  self.layers, self.weights,  self.biases)\n",
    "        return u\n",
    "\n",
    "    # Forward pass for du/dt\n",
    "    def net_u_t(self, t, x):\n",
    "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
    "        return u_t\n",
    "\n",
    "    # Forward pass for the residual\n",
    "    def net_r(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1),  self.layers, self.weights,  self.biases)\n",
    "        residual = self.operator(u, t, x, self.c, self.sigma_t, self.sigma_x)\n",
    "        return residual\n",
    "    \n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "        # Trains the model by minimizing the MSE loss\n",
    "\n",
    "\n",
    "    def lambda_balance(self  , term  ):\n",
    "                histoy_mean =  np.mean(self.loss_history[term])\n",
    "                m = 4 #len(self.loss_list)\n",
    "                num = np.exp( np.std(self.loss_history[term][-100::]) / (np.mean(self.loss_history[term][-100::]) + 1e-12) )#/(self.T * histoy_mean)) np.exp( )\n",
    "                denum = 0 \n",
    "                self.loss_list = [ \"loss_res\"  , \"loss_bc1\", \"loss_bc2\", \"loss_ics_us\"] \n",
    "\n",
    "                for  key in self.loss_list:\n",
    "                    denum +=  np.exp(  np.std(self.loss_history[key][-100::]) / (np.mean(self.loss_history[key][-100::]) + 1e-12) )# /(self.T * histoy_mean))  np.exp(self.loss_history[key][-1] )\n",
    "                return m * (num / denum)\n",
    "    \n",
    "    def trainmb(self, nIter, batch_size, a , c):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(1 , nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size // 3)\n",
    "            X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], batch_size // 3)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size // 3)\n",
    "            \n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "            _star = u(X_res_batch, a,c)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                       self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                       self.u_ics_tf: u_ics_batch,\n",
    "                       self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                        self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                       self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.u_r_tf: _star,\n",
    "                       self.adaptive_constant_bcs1_tf: self.adaptive_constant_bcs1_val,\n",
    "                       self.adaptive_constant_bcs2_tf: self.adaptive_constant_bcs2_val,\n",
    "                       self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
    "                       self.adaptive_constant_res_tf: self.adaptive_constant_res_val\n",
    "                       }#self.lam_r_val}\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "            \n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "            # Print\n",
    "            if it % 1000 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                # loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                # loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                # loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                # loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "                [loss ,  loss_res, loss_bc1 , loss_bc2 , loss_ics_us , loss_ics_u, loss_ics_u_t]   =  batch_losses \n",
    "                self.print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs1: %.3e,  Loss_bcs2: %.3e,  loss_ics_u: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss, loss_res, loss_bc1, loss_bc2, loss_ics_u ,  loss_ics_u_t, elapsed))\n",
    "                \n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "                max_grad_bcs1, max_grad_bcs2, max_grad_ics, max_grad_res = self.sess.run( [self.max_grad_bcs1, self.max_grad_bcs2,  self.max_grad_ics,  self.max_grad_res  ], tf_dict)\n",
    "                min_grad_bcs1, min_grad_bcs2, min_grad_ics, min_grad_res = self.sess.run( [self.min_grad_bcs1, self.min_grad_bcs2,  self.min_grad_ics,  self.min_grad_res  ], tf_dict)\n",
    "\n",
    "                # Print adaptive weights during training\n",
    "\n",
    "                max_grad = np.max( [max_grad_bcs1, max_grad_bcs2, max_grad_ics, max_grad_res])\n",
    "                min_grad = np.min( [min_grad_bcs1, min_grad_bcs2, min_grad_ics, min_grad_res])\n",
    "\n",
    "                ratio =  (max_grad/min_grad + 1e-8 )\n",
    "                update_res = self.lambda_balance( \"loss_res\"  )\n",
    "                update_ics_u = self.lambda_balance( \"loss_ics_us\"  )\n",
    "                update_bcs1 = self.lambda_balance( \"loss_bc1\"  )\n",
    "                update_bcs2 = self.lambda_balance( \"loss_bc2\"  )\n",
    "                \n",
    "                if not self.update:\n",
    "                    self.adaptive_constant_res_val =  max_grad_res  #/ (adaptive_constant_res_val + 1e-12) #( 1.0 - self.rate) + self.rate * self.adaptive_constant_res_val\n",
    "                    self.adaptive_constant_ics_val =  max_grad_res/ max_grad_ics  #/ (adaptive_constant_ics_val + 1e-12)#( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                    self.adaptive_constant_bcs1_val = max_grad_res/ max_grad_bcs1  #/ (adaptive_constant_bcs1_val + 1e-12) #( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "                    self.adaptive_constant_bcs2_val = max_grad_res / max_grad_bcs2  #/ (adaptive_constant_bcs2_val + 1e-12) #( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "                    self.update = True\n",
    "\n",
    "                self.print(\"minmax ratio : \"  ,ratio )\n",
    "                self.print('max_grad_bcs1: {:.3e} ,max_grad_bcs2: {:.3e} ,max_grad_ics: {:.3e} ,max_grad_res: {:.3e}  '.format( max_grad_bcs1 , max_grad_bcs2 ,max_grad_ics , max_grad_res ))\n",
    "                self.print('min_grad_bcs1: {:.3e} ,min_grad_bcs2: {:.3e} ,min_grad_ics: {:.3e} ,min_grad_res: {:.3e}  '.format( min_grad_bcs1 , min_grad_bcs2 ,min_grad_ics , min_grad_res ))\n",
    "\n",
    "                self.print('update_res: {:.3e}'.format( update_res))\n",
    "                self.print('update_ics_u_t: {:.3e}'.format( update_ics_u))\n",
    "                self.print('update_bcs1: {:.3e}'.format( update_bcs1))\n",
    "                self.print('update_bcs2: {:.3e}'.format( update_bcs2))\n",
    "                \n",
    "                self.print('adaptive_constant_res_val: {:.3e}'.format( self.adaptive_constant_res_val))\n",
    "                self.print('adaptive_constant_ics_val: {:.3e}'.format( self.adaptive_constant_ics_val))\n",
    "                self.print('adaptive_constant_bcs1_val: {:.3e}'.format( self.adaptive_constant_bcs1_val))\n",
    "                self.print('adaptive_constant_bcs2_val: {:.3e}'.format( self.adaptive_constant_bcs2_val))\n",
    "                \n",
    "                self.adaptive_constant_res_log.append(self.adaptive_constant_res_val)\n",
    "                self.adaptive_constant_bcs1_log.append(self.adaptive_constant_bcs1_val)\n",
    "                self.adaptive_constant_bcs2_log.append(self.adaptive_constant_bcs2_val)\n",
    "                self.adaptive_constant_ics_log.append(self.adaptive_constant_ics_val)\n",
    "\n",
    "                max_grad_res , mean_grad_bcs1, mean_grad_bcs2, mean_grad_ics = self.sess.run( [ self.max_grad_res , self.max_grad_bcs1, self.max_grad_bcs2, self.max_grad_ics  ], tf_dict)\n",
    "\n",
    "                self.mean_adaptive_constant_res_log.append( max_grad_res)\n",
    "                self.mean_adaptive_constant_bcs1_log.append( mean_grad_bcs1)\n",
    "                self.mean_adaptive_constant_bcs2_log.append( mean_grad_bcs2)\n",
    "                self.mean_adaptive_constant_ics_log.append( mean_grad_ics)\n",
    "\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                start_time = timeit.default_timer()\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "    def train(self, nIter , bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, bcbatch_size)\n",
    "        X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        \n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                    self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                    self.u_ics_tf: u_ics_batch,\n",
    "                    self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                    self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                    self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs1_val,\n",
    "                    self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val\n",
    "                    }#self.lam_r_val}\n",
    "        \n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 1000 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "                adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [self.adaptive_constant_bcs1, self.adaptive_constant_ics  ], tf_dict)\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs1_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs1_val\n",
    "\n",
    "\n",
    "                print('lambda_u: {:.3e}'.format(self.adaptive_constant_bcs1_val))\n",
    "                print('lambda_ut: {:.3e}'.format(self.adaptive_constant_ics_val))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                         \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "        # Evaluates predictions at test points\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "    \n",
    "   ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   \n",
    "     ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1  , grad_bc2 ,grad_ics  = self.sess.run([ self.grad_res[i],self.grad_bcs1[i],self.grad_bcs2[i],self.grad_ics[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs1_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "            self.dict_gradients_bcs2_layers['layer_' + str(i + 1)].append(grad_bc2.flatten())\n",
    "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs1_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc2 = self.dict_gradients_bcs2_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_ics = self.dict_gradients_ics_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "            sns.distplot(gradients_bc2, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc2}}$')\n",
    "            sns.distplot(gradients_ics, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{ics}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=4)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        #plt.show()\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , t , x , X_star , u_star , u_pred , r_star , r_pred):\n",
    "        \n",
    "        U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "        r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "        R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(18, 9))\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.pcolor(t, x, U_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$x_1$')\n",
    "        plt.ylabel('$x_2$')\n",
    "        plt.title('Exact u(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Predicted u(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Absolute error')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.pcolor(t, x, r_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Exact r(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.pcolor(t, x, R_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Predicted r(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Absolute error')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fontsize = 17\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([16,8])\n",
    "        ax.semilogy(self.mean_adaptive_constant_bcs1_log, label=r'$\\bar{\\nabla_\\theta {u_{bc1}}}$' , color = 'tab:green')\n",
    "        ax.semilogy(self.mean_adaptive_constant_bcs2_log, label=r'$\\bar{\\nabla_\\theta {u_{bc2}}}$' , color = 'tab:brown')\n",
    "        ax.semilogy(self.mean_adaptive_constant_ics_log, label=r'$\\bar{\\nabla_\\theta {u_{ics}}}$' , color = 'tab:blue')\n",
    "        ax.semilogy(self.mean_adaptive_constant_res_log, label=r'$Max{\\nabla_\\theta {u_{phy}}}$' , color = 'tab:red')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
    "        ax.tick_params(labelsize=fontsize)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
    "\n",
    "        ax2 = ax.twinx() \n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_size_inches([15,8])\n",
    "    \n",
    "        ax2.semilogy(self.adaptive_constant_bcs1_log, label=r'$\\bar{\\lambda_{bc1}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
    "        ax2.semilogy(self.adaptive_constant_bcs2_log, label=r'$\\bar{\\lambda_{bc2}}$'  ,  linestyle='dashed' , color = 'tab:brown') \n",
    "        ax2.semilogy(self.adaptive_constant_ics_log, label=r'$\\bar{\\lambda_{ics}}$' , linestyle='dashed'  , color = 'tab:blue')\n",
    "        ax2.semilogy(self.adaptive_constant_res_log, label=r'$\\bar{\\lambda_{phy}}$' ,  linestyle='dashed' , color = 'tab:red')\n",
    "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
    "        ax2.tick_params(labelsize=fontsize)\n",
    "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   #  \n",
    "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
    "def test_method(method , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "        # sess.run(init)\n",
    "\n",
    "        model = PINN(layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess)\n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "\n",
    "        if method ==\"full_batch\":\n",
    "            print(\"full_batch method is used\")\n",
    "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "        elif method ==\"mini_batch\":\n",
    "            print(\"mini_batch method is used\")\n",
    "            model.trainmb(nIter, mbbatch_size)\n",
    "        else:\n",
    "            print(\"unknown method!\")\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # Predictions\n",
    "        u_pred = model.predict_u(X_star)\n",
    "        r_pred = model.predict_r(X_star)\n",
    "        # Predictions\n",
    "\n",
    "        sess.close()   \n",
    "\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "\n",
    "    print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "\n",
    "    return [elapsed, error_u  ]\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19273/3933714513.py:65: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19273/3933714513.py:66: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19273/3933714513.py:67: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19273/3933714513.py:67: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19273/1071407323.py:127: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 00:53:41.863335: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-26 00:53:41.886276: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-26 00:53:41.886776: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bc5518e4c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-26 00:53:41.886791: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-26 00:53:41.887657: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_19273/1071407323.py:183: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19273/1071407323.py:185: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_19273/1071407323.py:264: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "mini_batch method is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1000, Loss: 2.380e-01, Loss_res: 1.591e-01,  Loss_bcs1: 5.928e-02,  Loss_bcs2: 5.736e-04,  loss_ics_u: 5.177e-02, Loss_ut_ics: 1.900e-02,, Time: 22.23\n",
      "minmax ratio : 1.2353e+09\n",
      "max_grad_bcs1: 1.043e-03 ,max_grad_bcs2: 4.340e-04 ,max_grad_ics: 4.183e-04 ,max_grad_res: 1.673e-02  \n",
      "min_grad_bcs1: 1.374e-10 ,min_grad_bcs2: 1.354e-11 ,min_grad_ics: 6.542e-11 ,min_grad_res: 1.588e-11  \n",
      "update_res: 6.252e-01\n",
      "update_ics_u_t: 1.310e+00\n",
      "update_bcs1: 6.327e-01\n",
      "update_bcs2: 1.432e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 2000, Loss: 5.670e-01, Loss_res: 6.328e-01,  Loss_bcs1: 3.404e-02,  Loss_bcs2: 6.116e-05,  loss_ics_u: 3.110e-02, Loss_ut_ics: 2.083e-04,, Time: 14.59\n",
      "minmax ratio : 8.6003e+10\n",
      "max_grad_bcs1: 8.276e-05 ,max_grad_bcs2: 2.398e-05 ,max_grad_ics: 1.276e-05 ,max_grad_res: 8.520e-02  \n",
      "min_grad_bcs1: 3.141e-11 ,min_grad_bcs2: 9.907e-13 ,min_grad_ics: 3.478e-12 ,min_grad_res: 2.471e-09  \n",
      "update_res: 7.490e-01\n",
      "update_ics_u_t: 7.967e-01\n",
      "update_bcs1: 7.549e-01\n",
      "update_bcs2: 1.699e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 3000, Loss: 2.688e-02, Loss_res: 8.568e-01,  Loss_bcs1: 6.479e-04,  Loss_bcs2: 1.273e-06,  loss_ics_u: 5.639e-04, Loss_ut_ics: 5.289e-05,, Time: 11.73\n",
      "minmax ratio : 9.6543e+11\n",
      "max_grad_bcs1: 2.463e-05 ,max_grad_bcs2: 1.727e-05 ,max_grad_ics: 5.552e-06 ,max_grad_res: 1.305e-01  \n",
      "min_grad_bcs1: 2.807e-13 ,min_grad_bcs2: 1.352e-13 ,min_grad_ics: 3.183e-12 ,min_grad_res: 3.108e-09  \n",
      "update_res: 7.433e-01\n",
      "update_ics_u_t: 7.561e-01\n",
      "update_bcs1: 7.891e-01\n",
      "update_bcs2: 1.711e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 4000, Loss: 1.734e-02, Loss_res: 8.605e-01,  Loss_bcs1: 9.275e-05,  Loss_bcs2: 1.647e-05,  loss_ics_u: 7.643e-05, Loss_ut_ics: 2.060e-05,, Time: 12.98\n",
      "minmax ratio : 1.3556e+12\n",
      "max_grad_bcs1: 1.599e-05 ,max_grad_bcs2: 1.891e-05 ,max_grad_ics: 4.201e-06 ,max_grad_res: 1.333e-01  \n",
      "min_grad_bcs1: 1.839e-11 ,min_grad_bcs2: 9.832e-14 ,min_grad_ics: 1.805e-12 ,min_grad_res: 3.884e-10  \n",
      "update_res: 6.335e-01\n",
      "update_ics_u_t: 6.441e-01\n",
      "update_bcs1: 6.454e-01\n",
      "update_bcs2: 2.077e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 5000, Loss: 2.123e-02, Loss_res: 6.016e-01,  Loss_bcs1: 2.914e-04,  Loss_bcs2: 1.511e-04,  loss_ics_u: 7.973e-05, Loss_ut_ics: 1.681e-05,, Time: 13.31\n",
      "minmax ratio : 4.9633e+11\n",
      "max_grad_bcs1: 9.750e-05 ,max_grad_bcs2: 7.748e-05 ,max_grad_ics: 6.550e-06 ,max_grad_res: 9.561e-02  \n",
      "min_grad_bcs1: 3.622e-12 ,min_grad_bcs2: 1.926e-13 ,min_grad_ics: 3.183e-12 ,min_grad_res: 6.694e-10  \n",
      "update_res: 6.754e-01\n",
      "update_ics_u_t: 7.504e-01\n",
      "update_bcs1: 9.289e-01\n",
      "update_bcs2: 1.645e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 6000, Loss: 4.174e-02, Loss_res: 6.835e-01,  Loss_bcs1: 9.390e-04,  Loss_bcs2: 3.718e-04,  loss_ics_u: 3.329e-04, Loss_ut_ics: 2.305e-05,, Time: 13.35\n",
      "minmax ratio : 1.5679e+12\n",
      "max_grad_bcs1: 1.643e-04 ,max_grad_bcs2: 1.061e-04 ,max_grad_ics: 1.657e-05 ,max_grad_res: 9.508e-02  \n",
      "min_grad_bcs1: 8.664e-11 ,min_grad_bcs2: 6.064e-14 ,min_grad_ics: 1.468e-12 ,min_grad_res: 1.310e-09  \n",
      "update_res: 5.676e-01\n",
      "update_ics_u_t: 1.045e+00\n",
      "update_bcs1: 1.079e+00\n",
      "update_bcs2: 1.309e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 7000, Loss: 1.072e-02, Loss_res: 5.399e-01,  Loss_bcs1: 7.619e-05,  Loss_bcs2: 8.437e-06,  loss_ics_u: 5.025e-05, Loss_ut_ics: 3.552e-06,, Time: 14.03\n",
      "minmax ratio : 1.8343e+12\n",
      "max_grad_bcs1: 3.887e-05 ,max_grad_bcs2: 1.451e-05 ,max_grad_ics: 4.823e-06 ,max_grad_res: 6.969e-02  \n",
      "min_grad_bcs1: 1.592e-12 ,min_grad_bcs2: 3.799e-14 ,min_grad_ics: 1.499e-13 ,min_grad_res: 2.595e-09  \n",
      "update_res: 2.067e-01\n",
      "update_ics_u_t: 8.356e-01\n",
      "update_bcs1: 1.340e+00\n",
      "update_bcs2: 1.618e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 8000, Loss: 1.225e-02, Loss_res: 6.753e-01,  Loss_bcs1: 4.015e-05,  Loss_bcs2: 3.850e-06,  loss_ics_u: 2.344e-05, Loss_ut_ics: 4.023e-06,, Time: 13.00\n",
      "minmax ratio : 9.7552e+11\n",
      "max_grad_bcs1: 6.917e-06 ,max_grad_bcs2: 3.651e-06 ,max_grad_ics: 1.942e-06 ,max_grad_res: 7.465e-02  \n",
      "min_grad_bcs1: 1.954e-13 ,min_grad_bcs2: 7.652e-14 ,min_grad_ics: 8.207e-13 ,min_grad_res: 5.148e-10  \n",
      "update_res: 2.571e-01\n",
      "update_ics_u_t: 8.496e-01\n",
      "update_bcs1: 1.267e+00\n",
      "update_bcs2: 1.626e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 9000, Loss: 8.664e-03, Loss_res: 4.480e-01,  Loss_bcs1: 2.593e-05,  Loss_bcs2: 8.268e-06,  loss_ics_u: 2.182e-05, Loss_ut_ics: 1.091e-05,, Time: 12.64\n",
      "minmax ratio : 1.0605e+12\n",
      "max_grad_bcs1: 1.145e-05 ,max_grad_bcs2: 1.988e-06 ,max_grad_ics: 4.482e-06 ,max_grad_res: 4.910e-02  \n",
      "min_grad_bcs1: 1.528e-12 ,min_grad_bcs2: 4.630e-14 ,min_grad_ics: 1.381e-13 ,min_grad_res: 5.994e-10  \n",
      "update_res: 4.675e-01\n",
      "update_ics_u_t: 8.711e-01\n",
      "update_bcs1: 1.172e+00\n",
      "update_bcs2: 1.489e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 10000, Loss: 1.041e-02, Loss_res: 5.044e-01,  Loss_bcs1: 4.672e-05,  Loss_bcs2: 1.738e-05,  loss_ics_u: 1.699e-05, Loss_ut_ics: 1.374e-05,, Time: 12.70\n",
      "minmax ratio : 1.7971e+11\n",
      "max_grad_bcs1: 1.584e-05 ,max_grad_bcs2: 1.926e-05 ,max_grad_ics: 2.551e-05 ,max_grad_res: 5.838e-02  \n",
      "min_grad_bcs1: 1.209e-11 ,min_grad_bcs2: 3.249e-13 ,min_grad_ics: 3.638e-12 ,min_grad_res: 3.147e-10  \n",
      "update_res: 4.769e-01\n",
      "update_ics_u_t: 9.327e-01\n",
      "update_bcs1: 1.237e+00\n",
      "update_bcs2: 1.353e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 11000, Loss: 7.648e-03, Loss_res: 3.460e-01,  Loss_bcs1: 6.131e-05,  Loss_bcs2: 3.334e-07,  loss_ics_u: 3.839e-05, Loss_ut_ics: 2.163e-05,, Time: 12.73\n",
      "minmax ratio : 8.2878e+11\n",
      "max_grad_bcs1: 4.480e-05 ,max_grad_bcs2: 1.999e-05 ,max_grad_ics: 6.037e-06 ,max_grad_res: 4.375e-02  \n",
      "min_grad_bcs1: 2.265e-12 ,min_grad_bcs2: 5.279e-14 ,min_grad_ics: 2.700e-13 ,min_grad_res: 5.730e-11  \n",
      "update_res: 3.800e-01\n",
      "update_ics_u_t: 8.157e-01\n",
      "update_bcs1: 1.466e+00\n",
      "update_bcs2: 1.339e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 12000, Loss: 8.764e-03, Loss_res: 4.388e-01,  Loss_bcs1: 5.513e-05,  Loss_bcs2: 8.951e-06,  loss_ics_u: 2.343e-05, Loss_ut_ics: 4.882e-06,, Time: 12.67\n",
      "minmax ratio : 1.9576e+11\n",
      "max_grad_bcs1: 2.628e-05 ,max_grad_bcs2: 1.065e-05 ,max_grad_ics: 4.826e-06 ,max_grad_res: 4.097e-02  \n",
      "min_grad_bcs1: 1.251e-12 ,min_grad_bcs2: 2.093e-13 ,min_grad_ics: 6.437e-13 ,min_grad_res: 5.585e-11  \n",
      "update_res: 5.408e-01\n",
      "update_ics_u_t: 1.228e+00\n",
      "update_bcs1: 9.075e-01\n",
      "update_bcs2: 1.323e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 13000, Loss: 6.652e-03, Loss_res: 3.789e-01,  Loss_bcs1: 1.135e-05,  Loss_bcs2: 9.991e-07,  loss_ics_u: 1.069e-05, Loss_ut_ics: 2.317e-06,, Time: 12.86\n",
      "minmax ratio : 5.5418e+11\n",
      "max_grad_bcs1: 5.596e-06 ,max_grad_bcs2: 2.930e-06 ,max_grad_ics: 2.802e-06 ,max_grad_res: 3.544e-02  \n",
      "min_grad_bcs1: 3.979e-13 ,min_grad_bcs2: 1.620e-13 ,min_grad_ics: 6.395e-14 ,min_grad_res: 2.032e-10  \n",
      "update_res: 4.930e-01\n",
      "update_ics_u_t: 1.266e+00\n",
      "update_bcs1: 9.122e-01\n",
      "update_bcs2: 1.328e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 14000, Loss: 7.104e-03, Loss_res: 3.597e-01,  Loss_bcs1: 1.027e-05,  Loss_bcs2: 4.304e-07,  loss_ics_u: 9.172e-06, Loss_ut_ics: 2.263e-05,, Time: 12.76\n",
      "minmax ratio : 8.6181e+12\n",
      "max_grad_bcs1: 1.167e-05 ,max_grad_bcs2: 6.454e-06 ,max_grad_ics: 1.099e-05 ,max_grad_res: 3.748e-02  \n",
      "min_grad_bcs1: 1.160e-12 ,min_grad_bcs2: 4.349e-15 ,min_grad_ics: 2.806e-13 ,min_grad_res: 3.082e-10  \n",
      "update_res: 2.962e-01\n",
      "update_ics_u_t: 8.860e-01\n",
      "update_bcs1: 1.240e+00\n",
      "update_bcs2: 1.578e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 15000, Loss: 1.273e-02, Loss_res: 4.765e-01,  Loss_bcs1: 1.776e-04,  Loss_bcs2: 3.734e-05,  loss_ics_u: 6.597e-05, Loss_ut_ics: 1.176e-05,, Time: 12.66\n",
      "minmax ratio : 5.7263e+11\n",
      "max_grad_bcs1: 3.508e-05 ,max_grad_bcs2: 1.850e-05 ,max_grad_ics: 7.443e-06 ,max_grad_res: 4.798e-02  \n",
      "min_grad_bcs1: 9.415e-14 ,min_grad_bcs2: 2.474e-13 ,min_grad_ics: 8.379e-14 ,min_grad_res: 6.587e-11  \n",
      "update_res: 5.060e-01\n",
      "update_ics_u_t: 8.572e-01\n",
      "update_bcs1: 1.064e+00\n",
      "update_bcs2: 1.573e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 16000, Loss: 8.712e-03, Loss_res: 4.970e-01,  Loss_bcs1: 1.455e-05,  Loss_bcs2: 4.501e-07,  loss_ics_u: 6.824e-06, Loss_ut_ics: 3.700e-06,, Time: 12.75\n",
      "minmax ratio : 3.6229e+12\n",
      "max_grad_bcs1: 7.071e-06 ,max_grad_bcs2: 2.098e-06 ,max_grad_ics: 2.304e-06 ,max_grad_res: 4.738e-02  \n",
      "min_grad_bcs1: 3.360e-13 ,min_grad_bcs2: 2.394e-14 ,min_grad_ics: 1.308e-14 ,min_grad_res: 1.697e-12  \n",
      "update_res: 3.505e-01\n",
      "update_ics_u_t: 6.913e-01\n",
      "update_bcs1: 1.196e+00\n",
      "update_bcs2: 1.762e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 17000, Loss: 6.922e-03, Loss_res: 3.254e-01,  Loss_bcs1: 2.262e-05,  Loss_bcs2: 1.608e-06,  loss_ics_u: 1.658e-05, Loss_ut_ics: 2.637e-05,, Time: 12.85\n",
      "minmax ratio : 1.9726e+12\n",
      "max_grad_bcs1: 8.379e-06 ,max_grad_bcs2: 1.578e-06 ,max_grad_ics: 8.689e-06 ,max_grad_res: 3.126e-02  \n",
      "min_grad_bcs1: 1.935e-12 ,min_grad_bcs2: 1.585e-14 ,min_grad_ics: 3.821e-14 ,min_grad_res: 4.834e-11  \n",
      "update_res: 5.216e-01\n",
      "update_ics_u_t: 1.177e+00\n",
      "update_bcs1: 1.044e+00\n",
      "update_bcs2: 1.257e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 18000, Loss: 8.746e-03, Loss_res: 3.025e-01,  Loss_bcs1: 7.478e-05,  Loss_bcs2: 5.119e-05,  loss_ics_u: 5.164e-05, Loss_ut_ics: 1.285e-05,, Time: 12.91\n",
      "minmax ratio : 2.0528e+13\n",
      "max_grad_bcs1: 2.664e-05 ,max_grad_bcs2: 6.350e-06 ,max_grad_ics: 5.989e-06 ,max_grad_res: 2.792e-02  \n",
      "min_grad_bcs1: 4.906e-12 ,min_grad_bcs2: 3.041e-13 ,min_grad_ics: 1.360e-15 ,min_grad_res: 4.429e-11  \n",
      "update_res: 2.826e-01\n",
      "update_ics_u_t: 8.612e-01\n",
      "update_bcs1: 1.157e+00\n",
      "update_bcs2: 1.699e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 19000, Loss: 6.222e-03, Loss_res: 2.870e-01,  Loss_bcs1: 5.066e-05,  Loss_bcs2: 1.358e-05,  loss_ics_u: 1.272e-05, Loss_ut_ics: 2.140e-06,, Time: 12.69\n",
      "minmax ratio : 6.5545e+13\n",
      "max_grad_bcs1: 2.140e-05 ,max_grad_bcs2: 7.041e-06 ,max_grad_ics: 2.824e-06 ,max_grad_res: 2.365e-02  \n",
      "min_grad_bcs1: 1.478e-12 ,min_grad_bcs2: 1.051e-13 ,min_grad_ics: 3.608e-16 ,min_grad_res: 4.638e-11  \n",
      "update_res: 5.285e-01\n",
      "update_ics_u_t: 9.906e-01\n",
      "update_bcs1: 1.113e+00\n",
      "update_bcs2: 1.368e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 20000, Loss: 4.904e-03, Loss_res: 2.711e-01,  Loss_bcs1: 9.568e-06,  Loss_bcs2: 1.400e-07,  loss_ics_u: 9.004e-06, Loss_ut_ics: 5.289e-06,, Time: 13.15\n",
      "minmax ratio : 3.0806e+11\n",
      "max_grad_bcs1: 9.922e-06 ,max_grad_bcs2: 3.761e-06 ,max_grad_ics: 1.004e-05 ,max_grad_res: 2.235e-02  \n",
      "min_grad_bcs1: 7.255e-14 ,min_grad_bcs2: 2.628e-13 ,min_grad_ics: 8.467e-14 ,min_grad_res: 4.947e-12  \n",
      "update_res: 4.554e-01\n",
      "update_ics_u_t: 9.883e-01\n",
      "update_bcs1: 9.830e-01\n",
      "update_bcs2: 1.573e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 21000, Loss: 5.541e-03, Loss_res: 3.115e-01,  Loss_bcs1: 4.451e-06,  Loss_bcs2: 6.588e-07,  loss_ics_u: 3.171e-06, Loss_ut_ics: 5.861e-06,, Time: 19.74\n",
      "minmax ratio : 5.7758e+12\n",
      "max_grad_bcs1: 5.525e-06 ,max_grad_bcs2: 8.827e-07 ,max_grad_ics: 2.708e-06 ,max_grad_res: 2.705e-02  \n",
      "min_grad_bcs1: 4.547e-13 ,min_grad_bcs2: 1.330e-14 ,min_grad_ics: 4.683e-15 ,min_grad_res: 2.033e-11  \n",
      "update_res: 3.268e-01\n",
      "update_ics_u_t: 1.165e+00\n",
      "update_bcs1: 1.000e+00\n",
      "update_bcs2: 1.508e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 22000, Loss: 6.868e-03, Loss_res: 4.017e-01,  Loss_bcs1: 2.857e-06,  Loss_bcs2: 2.275e-07,  loss_ics_u: 1.810e-06, Loss_ut_ics: 2.339e-06,, Time: 12.85\n",
      "minmax ratio : 1.2379e+12\n",
      "max_grad_bcs1: 4.753e-06 ,max_grad_bcs2: 1.573e-06 ,max_grad_ics: 4.446e-06 ,max_grad_res: 3.412e-02  \n",
      "min_grad_bcs1: 1.669e-13 ,min_grad_bcs2: 3.598e-14 ,min_grad_ics: 2.756e-14 ,min_grad_res: 1.206e-11  \n",
      "update_res: 4.976e-01\n",
      "update_ics_u_t: 1.287e+00\n",
      "update_bcs1: 9.233e-01\n",
      "update_bcs2: 1.292e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 23000, Loss: 6.280e-03, Loss_res: 3.629e-01,  Loss_bcs1: 4.081e-06,  Loss_bcs2: 2.009e-06,  loss_ics_u: 2.939e-06, Loss_ut_ics: 1.658e-06,, Time: 15.67\n",
      "minmax ratio : 8.9380e+11\n",
      "max_grad_bcs1: 2.970e-06 ,max_grad_bcs2: 1.677e-06 ,max_grad_ics: 2.065e-06 ,max_grad_res: 2.812e-02  \n",
      "min_grad_bcs1: 6.951e-14 ,min_grad_bcs2: 6.513e-14 ,min_grad_ics: 3.146e-14 ,min_grad_res: 8.280e-11  \n",
      "update_res: 5.231e-01\n",
      "update_ics_u_t: 9.704e-01\n",
      "update_bcs1: 7.580e-01\n",
      "update_bcs2: 1.748e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 24000, Loss: 5.457e-03, Loss_res: 3.029e-01,  Loss_bcs1: 9.944e-06,  Loss_bcs2: 1.214e-07,  loss_ics_u: 4.402e-06, Loss_ut_ics: 5.670e-06,, Time: 17.80\n",
      "minmax ratio : 7.3010e+13\n",
      "max_grad_bcs1: 1.036e-06 ,max_grad_bcs2: 7.715e-08 ,max_grad_ics: 1.585e-06 ,max_grad_res: 2.576e-02  \n",
      "min_grad_bcs1: 5.621e-15 ,min_grad_bcs2: 3.529e-16 ,min_grad_ics: 1.074e-15 ,min_grad_res: 2.051e-12  \n",
      "update_res: 5.084e-01\n",
      "update_ics_u_t: 9.851e-01\n",
      "update_bcs1: 1.196e+00\n",
      "update_bcs2: 1.311e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 25000, Loss: 6.511e-03, Loss_res: 3.837e-01,  Loss_bcs1: 2.384e-06,  Loss_bcs2: 1.235e-07,  loss_ics_u: 1.811e-06, Loss_ut_ics: 1.231e-06,, Time: 12.41\n",
      "minmax ratio : 4.0103e+13\n",
      "max_grad_bcs1: 1.940e-06 ,max_grad_bcs2: 7.636e-08 ,max_grad_ics: 2.798e-06 ,max_grad_res: 2.652e-02  \n",
      "min_grad_bcs1: 1.866e-14 ,min_grad_bcs2: 6.614e-16 ,min_grad_ics: 2.337e-14 ,min_grad_res: 2.000e-11  \n",
      "update_res: 6.650e-01\n",
      "update_ics_u_t: 8.178e-01\n",
      "update_bcs1: 8.074e-01\n",
      "update_bcs2: 1.710e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 26000, Loss: 4.493e-03, Loss_res: 2.533e-01,  Loss_bcs1: 3.255e-06,  Loss_bcs2: 1.236e-06,  loss_ics_u: 1.220e-06, Loss_ut_ics: 3.926e-06,, Time: 12.39\n",
      "minmax ratio : 2.9824e+12\n",
      "max_grad_bcs1: 2.612e-06 ,max_grad_bcs2: 1.345e-06 ,max_grad_ics: 3.621e-06 ,max_grad_res: 2.252e-02  \n",
      "min_grad_bcs1: 1.341e-13 ,min_grad_bcs2: 2.799e-14 ,min_grad_ics: 7.550e-15 ,min_grad_res: 7.963e-13  \n",
      "update_res: 5.580e-01\n",
      "update_ics_u_t: 1.126e+00\n",
      "update_bcs1: 9.418e-01\n",
      "update_bcs2: 1.375e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 27000, Loss: 5.275e-03, Loss_res: 2.799e-01,  Loss_bcs1: 1.394e-05,  Loss_bcs2: 8.064e-06,  loss_ics_u: 1.066e-05, Loss_ut_ics: 1.453e-06,, Time: 12.86\n",
      "minmax ratio : 5.8043e+11\n",
      "max_grad_bcs1: 1.288e-05 ,max_grad_bcs2: 7.039e-06 ,max_grad_ics: 2.392e-06 ,max_grad_res: 2.569e-02  \n",
      "min_grad_bcs1: 2.047e-13 ,min_grad_bcs2: 8.849e-14 ,min_grad_ics: 4.426e-14 ,min_grad_res: 6.079e-12  \n",
      "update_res: 5.002e-01\n",
      "update_ics_u_t: 1.043e+00\n",
      "update_bcs1: 8.748e-01\n",
      "update_bcs2: 1.582e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 28000, Loss: 5.869e-03, Loss_res: 3.286e-01,  Loss_bcs1: 3.250e-06,  Loss_bcs2: 1.267e-06,  loss_ics_u: 2.551e-06, Loss_ut_ics: 6.815e-06,, Time: 11.43\n",
      "minmax ratio : 2.7574e+12\n",
      "max_grad_bcs1: 3.220e-06 ,max_grad_bcs2: 1.773e-06 ,max_grad_ics: 1.603e-05 ,max_grad_res: 2.566e-02  \n",
      "min_grad_bcs1: 1.274e-14 ,min_grad_bcs2: 9.307e-15 ,min_grad_ics: 3.575e-14 ,min_grad_res: 2.340e-11  \n",
      "update_res: 5.211e-01\n",
      "update_ics_u_t: 1.258e+00\n",
      "update_bcs1: 1.023e+00\n",
      "update_bcs2: 1.197e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 29000, Loss: 4.847e-03, Loss_res: 2.724e-01,  Loss_bcs1: 5.097e-06,  Loss_bcs2: 6.292e-07,  loss_ics_u: 2.353e-06, Loss_ut_ics: 4.594e-06,, Time: 14.40\n",
      "minmax ratio : 3.9640e+11\n",
      "max_grad_bcs1: 4.129e-06 ,max_grad_bcs2: 4.345e-06 ,max_grad_ics: 7.232e-06 ,max_grad_res: 1.832e-02  \n",
      "min_grad_bcs1: 8.527e-14 ,min_grad_bcs2: 4.622e-14 ,min_grad_ics: 3.148e-13 ,min_grad_res: 3.835e-13  \n",
      "update_res: 3.867e-01\n",
      "update_ics_u_t: 9.491e-01\n",
      "update_bcs1: 1.024e+00\n",
      "update_bcs2: 1.641e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 30000, Loss: 4.179e-03, Loss_res: 2.453e-01,  Loss_bcs1: 2.007e-06,  Loss_bcs2: 5.514e-07,  loss_ics_u: 1.473e-06, Loss_ut_ics: 5.658e-07,, Time: 12.42\n",
      "minmax ratio : 5.6056e+12\n",
      "max_grad_bcs1: 5.106e-06 ,max_grad_bcs2: 1.461e-06 ,max_grad_ics: 1.984e-06 ,max_grad_res: 1.926e-02  \n",
      "min_grad_bcs1: 4.547e-13 ,min_grad_bcs2: 3.435e-15 ,min_grad_ics: 1.040e-13 ,min_grad_res: 3.328e-12  \n",
      "update_res: 4.926e-01\n",
      "update_ics_u_t: 8.132e-01\n",
      "update_bcs1: 8.263e-01\n",
      "update_bcs2: 1.868e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 31000, Loss: 4.001e-03, Loss_res: 2.326e-01,  Loss_bcs1: 1.930e-06,  Loss_bcs2: 2.810e-07,  loss_ics_u: 1.496e-06, Loss_ut_ics: 1.725e-06,, Time: 14.08\n",
      "minmax ratio : 3.6057e+13\n",
      "max_grad_bcs1: 1.915e-06 ,max_grad_bcs2: 1.094e-06 ,max_grad_ics: 5.766e-06 ,max_grad_res: 2.379e-02  \n",
      "min_grad_bcs1: 7.137e-14 ,min_grad_bcs2: 6.597e-16 ,min_grad_ics: 5.465e-14 ,min_grad_res: 9.382e-12  \n",
      "update_res: 6.367e-01\n",
      "update_ics_u_t: 9.454e-01\n",
      "update_bcs1: 9.667e-01\n",
      "update_bcs2: 1.451e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 32000, Loss: 4.136e-03, Loss_res: 2.401e-01,  Loss_bcs1: 2.582e-06,  Loss_bcs2: 6.668e-07,  loss_ics_u: 1.328e-06, Loss_ut_ics: 1.311e-06,, Time: 12.77\n",
      "minmax ratio : 3.0160e+13\n",
      "max_grad_bcs1: 1.160e-06 ,max_grad_bcs2: 1.518e-06 ,max_grad_ics: 1.437e-06 ,max_grad_res: 1.781e-02  \n",
      "min_grad_bcs1: 3.454e-14 ,min_grad_bcs2: 4.986e-14 ,min_grad_ics: 5.907e-16 ,min_grad_res: 4.883e-12  \n",
      "update_res: 5.313e-01\n",
      "update_ics_u_t: 9.455e-01\n",
      "update_bcs1: 9.695e-01\n",
      "update_bcs2: 1.554e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 33000, Loss: 5.505e-03, Loss_res: 3.142e-01,  Loss_bcs1: 4.763e-06,  Loss_bcs2: 1.946e-06,  loss_ics_u: 1.034e-06, Loss_ut_ics: 2.460e-06,, Time: 12.48\n",
      "minmax ratio : 6.4508e+11\n",
      "max_grad_bcs1: 9.376e-06 ,max_grad_bcs2: 3.893e-06 ,max_grad_ics: 2.622e-06 ,max_grad_res: 3.154e-02  \n",
      "min_grad_bcs1: 6.018e-12 ,min_grad_bcs2: 9.200e-14 ,min_grad_ics: 4.889e-14 ,min_grad_res: 1.096e-10  \n",
      "update_res: 6.681e-01\n",
      "update_ics_u_t: 1.206e+00\n",
      "update_bcs1: 8.972e-01\n",
      "update_bcs2: 1.229e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 34000, Loss: 3.594e-03, Loss_res: 2.120e-01,  Loss_bcs1: 1.045e-06,  Loss_bcs2: 4.536e-07,  loss_ics_u: 8.553e-07, Loss_ut_ics: 3.511e-07,, Time: 12.91\n",
      "minmax ratio : 2.0983e+12\n",
      "max_grad_bcs1: 3.839e-06 ,max_grad_bcs2: 6.112e-07 ,max_grad_ics: 3.270e-06 ,max_grad_res: 1.851e-02  \n",
      "min_grad_bcs1: 1.821e-14 ,min_grad_bcs2: 8.822e-15 ,min_grad_ics: 1.193e-14 ,min_grad_res: 8.921e-12  \n",
      "update_res: 5.846e-01\n",
      "update_ics_u_t: 1.138e+00\n",
      "update_bcs1: 9.206e-01\n",
      "update_bcs2: 1.356e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 35000, Loss: 4.723e-03, Loss_res: 2.773e-01,  Loss_bcs1: 1.443e-06,  Loss_bcs2: 3.673e-07,  loss_ics_u: 7.911e-07, Loss_ut_ics: 1.182e-06,, Time: 12.80\n",
      "minmax ratio : 1.0041e+12\n",
      "max_grad_bcs1: 2.175e-06 ,max_grad_bcs2: 8.712e-07 ,max_grad_ics: 2.152e-06 ,max_grad_res: 2.586e-02  \n",
      "min_grad_bcs1: 1.442e-13 ,min_grad_bcs2: 2.575e-14 ,min_grad_ics: 1.769e-13 ,min_grad_res: 9.616e-11  \n",
      "update_res: 8.096e-01\n",
      "update_ics_u_t: 1.128e+00\n",
      "update_bcs1: 9.739e-01\n",
      "update_bcs2: 1.089e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 36000, Loss: 3.743e-03, Loss_res: 2.148e-01,  Loss_bcs1: 3.503e-06,  Loss_bcs2: 1.760e-06,  loss_ics_u: 1.161e-06, Loss_ut_ics: 6.305e-07,, Time: 12.77\n",
      "minmax ratio : 3.7824e+11\n",
      "max_grad_bcs1: 5.541e-06 ,max_grad_bcs2: 3.516e-06 ,max_grad_ics: 2.601e-06 ,max_grad_res: 1.335e-02  \n",
      "min_grad_bcs1: 2.635e-12 ,min_grad_bcs2: 3.530e-14 ,min_grad_ics: 6.537e-13 ,min_grad_res: 7.133e-12  \n",
      "update_res: 6.140e-01\n",
      "update_ics_u_t: 1.294e+00\n",
      "update_bcs1: 9.066e-01\n",
      "update_bcs2: 1.185e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 37000, Loss: 3.992e-03, Loss_res: 2.360e-01,  Loss_bcs1: 8.367e-07,  Loss_bcs2: 3.930e-07,  loss_ics_u: 6.689e-07, Loss_ut_ics: 3.862e-07,, Time: 13.01\n",
      "minmax ratio : 5.6987e+12\n",
      "max_grad_bcs1: 1.696e-06 ,max_grad_bcs2: 1.315e-07 ,max_grad_ics: 1.425e-06 ,max_grad_res: 2.242e-02  \n",
      "min_grad_bcs1: 1.828e-13 ,min_grad_bcs2: 3.934e-15 ,min_grad_ics: 1.991e-14 ,min_grad_res: 1.267e-12  \n",
      "update_res: 7.123e-01\n",
      "update_ics_u_t: 9.796e-01\n",
      "update_bcs1: 1.072e+00\n",
      "update_bcs2: 1.236e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 38000, Loss: 4.466e-03, Loss_res: 2.647e-01,  Loss_bcs1: 9.809e-07,  Loss_bcs2: 2.583e-07,  loss_ics_u: 6.959e-07, Loss_ut_ics: 3.125e-07,, Time: 12.70\n",
      "minmax ratio : 1.4449e+13\n",
      "max_grad_bcs1: 5.616e-07 ,max_grad_bcs2: 1.127e-07 ,max_grad_ics: 1.509e-06 ,max_grad_res: 1.604e-02  \n",
      "min_grad_bcs1: 1.110e-15 ,min_grad_bcs2: 4.828e-15 ,min_grad_ics: 4.003e-14 ,min_grad_res: 2.778e-11  \n",
      "update_res: 8.714e-01\n",
      "update_ics_u_t: 1.104e+00\n",
      "update_bcs1: 9.713e-01\n",
      "update_bcs2: 1.053e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "It: 39000, Loss: 3.932e-03, Loss_res: 2.317e-01,  Loss_bcs1: 6.746e-07,  Loss_bcs2: 5.903e-07,  loss_ics_u: 4.979e-07, Loss_ut_ics: 5.529e-07,, Time: 12.48\n",
      "minmax ratio : 4.2862e+12\n",
      "max_grad_bcs1: 2.069e-06 ,max_grad_bcs2: 1.154e-06 ,max_grad_ics: 1.987e-06 ,max_grad_res: 1.599e-02  \n",
      "min_grad_bcs1: 1.386e-13 ,min_grad_bcs2: 3.731e-15 ,min_grad_ics: 2.040e-14 ,min_grad_res: 1.777e-11  \n",
      "update_res: 8.563e-01\n",
      "update_ics_u_t: 1.161e+00\n",
      "update_bcs1: 9.800e-01\n",
      "update_bcs2: 1.002e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 40000, Loss: 3.986e-03, Loss_res: 2.366e-01,  Loss_bcs1: 6.724e-07,  Loss_bcs2: 2.056e-07,  loss_ics_u: 4.575e-07, Loss_ut_ics: 2.416e-07,, Time: 17.34\n",
      "minmax ratio : 1.6336e+12\n",
      "max_grad_bcs1: 2.138e-06 ,max_grad_bcs2: 1.003e-06 ,max_grad_ics: 1.888e-06 ,max_grad_res: 1.903e-02  \n",
      "min_grad_bcs1: 3.979e-13 ,min_grad_bcs2: 4.958e-14 ,min_grad_ics: 1.165e-14 ,min_grad_res: 3.185e-12  \n",
      "update_res: 8.064e-01\n",
      "update_ics_u_t: 1.122e+00\n",
      "update_bcs1: 9.466e-01\n",
      "update_bcs2: 1.125e+00\n",
      "adaptive_constant_res_val: 1.673e-02\n",
      "adaptive_constant_ics_val: 3.999e+01\n",
      "adaptive_constant_bcs1_val: 1.603e+01\n",
      "adaptive_constant_bcs2_val: 3.854e+01\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [?,1]\n\t [[node Placeholder (defined at home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'Placeholder':\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n    await self.process_one()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n    await dispatch(*args)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n    await result\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n    reply_content = await reply_content\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2915, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n    return runner(coro)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3186, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"tmp/ipykernel_19273/3933714513.py\", line 70, in <module>\n    model = PINN(layers, operator ,  ics_sampler, bcs_sampler, res_sampler, c , mode , sess)\n  File \"tmp/ipykernel_19273/1071407323.py\", line 127, in __init__\n    self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 2619, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 6669, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [?,1]\n\t [[{{node Placeholder}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19273/3933714513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mu_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19273/1071407323.py\u001b[0m in \u001b[0;36mpredict_r\u001b[0;34m(self, X_star)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mX_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_star\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_X\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mtf_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_r_tf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_star\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_r_tf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_star\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mr_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr_star\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [?,1]\n\t [[node Placeholder (defined at home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'Placeholder':\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n    await self.process_one()\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n    await dispatch(*args)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n    await result\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n    reply_content = await reply_content\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2915, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n    return runner(coro)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3186, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"tmp/ipykernel_19273/3933714513.py\", line 70, in <module>\n    model = PINN(layers, operator ,  ics_sampler, bcs_sampler, res_sampler, c , mode , sess)\n  File \"tmp/ipykernel_19273/1071407323.py\", line 127, in __init__\n    self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 2619, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 6669, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define PINN model\n",
    "a = 0.5\n",
    "c = 2\n",
    "\n",
    "kernel_size = 300\n",
    "\n",
    "# Domain boundaries\n",
    "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
    "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
    "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
    "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
    "\n",
    "# Create boundary conditions samplers\n",
    "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
    "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
    "bcs_sampler = [bc1, bc2]\n",
    "\n",
    "# Create residual sampler\n",
    "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
    "\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 300\n",
    "\n",
    "\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'\n",
    "layers = [2, 500, 500, 500, 1]\n",
    "\n",
    "\n",
    "nn = 200\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "u_star = u(X_star, a,c)\n",
    "r_star = r(X_star, a, c)\n",
    "\n",
    "iterations = 1\n",
    "methods = [  \"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_r_list = []\n",
    "\n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "     # Create residual sampler\n",
    "\n",
    "        # [elapsed, error_u , model] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            # sess.run(init)\n",
    "\n",
    "            model = PINN(layers, operator ,  ics_sampler, bcs_sampler, res_sampler, c , mode , sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                print(\"full_batch method is used\")\n",
    "                model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                print(\"mini_batch method is used\")\n",
    "                model.trainmb(nIter, mbbatch_size , a , c)\n",
    "            else:\n",
    "                print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            r_pred = model.predict_r(X_star)\n",
    "            # Predictions\n",
    "\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
    "\n",
    "            model.print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "            model.plot_lambda()\n",
    "            # model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
    "            model.print(\"average lambda_bc : \" , np.average(model.adaptive_constant_bcs1_log))\n",
    "            model.print(\"average lambda_ic : \" , np.average(model.adaptive_constant_ics_log))\n",
    "            model.print(\"average lambda_res : \" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "        # print('elapsed: {:.2e}'.format(elapsed))\n",
    "        # print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "        # print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_r_list.append(error_r)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list,error_r_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "elapsed: 6.79e+02\n",
      "Relative L2 error_u: 9.86e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Save uv NN parameters successfully in %s ...checkpoints/Dec-26-2023_00-53-41-887955_M2\n",
      "Final loss total loss: 3.986412e-03\n",
      "Final loss loss_res: 2.366266e-01\n",
      "Final loss loss_bc1: 6.724138e-07\n",
      "Final loss loss_bc2: 2.055639e-07\n",
      "Final loss loss_ics_us: 2.415584e-07\n",
      "Final loss loss_ics_u: 4.575136e-07\n",
      "Final loss loss_ics_u_t: 2.415584e-07\n",
      "average lambda_bc : 1.6033e+01\n",
      "average lambda_ic : 3.9993e+01\n",
      "average lambda_res : 1.0\n",
      "\n",
      "\n",
      "Method: mini_batch\n",
      "\n",
      "average of time_list:6.7892e+02\n",
      "average of error_u_list:9.8603e-01\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/scipy/io/matlab/mio5.py:493: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  narr = np.asanyarray(source)\n"
     ]
    }
   ],
   "source": [
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "u_pred = model.predict_u(X_star)\n",
    "# r_pred = model.predict_r(X_star)\n",
    "# Predictions\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "# error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
    "\n",
    "model.print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "# model.print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "model.plot_lambda()\n",
    "# model.plot_grad()\n",
    "model.save_NN()\n",
    "model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , u_pred)\n",
    "model.print(\"average lambda_bc : \" , np.average(model.adaptive_constant_bcs1_log))\n",
    "model.print(\"average lambda_ic : \" , np.average(model.adaptive_constant_ics_log))\n",
    "model.print(\"average lambda_res : \" , str(1.0))\n",
    "# sess.close()  \n",
    "\n",
    "# print('elapsed: {:.2e}'.format(elapsed))\n",
    "# print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "# print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "\n",
    "time_list.append(elapsed)\n",
    "error_u_list.append(error_u)\n",
    "# error_r_list.append(error_r)\n",
    "\n",
    "model.print(\"\\n\\nMethod: \", mtd)\n",
    "model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "# model.print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
    "\n",
    "result_dict[mtd] = [time_list ,error_u_list,error_r_list]\n",
    "# scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
