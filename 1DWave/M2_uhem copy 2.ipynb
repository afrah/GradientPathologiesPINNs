{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
    "import numpy as np\n",
    "import timeit\n",
    "from scipy.interpolate import griddata\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "\n",
    "import timeit\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name = None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1,:] + (self.coords[1:2,:]-self.coords[0:1,:])*np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "# Define the exact solution and its derivatives\n",
    "def u(x, a, c):\n",
    "    \"\"\"\n",
    "    :param x: x = (t, x)\n",
    "    \"\"\"\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    return np.sin(np.pi * x) * np.cos(c * np.pi * t) + a * np.sin(2 * c * np.pi* x) * np.cos(4 * c  * np.pi * t)\n",
    "\n",
    "def u_t(x,a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_t = -  c * np.pi * np.sin(np.pi * x) * np.sin(c * np.pi * t) -  a * 4 * c * np.pi * np.sin(2 * c * np.pi* x) * np.sin(4 * c * np.pi * t)\n",
    "    return u_t\n",
    "\n",
    "def u_tt(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_tt = -(c * np.pi)**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) - a * (4 * c * np.pi)**2 *  np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return u_tt\n",
    "\n",
    "def u_xx(x, a, c):\n",
    "    t = x[:,0:1]\n",
    "    x = x[:,1:2]\n",
    "    u_xx = - np.pi**2 * np.sin( np.pi * x) * np.cos(c * np.pi * t) -  a * (2 * c * np.pi)** 2 * np.sin(2 * c * np.pi* x) * np.cos(4 * c * np.pi * t)\n",
    "    return  u_xx\n",
    "\n",
    "\n",
    "def r(x, a, c):\n",
    "    return u_tt(x, a, c) - c**2 * u_xx(x, a, c)\n",
    "\n",
    "def operator(u, t, x, c, sigma_t=1.0, sigma_x=1.0):\n",
    "    u_t = tf.gradients(u, t)[0] / sigma_t\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_tt = tf.gradients(u_t, t)[0] / sigma_t\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    residual = u_tt - c**2 * u_xx\n",
    "    return residual\n",
    "\n",
    "class PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, layers, operator, ics_sampler, bcs_sampler, res_sampler, c , mode ,  sess):\n",
    "        # Normalization \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_t, self.sigma_t = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_x, self.sigma_x = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.ics_sampler = ics_sampler\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        self.sess = sess\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # weights\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "        self.adaptive_constant_ics_val = np.array(1.0)\n",
    "        self.adaptive_constant_res_val = np.array(1.0)\n",
    "        self.rate = 0.9\n",
    "\n",
    "        # Wave constant\n",
    "        self.c = tf.constant(c, dtype=tf.float32)\n",
    "        \n",
    "        # self.kernel_size = kernel_size # Size of the NTK matrix\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.t_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_ics_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.t_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        \n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "        self.adaptive_constant_ics_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_ics_val.shape)\n",
    "        self.adaptive_constant_res_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_res_val.shape)\n",
    "        \n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_ics_pred = self.net_u(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_t_ics_pred = self.net_u_t(self.t_ics_tf, self.x_ics_tf)\n",
    "        self.u_bc1_pred = self.net_u(self.t_bc1_tf, self.x_bc1_tf)\n",
    "        self.u_bc2_pred = self.net_u(self.t_bc2_tf, self.x_bc2_tf)\n",
    "\n",
    "        self.u_pred = self.net_u(self.t_u_tf, self.x_u_tf)\n",
    "        self.r_pred = self.net_r(self.t_r_tf, self.x_r_tf)\n",
    "        \n",
    "\n",
    "        # Boundary loss and Initial loss\n",
    "        self.loss_ics_u = tf.reduce_mean(tf.square(self.u_ics_tf - self.u_ics_pred))\n",
    "        self.loss_ics_u_t = tf.reduce_mean(tf.square(self.u_t_ics_pred))\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.u_bc1_pred))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.u_bc2_pred))\n",
    "\n",
    "        self.loss_bcs = self.loss_ics_u + self.loss_bc1 + self.loss_bc2\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_res = tf.reduce_mean(tf.square(self.r_pred))\n",
    "\n",
    "        # Total loss\n",
    "        self.loss =  self.adaptive_constant_res_tf * self.loss_res + self.adaptive_constant_bcs_tf * self.loss_bcs + self.adaptive_constant_ics_tf * self.loss_ics_u_t \n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bcs,  self.loss_bc1 , self.loss_bc2 , self.loss_ics_u , self.loss_ics_u_t] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs\" , \"loss_bc1\", \"loss_bc2\", \"loss_ics_u\", \"loss_ics_u_t\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_u_log = []\n",
    "        self.loss_r_log = []\n",
    "\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "         # # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_ics_layers = self.generate_grad_dict()\n",
    "        \n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_ics = []\n",
    "        self.grad_bcs = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "            self.grad_ics.append(tf.gradients(self.loss_ics_u_t, self.weights[i])[0])\n",
    "          \n",
    "        self.max_grad_res_list = []\n",
    "        self.mean_grad_bcs_list = []\n",
    "        self.mean_grad_ics_list = []\n",
    "\n",
    "        self.adaptive_constant_bcs_log = []\n",
    "        self.adaptive_constant_ics_log = []\n",
    "        self.adaptive_constant_res_log = []\n",
    "\n",
    "        self.mean_adaptive_constant_res_log = []\n",
    "        self.mean_adaptive_constant_bcs_log = []\n",
    "        self.mean_adaptive_constant_ics_log = []\n",
    "\n",
    "        for i in range(1 , len( self.layers) - 2):\n",
    "            self.max_grad_res_list.append(tf.reduce_mean(tf.abs(self.grad_res[i]))) \n",
    "            self.mean_grad_bcs_list.append(tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "            self.mean_grad_ics_list.append(tf.reduce_mean(tf.abs(self.grad_ics[i])))\n",
    "        \n",
    "        self.max_grad_res = tf.reduce_mean(tf.stack(self.max_grad_res_list))\n",
    "        self.mean_grad_bcs = tf.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
    "        self.mean_grad_ics = tf.reduce_mean(tf.stack(self.mean_grad_ics_list))\n",
    "        \n",
    "        self.adaptive_constant_bcs = self.max_grad_res/self.mean_grad_bcs\n",
    "        self.adaptive_constant_ics = self.max_grad_res/ self.mean_grad_ics\n",
    "        self.adaptive_constant_res = self.max_grad_res \n",
    "\n",
    "         # Initialize Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        # Xavier initialization\n",
    "        def xavier_init(size):\n",
    "            in_dim = size[0]\n",
    "            out_dim = size[1]\n",
    "            xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "            return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H, layers, weights, biases):\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "    # Forward pass for u\n",
    "    def net_u(self, t, x):\n",
    "        u = self.forward_pass(tf.concat([t, x], 1),\n",
    "                              self.layers,\n",
    "                              self.weights,\n",
    "                              self.biases)\n",
    "        return u\n",
    "\n",
    "    # Forward pass for du/dt\n",
    "    def net_u_t(self, t, x):\n",
    "        u_t = tf.gradients(self.net_u(t, x), t)[0] / self.sigma_t\n",
    "        return u_t\n",
    "\n",
    "    # Forward pass for the residual\n",
    "    def net_r(self, t, x):\n",
    "        u = self.net_u(t, x)\n",
    "        residual = self.operator(u, t, x,\n",
    "                                 self.c,\n",
    "                                 self.sigma_t,\n",
    "                                 self.sigma_x)\n",
    "        return residual\n",
    "    \n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "        # Trains the model by minimizing the MSE loss\n",
    "\n",
    "\n",
    "    def lambda_balance(self  , term  ):\n",
    "                histoy_mean =  np.mean(self.loss_history[term])\n",
    "                m = 3.0 #len(self.loss_list)\n",
    "                num = np.exp(10*  np.mean(self.loss_history[term][-99::]) )#/(self.T * histoy_mean)) np.exp( )\n",
    "                denum = 0 \n",
    "                loss_list = [ \"loss_res\"   , \"loss_bcs\" ,\"loss_ics_u_t\"  ]\n",
    "                for  key in loss_list:\n",
    "                    denum +=  np.exp( 10* np.mean(self.loss_history[key][-99::]) + 1e-12 )# /(self.T * histoy_mean))  np.exp(self.loss_history[key][-1] )\n",
    "\n",
    "                return m * (num / denum)\n",
    "    \n",
    "    def trainmb(self, nIter=10000, batch_size=128, log_NTK=False, update_lam=False):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range( 1, nIter):\n",
    "            # Fetch boundary mini-batches , \n",
    "            X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, batch_size // 3)\n",
    "            X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], batch_size // 3)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size // 3)\n",
    "            \n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                       self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                       self.u_ics_tf: u_ics_batch,\n",
    "                       self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                        self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                       self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
    "                       self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val,\n",
    "                       self.adaptive_constant_res_tf: self.adaptive_constant_res_val\n",
    "                       }#self.lam_r_val}\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _ , batch_losses = self.sess.run( [  self.train_op , self.loss_tensor_list ] ,tf_dict)\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                loss_ics_u_t = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "\n",
    "                self.print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs, loss_ics_u_t, elapsed))\n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "                \n",
    "                adaptive_constant_res_val, adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [self.adaptive_constant_res, self.adaptive_constant_bcs, self.adaptive_constant_ics  ], tf_dict)\n",
    "\n",
    "                update_loss_res = self.lambda_balance( \"loss_res\"  )\n",
    "                update_loss_bcs = self.lambda_balance( \"loss_bcs\"  )\n",
    "                update_loss_ics_u_t1 = self.lambda_balance( \"loss_ics_u_t\"  )\n",
    "\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_res_val = update_loss_res / adaptive_constant_res_val #  * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_res_val\n",
    "                self.adaptive_constant_ics_val = update_loss_ics_u_t1 / adaptive_constant_ics_val # * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs_val = update_loss_bcs /  adaptive_constant_bcs_val # * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "                self.print('update_loss_res: ' , ( update_loss_res ))\n",
    "                self.print('update_loss_ics_u_t:' , (update_loss_ics_u_t1))\n",
    "                self.print('update_loss_bcs: ' , ( update_loss_bcs))\n",
    "                \n",
    "                self.print('adaptive_constant_res_val: ' , ( self.adaptive_constant_res_val ))\n",
    "                self.print('adaptive_constant_ics_val:' , (self.adaptive_constant_ics_val))\n",
    "                self.print('adaptive_constant_bcs_val: ' , ( self.adaptive_constant_bcs_val))\n",
    "                \n",
    "                self.adaptive_constant_res_log.append(adaptive_constant_res_val)\n",
    "                self.adaptive_constant_bcs_log.append(adaptive_constant_bcs_val)\n",
    "                self.adaptive_constant_ics_log.append(adaptive_constant_ics_val)\n",
    "\n",
    "                max_grad_res , mean_grad_bcs, mean_grad_ics = self.sess.run( [ self.max_grad_res , self.mean_grad_bcs, self.mean_grad_ics  ], tf_dict)\n",
    "\n",
    "                self.mean_adaptive_constant_res_log.append( max_grad_res)\n",
    "                self.mean_adaptive_constant_bcs_log.append( mean_grad_bcs)\n",
    "                self.mean_adaptive_constant_ics_log.append( mean_grad_ics)\n",
    "\n",
    "                self.print('max_grad_res: {:.3e}'.format( max_grad_res))\n",
    "                self.print('mean_grad_ics: {:.3e}'.format( mean_grad_ics))\n",
    "                self.print('mean_grad_bcs: {:.3e}'.format( mean_grad_bcs))\n",
    "                \n",
    "                sys.stdout.flush()\n",
    "                start_time = timeit.default_timer()\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            sys.stdout.flush()\n",
    " \n",
    "\n",
    "    def train(self, nIter , bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        X_ics_batch, u_ics_batch = self.fetch_minibatch(self.ics_sampler, bcbatch_size)\n",
    "        X_bc1_batch, _ = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        \n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.t_ics_tf: X_ics_batch[:, 0:1],\n",
    "                    self.x_ics_tf: X_ics_batch[:, 1:2],\n",
    "                    self.u_ics_tf: u_ics_batch,\n",
    "                    self.t_bc1_tf: X_bc1_batch[:, 0:1],\n",
    "                    self.x_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.t_bc2_tf: X_bc2_batch[:, 0:1], \n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.t_r_tf: X_res_batch[:, 0:1], \n",
    "                    self.x_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val,\n",
    "                    self.adaptive_constant_ics_tf: self.adaptive_constant_ics_val\n",
    "                    }#self.lam_r_val}\n",
    "        \n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 1000 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_bcs_value = self.sess.run(self.loss_bcs, tf_dict)\n",
    "                loss_ics_ut_value = self.sess.run(self.loss_ics_u_t, tf_dict)\n",
    "                loss_res_value = self.sess.run(self.loss_res, tf_dict)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_res: %.3e,  Loss_bcs: %.3e, Loss_ut_ics: %.3e,, Time: %.2f' %(it, loss_value, loss_res_value, loss_bcs_value, loss_ics_ut_value, elapsed))\n",
    "                \n",
    "                # Compute and Print adaptive weights during training\n",
    "                    # Compute the adaptive constant\n",
    "                adaptive_constant_bcs_val, adaptive_constant_ics_val = self.sess.run( [self.adaptive_constant_bcs, self.adaptive_constant_ics  ], tf_dict)\n",
    "                # Print adaptive weights during training\n",
    "                self.adaptive_constant_ics_val = adaptive_constant_ics_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_ics_val\n",
    "                self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * ( 1.0 - self.rate) + self.rate * self.adaptive_constant_bcs_val\n",
    "\n",
    "\n",
    "                print('lambda_u: {:.3e}'.format(self.adaptive_constant_bcs_val))\n",
    "                print('lambda_ut: {:.3e}'.format(self.adaptive_constant_ics_val))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                         \n",
    "    # Evaluates predictions at test points\n",
    "    def predict_u(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_u_tf: X_star[:, 0:1], self.x_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        return u_star\n",
    "\n",
    "        # Evaluates predictions at test points\n",
    "\n",
    "    def predict_r(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.t_r_tf: X_star[:, 0:1], self.x_r_tf: X_star[:, 1:2]}\n",
    "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
    "        return r_star\n",
    "    \n",
    "   ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   \n",
    "     ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1  , grad_ics  = self.sess.run([ self.grad_res[i],self.grad_bcs[i],self.grad_ics[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "            self.dict_gradients_ics_layers['layer_' + str(i + 1)].append(grad_ics.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_ics = self.dict_gradients_ics_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "            sns.distplot(gradients_ics, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{ics}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=3)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        #plt.show()\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "  \n",
    "            \n",
    "    def plt_prediction(self , t , x , X_star , u_star , u_pred , r_star , r_pred):\n",
    "        \n",
    "        U_star = griddata(X_star, u_star.flatten(), (t, x), method='cubic')\n",
    "        r_star = griddata(X_star, r_star.flatten(), (t, x), method='cubic')\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (t, x), method='cubic')\n",
    "        R_pred = griddata(X_star, r_pred.flatten(), (t, x), method='cubic')\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(18, 9))\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.pcolor(t, x, U_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$x_1$')\n",
    "        plt.ylabel('$x_2$')\n",
    "        plt.title('Exact u(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.pcolor(t, x, U_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Predicted u(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.pcolor(t, x, np.abs(U_star - U_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Absolute error')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.pcolor(t, x, r_star, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Exact r(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.pcolor(t, x, R_pred, cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Predicted r(t, x)')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.pcolor(t, x, np.abs(r_star - R_pred), cmap='jet')\n",
    "        plt.colorbar()\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$x$')\n",
    "        plt.title('Absolute error')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def plot_lambda(self ):\n",
    "\n",
    "        fontsize = 17\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([16,8])\n",
    "        ax.semilogy(self.mean_adaptive_constant_bcs_log, label=r'$\\bar{\\nabla_\\theta {u_{bc}}}$' , color = 'tab:green')\n",
    "        ax.semilogy(self.mean_adaptive_constant_ics_log, label=r'$\\bar{\\nabla_\\theta {u_{ics}}}$' , color = 'tab:blue')\n",
    "        ax.semilogy(self.mean_adaptive_constant_res_log, label=r'$Max{\\nabla_\\theta {u_{phy}}}$' , color = 'tab:red')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
    "        ax.tick_params(labelsize=fontsize)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
    "\n",
    "        ax2 = ax.twinx() \n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # fig.set_size_inches([15,8])\n",
    "    \n",
    "        ax2.semilogy(self.adaptive_constant_bcs_log, label=r'$\\bar{\\lambda_{bc}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
    "        ax2.semilogy(self.adaptive_constant_ics_log, label=r'$\\bar{\\lambda_{ics}}$' , linestyle='dashed'  , color = 'tab:blue')\n",
    "        ax.semilogy(self.adaptive_constant_res_log, label=r'$Max{\\lambda_{phy}}$' ,  linestyle='dashed' , color = 'tab:red')\n",
    "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
    "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
    "        ax2.tick_params(labelsize=fontsize)\n",
    "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   #  \n",
    "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
    "def test_method(method , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "        # sess.run(init)\n",
    "\n",
    "        model = PINN(layers, operator, ics_sampler, bcs_sampler, res_sampler, c, kernel_size , sess)\n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "\n",
    "        if method ==\"full_batch\":\n",
    "            print(\"full_batch method is used\")\n",
    "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "        elif method ==\"mini_batch\":\n",
    "            print(\"mini_batch method is used\")\n",
    "            model.trainmb(nIter, mbbatch_size)\n",
    "        else:\n",
    "            print(\"unknown method!\")\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # Predictions\n",
    "        u_pred = model.predict_u(X_star)\n",
    "        r_pred = model.predict_r(X_star)\n",
    "        # Predictions\n",
    "\n",
    "        sess.close()   \n",
    "\n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "\n",
    "    print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "\n",
    "\n",
    "    return [elapsed, error_u  ]\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28471/181881221.py:65: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28471/181881221.py:66: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28471/181881221.py:67: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28471/181881221.py:67: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28471/3833956971.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 03:06:47.333688: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-24 03:06:47.361179: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-24 03:06:47.361675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f3b8ed80e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-24 03:06:47.361688: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-24 03:06:47.362142: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_28471/3833956971.py:174: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28471/3833956971.py:176: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_28471/3833956971.py:230: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "mini_batch method is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 100, Loss: 4.275e-01, Loss_res: 5.690e-04,  Loss_bcs: 4.019e-01, Loss_ut_ics: 2.501e-02,, Time: 12.61\n",
      "update_loss_res: 2.2325e-02\n",
      "update_loss_ics_u_t:3.8340e-02\n",
      "update_loss_bcs: 2.9393e+00\n",
      "adaptive_constant_res_val: 6.6545e+02\n",
      "adaptive_constant_ics_val:1.5721e+00\n",
      "adaptive_constant_bcs_val: 9.4592e+01\n",
      "max_grad_res: 3.355e-05\n",
      "mean_grad_ics: 1.376e-03\n",
      "mean_grad_bcs: 1.080e-03\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 200, Loss: 3.771e+01, Loss_res: 4.322e-05,  Loss_bcs: 3.929e-01, Loss_ut_ics: 3.311e-01,, Time: 6.31\n",
      "update_loss_res: 2.9793e-02\n",
      "update_loss_ics_u_t:1.5766e+00\n",
      "update_loss_bcs: 1.3936e+00\n",
      "adaptive_constant_res_val: 4.6576e+03\n",
      "adaptive_constant_ics_val:9.8248e+02\n",
      "adaptive_constant_bcs_val: 3.8366e+01\n",
      "max_grad_res: 6.397e-06\n",
      "mean_grad_ics: 3.986e-03\n",
      "mean_grad_bcs: 1.761e-04\n",
      "It: 300, Loss: 1.901e+01, Loss_res: 1.501e-06,  Loss_bcs: 4.931e-01, Loss_ut_ics: 9.094e-05,, Time: 3.17\n",
      "update_loss_res: 1.7266e-02\n",
      "update_loss_ics_u_t:2.6157e-02\n",
      "update_loss_bcs: 2.9566e+00\n",
      "adaptive_constant_res_val: 4.0781e+04\n",
      "adaptive_constant_ics_val:1.2189e+00\n",
      "adaptive_constant_bcs_val: 3.2658e+03\n",
      "max_grad_res: 4.234e-07\n",
      "mean_grad_ics: 1.973e-05\n",
      "mean_grad_bcs: 4.677e-04\n",
      "It: 400, Loss: 1.158e+03, Loss_res: 2.474e-05,  Loss_bcs: 3.541e-01, Loss_ut_ics: 2.626e-01,, Time: 3.17\n",
      "update_loss_res: 3.5386e-02\n",
      "update_loss_ics_u_t:1.6898e+00\n",
      "update_loss_bcs: 1.2748e+00\n",
      "adaptive_constant_res_val: 2.9414e+04\n",
      "adaptive_constant_ics_val:1.1517e+03\n",
      "adaptive_constant_bcs_val: 4.9109e+01\n",
      "max_grad_res: 1.203e-06\n",
      "mean_grad_ics: 8.199e-04\n",
      "mean_grad_bcs: 4.634e-05\n",
      "It: 500, Loss: 2.244e+01, Loss_res: 1.168e-07,  Loss_bcs: 4.546e-01, Loss_ut_ics: 9.667e-05,, Time: 3.62\n",
      "update_loss_res: 1.7003e-02\n",
      "update_loss_ics_u_t:1.7910e-02\n",
      "update_loss_bcs: 2.9651e+00\n",
      "adaptive_constant_res_val: 4.9937e+05\n",
      "adaptive_constant_ics_val:5.6156e+00\n",
      "adaptive_constant_bcs_val: 2.0525e+04\n",
      "max_grad_res: 3.405e-08\n",
      "mean_grad_ics: 1.068e-05\n",
      "mean_grad_bcs: 2.357e-04\n",
      "It: 600, Loss: 8.319e+03, Loss_res: 1.135e-06,  Loss_bcs: 4.052e-01, Loss_ut_ics: 3.943e-01,, Time: 3.59\n",
      "update_loss_res: 3.7369e-02\n",
      "update_loss_ics_u_t:1.5600e+00\n",
      "update_loss_bcs: 1.4026e+00\n",
      "adaptive_constant_res_val: 5.3110e+05\n",
      "adaptive_constant_ics_val:1.5641e+04\n",
      "adaptive_constant_bcs_val: 1.1383e+03\n",
      "max_grad_res: 7.036e-08\n",
      "mean_grad_ics: 7.055e-04\n",
      "mean_grad_bcs: 5.710e-05\n",
      "It: 700, Loss: 5.978e+02, Loss_res: 8.768e-09,  Loss_bcs: 5.220e-01, Loss_ut_ics: 2.364e-04,, Time: 3.75\n",
      "update_loss_res: 2.5044e-02\n",
      "update_loss_ics_u_t:2.6236e-02\n",
      "update_loss_bcs: 2.9487e+00\n",
      "adaptive_constant_res_val: 5.4139e+06\n",
      "adaptive_constant_ics_val:3.2669e+01\n",
      "adaptive_constant_bcs_val: 5.1116e+04\n",
      "max_grad_res: 4.626e-09\n",
      "mean_grad_ics: 5.760e-06\n",
      "mean_grad_bcs: 8.019e-05\n",
      "It: 800, Loss: 2.018e+04, Loss_res: 9.054e-06,  Loss_bcs: 3.936e-01, Loss_ut_ics: 4.221e-01,, Time: 3.60\n",
      "update_loss_res: 4.1266e-02\n",
      "update_loss_ics_u_t:1.4043e+00\n",
      "update_loss_bcs: 1.5544e+00\n",
      "adaptive_constant_res_val: 2.4696e+05\n",
      "adaptive_constant_ics_val:5.2676e+03\n",
      "adaptive_constant_bcs_val: 7.4550e+01\n",
      "max_grad_res: 1.671e-07\n",
      "mean_grad_ics: 6.268e-04\n",
      "mean_grad_bcs: 8.014e-06\n",
      "It: 900, Loss: 4.101e+01, Loss_res: 6.969e-07,  Loss_bcs: 5.473e-01, Loss_ut_ics: 5.725e-06,, Time: 3.32\n",
      "update_loss_res: 2.1755e-02\n",
      "update_loss_ics_u_t:2.4547e-02\n",
      "update_loss_bcs: 2.9537e+00\n",
      "adaptive_constant_res_val: 1.7623e+06\n",
      "adaptive_constant_ics_val:1.4305e+00\n",
      "adaptive_constant_bcs_val: 1.8224e+04\n",
      "max_grad_res: 1.234e-08\n",
      "mean_grad_ics: 7.194e-07\n",
      "mean_grad_bcs: 7.617e-05\n",
      "It: 1000, Loss: 6.411e+03, Loss_res: 1.696e-06,  Loss_bcs: 3.516e-01, Loss_ut_ics: 4.008e-01,, Time: 3.33\n",
      "update_loss_res: 4.0302e-02\n",
      "update_loss_ics_u_t:1.4287e+00\n",
      "update_loss_bcs: 1.5310e+00\n",
      "adaptive_constant_res_val: 5.2496e+05\n",
      "adaptive_constant_ics_val:1.1331e+04\n",
      "adaptive_constant_bcs_val: 4.5469e+02\n",
      "max_grad_res: 7.677e-08\n",
      "mean_grad_ics: 6.089e-04\n",
      "mean_grad_bcs: 2.280e-05\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1100, Loss: 2.335e+02, Loss_res: 4.110e-07,  Loss_bcs: 5.109e-01, Loss_ut_ics: 8.885e-05,, Time: 6.34\n",
      "update_loss_res: 2.1917e-02\n",
      "update_loss_ics_u_t:2.2963e-02\n",
      "update_loss_bcs: 2.9551e+00\n",
      "adaptive_constant_res_val: 2.3386e+06\n",
      "adaptive_constant_ics_val:7.6867e+00\n",
      "adaptive_constant_bcs_val: 2.5702e+04\n",
      "max_grad_res: 9.372e-09\n",
      "mean_grad_ics: 3.137e-06\n",
      "mean_grad_bcs: 8.151e-05\n",
      "It: 1200, Loss: 7.517e+03, Loss_res: 9.554e-07,  Loss_bcs: 2.922e-01, Loss_ut_ics: 3.842e-01,, Time: 3.21\n",
      "update_loss_res: 4.1138e-02\n",
      "update_loss_ics_u_t:1.4936e+00\n",
      "update_loss_bcs: 1.4653e+00\n",
      "adaptive_constant_res_val: 1.6834e+06\n",
      "adaptive_constant_ics_val:3.7908e+04\n",
      "adaptive_constant_bcs_val: 2.9463e+03\n",
      "max_grad_res: 2.444e-08\n",
      "mean_grad_ics: 6.202e-04\n",
      "mean_grad_bcs: 4.914e-05\n",
      "It: 1300, Loss: 1.385e+03, Loss_res: 1.338e-07,  Loss_bcs: 4.684e-01, Loss_ut_ics: 1.254e-04,, Time: 3.93\n",
      "update_loss_res: 2.1475e-02\n",
      "update_loss_ics_u_t:2.1799e-02\n",
      "update_loss_bcs: 2.9567e+00\n",
      "adaptive_constant_res_val: 1.0509e+06\n",
      "adaptive_constant_ics_val:7.7436e+00\n",
      "adaptive_constant_bcs_val: 2.0047e+04\n",
      "max_grad_res: 2.043e-08\n",
      "mean_grad_ics: 7.259e-06\n",
      "mean_grad_bcs: 1.385e-04\n",
      "It: 1400, Loss: 5.777e+03, Loss_res: 1.305e-05,  Loss_bcs: 2.874e-01, Loss_ut_ics: 3.387e-01,, Time: 3.53\n",
      "update_loss_res: 3.5971e-02\n",
      "update_loss_ics_u_t:1.6889e+00\n",
      "update_loss_bcs: 1.2752e+00\n",
      "adaptive_constant_res_val: 1.8534e+05\n",
      "adaptive_constant_ics_val:4.7900e+03\n",
      "adaptive_constant_bcs_val: 1.3506e+02\n",
      "max_grad_res: 1.941e-07\n",
      "mean_grad_ics: 5.505e-04\n",
      "mean_grad_bcs: 2.056e-05\n",
      "It: 1500, Loss: 6.401e+01, Loss_res: 1.037e-06,  Loss_bcs: 4.707e-01, Loss_ut_ics: 4.915e-05,, Time: 3.76\n",
      "update_loss_res: 2.2807e-02\n",
      "update_loss_ics_u_t:2.4471e-02\n",
      "update_loss_bcs: 2.9527e+00\n",
      "adaptive_constant_res_val: 2.0059e+06\n",
      "adaptive_constant_ics_val:3.0202e+00\n",
      "adaptive_constant_bcs_val: 1.5346e+04\n",
      "max_grad_res: 1.137e-08\n",
      "mean_grad_ics: 1.403e-06\n",
      "mean_grad_bcs: 5.909e-05\n",
      "It: 1600, Loss: 4.693e+03, Loss_res: 1.925e-07,  Loss_bcs: 3.057e-01, Loss_ut_ics: 3.531e-01,, Time: 4.04\n",
      "update_loss_res: 4.4846e-02\n",
      "update_loss_ics_u_t:1.3635e+00\n",
      "update_loss_bcs: 1.5916e+00\n",
      "adaptive_constant_res_val: 4.1922e+06\n",
      "adaptive_constant_ics_val:6.9604e+04\n",
      "adaptive_constant_bcs_val: 5.2965e+03\n",
      "max_grad_res: 1.070e-08\n",
      "mean_grad_ics: 5.461e-04\n",
      "mean_grad_bcs: 3.560e-05\n",
      "It: 1700, Loss: 2.234e+03, Loss_res: 1.953e-08,  Loss_bcs: 4.203e-01, Loss_ut_ics: 1.201e-04,, Time: 3.15\n",
      "update_loss_res: 2.2237e-02\n",
      "update_loss_ics_u_t:2.2474e-02\n",
      "update_loss_bcs: 2.9553e+00\n",
      "adaptive_constant_res_val: 4.5986e+06\n",
      "adaptive_constant_ics_val:3.1149e+01\n",
      "adaptive_constant_bcs_val: 6.8273e+04\n",
      "max_grad_res: 4.836e-09\n",
      "mean_grad_ics: 6.702e-06\n",
      "mean_grad_bcs: 1.117e-04\n",
      "It: 1800, Loss: 3.153e+04, Loss_res: 1.291e-06,  Loss_bcs: 4.616e-01, Loss_ut_ics: 4.197e-01,, Time: 3.12\n",
      "update_loss_res: 3.7786e-02\n",
      "update_loss_ics_u_t:1.5529e+00\n",
      "update_loss_bcs: 1.4093e+00\n",
      "adaptive_constant_res_val: 9.1193e+05\n",
      "adaptive_constant_ics_val:2.0611e+04\n",
      "adaptive_constant_bcs_val: 4.1053e+02\n",
      "max_grad_res: 4.143e-08\n",
      "mean_grad_ics: 5.499e-04\n",
      "mean_grad_bcs: 1.207e-05\n",
      "It: 1900, Loss: 2.467e+02, Loss_res: 2.222e-07,  Loss_bcs: 5.998e-01, Loss_ut_ics: 1.371e-05,, Time: 3.14\n",
      "update_loss_res: 2.2784e-02\n",
      "update_loss_ics_u_t:2.4254e-02\n",
      "update_loss_bcs: 2.9530e+00\n",
      "adaptive_constant_res_val: 7.8717e+06\n",
      "adaptive_constant_ics_val:3.7770e+00\n",
      "adaptive_constant_bcs_val: 3.8150e+04\n",
      "max_grad_res: 2.894e-09\n",
      "mean_grad_ics: 4.507e-07\n",
      "mean_grad_bcs: 3.739e-05\n",
      "It: 2000, Loss: 1.420e+04, Loss_res: 2.696e-06,  Loss_bcs: 3.717e-01, Loss_ut_ics: 3.371e-01,, Time: 3.10\n",
      "update_loss_res: 4.3174e-02\n",
      "update_loss_ics_u_t:1.3430e+00\n",
      "update_loss_bcs: 1.6138e+00\n",
      "adaptive_constant_res_val: 1.1010e+06\n",
      "adaptive_constant_ics_val:1.7171e+04\n",
      "adaptive_constant_bcs_val: 2.0071e+03\n",
      "max_grad_res: 3.921e-08\n",
      "mean_grad_ics: 5.014e-04\n",
      "mean_grad_bcs: 4.877e-05\n",
      "It: 2100, Loss: 7.321e+02, Loss_res: 9.470e-07,  Loss_bcs: 3.581e-01, Loss_ut_ics: 7.116e-04,, Time: 3.10\n",
      "update_loss_res: 2.3838e-02\n",
      "update_loss_ics_u_t:2.5050e-02\n",
      "update_loss_bcs: 2.9511e+00\n",
      "adaptive_constant_res_val: 2.9320e+06\n",
      "adaptive_constant_ics_val:9.8658e+00\n",
      "adaptive_constant_bcs_val: 9.1747e+03\n",
      "max_grad_res: 8.130e-09\n",
      "mean_grad_ics: 3.202e-06\n",
      "mean_grad_bcs: 2.528e-05\n",
      "It: 2200, Loss: 3.096e+03, Loss_res: 8.598e-07,  Loss_bcs: 3.367e-01, Loss_ut_ics: 3.956e-01,, Time: 3.09\n",
      "update_loss_res: 5.4232e-02\n",
      "update_loss_ics_u_t:8.1919e-01\n",
      "update_loss_bcs: 2.1266e+00\n",
      "adaptive_constant_res_val: 1.8924e+06\n",
      "adaptive_constant_ics_val:1.4704e+04\n",
      "adaptive_constant_bcs_val: 1.4142e+03\n",
      "max_grad_res: 2.866e-08\n",
      "mean_grad_ics: 5.144e-04\n",
      "mean_grad_bcs: 1.906e-05\n",
      "It: 2300, Loss: 6.585e+02, Loss_res: 5.583e-07,  Loss_bcs: 4.611e-01, Loss_ut_ics: 3.613e-04,, Time: 3.13\n",
      "update_loss_res: 2.3598e-02\n",
      "update_loss_ics_u_t:2.4458e-02\n",
      "update_loss_bcs: 2.9519e+00\n",
      "adaptive_constant_res_val: 5.2099e+06\n",
      "adaptive_constant_ics_val:1.5946e+01\n",
      "adaptive_constant_bcs_val: 2.2313e+04\n",
      "max_grad_res: 4.530e-09\n",
      "mean_grad_ics: 2.953e-06\n",
      "mean_grad_bcs: 3.424e-05\n",
      "It: 2400, Loss: 6.433e+03, Loss_res: 4.512e-07,  Loss_bcs: 2.879e-01, Loss_ut_ics: 3.640e-01,, Time: 3.21\n",
      "update_loss_res: 4.6444e-02\n",
      "update_loss_ics_u_t:1.2338e+00\n",
      "update_loss_bcs: 1.7198e+00\n",
      "adaptive_constant_res_val: 1.5661e+06\n",
      "adaptive_constant_ics_val:2.0370e+04\n",
      "adaptive_constant_bcs_val: 7.5267e+02\n",
      "max_grad_res: 2.966e-08\n",
      "mean_grad_ics: 4.896e-04\n",
      "mean_grad_bcs: 1.298e-05\n",
      "It: 2500, Loss: 4.550e+02, Loss_res: 3.957e-07,  Loss_bcs: 6.016e-01, Loss_ut_ics: 7.807e-05,, Time: 3.53\n",
      "update_loss_res: 1.8581e-02\n",
      "update_loss_ics_u_t:1.9092e-02\n",
      "update_loss_bcs: 2.9623e+00\n",
      "adaptive_constant_res_val: 6.7469e+06\n",
      "adaptive_constant_ics_val:8.2896e+00\n",
      "adaptive_constant_bcs_val: 5.0611e+04\n",
      "max_grad_res: 2.754e-09\n",
      "mean_grad_ics: 1.196e-06\n",
      "mean_grad_bcs: 4.705e-05\n",
      "It: 2600, Loss: 1.869e+04, Loss_res: 1.007e-06,  Loss_bcs: 3.692e-01, Loss_ut_ics: 4.597e-01,, Time: 3.21\n",
      "update_loss_res: 4.3410e-02\n",
      "update_loss_ics_u_t:1.3648e+00\n",
      "update_loss_bcs: 1.5918e+00\n",
      "adaptive_constant_res_val: 3.5442e+06\n",
      "adaptive_constant_ics_val:7.3236e+04\n",
      "adaptive_constant_bcs_val: 2.4965e+03\n",
      "max_grad_res: 1.225e-08\n",
      "mean_grad_ics: 6.572e-04\n",
      "mean_grad_bcs: 1.921e-05\n",
      "It: 2700, Loss: 1.051e+03, Loss_res: 6.158e-08,  Loss_bcs: 4.191e-01, Loss_ut_ics: 5.447e-05,, Time: 3.26\n",
      "update_loss_res: 2.3149e-02\n",
      "update_loss_ics_u_t:2.3469e-02\n",
      "update_loss_bcs: 2.9534e+00\n",
      "adaptive_constant_res_val: 1.4893e+07\n",
      "adaptive_constant_ics_val:1.2854e+01\n",
      "adaptive_constant_bcs_val: 3.8105e+04\n",
      "max_grad_res: 1.554e-09\n",
      "mean_grad_ics: 8.513e-07\n",
      "mean_grad_bcs: 2.005e-05\n",
      "It: 2800, Loss: 1.237e+04, Loss_res: 9.622e-07,  Loss_bcs: 3.240e-01, Loss_ut_ics: 4.507e-01,, Time: 3.27\n",
      "update_loss_res: 4.8332e-02\n",
      "update_loss_ics_u_t:5.2592e-01\n",
      "update_loss_bcs: 2.4257e+00\n",
      "adaptive_constant_res_val: 1.1685e+06\n",
      "adaptive_constant_ics_val:6.8971e+03\n",
      "adaptive_constant_bcs_val: 8.0076e+02\n",
      "max_grad_res: 4.136e-08\n",
      "mean_grad_ics: 5.425e-04\n",
      "mean_grad_bcs: 1.365e-05\n",
      "It: 2900, Loss: 4.499e+02, Loss_res: 3.391e-07,  Loss_bcs: 5.569e-01, Loss_ut_ics: 5.247e-04,, Time: 3.12\n",
      "update_loss_res: 2.2896e-02\n",
      "update_loss_ics_u_t:2.6336e-02\n",
      "update_loss_bcs: 2.9508e+00\n",
      "adaptive_constant_res_val: 4.5128e+06\n",
      "adaptive_constant_ics_val:2.0903e+01\n",
      "adaptive_constant_bcs_val: 2.0361e+04\n",
      "max_grad_res: 5.073e-09\n",
      "mean_grad_ics: 4.027e-06\n",
      "mean_grad_bcs: 3.501e-05\n",
      "It: 3000, Loss: 6.102e+03, Loss_res: 9.822e-07,  Loss_bcs: 2.991e-01, Loss_ut_ics: 3.851e-01,, Time: 3.24\n",
      "update_loss_res: 4.7258e-02\n",
      "update_loss_ics_u_t:9.6049e-01\n",
      "update_loss_bcs: 1.9922e+00\n",
      "adaptive_constant_res_val: 1.2764e+06\n",
      "adaptive_constant_ics_val:1.0903e+04\n",
      "adaptive_constant_bcs_val: 1.4261e+03\n",
      "max_grad_res: 3.702e-08\n",
      "mean_grad_ics: 4.203e-04\n",
      "mean_grad_bcs: 2.650e-05\n",
      "It: 3100, Loss: 7.363e+02, Loss_res: 6.504e-07,  Loss_bcs: 5.124e-01, Loss_ut_ics: 4.291e-04,, Time: 3.42\n",
      "update_loss_res: 2.3699e-02\n",
      "update_loss_ics_u_t:2.6222e-02\n",
      "update_loss_bcs: 2.9501e+00\n",
      "adaptive_constant_res_val: 3.8024e+06\n",
      "adaptive_constant_ics_val:7.7698e+00\n",
      "adaptive_constant_bcs_val: 1.0248e+04\n",
      "max_grad_res: 6.233e-09\n",
      "mean_grad_ics: 1.847e-06\n",
      "mean_grad_bcs: 2.165e-05\n",
      "It: 3200, Loss: 3.645e+03, Loss_res: 2.813e-07,  Loss_bcs: 3.553e-01, Loss_ut_ics: 3.740e-01,, Time: 3.29\n",
      "update_loss_res: 5.2083e-02\n",
      "update_loss_ics_u_t:5.0958e-01\n",
      "update_loss_bcs: 2.4383e+00\n",
      "adaptive_constant_res_val: 1.9808e+06\n",
      "adaptive_constant_ics_val:7.1202e+03\n",
      "adaptive_constant_bcs_val: 1.0543e+03\n",
      "max_grad_res: 2.629e-08\n",
      "mean_grad_ics: 3.674e-04\n",
      "mean_grad_bcs: 1.137e-05\n",
      "It: 3300, Loss: 4.349e+02, Loss_res: 1.391e-07,  Loss_bcs: 4.100e-01, Loss_ut_ics: 3.323e-04,, Time: 3.40\n",
      "update_loss_res: 2.7828e-02\n",
      "update_loss_ics_u_t:3.1692e-02\n",
      "update_loss_bcs: 2.9405e+00\n",
      "adaptive_constant_res_val: 1.1791e+07\n",
      "adaptive_constant_ics_val:1.5924e+01\n",
      "adaptive_constant_bcs_val: 2.5697e+04\n",
      "max_grad_res: 2.360e-09\n",
      "mean_grad_ics: 1.186e-06\n",
      "mean_grad_bcs: 2.062e-05\n",
      "It: 3400, Loss: 8.877e+03, Loss_res: 1.510e-06,  Loss_bcs: 3.445e-01, Loss_ut_ics: 3.969e-01,, Time: 3.71\n",
      "update_loss_res: 4.6835e-02\n",
      "update_loss_ics_u_t:6.2705e-01\n",
      "update_loss_bcs: 2.3261e+00\n",
      "adaptive_constant_res_val: 8.8889e+05\n",
      "adaptive_constant_ics_val:5.1351e+03\n",
      "adaptive_constant_bcs_val: 6.5779e+02\n",
      "max_grad_res: 5.269e-08\n",
      "mean_grad_ics: 4.315e-04\n",
      "mean_grad_bcs: 1.490e-05\n",
      "It: 3500, Loss: 2.981e+02, Loss_res: 1.027e-06,  Loss_bcs: 4.471e-01, Loss_ut_ics: 5.886e-04,, Time: 3.43\n",
      "update_loss_res: 2.4462e-02\n",
      "update_loss_ics_u_t:2.8928e-02\n",
      "update_loss_bcs: 2.9466e+00\n",
      "adaptive_constant_res_val: 3.3591e+06\n",
      "adaptive_constant_ics_val:1.2183e+01\n",
      "adaptive_constant_bcs_val: 1.3424e+04\n",
      "max_grad_res: 7.282e-09\n",
      "mean_grad_ics: 3.067e-06\n",
      "mean_grad_bcs: 3.318e-05\n",
      "It: 3600, Loss: 5.142e+03, Loss_res: 3.258e-06,  Loss_bcs: 3.818e-01, Loss_ut_ics: 4.475e-01,, Time: 4.11\n",
      "update_loss_res: 4.9309e-02\n",
      "update_loss_ics_u_t:5.9877e-01\n",
      "update_loss_bcs: 2.3519e+00\n",
      "adaptive_constant_res_val: 7.3082e+05\n",
      "adaptive_constant_ics_val:3.8689e+03\n",
      "adaptive_constant_bcs_val: 1.0023e+03\n",
      "max_grad_res: 6.747e-08\n",
      "mean_grad_ics: 4.360e-04\n",
      "mean_grad_bcs: 2.875e-05\n",
      "It: 3700, Loss: 5.227e+02, Loss_res: 1.522e-06,  Loss_bcs: 5.116e-01, Loss_ut_ics: 2.293e-03,, Time: 4.06\n",
      "update_loss_res: 3.0107e-02\n",
      "update_loss_ics_u_t:3.8252e-02\n",
      "update_loss_bcs: 2.9316e+00\n",
      "adaptive_constant_res_val: 3.7419e+06\n",
      "adaptive_constant_ics_val:3.7867e+01\n",
      "adaptive_constant_bcs_val: 1.3224e+04\n",
      "max_grad_res: 8.046e-09\n",
      "mean_grad_ics: 7.965e-06\n",
      "mean_grad_bcs: 3.629e-05\n",
      "It: 3800, Loss: 4.745e+03, Loss_res: 1.104e-07,  Loss_bcs: 3.577e-01, Loss_ut_ics: 3.871e-01,, Time: 3.97\n",
      "update_loss_res: 5.0855e-02\n",
      "update_loss_ics_u_t:7.7577e-01\n",
      "update_loss_bcs: 2.1734e+00\n",
      "adaptive_constant_res_val: 6.5637e+06\n",
      "adaptive_constant_ics_val:3.9124e+04\n",
      "adaptive_constant_bcs_val: 2.1568e+03\n",
      "max_grad_res: 7.748e-09\n",
      "mean_grad_ics: 3.907e-04\n",
      "mean_grad_bcs: 7.689e-06\n",
      "It: 3900, Loss: 1.085e+03, Loss_res: 3.863e-08,  Loss_bcs: 5.007e-01, Loss_ut_ics: 1.156e-04,, Time: 3.76\n",
      "update_loss_res: 2.2662e-02\n",
      "update_loss_ics_u_t:2.3525e-02\n",
      "update_loss_bcs: 2.9538e+00\n",
      "adaptive_constant_res_val: 2.8998e+07\n",
      "adaptive_constant_ics_val:6.1193e+01\n",
      "adaptive_constant_bcs_val: 1.4457e+05\n",
      "max_grad_res: 7.815e-10\n",
      "mean_grad_ics: 2.033e-06\n",
      "mean_grad_bcs: 3.825e-05\n",
      "It: 4000, Loss: 4.897e+04, Loss_res: 9.066e-07,  Loss_bcs: 3.384e-01, Loss_ut_ics: 4.077e-01,, Time: 3.61\n",
      "update_loss_res: 4.4657e-02\n",
      "update_loss_ics_u_t:1.3201e+00\n",
      "update_loss_bcs: 1.6352e+00\n",
      "adaptive_constant_res_val: 1.9447e+06\n",
      "adaptive_constant_ics_val:3.2924e+04\n",
      "adaptive_constant_bcs_val: 7.9581e+02\n",
      "max_grad_res: 2.296e-08\n",
      "mean_grad_ics: 5.727e-04\n",
      "mean_grad_bcs: 1.118e-05\n",
      "It: 4100, Loss: 4.529e+02, Loss_res: 3.112e-07,  Loss_bcs: 5.677e-01, Loss_ut_ics: 1.656e-05,, Time: 3.64\n",
      "update_loss_res: 2.1052e-02\n",
      "update_loss_ics_u_t:2.2261e-02\n",
      "update_loss_bcs: 2.9567e+00\n",
      "adaptive_constant_res_val: 1.2525e+07\n",
      "adaptive_constant_ics_val:1.0014e+01\n",
      "adaptive_constant_bcs_val: 6.1089e+04\n",
      "max_grad_res: 1.681e-09\n",
      "mean_grad_ics: 7.561e-07\n",
      "mean_grad_bcs: 3.473e-05\n",
      "It: 4200, Loss: 1.973e+04, Loss_res: 5.829e-07,  Loss_bcs: 3.228e-01, Loss_ut_ics: 4.242e-01,, Time: 3.71\n",
      "update_loss_res: 4.5275e-02\n",
      "update_loss_ics_u_t:1.2214e+00\n",
      "update_loss_bcs: 1.7333e+00\n",
      "adaptive_constant_res_val: 3.3009e+06\n",
      "adaptive_constant_ics_val:5.3806e+04\n",
      "adaptive_constant_bcs_val: 3.2432e+03\n",
      "max_grad_res: 1.372e-08\n",
      "mean_grad_ics: 6.042e-04\n",
      "mean_grad_bcs: 2.566e-05\n",
      "It: 4300, Loss: 1.366e+03, Loss_res: 5.256e-07,  Loss_bcs: 4.176e-01, Loss_ut_ics: 1.815e-04,, Time: 3.54\n",
      "update_loss_res: 2.3835e-02\n",
      "update_loss_ics_u_t:2.4528e-02\n",
      "update_loss_bcs: 2.9516e+00\n",
      "adaptive_constant_res_val: 4.7801e+06\n",
      "adaptive_constant_ics_val:1.2904e+01\n",
      "adaptive_constant_bcs_val: 2.2756e+04\n",
      "max_grad_res: 4.986e-09\n",
      "mean_grad_ics: 2.623e-06\n",
      "mean_grad_bcs: 3.844e-05\n",
      "It: 4400, Loss: 7.644e+03, Loss_res: 5.322e-06,  Loss_bcs: 3.346e-01, Loss_ut_ics: 3.436e-01,, Time: 3.59\n",
      "update_loss_res: 4.6925e-02\n",
      "update_loss_ics_u_t:9.6083e-01\n",
      "update_loss_bcs: 1.9922e+00\n",
      "adaptive_constant_res_val: 4.5439e+05\n",
      "adaptive_constant_ics_val:3.6790e+03\n",
      "adaptive_constant_bcs_val: 2.5949e+01\n",
      "max_grad_res: 1.033e-07\n",
      "mean_grad_ics: 3.954e-04\n",
      "mean_grad_bcs: 1.345e-06\n",
      "It: 4500, Loss: 1.370e+01, Loss_res: 7.750e-07,  Loss_bcs: 5.143e-01, Loss_ut_ics: 7.339e-07,, Time: 3.66\n",
      "update_loss_res: 2.0056e-02\n",
      "update_loss_ics_u_t:2.4358e-02\n",
      "update_loss_bcs: 2.9556e+00\n",
      "adaptive_constant_res_val: 3.5081e+06\n",
      "adaptive_constant_ics_val:5.9802e-01\n",
      "adaptive_constant_bcs_val: 3.0677e+04\n",
      "max_grad_res: 5.717e-09\n",
      "mean_grad_ics: 1.404e-07\n",
      "mean_grad_bcs: 5.934e-05\n",
      "It: 4600, Loss: 1.039e+04, Loss_res: 2.121e-06,  Loss_bcs: 3.384e-01, Loss_ut_ics: 4.210e-01,, Time: 3.58\n",
      "update_loss_res: 4.6412e-02\n",
      "update_loss_ics_u_t:1.0216e+00\n",
      "update_loss_bcs: 1.9320e+00\n",
      "adaptive_constant_res_val: 9.7678e+05\n",
      "adaptive_constant_ics_val:1.0156e+04\n",
      "adaptive_constant_bcs_val: 1.2908e+03\n",
      "max_grad_res: 4.751e-08\n",
      "mean_grad_ics: 4.724e-04\n",
      "mean_grad_bcs: 3.175e-05\n",
      "It: 4700, Loss: 6.281e+02, Loss_res: 1.098e-06,  Loss_bcs: 4.838e-01, Loss_ut_ics: 2.502e-04,, Time: 3.63\n",
      "update_loss_res: 2.4696e-02\n",
      "update_loss_ics_u_t:2.7680e-02\n",
      "update_loss_bcs: 2.9476e+00\n",
      "adaptive_constant_res_val: 3.5069e+06\n",
      "adaptive_constant_ics_val:3.8818e+00\n",
      "adaptive_constant_bcs_val: 6.9012e+03\n",
      "max_grad_res: 7.042e-09\n",
      "mean_grad_ics: 9.876e-07\n",
      "mean_grad_bcs: 1.649e-05\n",
      "It: 4800, Loss: 2.858e+03, Loss_res: 1.678e-06,  Loss_bcs: 4.132e-01, Loss_ut_ics: 2.702e-01,, Time: 3.63\n",
      "update_loss_res: 4.1243e-02\n",
      "update_loss_ics_u_t:1.3772e-01\n",
      "update_loss_bcs: 2.8210e+00\n",
      "adaptive_constant_res_val: 1.0067e+06\n",
      "adaptive_constant_ics_val:9.9681e+02\n",
      "adaptive_constant_bcs_val: 2.2700e+03\n",
      "max_grad_res: 4.097e-08\n",
      "mean_grad_ics: 2.965e-04\n",
      "mean_grad_bcs: 3.297e-05\n",
      "It: 4900, Loss: 9.383e+02, Loss_res: 2.019e-07,  Loss_bcs: 3.772e-01, Loss_ut_ics: 8.201e-02,, Time: 3.63\n",
      "update_loss_res: 5.6083e-02\n",
      "update_loss_ics_u_t:1.5400e-01\n",
      "update_loss_bcs: 2.7899e+00\n",
      "adaptive_constant_res_val: 1.3725e+07\n",
      "adaptive_constant_ics_val:4.2618e+03\n",
      "adaptive_constant_bcs_val: 3.6954e+04\n",
      "max_grad_res: 4.086e-09\n",
      "mean_grad_ics: 1.131e-04\n",
      "mean_grad_bcs: 5.412e-05\n",
      "It: 5000, Loss: 1.290e+04, Loss_res: 1.128e-06,  Loss_bcs: 3.272e-01, Loss_ut_ics: 1.851e-01,, Time: 3.58\n",
      "update_loss_res: 6.5178e-02\n",
      "update_loss_ics_u_t:5.2990e-01\n",
      "update_loss_bcs: 2.4049e+00\n",
      "adaptive_constant_res_val: 2.9756e+06\n",
      "adaptive_constant_ics_val:5.5292e+03\n",
      "adaptive_constant_bcs_val: 4.8162e+03\n",
      "max_grad_res: 2.190e-08\n",
      "mean_grad_ics: 2.286e-04\n",
      "mean_grad_bcs: 4.387e-05\n",
      "It: 5100, Loss: 2.176e+03, Loss_res: 5.369e-07,  Loss_bcs: 4.270e-01, Loss_ut_ics: 2.137e-02,, Time: 3.61\n",
      "update_loss_res: 3.7302e-02\n",
      "update_loss_ics_u_t:4.8928e-02\n",
      "update_loss_bcs: 2.9138e+00\n",
      "adaptive_constant_res_val: 1.0191e+07\n",
      "adaptive_constant_ics_val:4.6918e+02\n",
      "adaptive_constant_bcs_val: 3.3905e+04\n",
      "max_grad_res: 3.660e-09\n",
      "mean_grad_ics: 3.510e-05\n",
      "mean_grad_bcs: 4.259e-05\n",
      "It: 5200, Loss: 1.189e+04, Loss_res: 1.465e-07,  Loss_bcs: 3.454e-01, Loss_ut_ics: 3.892e-01,, Time: 3.63\n",
      "update_loss_res: 4.4796e-02\n",
      "update_loss_ics_u_t:1.2930e+00\n",
      "update_loss_bcs: 1.6622e+00\n",
      "adaptive_constant_res_val: 4.0290e+06\n",
      "adaptive_constant_ics_val:5.2641e+04\n",
      "adaptive_constant_bcs_val: 2.1202e+03\n",
      "max_grad_res: 1.112e-08\n",
      "mean_grad_ics: 4.527e-04\n",
      "mean_grad_bcs: 1.418e-05\n",
      "It: 5300, Loss: 1.164e+03, Loss_res: 1.449e-07,  Loss_bcs: 5.465e-01, Loss_ut_ics: 8.465e-05,, Time: 3.60\n",
      "update_loss_res: 2.2060e-02\n",
      "update_loss_ics_u_t:2.2632e-02\n",
      "update_loss_bcs: 2.9553e+00\n",
      "adaptive_constant_res_val: 1.2028e+07\n",
      "adaptive_constant_ics_val:2.6241e+01\n",
      "adaptive_constant_bcs_val: 9.1430e+04\n",
      "max_grad_res: 1.834e-09\n",
      "mean_grad_ics: 2.126e-06\n",
      "mean_grad_bcs: 5.674e-05\n",
      "It: 5400, Loss: 3.461e+04, Loss_res: 4.399e-06,  Loss_bcs: 3.779e-01, Loss_ut_ics: 4.299e-01,, Time: 3.58\n",
      "update_loss_res: 3.7990e-02\n",
      "update_loss_ics_u_t:1.5361e+00\n",
      "update_loss_bcs: 1.4259e+00\n",
      "adaptive_constant_res_val: 9.8345e+05\n",
      "adaptive_constant_ics_val:2.4523e+04\n",
      "adaptive_constant_bcs_val: 3.4815e+02\n",
      "max_grad_res: 3.863e-08\n",
      "mean_grad_ics: 6.167e-04\n",
      "mean_grad_bcs: 9.432e-06\n",
      "It: 5500, Loss: 1.619e+02, Loss_res: 5.969e-07,  Loss_bcs: 4.623e-01, Loss_ut_ics: 1.328e-05,, Time: 3.76\n",
      "update_loss_res: 2.1120e-02\n",
      "update_loss_ics_u_t:2.2279e-02\n",
      "update_loss_bcs: 2.9566e+00\n",
      "adaptive_constant_res_val: 5.6340e+06\n",
      "adaptive_constant_ics_val:2.3604e+00\n",
      "adaptive_constant_bcs_val: 1.9792e+04\n",
      "max_grad_res: 3.749e-09\n",
      "mean_grad_ics: 3.972e-07\n",
      "mean_grad_bcs: 2.509e-05\n",
      "It: 5600, Loss: 6.584e+03, Loss_res: 1.658e-06,  Loss_bcs: 3.322e-01, Loss_ut_ics: 3.785e-01,, Time: 4.30\n",
      "update_loss_res: 4.8044e-02\n",
      "update_loss_ics_u_t:9.5966e-01\n",
      "update_loss_bcs: 1.9923e+00\n",
      "adaptive_constant_res_val: 6.5222e+05\n",
      "adaptive_constant_ics_val:5.7582e+03\n",
      "adaptive_constant_bcs_val: 3.5066e+01\n",
      "max_grad_res: 7.366e-08\n",
      "mean_grad_ics: 4.420e-04\n",
      "mean_grad_bcs: 1.297e-06\n",
      "It: 5700, Loss: 1.470e+01, Loss_res: 1.360e-08,  Loss_bcs: 4.144e-01, Loss_ut_ics: 2.798e-05,, Time: 3.77\n",
      "update_loss_res: 2.1306e-02\n",
      "update_loss_ics_u_t:2.3965e-02\n",
      "update_loss_bcs: 2.9547e+00\n",
      "adaptive_constant_res_val: 5.9983e+07\n",
      "adaptive_constant_ics_val:2.9321e+01\n",
      "adaptive_constant_bcs_val: 1.5274e+05\n",
      "max_grad_res: 3.552e-10\n",
      "mean_grad_ics: 4.346e-07\n",
      "mean_grad_bcs: 1.836e-05\n",
      "It: 5800, Loss: 5.487e+04, Loss_res: 1.050e-06,  Loss_bcs: 3.588e-01, Loss_ut_ics: 4.020e-01,, Time: 3.78\n",
      "update_loss_res: 4.5169e-02\n",
      "update_loss_ics_u_t:1.2173e+00\n",
      "update_loss_bcs: 1.7375e+00\n",
      "adaptive_constant_res_val: 2.7538e+06\n",
      "adaptive_constant_ics_val:4.4923e+04\n",
      "adaptive_constant_bcs_val: 1.5613e+03\n",
      "max_grad_res: 1.640e-08\n",
      "mean_grad_ics: 6.053e-04\n",
      "mean_grad_bcs: 1.474e-05\n",
      "It: 5900, Loss: 7.302e+02, Loss_res: 3.813e-08,  Loss_bcs: 4.663e-01, Loss_ut_ics: 4.616e-05,, Time: 4.22\n",
      "update_loss_res: 2.2447e-02\n",
      "update_loss_ics_u_t:2.3350e-02\n",
      "update_loss_bcs: 2.9542e+00\n",
      "adaptive_constant_res_val: 5.3605e+07\n",
      "adaptive_constant_ics_val:7.9791e+01\n",
      "adaptive_constant_bcs_val: 3.0194e+05\n",
      "max_grad_res: 4.187e-10\n",
      "mean_grad_ics: 1.431e-06\n",
      "mean_grad_bcs: 4.280e-05\n",
      "It: 6000, Loss: 1.190e+05, Loss_res: 5.654e-07,  Loss_bcs: 3.939e-01, Loss_ut_ics: 4.040e-01,, Time: 3.95\n",
      "update_loss_res: 4.0615e-02\n",
      "update_loss_ics_u_t:1.4125e+00\n",
      "update_loss_bcs: 1.5469e+00\n",
      "adaptive_constant_res_val: 2.8121e+06\n",
      "adaptive_constant_ics_val:5.7499e+04\n",
      "adaptive_constant_bcs_val: 1.3829e+03\n",
      "max_grad_res: 1.444e-08\n",
      "mean_grad_ics: 5.879e-04\n",
      "mean_grad_bcs: 1.291e-05\n",
      "It: 6100, Loss: 6.280e+02, Loss_res: 3.047e-07,  Loss_bcs: 4.525e-01, Loss_ut_ics: 2.374e-05,, Time: 3.95\n",
      "update_loss_res: 2.3666e-02\n",
      "update_loss_ics_u_t:2.5040e-02\n",
      "update_loss_bcs: 2.9513e+00\n",
      "adaptive_constant_res_val: 1.6034e+07\n",
      "adaptive_constant_ics_val:9.5423e+00\n",
      "adaptive_constant_bcs_val: 5.3666e+04\n",
      "max_grad_res: 1.476e-09\n",
      "mean_grad_ics: 5.625e-07\n",
      "mean_grad_bcs: 2.684e-05\n",
      "It: 6200, Loss: 1.972e+04, Loss_res: 3.234e-07,  Loss_bcs: 3.673e-01, Loss_ut_ics: 4.167e-01,, Time: 3.72\n",
      "update_loss_res: 4.8515e-02\n",
      "update_loss_ics_u_t:9.2430e-01\n",
      "update_loss_bcs: 2.0272e+00\n",
      "adaptive_constant_res_val: 2.5041e+06\n",
      "adaptive_constant_ics_val:2.2983e+04\n",
      "adaptive_constant_bcs_val: 1.0700e+03\n",
      "max_grad_res: 1.937e-08\n",
      "mean_grad_ics: 4.818e-04\n",
      "mean_grad_bcs: 1.023e-05\n",
      "It: 6300, Loss: 5.355e+02, Loss_res: 5.795e-07,  Loss_bcs: 4.986e-01, Loss_ut_ics: 2.522e-05,, Time: 3.67\n",
      "update_loss_res: 2.3770e-02\n",
      "update_loss_ics_u_t:2.5873e-02\n",
      "update_loss_bcs: 2.9504e+00\n",
      "adaptive_constant_res_val: 8.0791e+06\n",
      "adaptive_constant_ics_val:2.9340e+00\n",
      "adaptive_constant_bcs_val: 1.7430e+04\n",
      "max_grad_res: 2.942e-09\n",
      "mean_grad_ics: 3.336e-07\n",
      "mean_grad_bcs: 1.738e-05\n",
      "It: 6400, Loss: 5.915e+03, Loss_res: 6.345e-06,  Loss_bcs: 3.364e-01, Loss_ut_ics: 2.980e-01,, Time: 3.58\n",
      "update_loss_res: 4.4339e-02\n",
      "update_loss_ics_u_t:1.4945e-01\n",
      "update_loss_bcs: 2.8062e+00\n",
      "adaptive_constant_res_val: 1.1203e+06\n",
      "adaptive_constant_ics_val:1.1590e+03\n",
      "adaptive_constant_bcs_val: 1.1255e+03\n",
      "max_grad_res: 3.958e-08\n",
      "mean_grad_ics: 3.069e-04\n",
      "mean_grad_bcs: 1.587e-05\n",
      "It: 6500, Loss: 5.230e+02, Loss_res: 5.763e-07,  Loss_bcs: 4.304e-01, Loss_ut_ics: 3.269e-02,, Time: 3.60\n",
      "update_loss_res: 5.0765e-02\n",
      "update_loss_ics_u_t:1.2243e-01\n",
      "update_loss_bcs: 2.8268e+00\n",
      "adaptive_constant_res_val: 1.5913e+07\n",
      "adaptive_constant_ics_val:1.9889e+03\n",
      "adaptive_constant_bcs_val: 4.3571e+04\n",
      "max_grad_res: 3.190e-09\n",
      "mean_grad_ics: 5.182e-05\n",
      "mean_grad_bcs: 4.917e-05\n",
      "It: 6600, Loss: 1.517e+04, Loss_res: 9.413e-07,  Loss_bcs: 3.335e-01, Loss_ut_ics: 3.133e-01,, Time: 3.52\n",
      "update_loss_res: 5.2649e-02\n",
      "update_loss_ics_u_t:9.7728e-01\n",
      "update_loss_bcs: 1.9701e+00\n",
      "adaptive_constant_res_val: 2.0464e+06\n",
      "adaptive_constant_ics_val:1.3061e+04\n",
      "adaptive_constant_bcs_val: 7.7943e+02\n",
      "max_grad_res: 2.573e-08\n",
      "mean_grad_ics: 3.438e-04\n",
      "mean_grad_bcs: 1.018e-05\n",
      "It: 6700, Loss: 3.436e+02, Loss_res: 2.545e-07,  Loss_bcs: 4.372e-01, Loss_ut_ics: 1.758e-04,, Time: 3.59\n",
      "update_loss_res: 2.6063e-02\n",
      "update_loss_ics_u_t:2.9001e-02\n",
      "update_loss_bcs: 2.9449e+00\n",
      "adaptive_constant_res_val: 1.3013e+07\n",
      "adaptive_constant_ics_val:1.0424e+01\n",
      "adaptive_constant_bcs_val: 2.1849e+04\n",
      "max_grad_res: 2.003e-09\n",
      "mean_grad_ics: 7.198e-07\n",
      "mean_grad_bcs: 1.486e-05\n",
      "It: 6800, Loss: 6.905e+03, Loss_res: 9.713e-07,  Loss_bcs: 3.153e-01, Loss_ut_ics: 3.679e-01,, Time: 3.63\n",
      "update_loss_res: 4.7095e-02\n",
      "update_loss_ics_u_t:2.1143e-01\n",
      "update_loss_bcs: 2.7415e+00\n",
      "adaptive_constant_res_val: 1.5176e+06\n",
      "adaptive_constant_ics_val:2.5347e+03\n",
      "adaptive_constant_bcs_val: 8.2933e+02\n",
      "max_grad_res: 3.103e-08\n",
      "mean_grad_ics: 3.720e-04\n",
      "mean_grad_bcs: 9.388e-06\n",
      "It: 6900, Loss: 3.871e+02, Loss_res: 7.826e-07,  Loss_bcs: 4.446e-01, Loss_ut_ics: 6.767e-03,, Time: 3.59\n",
      "update_loss_res: 4.1612e-02\n",
      "update_loss_ics_u_t:6.8882e-02\n",
      "update_loss_bcs: 2.8895e+00\n",
      "adaptive_constant_res_val: 1.7401e+07\n",
      "adaptive_constant_ics_val:4.4762e+02\n",
      "adaptive_constant_bcs_val: 4.8775e+04\n",
      "max_grad_res: 2.391e-09\n",
      "mean_grad_ics: 1.554e-05\n",
      "mean_grad_bcs: 4.037e-05\n",
      "It: 7000, Loss: 1.703e+04, Loss_res: 5.079e-07,  Loss_bcs: 3.455e-01, Loss_ut_ics: 3.731e-01,, Time: 3.62\n",
      "update_loss_res: 4.7680e-02\n",
      "update_loss_ics_u_t:1.2096e+00\n",
      "update_loss_bcs: 1.7427e+00\n",
      "adaptive_constant_res_val: 4.8967e+06\n",
      "adaptive_constant_ics_val:5.0635e+04\n",
      "adaptive_constant_bcs_val: 2.7189e+03\n",
      "max_grad_res: 9.737e-09\n",
      "mean_grad_ics: 4.076e-04\n",
      "mean_grad_bcs: 1.519e-05\n",
      "It: 7100, Loss: 1.234e+03, Loss_res: 2.597e-07,  Loss_bcs: 4.508e-01, Loss_ut_ics: 1.412e-04,, Time: 3.59\n",
      "update_loss_res: 2.3839e-02\n",
      "update_loss_ics_u_t:2.5006e-02\n",
      "update_loss_bcs: 2.9512e+00\n",
      "adaptive_constant_res_val: 1.5454e+07\n",
      "adaptive_constant_ics_val:1.5266e+01\n",
      "adaptive_constant_bcs_val: 3.2098e+04\n",
      "max_grad_res: 1.543e-09\n",
      "mean_grad_ics: 9.417e-07\n",
      "mean_grad_bcs: 1.678e-05\n",
      "It: 7200, Loss: 1.085e+04, Loss_res: 1.128e-06,  Loss_bcs: 3.374e-01, Loss_ut_ics: 4.367e-01,, Time: 3.61\n",
      "update_loss_res: 5.0610e-02\n",
      "update_loss_ics_u_t:6.5047e-01\n",
      "update_loss_bcs: 2.2989e+00\n",
      "adaptive_constant_res_val: 1.7827e+06\n",
      "adaptive_constant_ics_val:9.7802e+03\n",
      "adaptive_constant_bcs_val: 7.5309e+02\n",
      "max_grad_res: 2.839e-08\n",
      "mean_grad_ics: 4.269e-04\n",
      "mean_grad_bcs: 9.300e-06\n",
      "It: 7300, Loss: 4.050e+02, Loss_res: 4.629e-07,  Loss_bcs: 5.361e-01, Loss_ut_ics: 4.269e-05,, Time: 3.49\n",
      "update_loss_res: 2.2028e-02\n",
      "update_loss_ics_u_t:2.5222e-02\n",
      "update_loss_bcs: 2.9527e+00\n",
      "adaptive_constant_res_val: 6.3298e+06\n",
      "adaptive_constant_ics_val:3.3322e+00\n",
      "adaptive_constant_bcs_val: 1.9349e+04\n",
      "max_grad_res: 3.480e-09\n",
      "mean_grad_ics: 4.598e-07\n",
      "mean_grad_bcs: 2.280e-05\n",
      "It: 7400, Loss: 6.894e+03, Loss_res: 2.706e-06,  Loss_bcs: 3.553e-01, Loss_ut_ics: 4.334e-01,, Time: 3.68\n",
      "update_loss_res: 4.8190e-02\n",
      "update_loss_ics_u_t:3.1300e-01\n",
      "update_loss_bcs: 2.6388e+00\n",
      "adaptive_constant_res_val: 8.7570e+05\n",
      "adaptive_constant_ics_val:2.4095e+03\n",
      "adaptive_constant_bcs_val: 1.8798e+03\n",
      "max_grad_res: 5.503e-08\n",
      "mean_grad_ics: 4.236e-04\n",
      "mean_grad_bcs: 3.920e-05\n",
      "It: 7500, Loss: 8.563e+02, Loss_res: 2.926e-06,  Loss_bcs: 4.336e-01, Loss_ut_ics: 1.605e-02,, Time: 3.73\n",
      "update_loss_res: 4.0361e-02\n",
      "update_loss_ics_u_t:6.5570e-02\n",
      "update_loss_bcs: 2.8941e+00\n",
      "adaptive_constant_res_val: 3.2758e+06\n",
      "adaptive_constant_ics_val:1.4797e+02\n",
      "adaptive_constant_bcs_val: 9.9982e+03\n",
      "max_grad_res: 1.232e-08\n",
      "mean_grad_ics: 2.781e-05\n",
      "mean_grad_bcs: 4.257e-05\n",
      "It: 7600, Loss: 3.663e+03, Loss_res: 5.887e-08,  Loss_bcs: 3.604e-01, Loss_ut_ics: 4.017e-01,, Time: 3.54\n",
      "update_loss_res: 5.1958e-02\n",
      "update_loss_ics_u_t:8.0546e-01\n",
      "update_loss_bcs: 2.1426e+00\n",
      "adaptive_constant_res_val: 1.6086e+07\n",
      "adaptive_constant_ics_val:9.8456e+04\n",
      "adaptive_constant_bcs_val: 1.1045e+04\n",
      "max_grad_res: 3.230e-09\n",
      "mean_grad_ics: 3.948e-04\n",
      "mean_grad_bcs: 1.665e-05\n",
      "It: 7700, Loss: 4.679e+03, Loss_res: 1.286e-07,  Loss_bcs: 4.184e-01, Loss_ut_ics: 5.582e-04,, Time: 3.73\n",
      "update_loss_res: 2.4585e-02\n",
      "update_loss_ics_u_t:2.5381e-02\n",
      "update_loss_bcs: 2.9500e+00\n",
      "adaptive_constant_res_val: 2.1488e+07\n",
      "adaptive_constant_ics_val:1.3974e+02\n",
      "adaptive_constant_bcs_val: 1.1993e+05\n",
      "max_grad_res: 1.144e-09\n",
      "mean_grad_ics: 6.299e-06\n",
      "mean_grad_bcs: 4.651e-05\n",
      "It: 7800, Loss: 4.193e+04, Loss_res: 1.510e-06,  Loss_bcs: 3.488e-01, Loss_ut_ics: 4.181e-01,, Time: 3.60\n",
      "update_loss_res: 4.2732e-02\n",
      "update_loss_ics_u_t:1.3724e+00\n",
      "update_loss_bcs: 1.5848e+00\n",
      "adaptive_constant_res_val: 1.1178e+06\n",
      "adaptive_constant_ics_val:1.9610e+04\n",
      "adaptive_constant_bcs_val: 9.8384e+02\n",
      "max_grad_res: 3.823e-08\n",
      "mean_grad_ics: 5.462e-04\n",
      "mean_grad_bcs: 2.373e-05\n",
      "It: 7900, Loss: 4.541e+02, Loss_res: 1.717e-07,  Loss_bcs: 4.602e-01, Loss_ut_ics: 5.865e-05,, Time: 3.63\n",
      "update_loss_res: 2.5796e-02\n",
      "update_loss_ics_u_t:2.8443e-02\n",
      "update_loss_bcs: 2.9458e+00\n",
      "adaptive_constant_res_val: 1.8432e+07\n",
      "adaptive_constant_ics_val:1.1697e+01\n",
      "adaptive_constant_bcs_val: 4.0177e+04\n",
      "max_grad_res: 1.400e-09\n",
      "mean_grad_ics: 5.755e-07\n",
      "mean_grad_bcs: 1.909e-05\n",
      "It: 8000, Loss: 1.294e+04, Loss_res: 1.200e-06,  Loss_bcs: 3.213e-01, Loss_ut_ics: 4.132e-01,, Time: 3.55\n",
      "update_loss_res: 4.8328e-02\n",
      "update_loss_ics_u_t:6.4400e-01\n",
      "update_loss_bcs: 2.3077e+00\n",
      "adaptive_constant_res_val: 1.4159e+06\n",
      "adaptive_constant_ics_val:9.3432e+03\n",
      "adaptive_constant_bcs_val: 9.9177e+02\n",
      "max_grad_res: 3.413e-08\n",
      "mean_grad_ics: 4.952e-04\n",
      "mean_grad_bcs: 1.467e-05\n",
      "It: 8100, Loss: 3.711e+02, Loss_res: 2.888e-07,  Loss_bcs: 3.721e-01, Loss_ut_ics: 1.752e-04,, Time: 3.73\n",
      "update_loss_res: 2.3884e-02\n",
      "update_loss_ics_u_t:2.7537e-02\n",
      "update_loss_bcs: 2.9486e+00\n",
      "adaptive_constant_res_val: 9.0925e+06\n",
      "adaptive_constant_ics_val:1.6215e+01\n",
      "adaptive_constant_bcs_val: 2.6264e+04\n",
      "max_grad_res: 2.627e-09\n",
      "mean_grad_ics: 1.547e-06\n",
      "mean_grad_bcs: 2.340e-05\n",
      "It: 8200, Loss: 8.585e+03, Loss_res: 1.631e-06,  Loss_bcs: 3.261e-01, Loss_ut_ics: 3.728e-01,, Time: 3.57\n",
      "update_loss_res: 4.9779e-02\n",
      "update_loss_ics_u_t:6.3878e-01\n",
      "update_loss_bcs: 2.3114e+00\n",
      "adaptive_constant_res_val: 1.5503e+06\n",
      "adaptive_constant_ics_val:8.1389e+03\n",
      "adaptive_constant_bcs_val: 3.9872e+02\n",
      "max_grad_res: 3.211e-08\n",
      "mean_grad_ics: 4.091e-04\n",
      "mean_grad_bcs: 5.539e-06\n",
      "It: 8300, Loss: 1.721e+02, Loss_res: 1.025e-07,  Loss_bcs: 4.298e-01, Loss_ut_ics: 7.139e-05,, Time: 3.60\n",
      "update_loss_res: 2.3320e-02\n",
      "update_loss_ics_u_t:2.7234e-02\n",
      "update_loss_bcs: 2.9494e+00\n",
      "adaptive_constant_res_val: 2.1201e+07\n",
      "adaptive_constant_ics_val:2.2152e+01\n",
      "adaptive_constant_bcs_val: 7.0829e+04\n",
      "max_grad_res: 1.100e-09\n",
      "mean_grad_ics: 8.947e-07\n",
      "mean_grad_bcs: 2.641e-05\n",
      "It: 8400, Loss: 2.368e+04, Loss_res: 7.584e-08,  Loss_bcs: 3.342e-01, Loss_ut_ics: 4.206e-01,, Time: 3.77\n",
      "update_loss_res: 4.4250e-02\n",
      "update_loss_ics_u_t:1.1166e+00\n",
      "update_loss_bcs: 1.8392e+00\n",
      "adaptive_constant_res_val: 2.4295e+07\n",
      "adaptive_constant_ics_val:2.7363e+05\n",
      "adaptive_constant_bcs_val: 1.6387e+04\n",
      "max_grad_res: 1.821e-09\n",
      "mean_grad_ics: 4.463e-04\n",
      "mean_grad_bcs: 1.623e-05\n",
      "It: 8500, Loss: 8.255e+03, Loss_res: 2.188e-08,  Loss_bcs: 5.026e-01, Loss_ut_ics: 6.450e-05,, Time: 3.74\n",
      "update_loss_res: 2.4051e-02\n",
      "update_loss_ics_u_t:2.4618e-02\n",
      "update_loss_bcs: 2.9513e+00\n",
      "adaptive_constant_res_val: 3.7191e+07\n",
      "adaptive_constant_ics_val:1.0447e+02\n",
      "adaptive_constant_bcs_val: 3.7641e+05\n",
      "max_grad_res: 6.467e-10\n",
      "mean_grad_ics: 2.744e-06\n",
      "mean_grad_bcs: 8.248e-05\n",
      "It: 8600, Loss: 1.332e+05, Loss_res: 6.963e-06,  Loss_bcs: 3.530e-01, Loss_ut_ics: 4.945e-01,, Time: 4.22\n",
      "update_loss_res: 3.9630e-02\n",
      "update_loss_ics_u_t:1.5124e+00\n",
      "update_loss_bcs: 1.4480e+00\n",
      "adaptive_constant_res_val: 3.8781e+05\n",
      "adaptive_constant_ics_val:9.0058e+03\n",
      "adaptive_constant_bcs_val: 1.8910e+02\n",
      "max_grad_res: 1.022e-07\n",
      "mean_grad_ics: 6.085e-04\n",
      "mean_grad_bcs: 1.335e-05\n",
      "It: 8700, Loss: 9.652e+01, Loss_res: 3.683e-06,  Loss_bcs: 4.906e-01, Loss_ut_ics: 2.579e-04,, Time: 3.50\n",
      "update_loss_res: 3.0411e-02\n",
      "update_loss_ics_u_t:5.2988e-02\n",
      "update_loss_bcs: 2.9166e+00\n",
      "adaptive_constant_res_val: 2.0846e+06\n",
      "adaptive_constant_ics_val:1.4528e+01\n",
      "adaptive_constant_bcs_val: 1.0631e+04\n",
      "max_grad_res: 1.459e-08\n",
      "mean_grad_ics: 4.000e-06\n",
      "mean_grad_bcs: 5.318e-05\n",
      "It: 8800, Loss: 3.601e+03, Loss_res: 1.801e-06,  Loss_bcs: 3.381e-01, Loss_ut_ics: 1.932e-01,, Time: 3.27\n",
      "update_loss_res: 3.8718e-02\n",
      "update_loss_ics_u_t:6.0753e-02\n",
      "update_loss_bcs: 2.9005e+00\n",
      "adaptive_constant_res_val: 2.5206e+06\n",
      "adaptive_constant_ics_val:9.4271e+02\n",
      "adaptive_constant_bcs_val: 5.9838e+03\n",
      "max_grad_res: 1.536e-08\n",
      "mean_grad_ics: 2.384e-04\n",
      "mean_grad_bcs: 3.169e-05\n",
      "It: 8900, Loss: 2.269e+03, Loss_res: 1.465e-07,  Loss_bcs: 3.501e-01, Loss_ut_ics: 1.835e-01,, Time: 3.45\n",
      "update_loss_res: 6.3925e-02\n",
      "update_loss_ics_u_t:4.5985e-01\n",
      "update_loss_bcs: 2.4762e+00\n",
      "adaptive_constant_res_val: 1.8116e+07\n",
      "adaptive_constant_ics_val:2.9634e+04\n",
      "adaptive_constant_bcs_val: 2.3524e+04\n",
      "max_grad_res: 3.529e-09\n",
      "mean_grad_ics: 2.274e-04\n",
      "mean_grad_bcs: 3.352e-05\n",
      "It: 9000, Loss: 9.368e+03, Loss_res: 3.391e-07,  Loss_bcs: 3.750e-01, Loss_ut_ics: 1.825e-02,, Time: 3.34\n",
      "update_loss_res: 3.6972e-02\n",
      "update_loss_ics_u_t:4.6939e-02\n",
      "update_loss_bcs: 2.9161e+00\n",
      "adaptive_constant_res_val: 1.7605e+07\n",
      "adaptive_constant_ics_val:7.0917e+02\n",
      "adaptive_constant_bcs_val: 5.4751e+04\n",
      "max_grad_res: 2.100e-09\n",
      "mean_grad_ics: 3.173e-05\n",
      "mean_grad_bcs: 3.943e-05\n",
      "It: 9100, Loss: 2.000e+04, Loss_res: 8.186e-08,  Loss_bcs: 3.606e-01, Loss_ut_ics: 3.577e-01,, Time: 3.27\n",
      "update_loss_res: 5.1547e-02\n",
      "update_loss_ics_u_t:9.2122e-01\n",
      "update_loss_bcs: 2.0272e+00\n",
      "adaptive_constant_res_val: 6.5746e+06\n",
      "adaptive_constant_ics_val:4.9365e+04\n",
      "adaptive_constant_bcs_val: 3.0228e+03\n",
      "max_grad_res: 7.840e-09\n",
      "mean_grad_ics: 4.201e-04\n",
      "mean_grad_bcs: 1.169e-05\n",
      "It: 9200, Loss: 1.530e+03, Loss_res: 8.246e-08,  Loss_bcs: 5.034e-01, Loss_ut_ics: 1.498e-04,, Time: 3.68\n",
      "update_loss_res: 2.5935e-02\n",
      "update_loss_ics_u_t:2.8337e-02\n",
      "update_loss_bcs: 2.9457e+00\n",
      "adaptive_constant_res_val: 5.5027e+07\n",
      "adaptive_constant_ics_val:5.4737e+01\n",
      "adaptive_constant_bcs_val: 1.0643e+05\n",
      "max_grad_res: 4.713e-10\n",
      "mean_grad_ics: 9.104e-07\n",
      "mean_grad_bcs: 1.703e-05\n",
      "It: 9300, Loss: 4.233e+04, Loss_res: 2.383e-07,  Loss_bcs: 3.974e-01, Loss_ut_ics: 4.139e-01,, Time: 3.39\n",
      "update_loss_res: 4.3811e-02\n",
      "update_loss_ics_u_t:1.1090e+00\n",
      "update_loss_bcs: 1.8472e+00\n",
      "adaptive_constant_res_val: 3.0603e+06\n",
      "adaptive_constant_ics_val:3.2152e+04\n",
      "adaptive_constant_bcs_val: 7.6518e+02\n",
      "max_grad_res: 1.432e-08\n",
      "mean_grad_ics: 4.150e-04\n",
      "mean_grad_bcs: 5.930e-06\n",
      "It: 9400, Loss: 4.130e+02, Loss_res: 8.980e-08,  Loss_bcs: 5.392e-01, Loss_ut_ics: 4.504e-06,, Time: 3.45\n",
      "update_loss_res: 2.2429e-02\n",
      "update_loss_ics_u_t:2.4632e-02\n",
      "update_loss_bcs: 2.9529e+00\n",
      "adaptive_constant_res_val: 1.2486e+07\n",
      "adaptive_constant_ics_val:1.7761e+00\n",
      "adaptive_constant_bcs_val: 2.8380e+04\n",
      "max_grad_res: 1.796e-09\n",
      "mean_grad_ics: 1.295e-07\n",
      "mean_grad_bcs: 1.726e-05\n",
      "It: 9500, Loss: 8.620e+03, Loss_res: 5.331e-06,  Loss_bcs: 3.014e-01, Loss_ut_ics: 2.894e-01,, Time: 3.24\n",
      "update_loss_res: 4.3198e-02\n",
      "update_loss_ics_u_t:1.5198e-01\n",
      "update_loss_bcs: 2.8048e+00\n",
      "adaptive_constant_res_val: 6.3060e+05\n",
      "adaptive_constant_ics_val:6.8698e+02\n",
      "adaptive_constant_bcs_val: 6.1466e+02\n",
      "max_grad_res: 6.850e-08\n",
      "mean_grad_ics: 3.096e-04\n",
      "mean_grad_bcs: 1.501e-05\n",
      "It: 9600, Loss: 3.120e+02, Loss_res: 4.781e-06,  Loss_bcs: 4.308e-01, Loss_ut_ics: 6.434e-02,, Time: 3.41\n",
      "update_loss_res: 6.2248e-02\n",
      "update_loss_ics_u_t:2.6523e-01\n",
      "update_loss_bcs: 2.6725e+00\n",
      "adaptive_constant_res_val: 3.2395e+06\n",
      "adaptive_constant_ics_val:1.2493e+03\n",
      "adaptive_constant_bcs_val: 7.2546e+03\n",
      "max_grad_res: 1.922e-08\n",
      "mean_grad_ics: 9.051e-05\n",
      "mean_grad_bcs: 5.216e-05\n",
      "It: 9700, Loss: 2.769e+03, Loss_res: 4.465e-08,  Loss_bcs: 3.524e-01, Loss_ut_ics: 1.702e-01,, Time: 3.23\n",
      "update_loss_res: 6.5112e-02\n",
      "update_loss_ics_u_t:2.8616e-01\n",
      "update_loss_bcs: 2.6487e+00\n",
      "adaptive_constant_res_val: 2.0165e+07\n",
      "adaptive_constant_ics_val:1.7782e+04\n",
      "adaptive_constant_bcs_val: 3.1271e+04\n",
      "max_grad_res: 3.229e-09\n",
      "mean_grad_ics: 2.006e-04\n",
      "mean_grad_bcs: 3.812e-05\n",
      "It: 9800, Loss: 1.421e+04, Loss_res: 9.508e-09,  Loss_bcs: 4.219e-01, Loss_ut_ics: 5.698e-02,, Time: 3.38\n",
      "update_loss_res: 4.8810e-02\n",
      "update_loss_ics_u_t:8.7672e-02\n",
      "update_loss_bcs: 2.8635e+00\n",
      "adaptive_constant_res_val: 8.3685e+07\n",
      "adaptive_constant_ics_val:1.1894e+04\n",
      "adaptive_constant_bcs_val: 2.0539e+05\n",
      "max_grad_res: 5.833e-10\n",
      "mean_grad_ics: 7.913e-05\n",
      "mean_grad_bcs: 4.183e-05\n",
      "It: 9900, Loss: 8.156e+04, Loss_res: 1.550e-06,  Loss_bcs: 3.830e-01, Loss_ut_ics: 2.315e-01,, Time: 3.65\n",
      "update_loss_res: 5.7593e-02\n",
      "update_loss_ics_u_t:8.9326e-01\n",
      "update_loss_bcs: 2.0491e+00\n",
      "adaptive_constant_res_val: 2.7011e+06\n",
      "adaptive_constant_ics_val:1.2365e+04\n",
      "adaptive_constant_bcs_val: 4.3110e+03\n",
      "max_grad_res: 2.132e-08\n",
      "mean_grad_ics: 2.952e-04\n",
      "mean_grad_bcs: 4.486e-05\n",
      "It: 10000, Loss: 2.229e+03, Loss_res: 2.650e-07,  Loss_bcs: 5.066e-01, Loss_ut_ics: 3.616e-03,, Time: 3.44\n",
      "update_loss_res: 3.2102e-02\n",
      "update_loss_ics_u_t:4.1817e-02\n",
      "update_loss_bcs: 2.9261e+00\n",
      "adaptive_constant_res_val: 3.6862e+07\n",
      "adaptive_constant_ics_val:4.6785e+02\n",
      "adaptive_constant_bcs_val: 1.1004e+05\n",
      "max_grad_res: 8.708e-10\n",
      "mean_grad_ics: 9.743e-06\n",
      "mean_grad_bcs: 3.275e-05\n",
      "It: 10100, Loss: 3.760e+04, Loss_res: 1.125e-06,  Loss_bcs: 3.396e-01, Loss_ut_ics: 4.131e-01,, Time: 3.32\n",
      "update_loss_res: 4.4985e-02\n",
      "update_loss_ics_u_t:1.2776e+00\n",
      "update_loss_bcs: 1.6774e+00\n",
      "adaptive_constant_res_val: 2.0310e+06\n",
      "adaptive_constant_ics_val:2.5125e+04\n",
      "adaptive_constant_bcs_val: 2.6738e+02\n",
      "max_grad_res: 2.215e-08\n",
      "mean_grad_ics: 4.356e-04\n",
      "mean_grad_bcs: 3.531e-06\n",
      "It: 10200, Loss: 1.550e+02, Loss_res: 9.399e-07,  Loss_bcs: 5.725e-01, Loss_ut_ics: 1.941e-07,, Time: 3.30\n",
      "update_loss_res: 2.2926e-02\n",
      "update_loss_ics_u_t:2.5855e-02\n",
      "update_loss_bcs: 2.9512e+00\n",
      "adaptive_constant_res_val: 4.5970e+06\n",
      "adaptive_constant_ics_val:2.3423e-01\n",
      "adaptive_constant_bcs_val: 1.5777e+04\n",
      "max_grad_res: 4.987e-09\n",
      "mean_grad_ics: 4.518e-08\n",
      "mean_grad_bcs: 2.666e-05\n",
      "It: 10300, Loss: 6.251e+03, Loss_res: 8.543e-06,  Loss_bcs: 3.937e-01, Loss_ut_ics: 2.967e-01,, Time: 3.41\n",
      "update_loss_res: 3.7393e-02\n",
      "update_loss_ics_u_t:8.0509e-02\n",
      "update_loss_bcs: 2.8821e+00\n",
      "adaptive_constant_res_val: 4.3285e+05\n",
      "adaptive_constant_ics_val:2.7862e+02\n",
      "adaptive_constant_bcs_val: 8.0357e+02\n",
      "max_grad_res: 8.639e-08\n",
      "mean_grad_ics: 2.990e-04\n",
      "mean_grad_bcs: 2.409e-05\n",
      "It: 10400, Loss: 3.849e+02, Loss_res: 1.234e-06,  Loss_bcs: 4.280e-01, Loss_ut_ics: 1.450e-01,, Time: 3.82\n",
      "update_loss_res: 6.3295e-02\n",
      "update_loss_ics_u_t:4.7673e-01\n",
      "update_loss_bcs: 2.4600e+00\n",
      "adaptive_constant_res_val: 5.6281e+06\n",
      "adaptive_constant_ics_val:6.9414e+03\n",
      "adaptive_constant_bcs_val: 1.0441e+04\n",
      "max_grad_res: 1.125e-08\n",
      "mean_grad_ics: 1.638e-04\n",
      "mean_grad_bcs: 4.773e-05\n",
      "It: 10500, Loss: 5.186e+03, Loss_res: 3.683e-07,  Loss_bcs: 4.655e-01, Loss_ut_ics: 4.665e-02,, Time: 3.56\n",
      "update_loss_res: 4.4846e-02\n",
      "update_loss_ics_u_t:7.6687e-02\n",
      "update_loss_bcs: 2.8785e+00\n",
      "adaptive_constant_res_val: 1.4921e+07\n",
      "adaptive_constant_ics_val:1.5805e+03\n",
      "adaptive_constant_bcs_val: 4.8463e+04\n",
      "max_grad_res: 3.005e-09\n",
      "mean_grad_ics: 6.194e-05\n",
      "mean_grad_bcs: 5.060e-05\n",
      "It: 10600, Loss: 1.800e+04, Loss_res: 3.698e-06,  Loss_bcs: 3.614e-01, Loss_ut_ics: 2.706e-01,, Time: 3.91\n",
      "update_loss_res: 5.4834e-02\n",
      "update_loss_ics_u_t:1.0382e+00\n",
      "update_loss_bcs: 1.9070e+00\n",
      "adaptive_constant_res_val: 1.2408e+06\n",
      "adaptive_constant_ics_val:7.0101e+03\n",
      "adaptive_constant_bcs_val: 9.6653e+02\n",
      "max_grad_res: 4.419e-08\n",
      "mean_grad_ics: 2.984e-04\n",
      "mean_grad_bcs: 2.240e-05\n",
      "It: 10700, Loss: 5.090e+02, Loss_res: 1.048e-06,  Loss_bcs: 5.080e-01, Loss_ut_ics: 2.375e-03,, Time: 3.58\n",
      "update_loss_res: 3.0754e-02\n",
      "update_loss_ics_u_t:3.8676e-02\n",
      "update_loss_bcs: 2.9306e+00\n",
      "adaptive_constant_res_val: 1.0039e+07\n",
      "adaptive_constant_ics_val:6.6543e+01\n",
      "adaptive_constant_bcs_val: 2.5657e+04\n",
      "max_grad_res: 3.064e-09\n",
      "mean_grad_ics: 5.271e-06\n",
      "mean_grad_bcs: 2.682e-05\n",
      "It: 10800, Loss: 9.421e+03, Loss_res: 3.447e-07,  Loss_bcs: 3.661e-01, Loss_ut_ics: 3.927e-01,, Time: 3.87\n",
      "update_loss_res: 5.0906e-02\n",
      "update_loss_ics_u_t:5.2339e-01\n",
      "update_loss_bcs: 2.4257e+00\n",
      "adaptive_constant_res_val: 3.3377e+06\n",
      "adaptive_constant_ics_val:1.3361e+04\n",
      "adaptive_constant_bcs_val: 1.7756e+03\n",
      "max_grad_res: 1.525e-08\n",
      "mean_grad_ics: 3.894e-04\n",
      "mean_grad_bcs: 1.116e-05\n",
      "It: 10900, Loss: 8.502e+02, Loss_res: 4.508e-07,  Loss_bcs: 4.705e-01, Loss_ut_ics: 9.854e-04,, Time: 4.07\n",
      "update_loss_res: 2.9180e-02\n",
      "update_loss_ics_u_t:3.5091e-02\n",
      "update_loss_bcs: 2.9357e+00\n",
      "adaptive_constant_res_val: 1.9446e+07\n",
      "adaptive_constant_ics_val:5.3256e+01\n",
      "adaptive_constant_bcs_val: 3.2228e+04\n",
      "max_grad_res: 1.501e-09\n",
      "mean_grad_ics: 2.277e-06\n",
      "mean_grad_bcs: 1.647e-05\n",
      "It: 11000, Loss: 1.338e+04, Loss_res: 4.537e-07,  Loss_bcs: 4.142e-01, Loss_ut_ics: 3.890e-01,, Time: 4.68\n",
      "update_loss_res: 4.7292e-02\n",
      "update_loss_ics_u_t:4.2666e-01\n",
      "update_loss_bcs: 2.5261e+00\n",
      "adaptive_constant_res_val: 6.1352e+06\n",
      "adaptive_constant_ics_val:2.1618e+04\n",
      "adaptive_constant_bcs_val: 9.5622e+03\n",
      "max_grad_res: 7.708e-09\n",
      "mean_grad_ics: 3.906e-04\n",
      "mean_grad_bcs: 2.918e-05\n",
      "It: 11100, Loss: 5.287e+03, Loss_res: 1.282e-06,  Loss_bcs: 5.371e-01, Loss_ut_ics: 6.630e-03,, Time: 4.60\n",
      "update_loss_res: 3.0167e-02\n",
      "update_loss_ics_u_t:3.5312e-02\n",
      "update_loss_bcs: 2.9345e+00\n",
      "adaptive_constant_res_val: 7.1796e+06\n",
      "adaptive_constant_ics_val:8.1382e+01\n",
      "adaptive_constant_bcs_val: 1.8725e+04\n",
      "max_grad_res: 4.202e-09\n",
      "mean_grad_ics: 9.684e-06\n",
      "mean_grad_bcs: 2.681e-05\n",
      "It: 11200, Loss: 6.645e+03, Loss_res: 2.186e-06,  Loss_bcs: 3.523e-01, Loss_ut_ics: 4.023e-01,, Time: 4.18\n",
      "update_loss_res: 5.3375e-02\n",
      "update_loss_ics_u_t:5.4908e-01\n",
      "update_loss_bcs: 2.3975e+00\n",
      "adaptive_constant_res_val: 1.0965e+06\n",
      "adaptive_constant_ics_val:4.3013e+03\n",
      "adaptive_constant_bcs_val: 1.0021e+03\n",
      "max_grad_res: 4.868e-08\n",
      "mean_grad_ics: 3.813e-04\n",
      "mean_grad_bcs: 2.034e-05\n",
      "It: 11300, Loss: 5.685e+02, Loss_res: 1.819e-06,  Loss_bcs: 5.469e-01, Loss_ut_ics: 4.303e-03,, Time: 4.21\n",
      "update_loss_res: 3.5636e-02\n",
      "update_loss_ics_u_t:5.0878e-02\n",
      "update_loss_bcs: 2.9135e+00\n",
      "adaptive_constant_res_val: 9.1133e+06\n",
      "adaptive_constant_ics_val:1.0550e+02\n",
      "adaptive_constant_bcs_val: 2.1911e+04\n",
      "max_grad_res: 3.910e-09\n",
      "mean_grad_ics: 8.108e-06\n",
      "mean_grad_bcs: 2.941e-05\n",
      "It: 11400, Loss: 6.050e+03, Loss_res: 3.031e-07,  Loss_bcs: 2.741e-01, Loss_ut_ics: 3.868e-01,, Time: 3.86\n",
      "update_loss_res: 5.2848e-02\n",
      "update_loss_ics_u_t:6.7365e-01\n",
      "update_loss_bcs: 2.2735e+00\n",
      "adaptive_constant_res_val: 5.3799e+06\n",
      "adaptive_constant_ics_val:2.5047e+04\n",
      "adaptive_constant_bcs_val: 1.2513e+03\n",
      "max_grad_res: 9.823e-09\n",
      "mean_grad_ics: 3.652e-04\n",
      "mean_grad_bcs: 5.406e-06\n",
      "It: 11500, Loss: 6.231e+02, Loss_res: 6.519e-08,  Loss_bcs: 4.950e-01, Loss_ut_ics: 1.360e-04,, Time: 7.18\n",
      "update_loss_res: 2.2934e-02\n",
      "update_loss_ics_u_t:2.5442e-02\n",
      "update_loss_bcs: 2.9516e+00\n",
      "adaptive_constant_res_val: 6.7390e+07\n",
      "adaptive_constant_ics_val:6.5878e+01\n",
      "adaptive_constant_bcs_val: 1.8738e+05\n",
      "max_grad_res: 3.403e-10\n",
      "mean_grad_ics: 8.812e-07\n",
      "mean_grad_bcs: 2.160e-05\n",
      "It: 11600, Loss: 6.496e+04, Loss_res: 1.185e-07,  Loss_bcs: 3.465e-01, Loss_ut_ics: 3.671e-01,, Time: 4.06\n",
      "update_loss_res: 4.6032e-02\n",
      "update_loss_ics_u_t:1.1740e+00\n",
      "update_loss_bcs: 1.7800e+00\n",
      "adaptive_constant_res_val: 1.7806e+07\n",
      "adaptive_constant_ics_val:1.8952e+05\n",
      "adaptive_constant_bcs_val: 9.6681e+03\n",
      "max_grad_res: 2.585e-09\n",
      "mean_grad_ics: 4.173e-04\n",
      "mean_grad_bcs: 1.404e-05\n",
      "It: 11700, Loss: 4.684e+03, Loss_res: 1.057e-07,  Loss_bcs: 4.819e-01, Loss_ut_ics: 1.210e-04,, Time: 4.45\n",
      "update_loss_res: 2.3222e-02\n",
      "update_loss_ics_u_t:2.4124e-02\n",
      "update_loss_bcs: 2.9527e+00\n",
      "adaptive_constant_res_val: 3.0945e+07\n",
      "adaptive_constant_ics_val:3.8967e+01\n",
      "adaptive_constant_bcs_val: 9.0900e+04\n",
      "max_grad_res: 7.504e-10\n",
      "mean_grad_ics: 1.212e-06\n",
      "mean_grad_bcs: 2.310e-05\n",
      "It: 11800, Loss: 3.518e+04, Loss_res: 6.975e-07,  Loss_bcs: 3.866e-01, Loss_ut_ics: 4.186e-01,, Time: 7.39\n",
      "update_loss_res: 4.7560e-02\n",
      "update_loss_ics_u_t:9.3452e-01\n",
      "update_loss_bcs: 2.0179e+00\n",
      "adaptive_constant_res_val: 2.4739e+06\n",
      "adaptive_constant_ics_val:2.4257e+04\n",
      "adaptive_constant_bcs_val: 3.1260e+03\n",
      "max_grad_res: 1.922e-08\n",
      "mean_grad_ics: 4.990e-04\n",
      "mean_grad_bcs: 2.978e-05\n",
      "It: 11900, Loss: 1.332e+03, Loss_res: 3.550e-07,  Loss_bcs: 4.206e-01, Loss_ut_ics: 6.811e-04,, Time: 7.44\n",
      "update_loss_res: 2.4056e-02\n",
      "update_loss_ics_u_t:2.7002e-02\n",
      "update_loss_bcs: 2.9489e+00\n",
      "adaptive_constant_res_val: 1.1720e+07\n",
      "adaptive_constant_ics_val:4.1092e+01\n",
      "adaptive_constant_bcs_val: 4.1411e+04\n",
      "max_grad_res: 2.053e-09\n",
      "mean_grad_ics: 3.124e-06\n",
      "mean_grad_bcs: 2.882e-05\n",
      "It: 12000, Loss: 1.473e+04, Loss_res: 3.365e-07,  Loss_bcs: 3.552e-01, Loss_ut_ics: 4.197e-01,, Time: 7.45\n",
      "update_loss_res: 5.1171e-02\n",
      "update_loss_ics_u_t:6.8394e-01\n",
      "update_loss_bcs: 2.2649e+00\n",
      "adaptive_constant_res_val: 3.3559e+06\n",
      "adaptive_constant_ics_val:1.9493e+04\n",
      "adaptive_constant_bcs_val: 2.5415e+03\n",
      "max_grad_res: 1.525e-08\n",
      "mean_grad_ics: 4.346e-04\n",
      "mean_grad_bcs: 1.711e-05\n",
      "It: 12100, Loss: 1.335e+03, Loss_res: 4.660e-07,  Loss_bcs: 5.210e-01, Loss_ut_ics: 4.733e-04,, Time: 7.55\n",
      "update_loss_res: 2.4059e-02\n",
      "update_loss_ics_u_t:2.7297e-02\n",
      "update_loss_bcs: 2.9486e+00\n",
      "adaptive_constant_res_val: 8.0141e+06\n",
      "adaptive_constant_ics_val:1.8877e+01\n",
      "adaptive_constant_bcs_val: 2.1462e+04\n",
      "max_grad_res: 3.002e-09\n",
      "mean_grad_ics: 2.076e-06\n",
      "mean_grad_bcs: 2.185e-05\n",
      "It: 12200, Loss: 7.890e+03, Loss_res: 1.792e-08,  Loss_bcs: 3.673e-01, Loss_ut_ics: 4.188e-01,, Time: 7.42\n",
      "update_loss_res: 4.8977e-02\n",
      "update_loss_ics_u_t:2.4581e-01\n",
      "update_loss_bcs: 2.7052e+00\n",
      "adaptive_constant_res_val: 1.4973e+07\n",
      "adaptive_constant_ics_val:3.0814e+04\n",
      "adaptive_constant_bcs_val: 5.8012e+03\n",
      "max_grad_res: 3.271e-09\n",
      "mean_grad_ics: 4.101e-04\n",
      "mean_grad_bcs: 7.015e-06\n",
      "It: 12300, Loss: 2.888e+03, Loss_res: 2.301e-07,  Loss_bcs: 4.915e-01, Loss_ut_ics: 1.089e-03,, Time: 7.46\n",
      "update_loss_res: 2.3958e-02\n",
      "update_loss_ics_u_t:2.6619e-02\n",
      "update_loss_bcs: 2.9494e+00\n",
      "adaptive_constant_res_val: 2.1662e+07\n",
      "adaptive_constant_ics_val:6.6309e+01\n",
      "adaptive_constant_bcs_val: 5.0039e+04\n",
      "max_grad_res: 1.106e-09\n",
      "mean_grad_ics: 2.755e-06\n",
      "mean_grad_bcs: 1.876e-05\n",
      "It: 12400, Loss: 1.960e+04, Loss_res: 6.584e-07,  Loss_bcs: 3.908e-01, Loss_ut_ics: 4.399e-01,, Time: 7.39\n",
      "update_loss_res: 4.7965e-02\n",
      "update_loss_ics_u_t:7.1175e-01\n",
      "update_loss_bcs: 2.2403e+00\n",
      "adaptive_constant_res_val: 3.6812e+06\n",
      "adaptive_constant_ics_val:2.5858e+04\n",
      "adaptive_constant_bcs_val: 1.1174e+03\n",
      "max_grad_res: 1.303e-08\n",
      "mean_grad_ics: 4.734e-04\n",
      "mean_grad_bcs: 6.499e-06\n",
      "It: 12500, Loss: 6.078e+02, Loss_res: 1.999e-07,  Loss_bcs: 5.415e-01, Loss_ut_ics: 7.644e-05,, Time: 7.37\n",
      "update_loss_res: 2.5264e-02\n",
      "update_loss_ics_u_t:2.8439e-02\n",
      "update_loss_bcs: 2.9463e+00\n",
      "adaptive_constant_res_val: 2.5638e+07\n",
      "adaptive_constant_ics_val:1.5686e+01\n",
      "adaptive_constant_bcs_val: 5.7589e+04\n",
      "max_grad_res: 9.854e-10\n",
      "mean_grad_ics: 5.435e-07\n",
      "mean_grad_bcs: 1.926e-05\n",
      "It: 12600, Loss: 2.239e+04, Loss_res: 3.600e-08,  Loss_bcs: 3.886e-01, Loss_ut_ics: 4.242e-01,, Time: 6.07\n",
      "update_loss_res: 5.1300e-02\n",
      "update_loss_ics_u_t:5.7384e-01\n",
      "update_loss_bcs: 2.3749e+00\n",
      "adaptive_constant_res_val: 9.1697e+06\n",
      "adaptive_constant_ics_val:4.7212e+04\n",
      "adaptive_constant_bcs_val: 8.8515e+03\n",
      "max_grad_res: 5.595e-09\n",
      "mean_grad_ics: 4.603e-04\n",
      "mean_grad_bcs: 2.085e-05\n",
      "It: 12700, Loss: 4.653e+03, Loss_res: 4.819e-07,  Loss_bcs: 5.171e-01, Loss_ut_ics: 1.522e-03,, Time: 4.22\n",
      "update_loss_res: 2.6022e-02\n",
      "update_loss_ics_u_t:2.8312e-02\n",
      "update_loss_bcs: 2.9457e+00\n",
      "adaptive_constant_res_val: 1.2450e+07\n",
      "adaptive_constant_ics_val:4.7995e+01\n",
      "adaptive_constant_bcs_val: 2.9083e+04\n",
      "max_grad_res: 2.090e-09\n",
      "mean_grad_ics: 3.543e-06\n",
      "mean_grad_bcs: 2.064e-05\n",
      "It: 12800, Loss: 9.947e+03, Loss_res: 5.007e-07,  Loss_bcs: 3.411e-01, Loss_ut_ics: 4.224e-01,, Time: 3.82\n",
      "update_loss_res: 5.3891e-02\n",
      "update_loss_ics_u_t:4.1160e-01\n",
      "update_loss_bcs: 2.5345e+00\n",
      "adaptive_constant_res_val: 1.6980e+06\n",
      "adaptive_constant_ics_val:5.6721e+03\n",
      "adaptive_constant_bcs_val: 1.0091e+03\n",
      "max_grad_res: 3.174e-08\n",
      "mean_grad_ics: 4.374e-04\n",
      "mean_grad_bcs: 1.264e-05\n",
      "It: 12900, Loss: 4.359e+02, Loss_res: 6.832e-07,  Loss_bcs: 4.187e-01, Loss_ut_ics: 2.143e-03,, Time: 5.80\n",
      "update_loss_res: 3.0864e-02\n",
      "update_loss_ics_u_t:4.2569e-02\n",
      "update_loss_bcs: 2.9266e+00\n",
      "adaptive_constant_res_val: 1.4381e+07\n",
      "adaptive_constant_ics_val:1.4103e+02\n",
      "adaptive_constant_bcs_val: 5.2317e+04\n",
      "max_grad_res: 2.146e-09\n",
      "mean_grad_ics: 7.110e-06\n",
      "mean_grad_bcs: 3.837e-05\n",
      "It: 13000, Loss: 1.660e+04, Loss_res: 1.782e-06,  Loss_bcs: 3.157e-01, Loss_ut_ics: 3.827e-01,, Time: 7.58\n",
      "update_loss_res: 4.7034e-02\n",
      "update_loss_ics_u_t:1.0296e+00\n",
      "update_loss_bcs: 1.9233e+00\n",
      "adaptive_constant_res_val: 1.3972e+06\n",
      "adaptive_constant_ics_val:1.2962e+04\n",
      "adaptive_constant_bcs_val: 5.9531e+02\n",
      "max_grad_res: 3.366e-08\n",
      "mean_grad_ics: 4.238e-04\n",
      "mean_grad_bcs: 1.042e-05\n",
      "It: 13100, Loss: 2.739e+02, Loss_res: 5.676e-07,  Loss_bcs: 4.528e-01, Loss_ut_ics: 2.697e-04,, Time: 7.50\n",
      "update_loss_res: 2.6613e-02\n",
      "update_loss_ics_u_t:3.1885e-02\n",
      "update_loss_bcs: 2.9415e+00\n",
      "adaptive_constant_res_val: 9.9967e+06\n",
      "adaptive_constant_ics_val:1.5626e+01\n",
      "adaptive_constant_bcs_val: 1.6386e+04\n",
      "max_grad_res: 2.662e-09\n",
      "mean_grad_ics: 1.305e-06\n",
      "mean_grad_bcs: 1.483e-05\n",
      "It: 13200, Loss: 5.748e+03, Loss_res: 1.725e-07,  Loss_bcs: 3.504e-01, Loss_ut_ics: 3.290e-01,, Time: 7.35\n",
      "update_loss_res: 3.9993e-02\n",
      "update_loss_ics_u_t:9.5924e-02\n",
      "update_loss_bcs: 2.8641e+00\n",
      "adaptive_constant_res_val: 6.9022e+06\n",
      "adaptive_constant_ics_val:5.2980e+03\n",
      "adaptive_constant_bcs_val: 7.3841e+03\n",
      "max_grad_res: 5.794e-09\n",
      "mean_grad_ics: 3.200e-04\n",
      "mean_grad_bcs: 1.494e-05\n",
      "It: 13300, Loss: 3.397e+03, Loss_res: 6.272e-07,  Loss_bcs: 4.288e-01, Loss_ut_ics: 4.273e-02,, Time: 7.45\n",
      "update_loss_res: 4.5035e-02\n",
      "update_loss_ics_u_t:8.4656e-02\n",
      "update_loss_bcs: 2.8703e+00\n",
      "adaptive_constant_res_val: 1.1445e+07\n",
      "adaptive_constant_ics_val:1.1382e+03\n",
      "adaptive_constant_bcs_val: 3.1927e+04\n",
      "max_grad_res: 3.935e-09\n",
      "mean_grad_ics: 5.290e-05\n",
      "mean_grad_bcs: 4.377e-05\n",
      "It: 13400, Loss: 1.169e+04, Loss_res: 1.389e-08,  Loss_bcs: 3.545e-01, Loss_ut_ics: 3.312e-01,, Time: 7.49\n",
      "update_loss_res: 5.7240e-02\n",
      "update_loss_ics_u_t:9.0403e-01\n",
      "update_loss_bcs: 2.0387e+00\n",
      "adaptive_constant_res_val: 3.3403e+07\n",
      "adaptive_constant_ics_val:1.7322e+05\n",
      "adaptive_constant_bcs_val: 3.0010e+04\n",
      "max_grad_res: 1.714e-09\n",
      "mean_grad_ics: 3.284e-04\n",
      "mean_grad_bcs: 2.522e-05\n",
      "It: 13500, Loss: 1.526e+04, Loss_res: 1.280e-07,  Loss_bcs: 5.022e-01, Loss_ut_ics: 1.095e-03,, Time: 7.46\n",
      "update_loss_res: 2.6931e-02\n",
      "update_loss_ics_u_t:2.8244e-02\n",
      "update_loss_bcs: 2.9448e+00\n",
      "adaptive_constant_res_val: 3.0969e+07\n",
      "adaptive_constant_ics_val:2.2716e+02\n",
      "adaptive_constant_bcs_val: 1.4491e+05\n",
      "max_grad_res: 8.696e-10\n",
      "mean_grad_ics: 6.994e-06\n",
      "mean_grad_bcs: 4.279e-05\n",
      "It: 13600, Loss: 5.221e+04, Loss_res: 2.470e-07,  Loss_bcs: 3.596e-01, Loss_ut_ics: 4.035e-01,, Time: 7.40\n",
      "update_loss_res: 3.9216e-02\n",
      "update_loss_ics_u_t:1.3911e+00\n",
      "update_loss_bcs: 1.5697e+00\n",
      "adaptive_constant_res_val: 4.6262e+06\n",
      "adaptive_constant_ics_val:8.4264e+04\n",
      "adaptive_constant_bcs_val: 4.9061e+03\n",
      "max_grad_res: 8.477e-09\n",
      "mean_grad_ics: 5.135e-04\n",
      "mean_grad_bcs: 2.649e-05\n",
      "It: 13700, Loss: 2.295e+03, Loss_res: 1.646e-07,  Loss_bcs: 4.643e-01, Loss_ut_ics: 1.990e-04,, Time: 7.38\n",
      "update_loss_res: 2.3591e-02\n",
      "update_loss_ics_u_t:2.5001e-02\n",
      "update_loss_bcs: 2.9514e+00\n",
      "adaptive_constant_res_val: 2.6519e+07\n",
      "adaptive_constant_ics_val:2.2805e+01\n",
      "adaptive_constant_bcs_val: 4.0602e+04\n",
      "max_grad_res: 8.896e-10\n",
      "mean_grad_ics: 8.115e-07\n",
      "mean_grad_bcs: 1.224e-05\n",
      "It: 13800, Loss: 1.586e+04, Loss_res: 1.978e-07,  Loss_bcs: 3.903e-01, Loss_ut_ics: 4.524e-01,, Time: 7.49\n",
      "update_loss_res: 4.9766e-02\n",
      "update_loss_ics_u_t:3.7185e-01\n",
      "update_loss_bcs: 2.5784e+00\n",
      "adaptive_constant_res_val: 2.2640e+06\n",
      "adaptive_constant_ics_val:7.1276e+03\n",
      "adaptive_constant_bcs_val: 3.6583e+03\n",
      "max_grad_res: 2.198e-08\n",
      "mean_grad_ics: 4.213e-04\n",
      "mean_grad_bcs: 3.119e-05\n",
      "It: 13900, Loss: 1.797e+03, Loss_res: 1.201e-06,  Loss_bcs: 4.742e-01, Loss_ut_ics: 8.409e-03,, Time: 7.48\n",
      "update_loss_res: 3.0929e-02\n",
      "update_loss_ics_u_t:4.3267e-02\n",
      "update_loss_bcs: 2.9258e+00\n",
      "adaptive_constant_res_val: 4.4654e+06\n",
      "adaptive_constant_ics_val:1.1083e+02\n",
      "adaptive_constant_bcs_val: 1.7501e+04\n",
      "max_grad_res: 6.926e-09\n",
      "mean_grad_ics: 1.774e-05\n",
      "mean_grad_bcs: 4.143e-05\n",
      "It: 14000, Loss: 6.112e+03, Loss_res: 8.449e-08,  Loss_bcs: 3.465e-01, Loss_ut_ics: 4.234e-01,, Time: 7.44\n",
      "update_loss_res: 5.5096e-02\n",
      "update_loss_ics_u_t:6.3227e-01\n",
      "update_loss_bcs: 2.3126e+00\n",
      "adaptive_constant_res_val: 4.9127e+06\n",
      "adaptive_constant_ics_val:2.2109e+04\n",
      "adaptive_constant_bcs_val: 1.2889e+03\n",
      "max_grad_res: 1.122e-08\n",
      "mean_grad_ics: 3.922e-04\n",
      "mean_grad_bcs: 6.251e-06\n",
      "It: 14100, Loss: 5.975e+02, Loss_res: 5.725e-08,  Loss_bcs: 4.625e-01, Loss_ut_ics: 4.732e-05,, Time: 7.44\n",
      "update_loss_res: 2.3946e-02\n",
      "update_loss_ics_u_t:2.7502e-02\n",
      "update_loss_bcs: 2.9486e+00\n",
      "adaptive_constant_res_val: 6.4324e+07\n",
      "adaptive_constant_ics_val:2.5931e+01\n",
      "adaptive_constant_bcs_val: 1.3201e+05\n",
      "max_grad_res: 3.723e-10\n",
      "mean_grad_ics: 3.510e-07\n",
      "mean_grad_bcs: 1.667e-05\n",
      "It: 14200, Loss: 4.809e+04, Loss_res: 3.489e-07,  Loss_bcs: 3.641e-01, Loss_ut_ics: 4.201e-01,, Time: 7.43\n",
      "update_loss_res: 5.0838e-02\n",
      "update_loss_ics_u_t:6.8749e-01\n",
      "update_loss_bcs: 2.2617e+00\n",
      "adaptive_constant_res_val: 3.7772e+06\n",
      "adaptive_constant_ics_val:2.4374e+04\n",
      "adaptive_constant_bcs_val: 2.4482e+03\n",
      "max_grad_res: 1.346e-08\n",
      "mean_grad_ics: 4.772e-04\n",
      "mean_grad_bcs: 1.457e-05\n",
      "It: 14300, Loss: 1.274e+03, Loss_res: 9.386e-07,  Loss_bcs: 5.160e-01, Loss_ut_ics: 2.863e-04,, Time: 7.58\n",
      "update_loss_res: 2.5930e-02\n",
      "update_loss_ics_u_t:3.0314e-02\n",
      "update_loss_bcs: 2.9438e+00\n",
      "adaptive_constant_res_val: 5.5670e+06\n",
      "adaptive_constant_ics_val:1.2752e+01\n",
      "adaptive_constant_bcs_val: 1.8316e+04\n",
      "max_grad_res: 4.658e-09\n",
      "mean_grad_ics: 1.959e-06\n",
      "mean_grad_bcs: 2.898e-05\n",
      "It: 14400, Loss: 5.959e+03, Loss_res: 6.821e-06,  Loss_bcs: 3.231e-01, Loss_ut_ics: 2.791e-01,, Time: 7.50\n",
      "update_loss_res: 4.4731e-02\n",
      "update_loss_ics_u_t:1.1652e-01\n",
      "update_loss_bcs: 2.8388e+00\n",
      "adaptive_constant_res_val: 8.3020e+05\n",
      "adaptive_constant_ics_val:6.1153e+02\n",
      "adaptive_constant_bcs_val: 7.8442e+02\n",
      "max_grad_res: 5.388e-08\n",
      "mean_grad_ics: 2.828e-04\n",
      "mean_grad_bcs: 1.489e-05\n",
      "It: 14500, Loss: 3.291e+02, Loss_res: 1.275e-06,  Loss_bcs: 3.436e-01, Loss_ut_ics: 9.573e-02,, Time: 7.39\n",
      "update_loss_res: 6.3035e-02\n",
      "update_loss_ics_u_t:3.2241e-01\n",
      "update_loss_bcs: 2.6146e+00\n",
      "adaptive_constant_res_val: 6.3544e+06\n",
      "adaptive_constant_ics_val:3.6772e+03\n",
      "adaptive_constant_bcs_val: 1.0541e+04\n",
      "max_grad_res: 9.920e-09\n",
      "mean_grad_ics: 1.131e-04\n",
      "mean_grad_bcs: 4.000e-05\n",
      "It: 14600, Loss: 4.200e+03, Loss_res: 3.173e-08,  Loss_bcs: 3.623e-01, Loss_ut_ics: 1.035e-01,, Time: 7.43\n",
      "update_loss_res: 5.8828e-02\n",
      "update_loss_ics_u_t:1.6291e-01\n",
      "update_loss_bcs: 2.7783e+00\n",
      "adaptive_constant_res_val: 5.4765e+07\n",
      "adaptive_constant_ics_val:1.8409e+04\n",
      "adaptive_constant_bcs_val: 9.3658e+04\n",
      "max_grad_res: 1.074e-09\n",
      "mean_grad_ics: 1.214e-04\n",
      "mean_grad_bcs: 3.621e-05\n",
      "It: 14700, Loss: 4.043e+04, Loss_res: 1.368e-07,  Loss_bcs: 3.992e-01, Loss_ut_ics: 1.650e-01,, Time: 7.58\n",
      "update_loss_res: 6.6512e-02\n",
      "update_loss_ics_u_t:3.1619e-01\n",
      "update_loss_bcs: 2.6173e+00\n",
      "adaptive_constant_res_val: 1.5096e+07\n",
      "adaptive_constant_ics_val:1.3263e+04\n",
      "adaptive_constant_bcs_val: 2.6252e+04\n",
      "max_grad_res: 4.406e-09\n",
      "mean_grad_ics: 1.848e-04\n",
      "mean_grad_bcs: 4.419e-05\n",
      "It: 14800, Loss: 1.074e+04, Loss_res: 1.174e-07,  Loss_bcs: 3.791e-01, Loss_ut_ics: 5.903e-02,, Time: 7.44\n",
      "update_loss_res: 5.2346e-02\n",
      "update_loss_ics_u_t:1.0401e-01\n",
      "update_loss_bcs: 2.8436e+00\n",
      "adaptive_constant_res_val: 3.0097e+07\n",
      "adaptive_constant_ics_val:4.4670e+03\n",
      "adaptive_constant_bcs_val: 5.2945e+04\n",
      "max_grad_res: 1.739e-09\n",
      "mean_grad_ics: 7.470e-05\n",
      "mean_grad_bcs: 3.238e-05\n",
      "It: 14900, Loss: 2.218e+04, Loss_res: 8.830e-07,  Loss_bcs: 3.928e-01, Loss_ut_ics: 3.035e-01,, Time: 7.41\n",
      "update_loss_res: 6.0057e-02\n",
      "update_loss_ics_u_t:6.4835e-01\n",
      "update_loss_bcs: 2.2916e+00\n",
      "adaptive_constant_res_val: 3.2478e+06\n",
      "adaptive_constant_ics_val:1.1084e+04\n",
      "adaptive_constant_bcs_val: 2.6537e+03\n",
      "max_grad_res: 1.849e-08\n",
      "mean_grad_ics: 3.161e-04\n",
      "mean_grad_bcs: 2.141e-05\n",
      "It: 15000, Loss: 1.257e+03, Loss_res: 7.676e-07,  Loss_bcs: 4.598e-01, Loss_ut_ics: 3.126e-03,, Time: 7.48\n",
      "update_loss_res: 3.1332e-02\n",
      "update_loss_ics_u_t:4.0039e-02\n",
      "update_loss_bcs: 2.9286e+00\n",
      "adaptive_constant_res_val: 1.6286e+07\n",
      "adaptive_constant_ics_val:1.3491e+02\n",
      "adaptive_constant_bcs_val: 4.0185e+04\n",
      "max_grad_res: 1.924e-09\n",
      "mean_grad_ics: 6.482e-06\n",
      "mean_grad_bcs: 2.640e-05\n",
      "It: 15100, Loss: 1.476e+04, Loss_res: 4.372e-08,  Loss_bcs: 3.658e-01, Loss_ut_ics: 3.950e-01,, Time: 7.57\n",
      "update_loss_res: 5.3850e-02\n",
      "update_loss_ics_u_t:6.4378e-01\n",
      "update_loss_bcs: 2.3024e+00\n",
      "adaptive_constant_res_val: 3.2742e+07\n",
      "adaptive_constant_ics_val:1.4262e+05\n",
      "adaptive_constant_bcs_val: 2.5870e+04\n",
      "max_grad_res: 1.645e-09\n",
      "mean_grad_ics: 3.644e-04\n",
      "mean_grad_bcs: 1.848e-05\n",
      "It: 15200, Loss: 1.384e+04, Loss_res: 3.581e-07,  Loss_bcs: 5.274e-01, Loss_ut_ics: 1.311e-03,, Time: 7.48\n",
      "update_loss_res: 2.7155e-02\n",
      "update_loss_ics_u_t:2.9113e-02\n",
      "update_loss_bcs: 2.9437e+00\n",
      "adaptive_constant_res_val: 2.0415e+07\n",
      "adaptive_constant_ics_val:1.1241e+02\n",
      "adaptive_constant_bcs_val: 7.9888e+04\n",
      "max_grad_res: 1.330e-09\n",
      "mean_grad_ics: 5.136e-06\n",
      "mean_grad_bcs: 3.610e-05\n",
      "It: 15300, Loss: 2.659e+04, Loss_res: 7.198e-07,  Loss_bcs: 3.320e-01, Loss_ut_ics: 4.301e-01,, Time: 7.48\n",
      "update_loss_res: 5.0299e-02\n",
      "update_loss_ics_u_t:1.0491e+00\n",
      "update_loss_bcs: 1.9006e+00\n",
      "adaptive_constant_res_val: 2.6770e+06\n",
      "adaptive_constant_ics_val:2.6151e+04\n",
      "adaptive_constant_bcs_val: 2.3277e+03\n",
      "max_grad_res: 1.879e-08\n",
      "mean_grad_ics: 4.684e-04\n",
      "mean_grad_bcs: 2.301e-05\n",
      "It: 15400, Loss: 1.032e+03, Loss_res: 5.480e-07,  Loss_bcs: 4.410e-01, Loss_ut_ics: 1.643e-04,, Time: 7.54\n",
      "update_loss_res: 2.2765e-02\n",
      "update_loss_ics_u_t:2.5935e-02\n",
      "update_loss_bcs: 2.9513e+00\n",
      "adaptive_constant_res_val: 4.9497e+06\n",
      "adaptive_constant_ics_val:6.5850e+00\n",
      "adaptive_constant_bcs_val: 1.4840e+04\n",
      "max_grad_res: 4.599e-09\n",
      "mean_grad_ics: 1.168e-06\n",
      "mean_grad_bcs: 2.313e-05\n",
      "It: 15500, Loss: 6.557e+03, Loss_res: 3.340e-06,  Loss_bcs: 4.406e-01, Loss_ut_ics: 2.225e-01,, Time: 7.50\n",
      "update_loss_res: 3.6870e-02\n",
      "update_loss_ics_u_t:5.6227e-02\n",
      "update_loss_bcs: 2.9069e+00\n",
      "adaptive_constant_res_val: 1.0773e+06\n",
      "adaptive_constant_ics_val:3.6392e+02\n",
      "adaptive_constant_bcs_val: 3.1306e+03\n",
      "max_grad_res: 3.422e-08\n",
      "mean_grad_ics: 2.215e-04\n",
      "mean_grad_bcs: 3.686e-05\n",
      "It: 15600, Loss: 1.117e+03, Loss_res: 3.326e-06,  Loss_bcs: 3.353e-01, Loss_ut_ics: 1.759e-01,, Time: 7.47\n",
      "update_loss_res: 6.1635e-02\n",
      "update_loss_ics_u_t:4.6188e-01\n",
      "update_loss_bcs: 2.4765e+00\n",
      "adaptive_constant_res_val: 2.1723e+06\n",
      "adaptive_constant_ics_val:3.0280e+03\n",
      "adaptive_constant_bcs_val: 1.7249e+03\n",
      "max_grad_res: 2.837e-08\n",
      "mean_grad_ics: 1.860e-04\n",
      "mean_grad_bcs: 1.976e-05\n",
      "It: 15700, Loss: 7.987e+02, Loss_res: 5.621e-07,  Loss_bcs: 4.311e-01, Loss_ut_ics: 1.777e-02,, Time: 7.57\n",
      "update_loss_res: 4.3885e-02\n",
      "update_loss_ics_u_t:7.0561e-02\n",
      "update_loss_bcs: 2.8856e+00\n",
      "adaptive_constant_res_val: 1.6271e+07\n",
      "adaptive_constant_ics_val:7.1221e+02\n",
      "adaptive_constant_bcs_val: 3.8492e+04\n",
      "max_grad_res: 2.697e-09\n",
      "mean_grad_ics: 2.722e-05\n",
      "mean_grad_bcs: 3.598e-05\n",
      "It: 15800, Loss: 1.236e+04, Loss_res: 1.235e-07,  Loss_bcs: 3.137e-01, Loss_ut_ics: 3.939e-01,, Time: 7.54\n",
      "update_loss_res: 5.4786e-02\n",
      "update_loss_ics_u_t:8.5779e-01\n",
      "update_loss_bcs: 2.0874e+00\n",
      "adaptive_constant_res_val: 5.1775e+06\n",
      "adaptive_constant_ics_val:3.3220e+04\n",
      "adaptive_constant_bcs_val: 6.2934e+03\n",
      "max_grad_res: 1.058e-08\n",
      "mean_grad_ics: 4.098e-04\n",
      "mean_grad_bcs: 3.190e-05\n",
      "It: 15900, Loss: 2.593e+03, Loss_res: 1.116e-06,  Loss_bcs: 4.072e-01, Loss_ut_ics: 7.214e-04,, Time: 7.45\n",
      "update_loss_res: 2.6726e-02\n",
      "update_loss_ics_u_t:2.9761e-02\n",
      "update_loss_bcs: 2.9435e+00\n",
      "adaptive_constant_res_val: 4.3640e+06\n",
      "adaptive_constant_ics_val:7.0366e+00\n",
      "adaptive_constant_bcs_val: 7.1835e+03\n",
      "max_grad_res: 6.124e-09\n",
      "mean_grad_ics: 1.448e-06\n",
      "mean_grad_bcs: 1.495e-05\n",
      "It: 16000, Loss: 3.052e+03, Loss_res: 2.544e-06,  Loss_bcs: 4.233e-01, Loss_ut_ics: 1.214e-02,, Time: 7.43\n",
      "update_loss_res: 2.6638e-02\n",
      "update_loss_ics_u_t:2.7578e-02\n",
      "update_loss_bcs: 2.9458e+00\n",
      "adaptive_constant_res_val: 2.7187e+06\n",
      "adaptive_constant_ics_val:4.6837e+01\n",
      "adaptive_constant_bcs_val: 8.6809e+03\n",
      "max_grad_res: 9.798e-09\n",
      "mean_grad_ics: 1.664e-05\n",
      "mean_grad_bcs: 2.887e-05\n",
      "It: 16100, Loss: 3.873e+03, Loss_res: 9.684e-07,  Loss_bcs: 4.443e-01, Loss_ut_ics: 2.946e-01,, Time: 7.53\n",
      "update_loss_res: 5.7722e-02\n",
      "update_loss_ics_u_t:2.7220e-01\n",
      "update_loss_bcs: 2.6701e+00\n",
      "adaptive_constant_res_val: 2.1117e+06\n",
      "adaptive_constant_ics_val:2.7080e+03\n",
      "adaptive_constant_bcs_val: 3.4885e+03\n",
      "max_grad_res: 2.733e-08\n",
      "mean_grad_ics: 2.719e-04\n",
      "mean_grad_bcs: 3.571e-05\n",
      "It: 16200, Loss: 1.573e+03, Loss_res: 1.162e-06,  Loss_bcs: 4.194e-01, Loss_ut_ics: 3.970e-02,, Time: 7.43\n",
      "update_loss_res: 5.0016e-02\n",
      "update_loss_ics_u_t:1.0747e-01\n",
      "update_loss_bcs: 2.8425e+00\n",
      "adaptive_constant_res_val: 6.9461e+06\n",
      "adaptive_constant_ics_val:7.2561e+02\n",
      "adaptive_constant_bcs_val: 1.3378e+04\n",
      "max_grad_res: 7.201e-09\n",
      "mean_grad_ics: 4.862e-05\n",
      "mean_grad_bcs: 3.389e-05\n",
      "It: 16300, Loss: 5.391e+03, Loss_res: 1.466e-08,  Loss_bcs: 3.874e-01, Loss_ut_ics: 2.858e-01,, Time: 7.66\n",
      "update_loss_res: 6.1552e-02\n",
      "update_loss_ics_u_t:5.6510e-01\n",
      "update_loss_bcs: 2.3733e+00\n",
      "adaptive_constant_res_val: 7.4723e+07\n",
      "adaptive_constant_ics_val:1.8753e+05\n",
      "adaptive_constant_bcs_val: 4.8799e+04\n",
      "max_grad_res: 8.237e-10\n",
      "mean_grad_ics: 2.734e-04\n",
      "mean_grad_bcs: 1.694e-05\n",
      "It: 16400, Loss: 2.083e+04, Loss_res: 1.673e-07,  Loss_bcs: 4.174e-01, Loss_ut_ics: 2.379e-03,, Time: 7.74\n",
      "update_loss_res: 3.0149e-02\n",
      "update_loss_ics_u_t:3.2014e-02\n",
      "update_loss_bcs: 2.9378e+00\n",
      "adaptive_constant_res_val: 4.4721e+07\n",
      "adaptive_constant_ics_val:2.8069e+02\n",
      "adaptive_constant_bcs_val: 1.0225e+05\n",
      "max_grad_res: 6.742e-10\n",
      "mean_grad_ics: 5.911e-06\n",
      "mean_grad_bcs: 2.347e-05\n",
      "It: 16500, Loss: 3.293e+04, Loss_res: 8.558e-08,  Loss_bcs: 3.211e-01, Loss_ut_ics: 3.231e-01,, Time: 7.42\n",
      "update_loss_res: 5.0931e-02\n",
      "update_loss_ics_u_t:9.1578e-01\n",
      "update_loss_bcs: 2.0333e+00\n",
      "adaptive_constant_res_val: 1.5132e+07\n",
      "adaptive_constant_ics_val:1.0392e+05\n",
      "adaptive_constant_bcs_val: 1.9343e+04\n",
      "max_grad_res: 3.366e-09\n",
      "mean_grad_ics: 3.819e-04\n",
      "mean_grad_bcs: 3.202e-05\n",
      "It: 16600, Loss: 8.283e+03, Loss_res: 6.388e-07,  Loss_bcs: 4.203e-01, Loss_ut_ics: 1.378e-03,, Time: 7.33\n",
      "update_loss_res: 2.4078e-02\n",
      "update_loss_ics_u_t:2.5504e-02\n",
      "update_loss_bcs: 2.9504e+00\n",
      "adaptive_constant_res_val: 1.0398e+07\n",
      "adaptive_constant_ics_val:2.9470e+01\n",
      "adaptive_constant_bcs_val: 1.8013e+04\n",
      "max_grad_res: 2.316e-09\n",
      "mean_grad_ics: 2.676e-06\n",
      "mean_grad_bcs: 1.414e-05\n",
      "It: 16700, Loss: 6.328e+03, Loss_res: 3.058e-06,  Loss_bcs: 3.491e-01, Loss_ut_ics: 2.463e-01,, Time: 7.95\n",
      "update_loss_res: 4.3915e-02\n",
      "update_loss_ics_u_t:1.0541e-01\n",
      "update_loss_bcs: 2.8507e+00\n",
      "adaptive_constant_res_val: 1.2104e+06\n",
      "adaptive_constant_ics_val:6.2755e+02\n",
      "adaptive_constant_bcs_val: 9.1559e+02\n",
      "max_grad_res: 3.628e-08\n",
      "mean_grad_ics: 2.160e-04\n",
      "mean_grad_bcs: 1.165e-05\n",
      "It: 16800, Loss: 4.458e+02, Loss_res: 3.034e-07,  Loss_bcs: 4.265e-01, Loss_ut_ics: 8.762e-02,, Time: 7.85\n",
      "update_loss_res: 6.3155e-02\n",
      "update_loss_ics_u_t:2.6482e-01\n",
      "update_loss_bcs: 2.6720e+00\n",
      "adaptive_constant_res_val: 3.8645e+07\n",
      "adaptive_constant_ics_val:1.4790e+04\n",
      "adaptive_constant_bcs_val: 5.7895e+04\n",
      "max_grad_res: 1.634e-09\n",
      "mean_grad_ics: 9.127e-05\n",
      "mean_grad_bcs: 3.541e-05\n",
      "It: 16900, Loss: 2.458e+04, Loss_res: 9.282e-08,  Loss_bcs: 3.894e-01, Loss_ut_ics: 1.373e-01,, Time: 7.77\n",
      "update_loss_res: 6.1911e-02\n",
      "update_loss_ics_u_t:2.1078e-01\n",
      "update_loss_bcs: 2.7273e+00\n",
      "adaptive_constant_res_val: 1.0051e+07\n",
      "adaptive_constant_ics_val:4.8634e+03\n",
      "adaptive_constant_bcs_val: 1.6309e+04\n",
      "max_grad_res: 6.160e-09\n",
      "mean_grad_ics: 1.421e-04\n",
      "mean_grad_bcs: 3.683e-05\n",
      "It: 17000, Loss: 5.950e+03, Loss_res: 9.535e-10,  Loss_bcs: 3.315e-01, Loss_ut_ics: 1.118e-01,, Time: 7.36\n",
      "update_loss_res: 6.0611e-02\n",
      "update_loss_ics_u_t:1.9867e-01\n",
      "update_loss_bcs: 2.7407e+00\n",
      "adaptive_constant_res_val: 3.5467e+08\n",
      "adaptive_constant_ics_val:1.3866e+05\n",
      "adaptive_constant_bcs_val: 5.3777e+05\n",
      "max_grad_res: 1.709e-10\n",
      "mean_grad_ics: 1.193e-04\n",
      "mean_grad_bcs: 3.353e-05\n",
      "It: 17100, Loss: 2.263e+05, Loss_res: 9.896e-08,  Loss_bcs: 3.924e-01, Loss_ut_ics: 1.100e-01,, Time: 7.35\n",
      "update_loss_res: 6.2093e-02\n",
      "update_loss_ics_u_t:2.1916e-01\n",
      "update_loss_bcs: 2.7187e+00\n",
      "adaptive_constant_res_val: 1.5282e+07\n",
      "adaptive_constant_ics_val:6.4669e+03\n",
      "adaptive_constant_bcs_val: 2.4210e+04\n",
      "max_grad_res: 4.063e-09\n",
      "mean_grad_ics: 1.199e-04\n",
      "mean_grad_bcs: 3.618e-05\n",
      "It: 17200, Loss: 9.950e+03, Loss_res: 6.490e-09,  Loss_bcs: 3.788e-01, Loss_ut_ics: 1.205e-01,, Time: 7.28\n",
      "update_loss_res: 6.1574e-02\n",
      "update_loss_ics_u_t:1.9383e-01\n",
      "update_loss_bcs: 2.7446e+00\n",
      "adaptive_constant_res_val: 7.9879e+07\n",
      "adaptive_constant_ics_val:3.2897e+04\n",
      "adaptive_constant_bcs_val: 1.2594e+05\n",
      "max_grad_res: 7.708e-10\n",
      "mean_grad_ics: 1.308e-04\n",
      "mean_grad_bcs: 3.537e-05\n",
      "It: 17300, Loss: 5.652e+04, Loss_res: 1.984e-09,  Loss_bcs: 4.165e-01, Loss_ut_ics: 1.238e-01,, Time: 7.27\n",
      "update_loss_res: 6.4399e-02\n",
      "update_loss_ics_u_t:2.1558e-01\n",
      "update_loss_bcs: 2.7200e+00\n",
      "adaptive_constant_res_val: 1.0934e+08\n",
      "adaptive_constant_ics_val:4.8803e+04\n",
      "adaptive_constant_bcs_val: 1.8530e+05\n",
      "max_grad_res: 5.890e-10\n",
      "mean_grad_ics: 1.333e-04\n",
      "mean_grad_bcs: 4.012e-05\n",
      "It: 17400, Loss: 7.492e+04, Loss_res: 2.136e-08,  Loss_bcs: 3.657e-01, Loss_ut_ics: 1.465e-01,, Time: 7.25\n",
      "update_loss_res: 5.9060e-02\n",
      "update_loss_ics_u_t:2.1117e-01\n",
      "update_loss_bcs: 2.7298e+00\n",
      "adaptive_constant_res_val: 3.9839e+07\n",
      "adaptive_constant_ics_val:2.2110e+04\n",
      "adaptive_constant_bcs_val: 6.1623e+04\n",
      "max_grad_res: 1.482e-09\n",
      "mean_grad_ics: 1.552e-04\n",
      "mean_grad_bcs: 3.347e-05\n",
      "It: 17500, Loss: 2.681e+04, Loss_res: 2.556e-08,  Loss_bcs: 4.025e-01, Loss_ut_ics: 9.061e-02,, Time: 7.19\n",
      "update_loss_res: 5.8005e-02\n",
      "update_loss_ics_u_t:1.5326e-01\n",
      "update_loss_bcs: 2.7887e+00\n",
      "adaptive_constant_res_val: 1.6747e+07\n",
      "adaptive_constant_ics_val:4.5261e+03\n",
      "adaptive_constant_bcs_val: 3.5702e+04\n",
      "max_grad_res: 3.464e-09\n",
      "mean_grad_ics: 1.023e-04\n",
      "mean_grad_bcs: 4.434e-05\n",
      "It: 17600, Loss: 1.346e+04, Loss_res: 4.491e-08,  Loss_bcs: 3.502e-01, Loss_ut_ics: 2.117e-01,, Time: 7.27\n",
      "update_loss_res: 6.5260e-02\n",
      "update_loss_ics_u_t:4.0806e-01\n",
      "update_loss_bcs: 2.5267e+00\n",
      "adaptive_constant_res_val: 7.7325e+06\n",
      "adaptive_constant_ics_val:1.0713e+04\n",
      "adaptive_constant_bcs_val: 7.9479e+03\n",
      "max_grad_res: 8.440e-09\n",
      "mean_grad_ics: 2.216e-04\n",
      "mean_grad_bcs: 2.655e-05\n",
      "It: 17700, Loss: 4.080e+03, Loss_res: 1.818e-07,  Loss_bcs: 4.872e-01, Loss_ut_ics: 1.927e-02,, Time: 7.28\n",
      "update_loss_res: 4.1590e-02\n",
      "update_loss_ics_u_t:6.5378e-02\n",
      "update_loss_bcs: 2.8930e+00\n",
      "adaptive_constant_res_val: 2.7902e+07\n",
      "adaptive_constant_ics_val:1.4556e+03\n",
      "adaptive_constant_bcs_val: 8.1553e+04\n",
      "max_grad_res: 1.491e-09\n",
      "mean_grad_ics: 3.319e-05\n",
      "mean_grad_bcs: 4.202e-05\n",
      "It: 17800, Loss: 2.937e+04, Loss_res: 3.450e-07,  Loss_bcs: 3.533e-01, Loss_ut_ics: 3.778e-01,, Time: 7.23\n",
      "update_loss_res: 5.0457e-02\n",
      "update_loss_ics_u_t:1.0297e+00\n",
      "update_loss_bcs: 1.9199e+00\n",
      "adaptive_constant_res_val: 1.3500e+06\n",
      "adaptive_constant_ics_val:9.7183e+03\n",
      "adaptive_constant_bcs_val: 8.7183e+02\n",
      "max_grad_res: 3.738e-08\n",
      "mean_grad_ics: 3.528e-04\n",
      "mean_grad_bcs: 1.697e-05\n",
      "It: 17900, Loss: 4.593e+02, Loss_res: 2.522e-06,  Loss_bcs: 4.837e-01, Loss_ut_ics: 3.513e-03,, Time: 7.28\n",
      "update_loss_res: 3.9278e-02\n",
      "update_loss_ics_u_t:6.2463e-02\n",
      "update_loss_bcs: 2.8983e+00\n",
      "adaptive_constant_res_val: 2.5539e+06\n",
      "adaptive_constant_ics_val:4.7042e+01\n",
      "adaptive_constant_bcs_val: 8.5611e+03\n",
      "max_grad_res: 1.538e-08\n",
      "mean_grad_ics: 1.158e-05\n",
      "mean_grad_bcs: 4.543e-05\n",
      "It: 18000, Loss: 3.831e+03, Loss_res: 7.835e-07,  Loss_bcs: 4.469e-01, Loss_ut_ics: 7.248e-02,, Time: 7.24\n",
      "update_loss_res: 3.5779e-02\n",
      "update_loss_ics_u_t:4.6663e-02\n",
      "update_loss_bcs: 2.9176e+00\n",
      "adaptive_constant_res_val: 1.6928e+06\n",
      "adaptive_constant_ics_val:1.5729e+02\n",
      "adaptive_constant_bcs_val: 5.0543e+03\n",
      "max_grad_res: 2.114e-08\n",
      "mean_grad_ics: 7.124e-05\n",
      "mean_grad_bcs: 3.662e-05\n",
      "It: 18100, Loss: 1.819e+03, Loss_res: 7.347e-07,  Loss_bcs: 3.539e-01, Loss_ut_ics: 1.864e-01,, Time: 7.24\n",
      "update_loss_res: 6.0280e-02\n",
      "update_loss_ics_u_t:2.2304e-01\n",
      "update_loss_bcs: 2.7167e+00\n",
      "adaptive_constant_res_val: 1.8451e+06\n",
      "adaptive_constant_ics_val:1.1309e+03\n",
      "adaptive_constant_bcs_val: 2.0222e+03\n",
      "max_grad_res: 3.267e-08\n",
      "mean_grad_ics: 1.657e-04\n",
      "mean_grad_bcs: 2.432e-05\n",
      "It: 18200, Loss: 9.201e+02, Loss_res: 5.775e-08,  Loss_bcs: 3.970e-01, Loss_ut_ics: 1.037e-01,, Time: 7.25\n",
      "update_loss_res: 6.3379e-02\n",
      "update_loss_ics_u_t:2.5890e-01\n",
      "update_loss_bcs: 2.6777e+00\n",
      "adaptive_constant_res_val: 1.6094e+07\n",
      "adaptive_constant_ics_val:6.4962e+03\n",
      "adaptive_constant_bcs_val: 2.3688e+04\n",
      "max_grad_res: 3.938e-09\n",
      "mean_grad_ics: 9.881e-05\n",
      "mean_grad_bcs: 3.484e-05\n",
      "It: 18300, Loss: 9.336e+03, Loss_res: 4.325e-08,  Loss_bcs: 3.613e-01, Loss_ut_ics: 1.196e-01,, Time: 7.18\n",
      "update_loss_res: 6.1747e-02\n",
      "update_loss_ics_u_t:1.9740e-01\n",
      "update_loss_bcs: 2.7409e+00\n",
      "adaptive_constant_res_val: 8.4463e+06\n",
      "adaptive_constant_ics_val:3.1121e+03\n",
      "adaptive_constant_bcs_val: 1.0960e+04\n",
      "max_grad_res: 7.310e-09\n",
      "mean_grad_ics: 1.153e-04\n",
      "mean_grad_bcs: 2.923e-05\n",
      "It: 18400, Loss: 4.098e+03, Loss_res: 2.856e-08,  Loss_bcs: 3.409e-01, Loss_ut_ics: 1.161e-01,, Time: 7.28\n",
      "update_loss_res: 6.4799e-02\n",
      "update_loss_ics_u_t:2.1049e-01\n",
      "update_loss_bcs: 2.7247e+00\n",
      "adaptive_constant_res_val: 1.3247e+07\n",
      "adaptive_constant_ics_val:4.8629e+03\n",
      "adaptive_constant_bcs_val: 1.8776e+04\n",
      "max_grad_res: 4.892e-09\n",
      "mean_grad_ics: 1.130e-04\n",
      "mean_grad_bcs: 3.371e-05\n",
      "It: 18500, Loss: 7.267e+03, Loss_res: 6.856e-08,  Loss_bcs: 3.532e-01, Loss_ut_ics: 1.303e-01,, Time: 7.25\n",
      "update_loss_res: 6.1135e-02\n",
      "update_loss_ics_u_t:2.2039e-01\n",
      "update_loss_bcs: 2.7185e+00\n",
      "adaptive_constant_res_val: 2.8069e+07\n",
      "adaptive_constant_ics_val:1.2979e+04\n",
      "adaptive_constant_bcs_val: 3.9837e+04\n",
      "max_grad_res: 2.178e-09\n",
      "mean_grad_ics: 1.283e-04\n",
      "mean_grad_bcs: 3.192e-05\n",
      "It: 18600, Loss: 1.638e+04, Loss_res: 5.025e-08,  Loss_bcs: 3.797e-01, Loss_ut_ics: 9.621e-02,, Time: 7.27\n",
      "update_loss_res: 6.2008e-02\n",
      "update_loss_ics_u_t:1.7781e-01\n",
      "update_loss_bcs: 2.7602e+00\n",
      "adaptive_constant_res_val: 1.2166e+07\n",
      "adaptive_constant_ics_val:3.7688e+03\n",
      "adaptive_constant_bcs_val: 2.1033e+04\n",
      "max_grad_res: 5.097e-09\n",
      "mean_grad_ics: 1.080e-04\n",
      "mean_grad_bcs: 3.884e-05\n",
      "It: 18700, Loss: 8.336e+03, Loss_res: 8.932e-08,  Loss_bcs: 3.695e-01, Loss_ut_ics: 1.495e-01,, Time: 7.19\n",
      "update_loss_res: 6.3525e-02\n",
      "update_loss_ics_u_t:2.6265e-01\n",
      "update_loss_bcs: 2.6738e+00\n",
      "adaptive_constant_res_val: 5.6166e+06\n",
      "adaptive_constant_ics_val:3.8353e+03\n",
      "adaptive_constant_bcs_val: 8.2032e+03\n",
      "max_grad_res: 1.131e-08\n",
      "mean_grad_ics: 1.652e-04\n",
      "mean_grad_bcs: 3.470e-05\n",
      "It: 18800, Loss: 3.352e+03, Loss_res: 4.542e-08,  Loss_bcs: 3.757e-01, Loss_ut_ics: 7.042e-02,, Time: 7.43\n",
      "update_loss_res: 5.5733e-02\n",
      "update_loss_ics_u_t:1.3910e-01\n",
      "update_loss_bcs: 2.8052e+00\n",
      "adaptive_constant_res_val: 2.3190e+07\n",
      "adaptive_constant_ics_val:6.0257e+03\n",
      "adaptive_constant_bcs_val: 5.1020e+04\n",
      "max_grad_res: 2.403e-09\n",
      "mean_grad_ics: 1.041e-04\n",
      "mean_grad_bcs: 4.371e-05\n",
      "It: 18900, Loss: 1.991e+04, Loss_res: 2.209e-07,  Loss_bcs: 3.646e-01, Loss_ut_ics: 2.163e-01,, Time: 7.15\n",
      "update_loss_res: 6.3564e-02\n",
      "update_loss_ics_u_t:4.1142e-01\n",
      "update_loss_bcs: 2.5250e+00\n",
      "adaptive_constant_res_val: 1.7267e+06\n",
      "adaptive_constant_ics_val:2.5990e+03\n",
      "adaptive_constant_bcs_val: 1.7536e+03\n",
      "max_grad_res: 3.681e-08\n",
      "mean_grad_ics: 2.326e-04\n",
      "mean_grad_bcs: 2.557e-05\n",
      "It: 19000, Loss: 8.113e+02, Loss_res: 3.040e-06,  Loss_bcs: 4.146e-01, Loss_ut_ics: 3.037e-02,, Time: 7.21\n",
      "update_loss_res: 5.2807e-02\n",
      "update_loss_ics_u_t:1.1899e-01\n",
      "update_loss_bcs: 2.8282e+00\n",
      "adaptive_constant_res_val: 2.7446e+06\n",
      "adaptive_constant_ics_val:4.3078e+02\n",
      "adaptive_constant_bcs_val: 1.1789e+04\n",
      "max_grad_res: 1.924e-08\n",
      "mean_grad_ics: 6.965e-05\n",
      "mean_grad_bcs: 8.020e-05\n",
      "It: 19100, Loss: 4.004e+03, Loss_res: 1.820e-06,  Loss_bcs: 3.296e-01, Loss_ut_ics: 2.634e-01,, Time: 7.26\n",
      "update_loss_res: 5.9161e-02\n",
      "update_loss_ics_u_t:3.4456e-01\n",
      "update_loss_bcs: 2.5963e+00\n",
      "adaptive_constant_res_val: 2.3926e+06\n",
      "adaptive_constant_ics_val:3.6296e+03\n",
      "adaptive_constant_bcs_val: 9.9635e+02\n",
      "max_grad_res: 2.473e-08\n",
      "mean_grad_ics: 2.605e-04\n",
      "mean_grad_bcs: 9.489e-06\n",
      "It: 19200, Loss: 5.295e+02, Loss_res: 2.976e-06,  Loss_bcs: 4.911e-01, Loss_ut_ics: 9.120e-03,, Time: 7.22\n",
      "update_loss_res: 4.3523e-02\n",
      "update_loss_ics_u_t:8.2546e-02\n",
      "update_loss_bcs: 2.8739e+00\n",
      "adaptive_constant_res_val: 2.7053e+06\n",
      "adaptive_constant_ics_val:1.9429e+02\n",
      "adaptive_constant_bcs_val: 1.7343e+04\n",
      "max_grad_res: 1.609e-08\n",
      "mean_grad_ics: 3.787e-05\n",
      "mean_grad_bcs: 9.709e-05\n",
      "It: 19300, Loss: 6.626e+03, Loss_res: 2.882e-06,  Loss_bcs: 3.783e-01, Loss_ut_ics: 2.904e-01,, Time: 7.24\n",
      "update_loss_res: 5.9050e-02\n",
      "update_loss_ics_u_t:4.4129e-01\n",
      "update_loss_bcs: 2.4997e+00\n",
      "adaptive_constant_res_val: 1.1638e+06\n",
      "adaptive_constant_ics_val:2.3810e+03\n",
      "adaptive_constant_bcs_val: 6.8074e+02\n",
      "max_grad_res: 5.074e-08\n",
      "mean_grad_ics: 2.738e-04\n",
      "mean_grad_bcs: 1.382e-05\n",
      "It: 19400, Loss: 2.950e+02, Loss_res: 6.804e-06,  Loss_bcs: 3.559e-01, Loss_ut_ics: 1.882e-02,, Time: 7.30\n",
      "update_loss_res: 5.3477e-02\n",
      "update_loss_ics_u_t:1.3695e-01\n",
      "update_loss_bcs: 2.8096e+00\n",
      "adaptive_constant_res_val: 1.6666e+06\n",
      "adaptive_constant_ics_val:2.3600e+02\n",
      "adaptive_constant_bcs_val: 6.3200e+03\n",
      "max_grad_res: 3.209e-08\n",
      "mean_grad_ics: 5.530e-05\n",
      "mean_grad_bcs: 7.218e-05\n",
      "It: 19500, Loss: 2.361e+03, Loss_res: 1.428e-06,  Loss_bcs: 3.654e-01, Loss_ut_ics: 2.105e-01,, Time: 7.23\n",
      "update_loss_res: 5.7046e-02\n",
      "update_loss_ics_u_t:1.8800e-01\n",
      "update_loss_bcs: 2.7550e+00\n",
      "adaptive_constant_res_val: 5.4478e+06\n",
      "adaptive_constant_ics_val:3.4879e+03\n",
      "adaptive_constant_bcs_val: 3.9150e+03\n",
      "max_grad_res: 1.047e-08\n",
      "mean_grad_ics: 1.943e-04\n",
      "mean_grad_bcs: 1.488e-05\n",
      "It: 19600, Loss: 1.758e+03, Loss_res: 3.386e-07,  Loss_bcs: 4.142e-01, Loss_ut_ics: 3.865e-02,, Time: 7.24\n",
      "update_loss_res: 5.8565e-02\n",
      "update_loss_ics_u_t:1.2891e-01\n",
      "update_loss_bcs: 2.8125e+00\n",
      "adaptive_constant_res_val: 7.4312e+06\n",
      "adaptive_constant_ics_val:1.0866e+03\n",
      "adaptive_constant_bcs_val: 1.8875e+04\n",
      "max_grad_res: 7.881e-09\n",
      "mean_grad_ics: 6.643e-05\n",
      "mean_grad_bcs: 5.289e-05\n",
      "It: 19700, Loss: 6.062e+03, Loss_res: 5.838e-07,  Loss_bcs: 3.071e-01, Loss_ut_ics: 2.403e-01,, Time: 7.28\n",
      "update_loss_res: 6.2065e-02\n",
      "update_loss_ics_u_t:3.9672e-01\n",
      "update_loss_bcs: 2.5412e+00\n",
      "adaptive_constant_res_val: 1.9455e+06\n",
      "adaptive_constant_ics_val:2.9689e+03\n",
      "adaptive_constant_bcs_val: 1.3642e+03\n",
      "max_grad_res: 3.190e-08\n",
      "mean_grad_ics: 2.387e-04\n",
      "mean_grad_bcs: 1.713e-05\n",
      "It: 19800, Loss: 7.176e+02, Loss_res: 2.971e-06,  Loss_bcs: 4.866e-01, Loss_ut_ics: 1.614e-02,, Time: 7.21\n",
      "update_loss_res: 4.9896e-02\n",
      "update_loss_ics_u_t:9.8240e-02\n",
      "update_loss_bcs: 2.8519e+00\n",
      "adaptive_constant_res_val: 1.5313e+06\n",
      "adaptive_constant_ics_val:1.6109e+02\n",
      "adaptive_constant_bcs_val: 7.9412e+03\n",
      "max_grad_res: 3.258e-08\n",
      "mean_grad_ics: 5.343e-05\n",
      "mean_grad_bcs: 9.073e-05\n",
      "It: 19900, Loss: 3.111e+03, Loss_res: 2.277e-06,  Loss_bcs: 3.861e-01, Loss_ut_ics: 2.587e-01,, Time: 7.23\n",
      "update_loss_res: 5.8277e-02\n",
      "update_loss_ics_u_t:2.7056e-01\n",
      "update_loss_bcs: 2.6712e+00\n",
      "adaptive_constant_res_val: 2.6177e+06\n",
      "adaptive_constant_ics_val:2.7607e+03\n",
      "adaptive_constant_bcs_val: 1.7013e+03\n",
      "max_grad_res: 2.226e-08\n",
      "mean_grad_ics: 2.272e-04\n",
      "mean_grad_bcs: 1.418e-05\n",
      "It: 20000, Loss: 8.102e+02, Loss_res: 1.253e-06,  Loss_bcs: 4.439e-01, Loss_ut_ics: 1.874e-02,, Time: 7.32\n",
      "update_loss_res: 5.4661e-02\n",
      "update_loss_ics_u_t:1.1662e-01\n",
      "update_loss_bcs: 2.8287e+00\n",
      "adaptive_constant_res_val: 3.1631e+06\n",
      "adaptive_constant_ics_val:3.5576e+02\n",
      "adaptive_constant_bcs_val: 1.2830e+04\n",
      "max_grad_res: 1.728e-08\n",
      "mean_grad_ics: 5.272e-05\n",
      "mean_grad_bcs: 7.838e-05\n",
      "It: 20100, Loss: 4.381e+03, Loss_res: 1.113e-06,  Loss_bcs: 3.340e-01, Loss_ut_ics: 2.595e-01,, Time: 4.74\n",
      "update_loss_res: 6.3314e-02\n",
      "update_loss_ics_u_t:3.8103e-01\n",
      "update_loss_bcs: 2.5557e+00\n",
      "adaptive_constant_res_val: 1.1478e+06\n",
      "adaptive_constant_ics_val:1.6570e+03\n",
      "adaptive_constant_bcs_val: 5.7143e+02\n",
      "max_grad_res: 5.516e-08\n",
      "mean_grad_ics: 2.399e-04\n",
      "mean_grad_bcs: 1.233e-05\n",
      "It: 20200, Loss: 2.773e+02, Loss_res: 5.650e-06,  Loss_bcs: 4.178e-01, Loss_ut_ics: 1.936e-02,, Time: 3.45\n",
      "update_loss_res: 5.5952e-02\n",
      "update_loss_ics_u_t:1.3919e-01\n",
      "update_loss_bcs: 2.8049e+00\n",
      "adaptive_constant_res_val: 6.3522e+05\n",
      "adaptive_constant_ics_val:9.7111e+01\n",
      "adaptive_constant_bcs_val: 2.8983e+03\n",
      "max_grad_res: 8.808e-08\n",
      "mean_grad_ics: 6.145e-05\n",
      "mean_grad_bcs: 9.102e-05\n",
      "It: 20300, Loss: 1.087e+03, Loss_res: 1.037e-06,  Loss_bcs: 3.696e-01, Loss_ut_ics: 1.572e-01,, Time: 3.27\n",
      "update_loss_res: 6.0551e-02\n",
      "update_loss_ics_u_t:1.4996e-01\n",
      "update_loss_bcs: 2.7895e+00\n",
      "adaptive_constant_res_val: 2.4542e+06\n",
      "adaptive_constant_ics_val:9.5396e+02\n",
      "adaptive_constant_bcs_val: 3.7806e+03\n",
      "max_grad_res: 2.467e-08\n",
      "mean_grad_ics: 1.570e-04\n",
      "mean_grad_bcs: 3.344e-05\n",
      "It: 20400, Loss: 1.512e+03, Loss_res: 2.193e-07,  Loss_bcs: 3.728e-01, Loss_ut_ics: 1.065e-01,, Time: 3.40\n",
      "update_loss_res: 7.0250e-02\n",
      "update_loss_ics_u_t:2.5902e-01\n",
      "update_loss_bcs: 2.6707e+00\n",
      "adaptive_constant_res_val: 3.3222e+06\n",
      "adaptive_constant_ics_val:1.6549e+03\n",
      "adaptive_constant_bcs_val: 5.3428e+03\n",
      "max_grad_res: 2.115e-08\n",
      "mean_grad_ics: 1.351e-04\n",
      "mean_grad_bcs: 4.230e-05\n",
      "It: 20500, Loss: 2.201e+03, Loss_res: 2.860e-07,  Loss_bcs: 3.848e-01, Loss_ut_ics: 8.726e-02,, Time: 3.59\n",
      "update_loss_res: 6.2982e-02\n",
      "update_loss_ics_u_t:1.7069e-01\n",
      "update_loss_bcs: 2.7663e+00\n",
      "adaptive_constant_res_val: 1.7189e+06\n",
      "adaptive_constant_ics_val:6.0313e+02\n",
      "adaptive_constant_bcs_val: 4.0033e+03\n",
      "max_grad_res: 3.664e-08\n",
      "mean_grad_ics: 1.295e-04\n",
      "mean_grad_bcs: 5.303e-05\n",
      "It: 20600, Loss: 1.516e+03, Loss_res: 3.555e-07,  Loss_bcs: 3.577e-01, Loss_ut_ics: 1.388e-01,, Time: 3.65\n",
      "update_loss_res: 6.8901e-02\n",
      "update_loss_ics_u_t:2.4119e-01\n",
      "update_loss_bcs: 2.6899e+00\n",
      "adaptive_constant_res_val: 1.5739e+06\n",
      "adaptive_constant_ics_val:9.8126e+02\n",
      "adaptive_constant_bcs_val: 2.2389e+03\n",
      "max_grad_res: 4.378e-08\n",
      "mean_grad_ics: 1.781e-04\n",
      "mean_grad_bcs: 3.644e-05\n",
      "It: 20700, Loss: 9.358e+02, Loss_res: 8.887e-07,  Loss_bcs: 3.842e-01, Loss_ut_ics: 7.558e-02,, Time: 4.35\n",
      "update_loss_res: 6.4003e-02\n",
      "update_loss_ics_u_t:1.5768e-01\n",
      "update_loss_bcs: 2.7783e+00\n",
      "adaptive_constant_res_val: 4.5770e+05\n",
      "adaptive_constant_ics_val:1.5498e+02\n",
      "adaptive_constant_bcs_val: 1.0200e+03\n",
      "max_grad_res: 1.398e-07\n",
      "mean_grad_ics: 1.374e-04\n",
      "mean_grad_bcs: 5.134e-05\n",
      "It: 20800, Loss: 3.708e+02, Loss_res: 6.916e-07,  Loss_bcs: 3.484e-01, Loss_ut_ics: 9.730e-02,, Time: 4.29\n",
      "update_loss_res: 6.6717e-02\n",
      "update_loss_ics_u_t:1.5971e-01\n",
      "update_loss_bcs: 2.7736e+00\n",
      "adaptive_constant_res_val: 1.3741e+06\n",
      "adaptive_constant_ics_val:5.0266e+02\n",
      "adaptive_constant_bcs_val: 1.6698e+03\n",
      "max_grad_res: 4.855e-08\n",
      "mean_grad_ics: 1.528e-04\n",
      "mean_grad_bcs: 2.923e-05\n",
      "It: 20900, Loss: 6.816e+02, Loss_res: 6.874e-07,  Loss_bcs: 3.840e-01, Loss_ut_ics: 7.840e-02,, Time: 4.89\n",
      "update_loss_res: 6.6925e-02\n",
      "update_loss_ics_u_t:1.6773e-01\n",
      "update_loss_bcs: 2.7653e+00\n",
      "adaptive_constant_res_val: 1.3087e+06\n",
      "adaptive_constant_ics_val:4.6401e+02\n",
      "adaptive_constant_bcs_val: 3.1096e+03\n",
      "max_grad_res: 5.114e-08\n",
      "mean_grad_ics: 1.415e-04\n",
      "mean_grad_bcs: 5.751e-05\n",
      "It: 21000, Loss: 1.174e+03, Loss_res: 2.713e-06,  Loss_bcs: 3.567e-01, Loss_ut_ics: 1.323e-01,, Time: 3.50\n",
      "update_loss_res: 7.0525e-02\n",
      "update_loss_ics_u_t:2.2147e-01\n",
      "update_loss_bcs: 2.7080e+00\n",
      "adaptive_constant_res_val: 1.1954e+05\n",
      "adaptive_constant_ics_val:7.4605e+01\n",
      "adaptive_constant_bcs_val: 1.4392e+02\n",
      "max_grad_res: 5.900e-07\n",
      "mean_grad_ics: 1.987e-04\n",
      "mean_grad_bcs: 3.135e-05\n",
      "It: 21100, Loss: 5.708e+01, Loss_res: 1.260e-06,  Loss_bcs: 3.423e-01, Loss_ut_ics: 1.027e-01,, Time: 3.62\n",
      "update_loss_res: 7.2961e-02\n",
      "update_loss_ics_u_t:2.2845e-01\n",
      "update_loss_bcs: 2.6986e+00\n",
      "adaptive_constant_res_val: 1.7619e+06\n",
      "adaptive_constant_ics_val:9.8528e+02\n",
      "adaptive_constant_bcs_val: 2.0322e+03\n",
      "max_grad_res: 4.141e-08\n",
      "mean_grad_ics: 1.786e-04\n",
      "mean_grad_bcs: 3.118e-05\n",
      "It: 21200, Loss: 8.400e+02, Loss_res: 2.068e-07,  Loss_bcs: 3.815e-01, Loss_ut_ics: 6.537e-02,, Time: 3.27\n",
      "update_loss_res: 5.8504e-02\n",
      "update_loss_ics_u_t:1.0888e-01\n",
      "update_loss_bcs: 2.8326e+00\n",
      "adaptive_constant_res_val: 1.6079e+06\n",
      "adaptive_constant_ics_val:3.6134e+02\n",
      "adaptive_constant_bcs_val: 3.6567e+03\n",
      "max_grad_res: 3.639e-08\n",
      "mean_grad_ics: 1.207e-04\n",
      "mean_grad_bcs: 4.697e-05\n",
      "It: 21300, Loss: 1.600e+03, Loss_res: 4.639e-07,  Loss_bcs: 4.227e-01, Loss_ut_ics: 1.484e-01,, Time: 3.18\n",
      "update_loss_res: 7.1861e-02\n",
      "update_loss_ics_u_t:2.4159e-01\n",
      "update_loss_bcs: 2.6866e+00\n",
      "adaptive_constant_res_val: 6.1681e+06\n",
      "adaptive_constant_ics_val:3.9955e+03\n",
      "adaptive_constant_bcs_val: 8.4366e+03\n",
      "max_grad_res: 1.165e-08\n",
      "mean_grad_ics: 1.927e-04\n",
      "mean_grad_bcs: 3.659e-05\n",
      "It: 21400, Loss: 3.279e+03, Loss_res: 1.190e-06,  Loss_bcs: 3.618e-01, Loss_ut_ics: 5.471e-02,, Time: 3.20\n",
      "update_loss_res: 6.1547e-02\n",
      "update_loss_ics_u_t:1.1787e-01\n",
      "update_loss_bcs: 2.8206e+00\n",
      "adaptive_constant_res_val: 2.2406e+05\n",
      "adaptive_constant_ics_val:4.8677e+01\n",
      "adaptive_constant_bcs_val: 3.4427e+02\n",
      "max_grad_res: 2.747e-07\n",
      "mean_grad_ics: 1.134e-04\n",
      "mean_grad_bcs: 3.353e-05\n",
      "It: 21500, Loss: 1.342e+02, Loss_res: 3.110e-07,  Loss_bcs: 3.801e-01, Loss_ut_ics: 6.714e-02,, Time: 3.14\n",
      "update_loss_res: 5.8604e-02\n",
      "update_loss_ics_u_t:1.0184e-01\n",
      "update_loss_bcs: 2.8396e+00\n",
      "adaptive_constant_res_val: 4.4765e+06\n",
      "adaptive_constant_ics_val:9.8008e+02\n",
      "adaptive_constant_bcs_val: 1.0799e+04\n",
      "max_grad_res: 1.309e-08\n",
      "mean_grad_ics: 1.260e-04\n",
      "mean_grad_bcs: 4.979e-05\n",
      "It: 21600, Loss: 3.989e+03, Loss_res: 1.584e-06,  Loss_bcs: 3.513e-01, Loss_ut_ics: 1.922e-01,, Time: 3.22\n",
      "update_loss_res: 6.9934e-02\n",
      "update_loss_ics_u_t:3.1330e-01\n",
      "update_loss_bcs: 2.6168e+00\n",
      "adaptive_constant_res_val: 1.9730e+05\n",
      "adaptive_constant_ics_val:2.4445e+02\n",
      "adaptive_constant_bcs_val: 6.8191e+01\n",
      "max_grad_res: 3.544e-07\n",
      "mean_grad_ics: 2.766e-04\n",
      "mean_grad_bcs: 9.237e-06\n",
      "It: 21700, Loss: 4.594e+01, Loss_res: 8.432e-06,  Loss_bcs: 4.378e-01, Loss_ut_ics: 5.899e-02,, Time: 3.14\n",
      "update_loss_res: 7.0194e-02\n",
      "update_loss_ics_u_t:2.2922e-01\n",
      "update_loss_bcs: 2.7006e+00\n",
      "adaptive_constant_res_val: 6.2308e+05\n",
      "adaptive_constant_ics_val:3.1892e+02\n",
      "adaptive_constant_bcs_val: 2.6978e+03\n",
      "max_grad_res: 1.127e-07\n",
      "mean_grad_ics: 1.567e-04\n",
      "mean_grad_bcs: 1.125e-04\n",
      "It: 21800, Loss: 9.186e+02, Loss_res: 5.810e-07,  Loss_bcs: 3.207e-01, Loss_ut_ics: 1.666e-01,, Time: 3.13\n",
      "update_loss_res: 7.0806e-02\n",
      "update_loss_ics_u_t:2.8094e-01\n",
      "update_loss_bcs: 2.6483e+00\n",
      "adaptive_constant_res_val: 1.4728e+06\n",
      "adaptive_constant_ics_val:1.5008e+03\n",
      "adaptive_constant_bcs_val: 9.4643e+02\n",
      "max_grad_res: 4.808e-08\n",
      "mean_grad_ics: 2.568e-04\n",
      "mean_grad_bcs: 1.718e-05\n",
      "It: 21900, Loss: 3.632e+02, Loss_res: 8.405e-07,  Loss_bcs: 3.575e-01, Loss_ut_ics: 1.573e-02,, Time: 3.14\n",
      "update_loss_res: 5.1033e-02\n",
      "update_loss_ics_u_t:7.6518e-02\n",
      "update_loss_bcs: 2.8724e+00\n",
      "adaptive_constant_res_val: 1.1916e+06\n",
      "adaptive_constant_ics_val:1.1562e+02\n",
      "adaptive_constant_bcs_val: 7.3239e+03\n",
      "max_grad_res: 4.283e-08\n",
      "mean_grad_ics: 6.471e-05\n",
      "mean_grad_bcs: 1.092e-04\n",
      "It: 22000, Loss: 2.612e+03, Loss_res: 1.312e-06,  Loss_bcs: 3.521e-01, Loss_ut_ics: 2.790e-01,, Time: 3.44\n",
      "update_loss_res: 6.8446e-02\n",
      "update_loss_ics_u_t:5.2257e-01\n",
      "update_loss_bcs: 2.4090e+00\n",
      "adaptive_constant_res_val: 1.8908e+06\n",
      "adaptive_constant_ics_val:5.2601e+03\n",
      "adaptive_constant_bcs_val: 1.1417e+03\n",
      "max_grad_res: 3.620e-08\n",
      "mean_grad_ics: 3.644e-04\n",
      "mean_grad_bcs: 1.716e-05\n",
      "It: 22100, Loss: 6.023e+02, Loss_res: 6.077e-07,  Loss_bcs: 4.997e-01, Loss_ut_ics: 5.829e-03,, Time: 3.19\n",
      "update_loss_res: 3.9645e-02\n",
      "update_loss_ics_u_t:5.0130e-02\n",
      "update_loss_bcs: 2.9102e+00\n",
      "adaptive_constant_res_val: 1.5867e+06\n",
      "adaptive_constant_ics_val:3.5932e+01\n",
      "adaptive_constant_bcs_val: 1.6305e+04\n",
      "max_grad_res: 2.499e-08\n",
      "mean_grad_ics: 1.791e-05\n",
      "mean_grad_bcs: 1.400e-04\n",
      "It: 22200, Loss: 6.255e+03, Loss_res: 5.858e-06,  Loss_bcs: 3.821e-01, Loss_ut_ics: 4.249e-01,, Time: 3.21\n",
      "update_loss_res: 5.8129e-02\n",
      "update_loss_ics_u_t:8.5063e-01\n",
      "update_loss_bcs: 2.0912e+00\n",
      "adaptive_constant_res_val: 4.3140e+05\n",
      "adaptive_constant_ics_val:3.2261e+03\n",
      "adaptive_constant_bcs_val: 3.2094e+02\n",
      "max_grad_res: 1.347e-07\n",
      "mean_grad_ics: 5.110e-04\n",
      "mean_grad_bcs: 2.068e-05\n",
      "It: 22300, Loss: 1.379e+02, Loss_res: 1.869e-06,  Loss_bcs: 3.627e-01, Loss_ut_ics: 6.410e-03,, Time: 3.14\n",
      "update_loss_res: 2.9655e-02\n",
      "update_loss_ics_u_t:4.3816e-02\n",
      "update_loss_bcs: 2.9265e+00\n",
      "adaptive_constant_res_val: 8.2368e+05\n",
      "adaptive_constant_ics_val:2.2829e+01\n",
      "adaptive_constant_bcs_val: 1.2445e+04\n",
      "max_grad_res: 3.600e-08\n",
      "mean_grad_ics: 1.876e-05\n",
      "mean_grad_bcs: 1.531e-04\n",
      "It: 22400, Loss: 4.149e+03, Loss_res: 3.248e-06,  Loss_bcs: 3.324e-01, Loss_ut_ics: 4.164e-01,, Time: 3.21\n",
      "update_loss_res: 5.3647e-02\n",
      "update_loss_ics_u_t:1.0821e+00\n",
      "update_loss_bcs: 1.8643e+00\n",
      "adaptive_constant_res_val: 1.2515e+06\n",
      "adaptive_constant_ics_val:1.2672e+04\n",
      "adaptive_constant_bcs_val: 9.3058e+02\n",
      "max_grad_res: 4.287e-08\n",
      "mean_grad_ics: 5.020e-04\n",
      "mean_grad_bcs: 2.140e-05\n",
      "It: 22500, Loss: 5.066e+02, Loss_res: 2.312e-07,  Loss_bcs: 5.020e-01, Loss_ut_ics: 3.095e-03,, Time: 3.21\n",
      "update_loss_res: 2.8299e-02\n",
      "update_loss_ics_u_t:3.4930e-02\n",
      "update_loss_bcs: 2.9368e+00\n",
      "adaptive_constant_res_val: 2.2103e+06\n",
      "adaptive_constant_ics_val:2.9772e+01\n",
      "adaptive_constant_bcs_val: 2.5034e+04\n",
      "max_grad_res: 1.280e-08\n",
      "mean_grad_ics: 1.091e-05\n",
      "mean_grad_bcs: 1.091e-04\n",
      "It: 22600, Loss: 9.117e+03, Loss_res: 1.451e-05,  Loss_bcs: 3.623e-01, Loss_ut_ics: 4.852e-01,, Time: 3.26\n",
      "update_loss_res: 5.7582e-02\n",
      "update_loss_ics_u_t:7.9922e-01\n",
      "update_loss_bcs: 2.1432e+00\n",
      "adaptive_constant_res_val: 4.3480e+04\n",
      "adaptive_constant_ics_val:3.3323e+02\n",
      "adaptive_constant_bcs_val: 2.5214e+01\n",
      "max_grad_res: 1.324e-06\n",
      "mean_grad_ics: 5.522e-04\n",
      "mean_grad_bcs: 1.558e-05\n",
      "It: 22700, Loss: 4.563e+01, Loss_res: 1.870e-04,  Loss_bcs: 3.466e-01, Loss_ut_ics: 8.630e-02,, Time: 3.28\n",
      "update_loss_res: 5.9326e-02\n",
      "update_loss_ics_u_t:7.4684e-01\n",
      "update_loss_bcs: 2.1938e+00\n",
      "adaptive_constant_res_val: 6.7461e+04\n",
      "adaptive_constant_ics_val:1.9006e+02\n",
      "adaptive_constant_bcs_val: 3.1495e+02\n",
      "max_grad_res: 8.794e-07\n",
      "mean_grad_ics: 2.238e-04\n",
      "mean_grad_bcs: 1.262e-04\n",
      "It: 22800, Loss: 1.474e+02, Loss_res: 2.223e-05,  Loss_bcs: 4.043e-01, Loss_ut_ics: 9.749e-02,, Time: 3.16\n",
      "update_loss_res: 5.6967e-02\n",
      "update_loss_ics_u_t:1.4076e-01\n",
      "update_loss_bcs: 2.8023e+00\n",
      "adaptive_constant_res_val: 2.1584e+05\n",
      "adaptive_constant_ics_val:1.1875e+02\n",
      "adaptive_constant_bcs_val: 1.0562e+03\n",
      "max_grad_res: 2.639e-07\n",
      "mean_grad_ics: 2.227e-04\n",
      "mean_grad_bcs: 9.948e-05\n",
      "It: 22900, Loss: 4.366e+02, Loss_res: 2.959e-06,  Loss_bcs: 3.920e-01, Loss_ut_ics: 1.843e-01,, Time: 3.23\n",
      "update_loss_res: 7.0905e-02\n",
      "update_loss_ics_u_t:3.5714e-01\n",
      "update_loss_bcs: 2.5720e+00\n",
      "adaptive_constant_res_val: 1.1541e+06\n",
      "adaptive_constant_ics_val:1.6128e+03\n",
      "adaptive_constant_bcs_val: 2.7437e+03\n",
      "max_grad_res: 6.144e-08\n",
      "mean_grad_ics: 2.774e-04\n",
      "mean_grad_bcs: 6.554e-05\n",
      "It: 23000, Loss: 1.120e+03, Loss_res: 1.424e-06,  Loss_bcs: 3.754e-01, Loss_ut_ics: 5.488e-02,, Time: 3.15\n",
      "update_loss_res: 6.1042e-02\n",
      "update_loss_ics_u_t:1.3342e-01\n",
      "update_loss_bcs: 2.8055e+00\n",
      "adaptive_constant_res_val: 5.3711e+05\n",
      "adaptive_constant_ics_val:1.5864e+02\n",
      "adaptive_constant_bcs_val: 2.2914e+03\n",
      "max_grad_res: 1.136e-07\n",
      "mean_grad_ics: 1.351e-04\n",
      "mean_grad_bcs: 9.282e-05\n",
      "It: 23100, Loss: 8.591e+02, Loss_res: 1.451e-06,  Loss_bcs: 3.618e-01, Loss_ut_ics: 1.842e-01,, Time: 3.12\n",
      "update_loss_res: 7.3934e-02\n",
      "update_loss_ics_u_t:2.7549e-01\n",
      "update_loss_bcs: 2.6506e+00\n",
      "adaptive_constant_res_val: 2.4220e+06\n",
      "adaptive_constant_ics_val:2.2643e+03\n",
      "adaptive_constant_bcs_val: 1.8172e+03\n",
      "max_grad_res: 3.053e-08\n",
      "mean_grad_ics: 2.509e-04\n",
      "mean_grad_bcs: 2.093e-05\n",
      "It: 23200, Loss: 9.112e+02, Loss_res: 2.496e-07,  Loss_bcs: 4.731e-01, Loss_ut_ics: 2.248e-02,, Time: 3.12\n",
      "update_loss_res: 4.9426e-02\n",
      "update_loss_ics_u_t:7.3427e-02\n",
      "update_loss_bcs: 2.8771e+00\n",
      "adaptive_constant_res_val: 3.7104e+06\n",
      "adaptive_constant_ics_val:4.1441e+02\n",
      "adaptive_constant_bcs_val: 2.4702e+04\n",
      "max_grad_res: 1.332e-08\n",
      "mean_grad_ics: 7.518e-05\n",
      "mean_grad_bcs: 1.144e-04\n",
      "It: 23300, Loss: 7.819e+03, Loss_res: 4.256e-06,  Loss_bcs: 3.084e-01, Loss_ut_ics: 4.491e-01,, Time: 3.13\n",
      "update_loss_res: 5.6984e-02\n",
      "update_loss_ics_u_t:1.0947e+00\n",
      "update_loss_bcs: 1.8483e+00\n",
      "adaptive_constant_res_val: 2.7923e+05\n",
      "adaptive_constant_ics_val:2.8898e+03\n",
      "adaptive_constant_bcs_val: 1.6840e+02\n",
      "max_grad_res: 2.041e-07\n",
      "mean_grad_ics: 5.387e-04\n",
      "mean_grad_bcs: 1.859e-05\n",
      "It: 23400, Loss: 1.033e+02, Loss_res: 1.201e-05,  Loss_bcs: 4.776e-01, Loss_ut_ics: 6.741e-03,, Time: 3.16\n",
      "update_loss_res: 3.2854e-02\n",
      "update_loss_ics_u_t:5.6545e-02\n",
      "update_loss_bcs: 2.9106e+00\n",
      "adaptive_constant_res_val: 3.1770e+05\n",
      "adaptive_constant_ics_val:8.4062e+00\n",
      "adaptive_constant_bcs_val: 5.6896e+03\n",
      "max_grad_res: 1.034e-07\n",
      "mean_grad_ics: 1.537e-05\n",
      "mean_grad_bcs: 2.021e-04\n",
      "It: 23500, Loss: 2.063e+03, Loss_res: 7.174e-06,  Loss_bcs: 3.618e-01, Loss_ut_ics: 3.028e-01,, Time: 3.12\n",
      "update_loss_res: 6.2511e-02\n",
      "update_loss_ics_u_t:5.7037e-01\n",
      "update_loss_bcs: 2.3671e+00\n",
      "adaptive_constant_res_val: 1.0305e+06\n",
      "adaptive_constant_ics_val:2.9699e+03\n",
      "adaptive_constant_bcs_val: 6.9600e+02\n",
      "max_grad_res: 6.066e-08\n",
      "mean_grad_ics: 3.159e-04\n",
      "mean_grad_bcs: 1.784e-05\n",
      "It: 23600, Loss: 3.141e+02, Loss_res: 1.099e-06,  Loss_bcs: 4.122e-01, Loss_ut_ics: 8.788e-03,, Time: 3.14\n",
      "update_loss_res: 4.2389e-02\n",
      "update_loss_ics_u_t:6.9498e-02\n",
      "update_loss_bcs: 2.8881e+00\n",
      "adaptive_constant_res_val: 1.9603e+06\n",
      "adaptive_constant_ics_val:9.7546e+01\n",
      "adaptive_constant_bcs_val: 1.6715e+04\n",
      "max_grad_res: 2.162e-08\n",
      "mean_grad_ics: 3.035e-05\n",
      "mean_grad_bcs: 1.251e-04\n",
      "It: 23700, Loss: 5.822e+03, Loss_res: 1.635e-06,  Loss_bcs: 3.457e-01, Loss_ut_ics: 4.173e-01,, Time: 3.12\n",
      "update_loss_res: 6.1569e-02\n",
      "update_loss_ics_u_t:8.4846e-01\n",
      "update_loss_bcs: 2.0900e+00\n",
      "adaptive_constant_res_val: 5.2319e+05\n",
      "adaptive_constant_ics_val:3.0724e+03\n",
      "adaptive_constant_bcs_val: 3.3600e+02\n",
      "max_grad_res: 1.177e-07\n",
      "mean_grad_ics: 4.261e-04\n",
      "mean_grad_bcs: 1.892e-05\n",
      "It: 23800, Loss: 1.845e+02, Loss_res: 5.552e-06,  Loss_bcs: 4.540e-01, Loss_ut_ics: 9.465e-03,, Time: 3.17\n",
      "update_loss_res: 4.0802e-02\n",
      "update_loss_ics_u_t:7.4143e-02\n",
      "update_loss_bcs: 2.8851e+00\n",
      "adaptive_constant_res_val: 5.9391e+05\n",
      "adaptive_constant_ics_val:3.3538e+01\n",
      "adaptive_constant_bcs_val: 6.0527e+03\n",
      "max_grad_res: 6.870e-08\n",
      "mean_grad_ics: 3.108e-05\n",
      "mean_grad_bcs: 1.441e-04\n",
      "It: 23900, Loss: 1.985e+03, Loss_res: 4.566e-06,  Loss_bcs: 3.259e-01, Loss_ut_ics: 2.984e-01,, Time: 3.14\n",
      "update_loss_res: 7.1769e-02\n",
      "update_loss_ics_u_t:4.6164e-01\n",
      "update_loss_bcs: 2.4666e+00\n",
      "adaptive_constant_res_val: 1.2778e+06\n",
      "adaptive_constant_ics_val:2.3964e+03\n",
      "adaptive_constant_bcs_val: 5.3758e+02\n",
      "max_grad_res: 5.616e-08\n",
      "mean_grad_ics: 2.916e-04\n",
      "mean_grad_bcs: 1.224e-05\n",
      "It: 24000, Loss: 2.669e+02, Loss_res: 1.286e-06,  Loss_bcs: 4.478e-01, Loss_ut_ics: 1.025e-02,, Time: 3.13\n",
      "update_loss_res: 5.0222e-02\n",
      "update_loss_ics_u_t:8.8405e-02\n",
      "update_loss_bcs: 2.8614e+00\n",
      "adaptive_constant_res_val: 3.0007e+06\n",
      "adaptive_constant_ics_val:2.0130e+02\n",
      "adaptive_constant_bcs_val: 2.4093e+04\n",
      "max_grad_res: 1.674e-08\n",
      "mean_grad_ics: 3.811e-05\n",
      "mean_grad_bcs: 1.409e-04\n",
      "It: 24100, Loss: 9.515e+03, Loss_res: 1.211e-05,  Loss_bcs: 3.895e-01, Loss_ut_ics: 4.662e-01,, Time: 3.15\n",
      "update_loss_res: 5.5777e-02\n",
      "update_loss_ics_u_t:1.0169e+00\n",
      "update_loss_bcs: 1.9274e+00\n",
      "adaptive_constant_res_val: 3.7301e+04\n",
      "adaptive_constant_ics_val:3.3948e+02\n",
      "adaptive_constant_bcs_val: 1.8966e+01\n",
      "max_grad_res: 1.495e-06\n",
      "mean_grad_ics: 4.992e-04\n",
      "mean_grad_bcs: 1.471e-05\n",
      "It: 24200, Loss: 5.253e+01, Loss_res: 1.347e-04,  Loss_bcs: 3.351e-01, Loss_ut_ics: 1.212e-01,, Time: 3.14\n",
      "update_loss_res: 5.9090e-02\n",
      "update_loss_ics_u_t:9.1338e-01\n",
      "update_loss_bcs: 2.0275e+00\n",
      "adaptive_constant_res_val: 6.3122e+04\n",
      "adaptive_constant_ics_val:2.5267e+02\n",
      "adaptive_constant_bcs_val: 2.5788e+02\n",
      "max_grad_res: 9.361e-07\n",
      "mean_grad_ics: 2.590e-04\n",
      "mean_grad_bcs: 1.191e-04\n",
      "It: 24300, Loss: 1.163e+02, Loss_res: 4.846e-05,  Loss_bcs: 3.584e-01, Loss_ut_ics: 8.215e-02,, Time: 3.14\n",
      "update_loss_res: 5.8421e-02\n",
      "update_loss_ics_u_t:1.5693e-01\n",
      "update_loss_bcs: 2.7846e+00\n",
      "adaptive_constant_res_val: 1.5584e+05\n",
      "adaptive_constant_ics_val:8.6327e+01\n",
      "adaptive_constant_bcs_val: 9.5027e+02\n",
      "max_grad_res: 3.749e-07\n",
      "mean_grad_ics: 2.062e-04\n",
      "mean_grad_bcs: 1.279e-04\n",
      "It: 24400, Loss: 3.535e+02, Loss_res: 2.457e-06,  Loss_bcs: 3.522e-01, Loss_ut_ics: 2.136e-01,, Time: 3.12\n",
      "update_loss_res: 6.7116e-02\n",
      "update_loss_ics_u_t:3.1889e-01\n",
      "update_loss_bcs: 2.6140e+00\n",
      "adaptive_constant_res_val: 2.8778e+06\n",
      "adaptive_constant_ics_val:4.1506e+03\n",
      "adaptive_constant_bcs_val: 5.1240e+03\n",
      "max_grad_res: 2.332e-08\n",
      "mean_grad_ics: 3.036e-04\n",
      "mean_grad_bcs: 4.572e-05\n",
      "It: 24500, Loss: 2.071e+03, Loss_res: 5.753e-07,  Loss_bcs: 3.809e-01, Loss_ut_ics: 2.817e-02,, Time: 3.12\n",
      "update_loss_res: 5.4583e-02\n",
      "update_loss_ics_u_t:9.2749e-02\n",
      "update_loss_bcs: 2.8527e+00\n",
      "adaptive_constant_res_val: 5.3600e+05\n",
      "adaptive_constant_ics_val:7.4823e+01\n",
      "adaptive_constant_bcs_val: 2.2269e+03\n",
      "max_grad_res: 1.018e-07\n",
      "mean_grad_ics: 8.215e-05\n",
      "mean_grad_bcs: 7.949e-05\n",
      "It: 24600, Loss: 7.673e+02, Loss_res: 1.883e-06,  Loss_bcs: 3.391e-01, Loss_ut_ics: 1.492e-01,, Time: 3.23\n",
      "update_loss_res: 7.0413e-02\n",
      "update_loss_ics_u_t:1.8748e-01\n",
      "update_loss_bcs: 2.7421e+00\n",
      "adaptive_constant_res_val: 1.5119e+06\n",
      "adaptive_constant_ics_val:8.2861e+02\n",
      "adaptive_constant_bcs_val: 1.5251e+03\n",
      "max_grad_res: 4.657e-08\n",
      "mean_grad_ics: 2.058e-04\n",
      "mean_grad_bcs: 2.590e-05\n",
      "It: 24700, Loss: 5.996e+02, Loss_res: 4.868e-07,  Loss_bcs: 3.655e-01, Loss_ut_ics: 5.012e-02,, Time: 3.18\n",
      "update_loss_res: 6.9610e-02\n",
      "update_loss_ics_u_t:1.5636e-01\n",
      "update_loss_bcs: 2.7740e+00\n",
      "adaptive_constant_res_val: 2.4110e+06\n",
      "adaptive_constant_ics_val:6.7167e+02\n",
      "adaptive_constant_bcs_val: 5.5653e+03\n",
      "max_grad_res: 2.887e-08\n",
      "mean_grad_ics: 1.240e-04\n",
      "mean_grad_bcs: 5.792e-05\n",
      "It: 24800, Loss: 1.681e+03, Loss_res: 4.477e-07,  Loss_bcs: 2.831e-01, Loss_ut_ics: 1.555e-01,, Time: 3.29\n",
      "update_loss_res: 7.2463e-02\n",
      "update_loss_ics_u_t:2.3320e-01\n",
      "update_loss_bcs: 2.6943e+00\n",
      "adaptive_constant_res_val: 1.6051e+06\n",
      "adaptive_constant_ics_val:1.2068e+03\n",
      "adaptive_constant_bcs_val: 7.0991e+02\n",
      "max_grad_res: 4.515e-08\n",
      "mean_grad_ics: 2.336e-04\n",
      "mean_grad_bcs: 1.190e-05\n",
      "It: 24900, Loss: 2.999e+02, Loss_res: 8.147e-07,  Loss_bcs: 3.899e-01, Loss_ut_ics: 1.811e-02,, Time: 3.19\n",
      "update_loss_res: 5.7357e-02\n",
      "update_loss_ics_u_t:9.3414e-02\n",
      "update_loss_bcs: 2.8492e+00\n",
      "adaptive_constant_res_val: 3.9084e+06\n",
      "adaptive_constant_ics_val:3.3472e+02\n",
      "adaptive_constant_bcs_val: 2.0972e+04\n",
      "max_grad_res: 1.468e-08\n",
      "mean_grad_ics: 5.258e-05\n",
      "mean_grad_bcs: 1.080e-04\n",
      "It: 25000, Loss: 7.127e+03, Loss_res: 2.563e-06,  Loss_bcs: 3.330e-01, Loss_ut_ics: 4.013e-01,, Time: 3.13\n",
      "update_loss_res: 6.0551e-02\n",
      "update_loss_ics_u_t:9.6766e-01\n",
      "update_loss_bcs: 1.9718e+00\n",
      "adaptive_constant_res_val: 2.2260e+05\n",
      "adaptive_constant_ics_val:1.8357e+03\n",
      "adaptive_constant_bcs_val: 5.7741e+01\n",
      "max_grad_res: 2.720e-07\n",
      "mean_grad_ics: 5.160e-04\n",
      "mean_grad_bcs: 7.966e-06\n",
      "It: 25100, Loss: 5.948e+01, Loss_res: 4.068e-05,  Loss_bcs: 4.421e-01, Loss_ut_ics: 1.356e-02,, Time: 3.24\n",
      "update_loss_res: 3.3059e-02\n",
      "update_loss_ics_u_t:6.7957e-02\n",
      "update_loss_bcs: 2.8990e+00\n",
      "adaptive_constant_res_val: 1.1066e+05\n",
      "adaptive_constant_ics_val:7.5390e+00\n",
      "adaptive_constant_bcs_val: 2.1146e+03\n",
      "max_grad_res: 2.987e-07\n",
      "mean_grad_ics: 3.314e-05\n",
      "mean_grad_bcs: 2.179e-04\n",
      "It: 25200, Loss: 7.265e+02, Loss_res: 8.680e-06,  Loss_bcs: 3.421e-01, Loss_ut_ics: 2.899e-01,, Time: 3.19\n",
      "update_loss_res: 6.4193e-02\n",
      "update_loss_ics_u_t:3.3195e-01\n",
      "update_loss_bcs: 2.6039e+00\n",
      "adaptive_constant_res_val: 1.0126e+06\n",
      "adaptive_constant_ics_val:1.8499e+03\n",
      "adaptive_constant_bcs_val: 5.6454e+02\n",
      "max_grad_res: 6.340e-08\n",
      "mean_grad_ics: 3.533e-04\n",
      "mean_grad_bcs: 1.374e-05\n",
      "It: 25300, Loss: 2.625e+02, Loss_res: 1.170e-06,  Loss_bcs: 4.115e-01, Loss_ut_ics: 1.567e-02,, Time: 3.12\n",
      "update_loss_res: 4.9226e-02\n",
      "update_loss_ics_u_t:8.7620e-02\n",
      "update_loss_bcs: 2.8632e+00\n",
      "adaptive_constant_res_val: 1.1174e+06\n",
      "adaptive_constant_ics_val:1.1300e+02\n",
      "adaptive_constant_bcs_val: 8.2627e+03\n",
      "max_grad_res: 4.405e-08\n",
      "mean_grad_ics: 5.681e-05\n",
      "mean_grad_bcs: 1.271e-04\n",
      "It: 25400, Loss: 2.776e+03, Loss_res: 1.461e-06,  Loss_bcs: 3.313e-01, Loss_ut_ics: 3.258e-01,, Time: 3.23\n",
      "update_loss_res: 6.5562e-02\n",
      "update_loss_ics_u_t:5.7700e-01\n",
      "update_loss_bcs: 2.3574e+00\n",
      "adaptive_constant_res_val: 2.3103e+06\n",
      "adaptive_constant_ics_val:7.6041e+03\n",
      "adaptive_constant_bcs_val: 8.1237e+02\n",
      "max_grad_res: 2.838e-08\n",
      "mean_grad_ics: 3.740e-04\n",
      "mean_grad_bcs: 9.779e-06\n",
      "It: 25500, Loss: 4.475e+02, Loss_res: 6.455e-07,  Loss_bcs: 4.827e-01, Loss_ut_ics: 7.080e-03,, Time: 3.22\n",
      "update_loss_res: 3.9033e-02\n",
      "update_loss_ics_u_t:5.2388e-02\n",
      "update_loss_bcs: 2.9086e+00\n",
      "adaptive_constant_res_val: 1.2047e+06\n",
      "adaptive_constant_ics_val:1.1395e+01\n",
      "adaptive_constant_bcs_val: 1.3618e+04\n",
      "max_grad_res: 3.240e-08\n",
      "mean_grad_ics: 7.048e-06\n",
      "mean_grad_bcs: 1.517e-04\n",
      "It: 25600, Loss: 3.846e+03, Loss_res: 2.750e-06,  Loss_bcs: 2.819e-01, Loss_ut_ics: 3.290e-01,, Time: 3.22\n",
      "update_loss_res: 6.2698e-02\n",
      "update_loss_ics_u_t:5.5261e-01\n",
      "update_loss_bcs: 2.3847e+00\n",
      "adaptive_constant_res_val: 5.8745e+05\n",
      "adaptive_constant_ics_val:1.7708e+03\n",
      "adaptive_constant_bcs_val: 2.5745e+02\n",
      "max_grad_res: 1.067e-07\n",
      "mean_grad_ics: 3.420e-04\n",
      "mean_grad_bcs: 1.152e-05\n",
      "It: 25700, Loss: 1.621e+02, Loss_res: 7.325e-06,  Loss_bcs: 5.171e-01, Loss_ut_ics: 1.392e-02,, Time: 3.34\n",
      "update_loss_res: 5.1396e-02\n",
      "update_loss_ics_u_t:1.1454e-01\n",
      "update_loss_bcs: 2.8341e+00\n",
      "adaptive_constant_res_val: 6.8408e+05\n",
      "adaptive_constant_ics_val:7.3322e+01\n",
      "adaptive_constant_bcs_val: 6.8384e+03\n",
      "max_grad_res: 7.513e-08\n",
      "mean_grad_ics: 4.809e-05\n",
      "mean_grad_bcs: 1.813e-04\n",
      "It: 25800, Loss: 2.508e+03, Loss_res: 3.836e-06,  Loss_bcs: 3.632e-01, Loss_ut_ics: 2.970e-01,, Time: 3.46\n",
      "update_loss_res: 6.8546e-02\n",
      "update_loss_ics_u_t:5.0794e-01\n",
      "update_loss_bcs: 2.4235e+00\n",
      "adaptive_constant_res_val: 5.8624e+05\n",
      "adaptive_constant_ics_val:1.3486e+03\n",
      "adaptive_constant_bcs_val: 3.2889e+02\n",
      "max_grad_res: 1.169e-07\n",
      "mean_grad_ics: 3.104e-04\n",
      "mean_grad_bcs: 1.587e-05\n",
      "It: 25900, Loss: 1.803e+02, Loss_res: 1.077e-05,  Loss_bcs: 4.652e-01, Loss_ut_ics: 1.559e-02,, Time: 3.43\n",
      "update_loss_res: 5.7007e-02\n",
      "update_loss_ics_u_t:1.4662e-01\n",
      "update_loss_bcs: 2.7964e+00\n",
      "adaptive_constant_res_val: 5.8927e+05\n",
      "adaptive_constant_ics_val:7.6965e+01\n",
      "adaptive_constant_bcs_val: 4.1927e+03\n",
      "max_grad_res: 9.674e-08\n",
      "mean_grad_ics: 5.078e-05\n",
      "mean_grad_bcs: 1.451e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define PINN model\n",
    "a = 0.5\n",
    "c = 2\n",
    "\n",
    "kernel_size = 300\n",
    "\n",
    "# Domain boundaries\n",
    "ics_coords = np.array([[0.0, 0.0],  [0.0, 1.0]])\n",
    "bc1_coords = np.array([[0.0, 0.0],  [1.0, 0.0]])\n",
    "bc2_coords = np.array([[0.0, 1.0],  [1.0, 1.0]])\n",
    "dom_coords = np.array([[0.0, 0.0],  [1.0, 1.0]])\n",
    "\n",
    "# Create initial conditions samplers\n",
    "ics_sampler = Sampler(2, ics_coords, lambda x: u(x, a, c), name='Initial Condition 1')\n",
    "\n",
    "# Create boundary conditions samplers\n",
    "bc1 = Sampler(2, bc1_coords, lambda x: u(x, a, c), name='Dirichlet BC1')\n",
    "bc2 = Sampler(2, bc2_coords, lambda x: u(x, a, c), name='Dirichlet BC2')\n",
    "bcs_sampler = [bc1, bc2]\n",
    "\n",
    "# Create residual sampler\n",
    "res_sampler = Sampler(2, dom_coords, lambda x: r(x, a, c), name='Forcing')\n",
    "\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 300\n",
    "\n",
    "\n",
    "\n",
    "# Define model\n",
    "mode = 'M2'\n",
    "layers = [2, 500, 500, 500, 1]\n",
    "\n",
    "\n",
    "nn = 200\n",
    "t = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
    "x = np.linspace(dom_coords[0, 1], dom_coords[1, 1], nn)[:, None]\n",
    "t, x = np.meshgrid(t, x)\n",
    "X_star = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "\n",
    "u_star = u(X_star, a,c)\n",
    "r_star = r(X_star, a, c)\n",
    "\n",
    "iterations = 1\n",
    "methods = [  \"mini_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_r_list = []\n",
    "\n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "     # Create residual sampler\n",
    "\n",
    "        # [elapsed, error_u , model] = test_method(mtd , layers,  ics_sampler, bcs_sampler, res_sampler, c ,kernel_size , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size )\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "            # sess.run(init)\n",
    "\n",
    "            model = PINN(layers, operator ,  ics_sampler, bcs_sampler, res_sampler, c , mode , sess)\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                print(\"full_batch method is used\")\n",
    "                model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                print(\"mini_batch method is used\")\n",
    "                model.trainmb(nIter, mbbatch_size)\n",
    "            else:\n",
    "                print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Predictions\n",
    "            u_pred = model.predict_u(X_star)\n",
    "            r_pred = model.predict_r(X_star)\n",
    "            # Predictions\n",
    "\n",
    "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "            error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "\n",
    " \n",
    "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "            model.print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "            model.plot_lambda()\n",
    "            # model.plot_grad()\n",
    "            model.save_NN()\n",
    "            model.plt_prediction( t , x , X_star , u_star , u_pred , r_star , r_pred)\n",
    "            model.print(\"average lambda_bc : \" , np.average(model.adaptive_constant_bcs_log))\n",
    "            model.print(\"average lambda_ic : \" , np.average(model.adaptive_constant_ics_log))\n",
    "            model.print(\"average lambda_res : \" , str(1.0))\n",
    "            # sess.close()  \n",
    "\n",
    "        # print('elapsed: {:.2e}'.format(elapsed))\n",
    "        # print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "        # print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_r_list.append(error_r)\n",
    "\n",
    "    model.print(\"\\n\\nMethod: \", mtd)\n",
    "    model.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    model.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    model.print(\"average of error_r_list:\" , sum(error_r_list) / len(error_r_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list,error_r_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_1Dwave_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(mbbatch_size)+\"_exp\"+str(bcbatch_size)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "###############################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0009259"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.mean(model.loss_history[\"loss_res\"][-99::]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
