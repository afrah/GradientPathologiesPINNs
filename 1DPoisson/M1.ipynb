{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OybZJApDYGsi",
        "outputId": "dc7cdffa-8613-42ee-86cc-0bc5000b2998"
      },
      "outputs": [],
      "source": [
        "# # Switch to tensorflow 1.x\n",
        "# %tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkCgnRiYQSY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
        "import numpy as np\n",
        "import timeit\n",
        "from scipy.interpolate import griddata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
        "import timeit\n",
        "\n",
        "import sys\n",
        "\n",
        "import scipy\n",
        "import scipy.io\n",
        "import time\n",
        "import logging\n",
        "\n",
        "import os.path\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-y7cHTcJfBTR"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "    # Initialize the class\n",
        "    def __init__(self, dim, coords, func, name=None):\n",
        "        self.dim = dim\n",
        "        self.coords = coords\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "\n",
        "    def sample(self, N):\n",
        "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
        "        y = self.func(x)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SDqDWN3nfSAg"
      },
      "outputs": [],
      "source": [
        "class PINN:\n",
        "    def __init__(self, layers, X_u, Y_u, X_r, Y_r ,mode ,  sess):\n",
        "\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        self.dirname, logpath = self.make_output_dir()\n",
        "        self.logger = self.get_logger(logpath)     \n",
        "\n",
        "        self.mu_X, self.sigma_X = X_r.mean(0), X_r.std(0)\n",
        "        self.mu_x, self.sigma_x = self.mu_X[0], self.sigma_X[0]\n",
        "\n",
        "        # Normalize\n",
        "        self.X_u = (X_u - self.mu_X) / self.sigma_X\n",
        "        self.Y_u = Y_u\n",
        "        self.X_r = (X_r - self.mu_X) / self.sigma_X\n",
        "        self.Y_r = Y_r\n",
        "\n",
        "        # Initialize network weights and biases\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "            \n",
        "        # Define the size of the Kernel\n",
        "        self.kernel_size = X_u.shape[0]\n",
        "        # Define Tensorflow session\n",
        "        self.sess = sess# tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
        "\n",
        "        self.lam_bc =  np.array(1.0)\n",
        "        self.lam_res =  np.array(1.0)\n",
        "        self.lam_res_tf = tf.placeholder(tf.float32, shape=self.lam_res.shape)\n",
        "        self.lam_bc_tf = tf.placeholder(tf.float32, shape=self.lam_bc.shape)\n",
        "\n",
        "        # Define placeholders and computational graph\n",
        "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.x_bc_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_bc_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        \n",
        "        self.x_u_ntk_tf = tf.placeholder(tf.float32, shape=(self.kernel_size, 1))\n",
        "        self.x_r_ntk_tf = tf.placeholder(tf.float32, shape=(self.kernel_size, 1))\n",
        "\n",
        "        self.lambda1 = tf.constant(1.0 , dtype=tf.float32)\n",
        "        self.lambda2 = tf.constant(500.0 , dtype=tf.float32 )\n",
        "\n",
        "        # Evaluate predictions\n",
        "        self.u_bc_pred = self.net_u(self.x_bc_tf)\n",
        "\n",
        "        self.u_pred = self.net_u(self.x_u_tf)\n",
        "        self.r_pred = self.net_r(self.x_r_tf)\n",
        "        \n",
        "        self.u_ntk_pred = self.net_u(self.x_u_ntk_tf)\n",
        "        self.r_ntk_pred = self.net_r(self.x_r_ntk_tf)\n",
        "     \n",
        "        # Boundary loss\n",
        "        self.loss_bcs = tf.reduce_mean(tf.square(self.u_bc_pred - self.u_bc_tf))\n",
        "\n",
        "        # Residual loss        \n",
        "        self.loss_res =  tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
        "        \n",
        "        # Total loss\n",
        "        self.loss = self.lam_res_tf * self.loss_res + self.lam_bc_tf *  self.loss_bcs\n",
        "\n",
        "        # Define optimizer with learning rate schedule\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 1e-5\n",
        "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
        "        # Passing global_step to minimize() will increment it at each step.\n",
        "        # To compute NTK, it is better to use SGD optimizer\n",
        "        # since the corresponding gradient flow is not exactly same.\n",
        "        # self.train_op = tf.train.GradientDescentOptimizer(starter_learning_rate).minimize(self.loss)\n",
        "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
        "\n",
        "\n",
        "        \n",
        "        # Compute the Jacobian for weights and biases in each hidden layer  \n",
        "        self.J_u = self.compute_jacobian(self.u_ntk_pred) \n",
        "        self.J_r = self.compute_jacobian(self.r_ntk_pred)\n",
        "        \n",
        "        # The empirical NTK = J J^T, compute NTK of PINNs \n",
        "        self.K_uu = self.compute_ntk(self.J_u, self.x_u_ntk_tf, self.J_u, self.x_u_ntk_tf)\n",
        "        self.K_ur = self.compute_ntk(self.J_u, self.x_u_ntk_tf, self.J_r, self.x_r_ntk_tf)\n",
        "        self.K_rr = self.compute_ntk(self.J_r, self.x_r_ntk_tf, self.J_r, self.x_r_ntk_tf)\n",
        "        \n",
        "        # Logger\n",
        "        # Loss logger\n",
        "        self.loss_bcs_log = []\n",
        "        self.loss_res_log = []\n",
        "\n",
        "        # NTK logger \n",
        "        self.K_uu_log = []\n",
        "        self.K_rr_log = []\n",
        "        self.K_ur_log = []\n",
        "        \n",
        "        # Weights logger \n",
        "        self.weights_log = []\n",
        "        self.biases_log = []\n",
        "       # Gradients Storage\n",
        "\n",
        "\n",
        "\n",
        "        # Generate dicts for gradients storage\n",
        "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
        "        self.dict_gradients_bc_layers = self.generate_grad_dict()\n",
        "\n",
        "        self.grad_res = []\n",
        "        self.grad_bc = []\n",
        "        self.grad_res_list = []\n",
        "        self.grad_bc_list = []\n",
        "\n",
        "        for i in range(len(self.layers)-1):\n",
        "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
        "            self.grad_bc.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
        "\n",
        "\n",
        "        self.adaptive_constant_bcs_log = []\n",
        "\n",
        "        self.mean_grad_res_list = []\n",
        "        self.mean_grad_bcs_list = []\n",
        "    \n",
        "        self.mean_grad_res_list_log = []\n",
        "        self.mean_grad_bcs_list_log = []\n",
        "\n",
        "        for i in range( len(self.layers) -1):\n",
        "            self.mean_grad_res_list.append(tf.math.reduce_mean(tf.abs(self.grad_res[i]))) \n",
        "            self.mean_grad_bcs_list.append(tf.math.reduce_mean(tf.abs(self.grad_bc[i])))\n",
        "        \n",
        "        self.mean_grad_res = tf.math.reduce_mean(tf.stack(self.mean_grad_res_list))\n",
        "        self.mean_grad_bcs = tf.math.reduce_mean(tf.stack(self.mean_grad_bcs_list))\n",
        "    \n",
        "\n",
        "        # for i in range(1 , len(self.layers) - 2):\n",
        "        #     self.grad_res_list.append(tf.reduce_mean(tf.abs(self.grad_bc[i])))\n",
        "        #     self.grad_bc_list.append(tf.reduce_mean(tf.abs(self.grad_res[i])))\n",
        "\n",
        "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bcs] \n",
        "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs\"] \n",
        "\n",
        "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
        "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
        "        \n",
        "\n",
        "        # Initialize Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        self.sess.run(init)\n",
        "        \n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "    def generate_grad_dict(self):\n",
        "        num = len(self.layers) - 1\n",
        "        grad_dict = {}\n",
        "        for i in range(num):\n",
        "            grad_dict['layer_{}'.format(i + 1)] = []\n",
        "        return grad_dict\n",
        "\n",
        "    # Xavier initialization\n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "        return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
        "                           dtype=tf.float32)\n",
        "    \n",
        "    # NTK initialization\n",
        "    def NTK_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        std = 1. / np.sqrt(in_dim)\n",
        "        return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * std,\n",
        "                           dtype=tf.float32)\n",
        "\n",
        "     # Initialize network weights and biases using Xavier initialization\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.NTK_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.random.normal([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    # Evaluates the forward pass\n",
        "    def forward_pass(self, H):\n",
        "        num_layers = len(self.layers)\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = self.weights[l]\n",
        "            b = self.biases[l]\n",
        "            H = tf.nn.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = self.weights[-1]\n",
        "        b = self.biases[-1]\n",
        "        H = tf.add(tf.matmul(H, W), b)\n",
        "        return H\n",
        "\n",
        "    # Evaluates the PDE solution\n",
        "    def net_u(self, x):\n",
        "        u = self.forward_pass(x)\n",
        "        return u\n",
        "\n",
        "    # Forward pass for the residual\n",
        "    def net_r(self, x):\n",
        "        u = self.net_u(x)\n",
        "\n",
        "        u_x = tf.gradients(u, x)[0] / self.sigma_x\n",
        "        u_xx = tf.gradients(u_x, x)[0] / self.sigma_x\n",
        "\n",
        "        res_u = u_xx\n",
        "        return res_u\n",
        "    \n",
        "    # Compute Jacobian for each weights and biases in each layer and retrun a list \n",
        "    def compute_jacobian(self, f):\n",
        "        J_list =[]\n",
        "        L = len(self.weights)    \n",
        "        for i in range(L):\n",
        "            J_w = jacobian(f, self.weights[i])\n",
        "            J_list.append(J_w)\n",
        "     \n",
        "        for i in range(L):\n",
        "            J_b = jacobian(f, self.biases[i])\n",
        "            J_list.append(J_b)\n",
        "        return J_list\n",
        "    \n",
        "    # Compute the empirical NTK = J J^T\n",
        "    def compute_ntk(self, J1_list, x1, J2_list, x2):\n",
        "        D = x1.shape[0]\n",
        "        N = len(J1_list)\n",
        "        \n",
        "        Ker = tf.zeros((D,D))\n",
        "        for k in range(N):\n",
        "            J1 = tf.reshape(J1_list[k], shape=(D,-1))\n",
        "            J2 = tf.reshape(J2_list[k], shape=(D,-1))\n",
        "            \n",
        "            K = tf.matmul(J1, tf.transpose(J2))\n",
        "            Ker = Ker + K\n",
        "        return Ker\n",
        "            \n",
        " \n",
        "\n",
        "    def lambda_balance(self  , term  ):\n",
        "        histoy_mean =  np.mean(self.loss_history[term])\n",
        "        m = 3 #len(self.loss_list)\n",
        "        num = np.exp(  np.mean(self.loss_history[term][-99::]) )#/(self.T * histoy_mean)) np.exp( )\n",
        "        denum = 0 \n",
        "        loss_list = [ \"loss_res\" , \"loss_bcs\"] \n",
        "\n",
        "        for  key in loss_list:\n",
        "            denum +=  np.exp(   np.mean(self.loss_history[key][-99::]) +1e-8)# /(self.T * histoy_mean))  np.exp(self.loss_history[key][-1] )\n",
        "        return m * (num / denum)\n",
        "    \n",
        "    # Trains the model by minimizing the MSE loss\n",
        "    def trainmb(self, nIter=10000, batch_size=128, log_NTK=True, log_weights=True):\n",
        "\n",
        "\n",
        "        itValues = [1,100,1000,39999]\n",
        "        start_time = timeit.default_timer()\n",
        "        for it in range(nIter):\n",
        "            # Fetch boundary mini-batches\n",
        "            # Define a dictionary for associating placeholders with data\n",
        "            tf_dict = {self.x_bc_tf: self.X_u, self.u_bc_tf: self.Y_u,\n",
        "                       self.x_u_tf: self.X_u, self.x_r_tf: self.X_r,\n",
        "                       self.r_tf: self.Y_r,\n",
        "                       self.lam_bc_tf : self.lam_bc,\n",
        "                       self.lam_res_tf : self.lam_res\n",
        "                       }\n",
        "        \n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "\n",
        "            # print(self.lam_bc_tf.shape)\n",
        "            _, batch_losses = self.sess.run([self.train_op, self.loss_tensor_list] ,tf_dict)\n",
        "            self.assign_batch_losses(batch_losses)\n",
        "            for key in self.loss_history:\n",
        "                self.loss_history[key].append(self.epoch_loss[key])\n",
        "            \n",
        "            # self.print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = timeit.default_timer() - start_time\n",
        "                [loss ,  loss_res,  loss_bcs]  = batch_losses\n",
        "\n",
        "\n",
        "                self.print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e ,Time: %.2f' %  (it, loss, loss_bcs, loss_res, elapsed))\n",
        "                \n",
        "\n",
        "                update_res = self.lambda_balance( \"loss_res\"  )\n",
        "                update_bcs = self.lambda_balance( \"loss_bcs\"  )\n",
        "                \n",
        "                self.print('update_res: {:.3e}'.format( update_res))\n",
        "                self.print('update_bcs1: {:.3e}'.format( update_bcs))\n",
        "            \n",
        "            # provide x, x' for NTK\n",
        "            if it % 100 == 0:\n",
        "                mean_grad_bcs , mean_grad_res = self.sess.run([self.mean_grad_bcs , self.mean_grad_res],  tf_dict)\n",
        "\n",
        "                self.lam_bc = 1.0 # #mean_grad_res / (mean_grad_bcs  +1e-8 )\n",
        "                self.lam_res =  1.0 #mean_grad_res\n",
        "\n",
        "                self.print(\"mean_grad_bcs: \" ,  mean_grad_bcs)    \n",
        "                self.print(\"mean_grad_res: \" , mean_grad_res)    \n",
        "                self.mean_grad_bcs_list_log.append(mean_grad_bcs)\n",
        "                self.mean_grad_res_list_log.append(mean_grad_res)\n",
        "\n",
        "                self.adaptive_constant_bcs_log.append(self.lam_bc)\n",
        "                self.print(\"Compute NTK...\")\n",
        "                tf_dict2 = {self.x_u_ntk_tf: self.X_u, \n",
        "                           self.x_r_ntk_tf: self.X_r\n",
        "                           }\n",
        "                K_uu_value, K_ur_value, K_rr_value = self.sess.run([self.K_uu,  self.K_ur,  self.K_rr], tf_dict2)\n",
        "                self.K_uu_log.append(K_uu_value)\n",
        "                self.K_ur_log.append(K_ur_value)\n",
        "                self.K_rr_log.append(K_rr_value)\n",
        "\n",
        "                start_time = timeit.default_timer()\n",
        "\n",
        "            if it in itValues:\n",
        "                    self.plot_layerLoss(tf_dict , it)\n",
        "                    self.print(\"Gradients information stored ...\")\n",
        "\n",
        "            sys.stdout.flush()\n",
        "  \n",
        "    # Evaluates predictions at test points\n",
        "    def predict_u(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.x_u_tf: X_star}\n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        return u_star\n",
        "\n",
        "    # Evaluates predictions at test points\n",
        "    def predict_r(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.x_r_tf: X_star}\n",
        "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
        "        return r_star\n",
        " ############################################################\n",
        "\n",
        "    def assign_batch_losses(self, batch_losses):\n",
        "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
        "            self.epoch_loss[key] = loss_values\n",
        "\n",
        "  ############################################################\n",
        "###################################################################################################################\n",
        "\n",
        "\n",
        "    def plot_layerLoss(self , tf_dict , epoch):\n",
        "        ## Gradients #\n",
        "        num_layers = len(self.layers)\n",
        "        for i in range(num_layers - 1):\n",
        "            grad_res, grad_bc  = self.sess.run([ self.grad_res[i],self.grad_bc[i]], feed_dict=tf_dict)\n",
        "\n",
        "            # save gradients of loss_r and loss_u\n",
        "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
        "            self.dict_gradients_bc_layers['layer_' + str(i + 1)].append(grad_bc.flatten())\n",
        "\n",
        "        num_hidden_layers = num_layers -1\n",
        "        cnt = 1\n",
        "        fig = plt.figure(4, figsize=(13, 4))\n",
        "        for j in range(num_hidden_layers):\n",
        "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
        "            ax.set_title('Layer {}'.format(j + 1))\n",
        "            ax.set_yscale('symlog')\n",
        "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
        "            gradients_bc = self.dict_gradients_bc_layers['layer_' + str(j + 1)][-1]\n",
        "\n",
        "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
        "\n",
        "            sns.distplot(gradients_bc, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc}}$')\n",
        "\n",
        "            #ax.get_legend().remove()\n",
        "            ax.set_xlim([-1.0, 1.0])\n",
        "            #ax.set_ylim([0, 150])\n",
        "            cnt += 1\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
        "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
        "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    # #########################\n",
        "    # def make_output_dir(self):\n",
        "        \n",
        "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
        "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
        "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "    #     os.mkdir(dirname)\n",
        "    #     text = 'output.log'\n",
        "    #     logpath = os.path.join(dirname, text)\n",
        "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
        "\n",
        "    #     return dirname, logpath\n",
        "    \n",
        "    # # ###########################################################\n",
        "    def make_output_dir(self):\n",
        "        \n",
        "        if not os.path.exists(\"checkpoints\"):\n",
        "            os.mkdir(\"checkpoints\")\n",
        "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "        os.mkdir(dirname)\n",
        "        text = 'output.log'\n",
        "        logpath = os.path.join(dirname, text)\n",
        "        shutil.copyfile('M1.ipynb', os.path.join(dirname, 'M1.ipynb'))\n",
        "        return dirname, logpath\n",
        "    \n",
        "\n",
        "    def get_logger(self, logpath):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "        sh = logging.StreamHandler()\n",
        "        sh.setLevel(logging.DEBUG)        \n",
        "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
        "        fh = logging.FileHandler(logpath)\n",
        "        logger.addHandler(sh)\n",
        "        logger.addHandler(fh)\n",
        "        return logger\n",
        "    \n",
        "    def print(self, *args):\n",
        "        for word in args:\n",
        "            if len(args) == 1:\n",
        "                self.logger.info(word)\n",
        "            elif word != args[-1]:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "            else:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\\n\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "\n",
        "\n",
        "    def plot_loss_history(self , path):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        for key in self.loss_history:\n",
        "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
        "            ax.semilogy(self.loss_history[key], label=key)\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        plt.savefig(path)\n",
        "        #plt.show()\n",
        "       #######################\n",
        "    def save_NN(self):\n",
        "\n",
        "        uv_weights = self.sess.run(self.weights)\n",
        "        uv_biases = self.sess.run(self.biases)\n",
        "\n",
        "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
        "            pickle.dump([uv_weights, uv_biases], f)\n",
        "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
        "\n",
        "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
        "        #     pickle.dump(self.loss_rec, f)\n",
        "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
        "            self.plot_loss_history(f)\n",
        "\n",
        "        return self.dirname\n",
        "    \n",
        "    def plot_ntk(self):\n",
        "        # Create empty lists for storing the eigenvalues of NTK\n",
        "        lambda_K_log = []\n",
        "        lambda_K_uu_log = []\n",
        "        lambda_K_ur_log = []\n",
        "        lambda_K_rr_log = []\n",
        "\n",
        "        # Restore the NTK\n",
        "        K_uu_list = self.K_uu_log\n",
        "        K_ur_list = self.K_ur_log\n",
        "        K_rr_list = self.K_rr_log\n",
        "        K_list = []\n",
        "            \n",
        "        for k in range(len(K_uu_list)):\n",
        "            K_uu = K_uu_list[k]\n",
        "            K_ur = K_ur_list[k]\n",
        "            K_rr = K_rr_list[k]\n",
        "            \n",
        "            K = np.concatenate([np.concatenate([K_uu, K_ur], axis = 1), np.concatenate([K_ur.T, K_rr], axis = 1)], axis = 0)\n",
        "            K_list.append(K)\n",
        "\n",
        "            # Compute eigenvalues\n",
        "            lambda_K, _ = np.linalg.eig(K)\n",
        "            lambda_K_uu, _ = np.linalg.eig(K_uu)\n",
        "            lambda_K_rr, _ = np.linalg.eig(K_rr)\n",
        "            \n",
        "            # Sort in descresing order\n",
        "            lambda_K = np.sort(np.real(lambda_K))[::-1]\n",
        "            lambda_K_uu = np.sort(np.real(lambda_K_uu))[::-1]\n",
        "            lambda_K_rr = np.sort(np.real(lambda_K_rr))[::-1]\n",
        "            \n",
        "            # Store eigenvalues\n",
        "            lambda_K_log.append(lambda_K)\n",
        "            lambda_K_uu_log.append(lambda_K_uu)\n",
        "            lambda_K_rr_log.append(lambda_K_rr)\n",
        "        fig = plt.figure(figsize=(18, 5))\n",
        "        plt.subplot(1,3,1)\n",
        "        for i in range(1, len(lambda_K_log), 10):\n",
        "            plt.plot(lambda_K_log[i], '--')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.title(r'Eigenvalues of ${K}$')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(1,3,2)\n",
        "        for i in range(1, len(lambda_K_uu_log), 10):\n",
        "            plt.plot(lambda_K_uu_log[i], '--')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.title(r'Eigenvalues of ${K}_{uu}$')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(1,3,3)\n",
        "        for i in range(1, len(lambda_K_log), 10):\n",
        "            plt.plot(lambda_K_rr_log[i], '--')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.title(r'Eigenvalues of ${K}_{rr}$')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.dirname,\"ntk.png\"))\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    def plt_prediction(self , X_star , u_star , u_pred):\n",
        "        fig = plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(X_star, u_star, label='Exact')\n",
        "        plt.plot(X_star, u_pred, '--', label='Predicted')\n",
        "        plt.xlabel('$x$')\n",
        "        plt.ylabel('$y$')\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(X_star, np.abs(u_star - u_pred), label='Error')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('$x$')\n",
        "        plt.ylabel('Point-wise error')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
        "        plt.close(\"all\")\n",
        "\n",
        "\n",
        "\n",
        "    def plot_grad(self):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        ax.semilogy(self.mean_grad_res_list_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{phy}}}$')\n",
        "        ax.semilogy(self.mean_grad_bcs_list_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
        "\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        path = os.path.join(self.dirname,'grad_history.png')\n",
        "        plt.savefig(path)\n",
        "\n",
        "\n",
        "         \n",
        "    \n",
        "    def plot_lambda(self ):\n",
        "\n",
        "        fontsize = 17\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([16,8])\n",
        "        ax.semilogy(self.mean_grad_bcs_list_log, label=r'$\\bar{\\nabla_\\theta {u_{bc}}}$' , color = 'tab:green')\n",
        "        ax.semilogy(self.mean_grad_res_list_log, label=r'$Max{\\nabla_\\theta {u_{phy}}}$' , color = 'tab:red')\n",
        "        ax.set_xlabel(\"epochs\", fontsize=fontsize)\n",
        "        ax.set_ylabel(r'$\\bar{\\nabla_\\theta {u}}$', fontsize=fontsize)\n",
        "        ax.tick_params(labelsize=fontsize)\n",
        "        ax.legend(loc='center left', bbox_to_anchor=(-0.25, 0.5))\n",
        "\n",
        "        ax2 = ax.twinx() \n",
        "\n",
        "        # fig, ax = plt.subplots()\n",
        "        # fig.set_size_inches([15,8])\n",
        "    \n",
        "        ax2.semilogy(self.adaptive_constant_bcs_log, label=r'$\\bar{\\lambda_{bc}}$'  ,  linestyle='dashed' , color = 'tab:green') \n",
        "        ax2.set_xlabel(\"epochs\", fontsize=fontsize)\n",
        "        ax2.set_ylabel(r'$\\bar{\\lambda}$', fontsize=fontsize)\n",
        "        ax2.tick_params(labelsize=fontsize)\n",
        "        ax2.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        path = os.path.join(self.dirname,'grad_history.png')\n",
        "        plt.savefig(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FN1jEdRwY90i"
      },
      "outputs": [],
      "source": [
        "# Define solution and its Laplace\n",
        "a = 4\n",
        "\n",
        "def u(x, a):\n",
        "  return np.sin(np.pi * a * x)\n",
        "\n",
        "def u_xx(x, a):\n",
        "  return -(np.pi * a)**2 * np.sin(np.pi * a * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
        "def test_method(method , layers,  X_u, Y_u, X_r, Y_r , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size , mode):\n",
        "\n",
        "\n",
        "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "        # sess.run(init)\n",
        "\n",
        "        model = PINN(layers, X_u, Y_u, X_r, Y_r , mode , sess)    \n",
        "\n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "\n",
        "        if method ==\"full_batch\":\n",
        "            print(\"full_batch method is used\")\n",
        "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "        elif method ==\"mini_batch\":\n",
        "            print(\"mini_batch method is used\")\n",
        "            model.trainmb(nIter, mbbatch_size)\n",
        "        else:\n",
        "            print(\"unknown method!\")\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Predictions\n",
        "        u_pred = model.predict_u(X_star)\n",
        "        r_pred = model.predict_r(X_star)\n",
        "        # Predictions\n",
        "\n",
        "        sess.close()   \n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "    error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
        "\n",
        "    print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "    print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "\n",
        "    return [elapsed, error_u , error_r ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method:  mini_batch\n",
            "Epoch:  1\n",
            "WARNING:tensorflow:From /tmp/ipykernel_41824/3727794588.py:52: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_41824/3727794588.py:53: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_41824/3727794588.py:54: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_41824/3727794588.py:54: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_41824/618971952.py:30: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_41824/618971952.py:70: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_41824/618971952.py:75: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-29 22:59:52.437077: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-29 22:59:52.460274: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
            "2023-12-29 22:59:52.460843: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556b4a426d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-12-29 22:59:52.460858: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-12-29 22:59:52.461522: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_41824/618971952.py:147: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "mini_batch method is used\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "It: 0, Loss: 1.207e+04, Loss_bcs: 6.737e-01, Loss_res: 1.207e+04 ,Time: 0.54\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:248: RuntimeWarning: overflow encountered in exp\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:253: RuntimeWarning: overflow encountered in exp\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:254: RuntimeWarning: invalid value encountered in true_divide\n",
            "update_res: nan\n",
            "update_bcs1: 0.000e+00\n",
            "mean_grad_bcs: 2.5661e-01\n",
            "mean_grad_res: 1.3070e+02\n",
            "Compute NTK...\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 100, Loss: 8.535e+03, Loss_bcs: 3.322e-01, Loss_res: 8.535e+03 ,Time: 3.03\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:248: RuntimeWarning: overflow encountered in exp\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:253: RuntimeWarning: overflow encountered in exp\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:254: RuntimeWarning: invalid value encountered in true_divide\n",
            "update_res: nan\n",
            "update_bcs1: 0.000e+00\n",
            "mean_grad_bcs: 1.8697e-01\n",
            "mean_grad_res: 1.1538e+02\n",
            "Compute NTK...\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 200, Loss: 5.747e+03, Loss_bcs: 5.465e-01, Loss_res: 5.747e+03 ,Time: 1.91\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:248: RuntimeWarning: overflow encountered in exp\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:253: RuntimeWarning: overflow encountered in exp\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:254: RuntimeWarning: invalid value encountered in true_divide\n",
            "update_res: nan\n",
            "update_bcs1: 0.000e+00\n",
            "mean_grad_bcs: 2.3185e-01\n",
            "mean_grad_res: 1.1147e+02\n",
            "Compute NTK...\n",
            "It: 300, Loss: 3.758e+03, Loss_bcs: 2.142e+00, Loss_res: 3.755e+03 ,Time: 0.51\n",
            "update_res: nan\n",
            "update_bcs1: 0.000e+00\n",
            "mean_grad_bcs: 4.5717e-01\n",
            "mean_grad_res: 1.0123e+02\n",
            "Compute NTK...\n",
            "It: 400, Loss: 2.509e+03, Loss_bcs: 5.362e+00, Loss_res: 2.503e+03 ,Time: 0.51\n",
            "update_res: nan\n",
            "update_bcs1: 0.000e+00\n",
            "mean_grad_bcs: 7.8342e-01\n",
            "mean_grad_res: 8.2364e+01\n",
            "Compute NTK...\n",
            "It: 500, Loss: 1.785e+03, Loss_bcs: 9.302e+00, Loss_res: 1.776e+03 ,Time: 0.62\n",
            "update_res: nan\n",
            "update_bcs1: 0.000e+00\n",
            "mean_grad_bcs: 1.0956e+00\n",
            "mean_grad_res: 6.7598e+01\n",
            "Compute NTK...\n",
            "It: 600, Loss: 1.296e+03, Loss_bcs: 1.340e+01, Loss_res: 1.283e+03 ,Time: 0.64\n",
            "update_res: nan\n",
            "update_bcs1: 0.000e+00\n",
            "mean_grad_bcs: 1.3624e+00\n",
            "mean_grad_res: 5.9695e+01\n",
            "Compute NTK...\n",
            "It: 700, Loss: 9.406e+02, Loss_bcs: 1.719e+01, Loss_res: 9.234e+02 ,Time: 0.56\n",
            "update_res: nan\n",
            "update_bcs1: 0.000e+00\n",
            "mean_grad_bcs: 1.5809e+00\n",
            "mean_grad_res: 5.1717e+01\n",
            "Compute NTK...\n",
            "It: 800, Loss: 6.849e+02, Loss_bcs: 2.053e+01, Loss_res: 6.643e+02 ,Time: 0.63\n",
            "update_res: nan\n",
            "update_bcs1: 0.000e+00\n",
            "mean_grad_bcs: 1.7607e+00\n",
            "mean_grad_res: 4.3348e+01\n",
            "Compute NTK...\n",
            "It: 900, Loss: 5.087e+02, Loss_bcs: 2.333e+01, Loss_res: 4.854e+02 ,Time: 0.67\n",
            "update_res: inf\n",
            "update_bcs1: 5.212e-237\n",
            "mean_grad_bcs: 1.9069e+00\n",
            "mean_grad_res: 3.5198e+01\n",
            "Compute NTK...\n",
            "It: 1000, Loss: 3.913e+02, Loss_bcs: 2.544e+01, Loss_res: 3.658e+02 ,Time: 0.66\n",
            "update_res: inf\n",
            "update_bcs1: 3.810e-172\n",
            "mean_grad_bcs: 2.0188e+00\n",
            "mean_grad_res: 2.7878e+01\n",
            "Compute NTK...\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 1100, Loss: 3.131e+02, Loss_bcs: 2.680e+01, Loss_res: 2.863e+02 ,Time: 1.83\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/ipykernel_launcher.py:248: RuntimeWarning: overflow encountered in exp\n",
            "update_res: inf\n",
            "update_bcs1: 5.314e-129\n",
            "mean_grad_bcs: 2.0955e+00\n",
            "mean_grad_res: 2.1866e+01\n",
            "Compute NTK...\n",
            "It: 1200, Loss: 2.593e+02, Loss_bcs: 2.747e+01, Loss_res: 2.318e+02 ,Time: 0.51\n",
            "update_res: inf\n",
            "update_bcs1: 5.071e-100\n",
            "mean_grad_bcs: 2.1414e+00\n",
            "mean_grad_res: 1.7197e+01\n",
            "Compute NTK...\n",
            "It: 1300, Loss: 2.202e+02, Loss_bcs: 2.755e+01, Loss_res: 1.927e+02 ,Time: 0.53\n",
            "update_res: inf\n",
            "update_bcs1: 7.383e-80\n",
            "mean_grad_bcs: 2.1615e+00\n",
            "mean_grad_res: 1.3681e+01\n",
            "Compute NTK...\n",
            "It: 1400, Loss: 1.905e+02, Loss_bcs: 2.719e+01, Loss_res: 1.633e+02 ,Time: 0.51\n",
            "update_res: inf\n",
            "update_bcs1: 3.010e-65\n",
            "mean_grad_bcs: 2.1609e+00\n",
            "mean_grad_res: 1.1048e+01\n",
            "Compute NTK...\n",
            "It: 1500, Loss: 1.669e+02, Loss_bcs: 2.649e+01, Loss_res: 1.405e+02 ,Time: 0.50\n",
            "update_res: inf\n",
            "update_bcs1: 2.904e-54\n",
            "mean_grad_bcs: 2.1443e+00\n",
            "mean_grad_res: 9.0667e+00\n",
            "Compute NTK...\n",
            "It: 1600, Loss: 1.475e+02, Loss_bcs: 2.555e+01, Loss_res: 1.219e+02 ,Time: 0.62\n",
            "update_res: inf\n",
            "update_bcs1: 1.029e-45\n",
            "mean_grad_bcs: 2.1165e+00\n",
            "mean_grad_res: 7.5622e+00\n",
            "Compute NTK...\n",
            "It: 1700, Loss: 1.310e+02, Loss_bcs: 2.446e+01, Loss_res: 1.065e+02 ,Time: 0.54\n",
            "update_res: inf\n",
            "update_bcs1: 7.791e-39\n",
            "mean_grad_bcs: 2.0810e+00\n",
            "mean_grad_res: 6.4089e+00\n",
            "Compute NTK...\n",
            "It: 1800, Loss: 1.165e+02, Loss_bcs: 2.327e+01, Loss_res: 9.322e+01 ,Time: 0.52\n",
            "update_res: inf\n",
            "update_bcs1: 3.967e-33\n",
            "mean_grad_bcs: 2.0395e+00\n",
            "mean_grad_res: 5.5104e+00\n",
            "Compute NTK...\n",
            "It: 1900, Loss: 1.036e+02, Loss_bcs: 2.202e+01, Loss_res: 8.159e+01 ,Time: 0.50\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 2.837e-28\n",
            "mean_grad_bcs: 1.9926e+00\n",
            "mean_grad_res: 4.7935e+00\n",
            "Compute NTK...\n",
            "It: 2000, Loss: 9.201e+01, Loss_bcs: 2.076e+01, Loss_res: 7.125e+01 ,Time: 0.51\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 4.567e-24\n",
            "mean_grad_bcs: 1.9410e+00\n",
            "mean_grad_res: 4.2144e+00\n",
            "Compute NTK...\n",
            "It: 2100, Loss: 8.149e+01, Loss_bcs: 1.949e+01, Loss_res: 6.200e+01 ,Time: 0.51\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 2.245e-20\n",
            "mean_grad_bcs: 1.8857e+00\n",
            "mean_grad_res: 3.7338e+00\n",
            "Compute NTK...\n",
            "It: 2200, Loss: 7.195e+01, Loss_bcs: 1.824e+01, Loss_res: 5.371e+01 ,Time: 0.54\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 4.047e-17\n",
            "mean_grad_bcs: 1.8283e+00\n",
            "mean_grad_res: 3.3235e+00\n",
            "Compute NTK...\n",
            "It: 2300, Loss: 6.333e+01, Loss_bcs: 1.701e+01, Loss_res: 4.632e+01 ,Time: 0.51\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 2.941e-14\n",
            "mean_grad_bcs: 1.7703e+00\n",
            "mean_grad_res: 2.9637e+00\n",
            "Compute NTK...\n",
            "It: 2400, Loss: 5.560e+01, Loss_bcs: 1.581e+01, Loss_res: 3.979e+01 ,Time: 0.44\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 9.093e-12\n",
            "mean_grad_bcs: 1.7117e+00\n",
            "mean_grad_res: 2.6400e+00\n",
            "Compute NTK...\n",
            "It: 2500, Loss: 4.863e+01, Loss_bcs: 1.460e+01, Loss_res: 3.403e+01 ,Time: 0.49\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 1.255e-09\n",
            "mean_grad_bcs: 1.6503e+00\n",
            "mean_grad_res: 2.3535e+00\n",
            "Compute NTK...\n",
            "It: 2600, Loss: 4.249e+01, Loss_bcs: 1.345e+01, Loss_res: 2.904e+01 ,Time: 0.48\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 8.230e-08\n",
            "mean_grad_bcs: 1.5887e+00\n",
            "mean_grad_res: 2.1194e+00\n",
            "Compute NTK...\n",
            "It: 2700, Loss: 3.722e+01, Loss_bcs: 1.243e+01, Loss_res: 2.479e+01 ,Time: 0.44\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 2.764e-06\n",
            "mean_grad_bcs: 1.5305e+00\n",
            "mean_grad_res: 1.8745e+00\n",
            "Compute NTK...\n",
            "It: 2800, Loss: 3.264e+01, Loss_bcs: 1.146e+01, Loss_res: 2.117e+01 ,Time: 0.47\n",
            "update_res: 3.000e+00\n",
            "update_bcs1: 5.187e-05\n",
            "mean_grad_bcs: 1.4726e+00\n",
            "mean_grad_res: 1.6633e+00\n",
            "Compute NTK...\n",
            "It: 2900, Loss: 2.865e+01, Loss_bcs: 1.056e+01, Loss_res: 1.809e+01 ,Time: 0.46\n",
            "update_res: 2.999e+00\n",
            "update_bcs1: 5.753e-04\n",
            "mean_grad_bcs: 1.4155e+00\n",
            "mean_grad_res: 1.4810e+00\n",
            "Compute NTK...\n",
            "It: 3000, Loss: 2.519e+01, Loss_bcs: 9.712e+00, Loss_res: 1.548e+01 ,Time: 0.52\n",
            "update_res: 2.996e+00\n",
            "update_bcs1: 4.083e-03\n",
            "mean_grad_bcs: 1.3595e+00\n",
            "mean_grad_res: 1.3246e+00\n",
            "Compute NTK...\n",
            "It: 3100, Loss: 2.218e+01, Loss_bcs: 8.919e+00, Loss_res: 1.326e+01 ,Time: 0.45\n",
            "update_res: 2.980e+00\n",
            "update_bcs1: 1.987e-02\n",
            "mean_grad_bcs: 1.3047e+00\n",
            "mean_grad_res: 1.1894e+00\n",
            "Compute NTK...\n",
            "It: 3200, Loss: 1.956e+01, Loss_bcs: 8.181e+00, Loss_res: 1.138e+01 ,Time: 0.48\n",
            "update_res: 2.930e+00\n",
            "update_bcs1: 6.996e-02\n",
            "mean_grad_bcs: 1.2513e+00\n",
            "mean_grad_res: 1.0712e+00\n",
            "Compute NTK...\n",
            "It: 3300, Loss: 1.728e+01, Loss_bcs: 7.494e+00, Loss_res: 9.785e+00 ,Time: 0.46\n",
            "update_res: 2.814e+00\n",
            "update_bcs1: 1.859e-01\n",
            "mean_grad_bcs: 1.1994e+00\n",
            "mean_grad_res: 9.6833e-01\n",
            "Compute NTK...\n",
            "It: 3400, Loss: 1.529e+01, Loss_bcs: 6.858e+00, Loss_res: 8.431e+00 ,Time: 0.46\n",
            "update_res: 2.613e+00\n",
            "update_bcs1: 3.868e-01\n",
            "mean_grad_bcs: 1.1488e+00\n",
            "mean_grad_res: 8.7884e-01\n",
            "Compute NTK...\n",
            "It: 3500, Loss: 1.354e+01, Loss_bcs: 6.268e+00, Loss_res: 7.277e+00 ,Time: 0.44\n",
            "update_res: 2.344e+00\n",
            "update_bcs1: 6.559e-01\n",
            "mean_grad_bcs: 1.0995e+00\n",
            "mean_grad_res: 8.0063e-01\n",
            "Compute NTK...\n",
            "It: 3600, Loss: 1.201e+01, Loss_bcs: 5.723e+00, Loss_res: 6.292e+00 ,Time: 0.46\n",
            "update_res: 2.054e+00\n",
            "update_bcs1: 9.463e-01\n",
            "mean_grad_bcs: 1.0518e+00\n",
            "mean_grad_res: 7.3152e-01\n",
            "Compute NTK...\n",
            "It: 3700, Loss: 1.067e+01, Loss_bcs: 5.218e+00, Loss_res: 5.450e+00 ,Time: 0.45\n",
            "update_res: 1.788e+00\n",
            "update_bcs1: 1.212e+00\n",
            "mean_grad_bcs: 1.0054e+00\n",
            "mean_grad_res: 6.7035e-01\n",
            "Compute NTK...\n",
            "It: 3800, Loss: 9.483e+00, Loss_bcs: 4.752e+00, Loss_res: 4.731e+00 ,Time: 0.58\n",
            "update_res: 1.573e+00\n",
            "update_bcs1: 1.427e+00\n",
            "mean_grad_bcs: 9.6017e-01\n",
            "mean_grad_res: 6.1629e-01\n",
            "Compute NTK...\n",
            "It: 3900, Loss: 8.437e+00, Loss_bcs: 4.320e+00, Loss_res: 4.117e+00 ,Time: 0.57\n",
            "update_res: 1.411e+00\n",
            "update_bcs1: 1.589e+00\n",
            "mean_grad_bcs: 9.1619e-01\n",
            "mean_grad_res: 5.6766e-01\n",
            "Compute NTK...\n",
            "It: 4000, Loss: 7.511e+00, Loss_bcs: 3.920e+00, Loss_res: 3.591e+00 ,Time: 0.52\n",
            "update_res: 1.297e+00\n",
            "update_bcs1: 1.703e+00\n",
            "mean_grad_bcs: 8.7352e-01\n",
            "mean_grad_res: 5.2401e-01\n",
            "Compute NTK...\n",
            "It: 4100, Loss: 6.690e+00, Loss_bcs: 3.551e+00, Loss_res: 3.139e+00 ,Time: 0.52\n",
            "update_res: 1.222e+00\n",
            "update_bcs1: 1.778e+00\n",
            "mean_grad_bcs: 8.3207e-01\n",
            "mean_grad_res: 4.8453e-01\n",
            "Compute NTK...\n",
            "It: 4200, Loss: 5.960e+00, Loss_bcs: 3.208e+00, Loss_res: 2.751e+00 ,Time: 0.51\n",
            "update_res: 1.177e+00\n",
            "update_bcs1: 1.823e+00\n",
            "mean_grad_bcs: 7.9172e-01\n",
            "mean_grad_res: 4.4863e-01\n",
            "Compute NTK...\n",
            "It: 4300, Loss: 5.308e+00, Loss_bcs: 2.891e+00, Loss_res: 2.417e+00 ,Time: 0.51\n",
            "update_res: 1.155e+00\n",
            "update_bcs1: 1.845e+00\n",
            "mean_grad_bcs: 7.5252e-01\n",
            "mean_grad_res: 4.1630e-01\n",
            "Compute NTK...\n",
            "It: 4400, Loss: 4.723e+00, Loss_bcs: 2.596e+00, Loss_res: 2.127e+00 ,Time: 0.51\n",
            "update_res: 1.152e+00\n",
            "update_bcs1: 1.848e+00\n",
            "mean_grad_bcs: 7.1424e-01\n",
            "mean_grad_res: 3.8792e-01\n",
            "Compute NTK...\n",
            "It: 4500, Loss: 4.201e+00, Loss_bcs: 2.324e+00, Loss_res: 1.877e+00 ,Time: 0.52\n",
            "update_res: 1.162e+00\n",
            "update_bcs1: 1.838e+00\n",
            "mean_grad_bcs: 6.7693e-01\n",
            "mean_grad_res: 3.6269e-01\n",
            "Compute NTK...\n",
            "It: 4600, Loss: 3.738e+00, Loss_bcs: 2.076e+00, Loss_res: 1.662e+00 ,Time: 0.51\n",
            "update_res: 1.182e+00\n",
            "update_bcs1: 1.818e+00\n",
            "mean_grad_bcs: 6.4062e-01\n",
            "mean_grad_res: 3.3876e-01\n",
            "Compute NTK...\n",
            "It: 4700, Loss: 3.325e+00, Loss_bcs: 1.850e+00, Loss_res: 1.476e+00 ,Time: 0.51\n",
            "update_res: 1.208e+00\n",
            "update_bcs1: 1.792e+00\n",
            "mean_grad_bcs: 6.0542e-01\n",
            "mean_grad_res: 3.1683e-01\n",
            "Compute NTK...\n",
            "It: 4800, Loss: 2.958e+00, Loss_bcs: 1.643e+00, Loss_res: 1.315e+00 ,Time: 0.54\n",
            "update_res: 1.240e+00\n",
            "update_bcs1: 1.760e+00\n",
            "mean_grad_bcs: 5.7126e-01\n",
            "mean_grad_res: 2.9655e-01\n",
            "Compute NTK...\n",
            "It: 4900, Loss: 2.631e+00, Loss_bcs: 1.455e+00, Loss_res: 1.176e+00 ,Time: 0.53\n",
            "update_res: 1.275e+00\n",
            "update_bcs1: 1.725e+00\n",
            "mean_grad_bcs: 5.3812e-01\n",
            "mean_grad_res: 2.7808e-01\n",
            "Compute NTK...\n",
            "It: 5000, Loss: 2.340e+00, Loss_bcs: 1.283e+00, Loss_res: 1.057e+00 ,Time: 0.51\n",
            "update_res: 1.312e+00\n",
            "update_bcs1: 1.688e+00\n",
            "mean_grad_bcs: 5.0596e-01\n",
            "mean_grad_res: 2.6079e-01\n",
            "Compute NTK...\n",
            "It: 5100, Loss: 2.081e+00, Loss_bcs: 1.128e+00, Loss_res: 9.527e-01 ,Time: 0.51\n",
            "update_res: 1.350e+00\n",
            "update_bcs1: 1.650e+00\n",
            "mean_grad_bcs: 4.7479e-01\n",
            "mean_grad_res: 2.4443e-01\n",
            "Compute NTK...\n",
            "It: 5200, Loss: 1.850e+00, Loss_bcs: 9.877e-01, Loss_res: 8.626e-01 ,Time: 0.50\n",
            "update_res: 1.388e+00\n",
            "update_bcs1: 1.612e+00\n",
            "mean_grad_bcs: 4.4461e-01\n",
            "mean_grad_res: 2.2877e-01\n",
            "Compute NTK...\n",
            "It: 5300, Loss: 1.645e+00, Loss_bcs: 8.608e-01, Loss_res: 7.840e-01 ,Time: 0.51\n",
            "update_res: 1.425e+00\n",
            "update_bcs1: 1.575e+00\n",
            "mean_grad_bcs: 4.1548e-01\n",
            "mean_grad_res: 2.1364e-01\n",
            "Compute NTK...\n",
            "It: 5400, Loss: 1.462e+00, Loss_bcs: 7.464e-01, Loss_res: 7.152e-01 ,Time: 0.51\n",
            "update_res: 1.460e+00\n",
            "update_bcs1: 1.540e+00\n",
            "mean_grad_bcs: 3.8727e-01\n",
            "mean_grad_res: 1.9856e-01\n",
            "Compute NTK...\n",
            "It: 5500, Loss: 1.297e+00, Loss_bcs: 6.434e-01, Loss_res: 6.540e-01 ,Time: 0.55\n",
            "update_res: 1.493e+00\n",
            "update_bcs1: 1.507e+00\n",
            "mean_grad_bcs: 3.5993e-01\n",
            "mean_grad_res: 1.8299e-01\n",
            "Compute NTK...\n",
            "It: 5600, Loss: 1.148e+00, Loss_bcs: 5.509e-01, Loss_res: 5.972e-01 ,Time: 0.51\n",
            "update_res: 1.522e+00\n",
            "update_bcs1: 1.478e+00\n",
            "mean_grad_bcs: 3.3342e-01\n",
            "mean_grad_res: 1.6617e-01\n",
            "Compute NTK...\n",
            "It: 5700, Loss: 1.008e+00, Loss_bcs: 4.678e-01, Loss_res: 5.405e-01 ,Time: 0.51\n",
            "update_res: 1.546e+00\n",
            "update_bcs1: 1.454e+00\n",
            "mean_grad_bcs: 3.0762e-01\n",
            "mean_grad_res: 1.4861e-01\n",
            "Compute NTK...\n",
            "It: 5800, Loss: 8.797e-01, Loss_bcs: 3.939e-01, Loss_res: 4.858e-01 ,Time: 0.51\n",
            "update_res: 1.562e+00\n",
            "update_bcs1: 1.438e+00\n",
            "mean_grad_bcs: 2.8263e-01\n",
            "mean_grad_res: 1.3605e-01\n",
            "Compute NTK...\n",
            "It: 5900, Loss: 7.696e-01, Loss_bcs: 3.297e-01, Loss_res: 4.399e-01 ,Time: 0.52\n",
            "update_res: 1.576e+00\n",
            "update_bcs1: 1.424e+00\n",
            "mean_grad_bcs: 2.5884e-01\n",
            "mean_grad_res: 1.2815e-01\n",
            "Compute NTK...\n",
            "It: 6000, Loss: 6.773e-01, Loss_bcs: 2.751e-01, Loss_res: 4.023e-01 ,Time: 0.52\n",
            "update_res: 1.589e+00\n",
            "update_bcs1: 1.411e+00\n",
            "mean_grad_bcs: 2.3657e-01\n",
            "mean_grad_res: 1.2137e-01\n",
            "Compute NTK...\n",
            "It: 6100, Loss: 5.994e-01, Loss_bcs: 2.288e-01, Loss_res: 3.707e-01 ,Time: 0.54\n",
            "update_res: 1.601e+00\n",
            "update_bcs1: 1.399e+00\n",
            "mean_grad_bcs: 2.1585e-01\n",
            "mean_grad_res: 1.1438e-01\n",
            "Compute NTK...\n",
            "It: 6200, Loss: 5.331e-01, Loss_bcs: 1.896e-01, Loss_res: 3.435e-01 ,Time: 0.52\n",
            "update_res: 1.611e+00\n",
            "update_bcs1: 1.389e+00\n",
            "mean_grad_bcs: 1.9658e-01\n",
            "mean_grad_res: 1.0692e-01\n",
            "Compute NTK...\n",
            "It: 6300, Loss: 4.759e-01, Loss_bcs: 1.564e-01, Loss_res: 3.195e-01 ,Time: 0.51\n",
            "update_res: 1.619e+00\n",
            "update_bcs1: 1.381e+00\n",
            "mean_grad_bcs: 1.7856e-01\n",
            "mean_grad_res: 9.9198e-02\n",
            "Compute NTK...\n",
            "It: 6400, Loss: 4.262e-01, Loss_bcs: 1.281e-01, Loss_res: 2.981e-01 ,Time: 0.52\n",
            "update_res: 1.625e+00\n",
            "update_bcs1: 1.375e+00\n",
            "mean_grad_bcs: 1.6169e-01\n",
            "mean_grad_res: 9.1644e-02\n",
            "Compute NTK...\n",
            "It: 6500, Loss: 3.831e-01, Loss_bcs: 1.042e-01, Loss_res: 2.790e-01 ,Time: 0.51\n",
            "update_res: 1.629e+00\n",
            "update_bcs1: 1.371e+00\n",
            "mean_grad_bcs: 1.4586e-01\n",
            "mean_grad_res: 8.4473e-02\n",
            "Compute NTK...\n",
            "It: 6600, Loss: 3.458e-01, Loss_bcs: 8.399e-02, Loss_res: 2.618e-01 ,Time: 0.51\n",
            "update_res: 1.632e+00\n",
            "update_bcs1: 1.368e+00\n",
            "mean_grad_bcs: 1.3103e-01\n",
            "mean_grad_res: 7.7748e-02\n",
            "Compute NTK...\n",
            "It: 6700, Loss: 3.135e-01, Loss_bcs: 6.709e-02, Loss_res: 2.464e-01 ,Time: 0.53\n",
            "update_res: 1.634e+00\n",
            "update_bcs1: 1.366e+00\n",
            "mean_grad_bcs: 1.1715e-01\n",
            "mean_grad_res: 7.1441e-02\n",
            "Compute NTK...\n",
            "It: 6800, Loss: 2.855e-01, Loss_bcs: 5.303e-02, Loss_res: 2.325e-01 ,Time: 0.52\n",
            "update_res: 1.634e+00\n",
            "update_bcs1: 1.366e+00\n",
            "mean_grad_bcs: 1.0419e-01\n",
            "mean_grad_res: 6.5511e-02\n",
            "Compute NTK...\n",
            "It: 6900, Loss: 2.614e-01, Loss_bcs: 4.143e-02, Loss_res: 2.200e-01 ,Time: 0.51\n",
            "update_res: 1.634e+00\n",
            "update_bcs1: 1.366e+00\n",
            "mean_grad_bcs: 9.2114e-02\n",
            "mean_grad_res: 5.9929e-02\n",
            "Compute NTK...\n",
            "It: 7000, Loss: 2.406e-01, Loss_bcs: 3.194e-02, Loss_res: 2.086e-01 ,Time: 0.51\n",
            "update_res: 1.633e+00\n",
            "update_bcs1: 1.367e+00\n",
            "mean_grad_bcs: 8.0893e-02\n",
            "mean_grad_res: 5.4702e-02\n",
            "Compute NTK...\n",
            "It: 7100, Loss: 2.225e-01, Loss_bcs: 2.424e-02, Loss_res: 1.983e-01 ,Time: 0.51\n",
            "update_res: 1.631e+00\n",
            "update_bcs1: 1.369e+00\n",
            "mean_grad_bcs: 7.0494e-02\n",
            "mean_grad_res: 4.9811e-02\n",
            "Compute NTK...\n",
            "It: 7200, Loss: 2.069e-01, Loss_bcs: 1.808e-02, Loss_res: 1.888e-01 ,Time: 0.52\n",
            "update_res: 1.629e+00\n",
            "update_bcs1: 1.371e+00\n",
            "mean_grad_bcs: 6.0887e-02\n",
            "mean_grad_res: 4.5306e-02\n",
            "Compute NTK...\n",
            "It: 7300, Loss: 1.932e-01, Loss_bcs: 1.321e-02, Loss_res: 1.800e-01 ,Time: 0.54\n",
            "update_res: 1.626e+00\n",
            "update_bcs1: 1.374e+00\n",
            "mean_grad_bcs: 5.2046e-02\n",
            "mean_grad_res: 4.1143e-02\n",
            "Compute NTK...\n",
            "It: 7400, Loss: 1.814e-01, Loss_bcs: 9.413e-03, Loss_res: 1.719e-01 ,Time: 0.53\n",
            "update_res: 1.623e+00\n",
            "update_bcs1: 1.377e+00\n",
            "mean_grad_bcs: 4.3942e-02\n",
            "mean_grad_res: 3.7315e-02\n",
            "Compute NTK...\n",
            "It: 7500, Loss: 1.709e-01, Loss_bcs: 6.511e-03, Loss_res: 1.644e-01 ,Time: 0.52\n",
            "update_res: 1.620e+00\n",
            "update_bcs1: 1.380e+00\n",
            "mean_grad_bcs: 3.6547e-02\n",
            "mean_grad_res: 3.3819e-02\n",
            "Compute NTK...\n",
            "It: 7600, Loss: 1.618e-01, Loss_bcs: 4.337e-03, Loss_res: 1.574e-01 ,Time: 0.51\n",
            "update_res: 1.616e+00\n",
            "update_bcs1: 1.384e+00\n",
            "mean_grad_bcs: 2.9829e-02\n",
            "mean_grad_res: 3.0674e-02\n",
            "Compute NTK...\n",
            "It: 7700, Loss: 1.537e-01, Loss_bcs: 2.752e-03, Loss_res: 1.509e-01 ,Time: 0.51\n",
            "update_res: 1.613e+00\n",
            "update_bcs1: 1.387e+00\n",
            "mean_grad_bcs: 2.3758e-02\n",
            "mean_grad_res: 2.7854e-02\n",
            "Compute NTK...\n",
            "It: 7800, Loss: 1.464e-01, Loss_bcs: 1.634e-03, Loss_res: 1.448e-01 ,Time: 0.51\n",
            "update_res: 1.609e+00\n",
            "update_bcs1: 1.391e+00\n",
            "mean_grad_bcs: 1.8304e-02\n",
            "mean_grad_res: 2.5382e-02\n",
            "Compute NTK...\n",
            "It: 7900, Loss: 1.399e-01, Loss_bcs: 8.808e-04, Loss_res: 1.390e-01 ,Time: 0.51\n",
            "update_res: 1.605e+00\n",
            "update_bcs1: 1.395e+00\n",
            "mean_grad_bcs: 1.3433e-02\n",
            "mean_grad_res: 2.3401e-02\n",
            "Compute NTK...\n",
            "It: 8000, Loss: 1.340e-01, Loss_bcs: 4.057e-04, Loss_res: 1.336e-01 ,Time: 0.50\n",
            "update_res: 1.602e+00\n",
            "update_bcs1: 1.398e+00\n",
            "mean_grad_bcs: 9.1082e-03\n",
            "mean_grad_res: 2.2083e-02\n",
            "Compute NTK...\n",
            "It: 8100, Loss: 1.286e-01, Loss_bcs: 1.378e-04, Loss_res: 1.285e-01 ,Time: 0.51\n",
            "update_res: 1.598e+00\n",
            "update_bcs1: 1.402e+00\n",
            "mean_grad_bcs: 5.2980e-03\n",
            "mean_grad_res: 2.1304e-02\n",
            "Compute NTK...\n",
            "It: 8200, Loss: 1.236e-01, Loss_bcs: 1.932e-05, Loss_res: 1.236e-01 ,Time: 0.51\n",
            "update_res: 1.594e+00\n",
            "update_bcs1: 1.406e+00\n",
            "mean_grad_bcs: 1.9650e-03\n",
            "mean_grad_res: 2.0808e-02\n",
            "Compute NTK...\n",
            "It: 8300, Loss: 1.190e-01, Loss_bcs: 3.905e-06, Loss_res: 1.190e-01 ,Time: 0.51\n",
            "update_res: 1.591e+00\n",
            "update_bcs1: 1.409e+00\n",
            "mean_grad_bcs: 9.2549e-04\n",
            "mean_grad_res: 2.0408e-02\n",
            "Compute NTK...\n",
            "It: 8400, Loss: 1.146e-01, Loss_bcs: 5.534e-05, Loss_res: 1.146e-01 ,Time: 0.50\n",
            "update_res: 1.587e+00\n",
            "update_bcs1: 1.413e+00\n",
            "mean_grad_bcs: 3.4060e-03\n",
            "mean_grad_res: 2.0058e-02\n",
            "Compute NTK...\n",
            "It: 8500, Loss: 1.105e-01, Loss_bcs: 1.459e-04, Loss_res: 1.104e-01 ,Time: 0.50\n",
            "update_res: 1.584e+00\n",
            "update_bcs1: 1.416e+00\n",
            "mean_grad_bcs: 5.5127e-03\n",
            "mean_grad_res: 1.9720e-02\n",
            "Compute NTK...\n",
            "It: 8600, Loss: 1.066e-01, Loss_bcs: 2.551e-04, Loss_res: 1.063e-01 ,Time: 0.54\n",
            "update_res: 1.581e+00\n",
            "update_bcs1: 1.419e+00\n",
            "mean_grad_bcs: 7.2798e-03\n",
            "mean_grad_res: 1.9383e-02\n",
            "Compute NTK...\n",
            "It: 8700, Loss: 1.028e-01, Loss_bcs: 3.681e-04, Loss_res: 1.024e-01 ,Time: 0.52\n",
            "update_res: 1.578e+00\n",
            "update_bcs1: 1.422e+00\n",
            "mean_grad_bcs: 8.7409e-03\n",
            "mean_grad_res: 1.9046e-02\n",
            "Compute NTK...\n",
            "It: 8800, Loss: 9.916e-02, Loss_bcs: 4.752e-04, Loss_res: 9.869e-02 ,Time: 0.51\n",
            "update_res: 1.575e+00\n",
            "update_bcs1: 1.425e+00\n",
            "mean_grad_bcs: 9.9276e-03\n",
            "mean_grad_res: 1.8694e-02\n",
            "Compute NTK...\n",
            "It: 8900, Loss: 9.564e-02, Loss_bcs: 5.700e-04, Loss_res: 9.507e-02 ,Time: 0.50\n",
            "update_res: 1.572e+00\n",
            "update_bcs1: 1.428e+00\n",
            "mean_grad_bcs: 1.0872e-02\n",
            "mean_grad_res: 1.8333e-02\n",
            "Compute NTK...\n",
            "It: 9000, Loss: 9.222e-02, Loss_bcs: 6.490e-04, Loss_res: 9.157e-02 ,Time: 0.52\n",
            "update_res: 1.569e+00\n",
            "update_bcs1: 1.431e+00\n",
            "mean_grad_bcs: 1.1600e-02\n",
            "mean_grad_res: 1.7961e-02\n",
            "Compute NTK...\n",
            "It: 9100, Loss: 8.889e-02, Loss_bcs: 7.111e-04, Loss_res: 8.818e-02 ,Time: 0.53\n",
            "update_res: 1.567e+00\n",
            "update_bcs1: 1.433e+00\n",
            "mean_grad_bcs: 1.2141e-02\n",
            "mean_grad_res: 1.7573e-02\n",
            "Compute NTK...\n",
            "It: 9200, Loss: 8.565e-02, Loss_bcs: 7.562e-04, Loss_res: 8.489e-02 ,Time: 0.52\n",
            "update_res: 1.564e+00\n",
            "update_bcs1: 1.436e+00\n",
            "mean_grad_bcs: 1.2521e-02\n",
            "mean_grad_res: 1.7175e-02\n",
            "Compute NTK...\n",
            "It: 9300, Loss: 8.248e-02, Loss_bcs: 7.854e-04, Loss_res: 8.169e-02 ,Time: 0.50\n",
            "update_res: 1.562e+00\n",
            "update_bcs1: 1.438e+00\n",
            "mean_grad_bcs: 1.2761e-02\n",
            "mean_grad_res: 1.6774e-02\n",
            "Compute NTK...\n",
            "It: 9400, Loss: 7.938e-02, Loss_bcs: 8.005e-04, Loss_res: 7.858e-02 ,Time: 0.50\n",
            "update_res: 1.559e+00\n",
            "update_bcs1: 1.441e+00\n",
            "mean_grad_bcs: 1.2884e-02\n",
            "mean_grad_res: 1.6361e-02\n",
            "Compute NTK...\n",
            "It: 9500, Loss: 7.635e-02, Loss_bcs: 8.034e-04, Loss_res: 7.554e-02 ,Time: 0.51\n",
            "update_res: 1.557e+00\n",
            "update_bcs1: 1.443e+00\n",
            "mean_grad_bcs: 1.2908e-02\n",
            "mean_grad_res: 1.5948e-02\n",
            "Compute NTK...\n",
            "It: 9600, Loss: 7.338e-02, Loss_bcs: 7.961e-04, Loss_res: 7.258e-02 ,Time: 0.51\n",
            "update_res: 1.555e+00\n",
            "update_bcs1: 1.445e+00\n",
            "mean_grad_bcs: 1.2851e-02\n",
            "mean_grad_res: 1.5525e-02\n",
            "Compute NTK...\n",
            "It: 9700, Loss: 7.048e-02, Loss_bcs: 7.807e-04, Loss_res: 6.970e-02 ,Time: 0.50\n",
            "update_res: 1.553e+00\n",
            "update_bcs1: 1.447e+00\n",
            "mean_grad_bcs: 1.2727e-02\n",
            "mean_grad_res: 1.5101e-02\n",
            "Compute NTK...\n",
            "It: 9800, Loss: 6.763e-02, Loss_bcs: 7.586e-04, Loss_res: 6.687e-02 ,Time: 0.52\n",
            "update_res: 1.551e+00\n",
            "update_bcs1: 1.449e+00\n",
            "mean_grad_bcs: 1.2548e-02\n",
            "mean_grad_res: 1.4666e-02\n",
            "Compute NTK...\n",
            "It: 9900, Loss: 6.485e-02, Loss_bcs: 7.319e-04, Loss_res: 6.412e-02 ,Time: 0.50\n",
            "update_res: 1.549e+00\n",
            "update_bcs1: 1.451e+00\n",
            "mean_grad_bcs: 1.2327e-02\n",
            "mean_grad_res: 1.4242e-02\n",
            "Compute NTK...\n",
            "It: 10000, Loss: 6.213e-02, Loss_bcs: 7.018e-04, Loss_res: 6.142e-02 ,Time: 0.50\n",
            "update_res: 1.547e+00\n",
            "update_bcs1: 1.453e+00\n",
            "mean_grad_bcs: 1.2072e-02\n",
            "mean_grad_res: 1.3808e-02\n",
            "Compute NTK...\n",
            "It: 10100, Loss: 5.947e-02, Loss_bcs: 6.695e-04, Loss_res: 5.880e-02 ,Time: 0.50\n",
            "update_res: 1.545e+00\n",
            "update_bcs1: 1.455e+00\n",
            "mean_grad_bcs: 1.1793e-02\n",
            "mean_grad_res: 1.3379e-02\n",
            "Compute NTK...\n",
            "It: 10200, Loss: 5.687e-02, Loss_bcs: 6.359e-04, Loss_res: 5.624e-02 ,Time: 0.51\n",
            "update_res: 1.543e+00\n",
            "update_bcs1: 1.457e+00\n",
            "mean_grad_bcs: 1.1495e-02\n",
            "mean_grad_res: 1.2969e-02\n",
            "Compute NTK...\n",
            "It: 10300, Loss: 5.434e-02, Loss_bcs: 6.018e-04, Loss_res: 5.374e-02 ,Time: 0.52\n",
            "update_res: 1.541e+00\n",
            "update_bcs1: 1.459e+00\n",
            "mean_grad_bcs: 1.1184e-02\n",
            "mean_grad_res: 1.2539e-02\n",
            "Compute NTK...\n",
            "It: 10400, Loss: 5.188e-02, Loss_bcs: 5.680e-04, Loss_res: 5.131e-02 ,Time: 0.55\n",
            "update_res: 1.539e+00\n",
            "update_bcs1: 1.461e+00\n",
            "mean_grad_bcs: 1.0866e-02\n",
            "mean_grad_res: 1.2126e-02\n",
            "Compute NTK...\n",
            "It: 10500, Loss: 4.949e-02, Loss_bcs: 5.348e-04, Loss_res: 4.895e-02 ,Time: 0.50\n",
            "update_res: 1.537e+00\n",
            "update_bcs1: 1.463e+00\n",
            "mean_grad_bcs: 1.0546e-02\n",
            "mean_grad_res: 1.1709e-02\n",
            "Compute NTK...\n",
            "It: 10600, Loss: 4.717e-02, Loss_bcs: 5.025e-04, Loss_res: 4.666e-02 ,Time: 0.50\n",
            "update_res: 1.535e+00\n",
            "update_bcs1: 1.465e+00\n",
            "mean_grad_bcs: 1.0225e-02\n",
            "mean_grad_res: 1.1306e-02\n",
            "Compute NTK...\n",
            "It: 10700, Loss: 4.491e-02, Loss_bcs: 4.715e-04, Loss_res: 4.444e-02 ,Time: 0.50\n",
            "update_res: 1.534e+00\n",
            "update_bcs1: 1.466e+00\n",
            "mean_grad_bcs: 9.9052e-03\n",
            "mean_grad_res: 1.0909e-02\n",
            "Compute NTK...\n",
            "It: 10800, Loss: 4.272e-02, Loss_bcs: 4.418e-04, Loss_res: 4.228e-02 ,Time: 0.50\n",
            "update_res: 1.532e+00\n",
            "update_bcs1: 1.468e+00\n",
            "mean_grad_bcs: 9.5889e-03\n",
            "mean_grad_res: 1.0513e-02\n",
            "Compute NTK...\n",
            "It: 10900, Loss: 4.060e-02, Loss_bcs: 4.133e-04, Loss_res: 4.019e-02 ,Time: 0.51\n",
            "update_res: 1.531e+00\n",
            "update_bcs1: 1.469e+00\n",
            "mean_grad_bcs: 9.2755e-03\n",
            "mean_grad_res: 1.0146e-02\n",
            "Compute NTK...\n",
            "It: 11000, Loss: 3.854e-02, Loss_bcs: 3.860e-04, Loss_res: 3.816e-02 ,Time: 0.51\n",
            "update_res: 1.529e+00\n",
            "update_bcs1: 1.471e+00\n",
            "mean_grad_bcs: 8.9658e-03\n",
            "mean_grad_res: 9.7627e-03\n",
            "Compute NTK...\n",
            "It: 11100, Loss: 3.654e-02, Loss_bcs: 3.599e-04, Loss_res: 3.618e-02 ,Time: 0.50\n",
            "update_res: 1.528e+00\n",
            "update_bcs1: 1.472e+00\n",
            "mean_grad_bcs: 8.6584e-03\n",
            "mean_grad_res: 9.4122e-03\n",
            "Compute NTK...\n",
            "It: 11200, Loss: 3.460e-02, Loss_bcs: 3.348e-04, Loss_res: 3.426e-02 ,Time: 0.50\n",
            "update_res: 1.526e+00\n",
            "update_bcs1: 1.474e+00\n",
            "mean_grad_bcs: 8.3522e-03\n",
            "mean_grad_res: 9.0431e-03\n",
            "Compute NTK...\n",
            "It: 11300, Loss: 3.271e-02, Loss_bcs: 3.108e-04, Loss_res: 3.240e-02 ,Time: 0.50\n",
            "update_res: 1.525e+00\n",
            "update_bcs1: 1.475e+00\n",
            "mean_grad_bcs: 8.0474e-03\n",
            "mean_grad_res: 8.7134e-03\n",
            "Compute NTK...\n",
            "It: 11400, Loss: 3.088e-02, Loss_bcs: 2.877e-04, Loss_res: 3.059e-02 ,Time: 0.52\n",
            "update_res: 1.523e+00\n",
            "update_bcs1: 1.477e+00\n",
            "mean_grad_bcs: 7.7434e-03\n",
            "mean_grad_res: 8.3749e-03\n",
            "Compute NTK...\n",
            "It: 11500, Loss: 2.910e-02, Loss_bcs: 2.655e-04, Loss_res: 2.884e-02 ,Time: 0.56\n",
            "update_res: 1.522e+00\n",
            "update_bcs1: 1.478e+00\n",
            "mean_grad_bcs: 7.4387e-03\n",
            "mean_grad_res: 8.0347e-03\n",
            "Compute NTK...\n",
            "It: 11600, Loss: 2.738e-02, Loss_bcs: 2.440e-04, Loss_res: 2.714e-02 ,Time: 0.56\n",
            "update_res: 1.521e+00\n",
            "update_bcs1: 1.479e+00\n",
            "mean_grad_bcs: 7.1315e-03\n",
            "mean_grad_res: 7.7131e-03\n",
            "Compute NTK...\n",
            "It: 11700, Loss: 2.571e-02, Loss_bcs: 2.231e-04, Loss_res: 2.549e-02 ,Time: 0.53\n",
            "update_res: 1.520e+00\n",
            "update_bcs1: 1.480e+00\n",
            "mean_grad_bcs: 6.8219e-03\n",
            "mean_grad_res: 7.2973e-03\n",
            "Compute NTK...\n",
            "It: 11800, Loss: 2.413e-02, Loss_bcs: 2.030e-04, Loss_res: 2.393e-02 ,Time: 0.49\n",
            "update_res: 1.518e+00\n",
            "update_bcs1: 1.482e+00\n",
            "mean_grad_bcs: 6.5032e-03\n",
            "mean_grad_res: 7.0833e-03\n",
            "Compute NTK...\n",
            "It: 11900, Loss: 2.266e-02, Loss_bcs: 1.837e-04, Loss_res: 2.248e-02 ,Time: 0.50\n",
            "update_res: 1.517e+00\n",
            "update_bcs1: 1.483e+00\n",
            "mean_grad_bcs: 6.1835e-03\n",
            "mean_grad_res: 6.8534e-03\n",
            "Compute NTK...\n",
            "It: 12000, Loss: 2.131e-02, Loss_bcs: 1.657e-04, Loss_res: 2.115e-02 ,Time: 0.50\n",
            "update_res: 1.516e+00\n",
            "update_bcs1: 1.484e+00\n",
            "mean_grad_bcs: 5.8758e-03\n",
            "mean_grad_res: 6.4078e-03\n",
            "Compute NTK...\n",
            "It: 12100, Loss: 2.007e-02, Loss_bcs: 1.497e-04, Loss_res: 1.992e-02 ,Time: 0.52\n",
            "update_res: 1.515e+00\n",
            "update_bcs1: 1.485e+00\n",
            "mean_grad_bcs: 5.5876e-03\n",
            "mean_grad_res: 6.0261e-03\n",
            "Compute NTK...\n",
            "It: 12200, Loss: 1.892e-02, Loss_bcs: 1.359e-04, Loss_res: 1.878e-02 ,Time: 0.52\n",
            "update_res: 1.514e+00\n",
            "update_bcs1: 1.486e+00\n",
            "mean_grad_bcs: 5.3220e-03\n",
            "mean_grad_res: 5.8000e-03\n",
            "Compute NTK...\n",
            "It: 12300, Loss: 1.783e-02, Loss_bcs: 1.233e-04, Loss_res: 1.771e-02 ,Time: 0.53\n",
            "update_res: 1.514e+00\n",
            "update_bcs1: 1.486e+00\n",
            "mean_grad_bcs: 5.0845e-03\n",
            "mean_grad_res: 5.4867e-03\n",
            "Compute NTK...\n",
            "It: 12400, Loss: 1.682e-02, Loss_bcs: 1.127e-04, Loss_res: 1.670e-02 ,Time: 0.51\n",
            "update_res: 1.513e+00\n",
            "update_bcs1: 1.487e+00\n",
            "mean_grad_bcs: 4.8438e-03\n",
            "mean_grad_res: 5.4508e-03\n",
            "Compute NTK...\n",
            "It: 12500, Loss: 1.587e-02, Loss_bcs: 1.025e-04, Loss_res: 1.576e-02 ,Time: 0.50\n",
            "update_res: 1.512e+00\n",
            "update_bcs1: 1.488e+00\n",
            "mean_grad_bcs: 4.6254e-03\n",
            "mean_grad_res: 4.9707e-03\n",
            "Compute NTK...\n",
            "It: 12600, Loss: 1.498e-02, Loss_bcs: 9.315e-05, Loss_res: 1.489e-02 ,Time: 0.51\n",
            "update_res: 1.511e+00\n",
            "update_bcs1: 1.489e+00\n",
            "mean_grad_bcs: 4.4099e-03\n",
            "mean_grad_res: 4.7473e-03\n",
            "Compute NTK...\n",
            "It: 12700, Loss: 1.416e-02, Loss_bcs: 8.476e-05, Loss_res: 1.407e-02 ,Time: 0.51\n",
            "update_res: 1.511e+00\n",
            "update_bcs1: 1.489e+00\n",
            "mean_grad_bcs: 4.2032e-03\n",
            "mean_grad_res: 4.6339e-03\n",
            "Compute NTK...\n",
            "It: 12800, Loss: 1.338e-02, Loss_bcs: 7.713e-05, Loss_res: 1.331e-02 ,Time: 0.50\n",
            "update_res: 1.510e+00\n",
            "update_bcs1: 1.490e+00\n",
            "mean_grad_bcs: 4.0031e-03\n",
            "mean_grad_res: 4.8467e-03\n",
            "Compute NTK...\n",
            "It: 12900, Loss: 1.266e-02, Loss_bcs: 6.982e-05, Loss_res: 1.259e-02 ,Time: 0.52\n",
            "update_res: 1.510e+00\n",
            "update_bcs1: 1.490e+00\n",
            "mean_grad_bcs: 3.8201e-03\n",
            "mean_grad_res: 4.1040e-03\n",
            "Compute NTK...\n",
            "It: 13000, Loss: 1.199e-02, Loss_bcs: 6.345e-05, Loss_res: 1.192e-02 ,Time: 0.51\n",
            "update_res: 1.509e+00\n",
            "update_bcs1: 1.491e+00\n",
            "mean_grad_bcs: 3.6394e-03\n",
            "mean_grad_res: 4.0195e-03\n",
            "Compute NTK...\n",
            "It: 13100, Loss: 1.136e-02, Loss_bcs: 5.747e-05, Loss_res: 1.130e-02 ,Time: 0.55\n",
            "update_res: 1.509e+00\n",
            "update_bcs1: 1.491e+00\n",
            "mean_grad_bcs: 3.4733e-03\n",
            "mean_grad_res: 3.9146e-03\n",
            "Compute NTK...\n",
            "It: 13200, Loss: 1.077e-02, Loss_bcs: 5.264e-05, Loss_res: 1.071e-02 ,Time: 0.51\n",
            "update_res: 1.508e+00\n",
            "update_bcs1: 1.492e+00\n",
            "mean_grad_bcs: 3.2937e-03\n",
            "mean_grad_res: 6.0368e-03\n",
            "Compute NTK...\n",
            "It: 13300, Loss: 1.020e-02, Loss_bcs: 4.736e-05, Loss_res: 1.016e-02 ,Time: 0.51\n",
            "update_res: 1.508e+00\n",
            "update_bcs1: 1.492e+00\n",
            "mean_grad_bcs: 3.1535e-03\n",
            "mean_grad_res: 3.4354e-03\n",
            "Compute NTK...\n",
            "It: 13400, Loss: 9.679e-03, Loss_bcs: 4.325e-05, Loss_res: 9.636e-03 ,Time: 0.51\n",
            "update_res: 1.507e+00\n",
            "update_bcs1: 1.493e+00\n",
            "mean_grad_bcs: 2.9966e-03\n",
            "mean_grad_res: 4.0925e-03\n",
            "Compute NTK...\n",
            "It: 13500, Loss: 9.184e-03, Loss_bcs: 3.901e-05, Loss_res: 9.145e-03 ,Time: 0.52\n",
            "update_res: 1.507e+00\n",
            "update_bcs1: 1.493e+00\n",
            "mean_grad_bcs: 2.8602e-03\n",
            "mean_grad_res: 3.0640e-03\n",
            "Compute NTK...\n",
            "It: 13600, Loss: 8.719e-03, Loss_bcs: 3.550e-05, Loss_res: 8.683e-03 ,Time: 0.51\n",
            "update_res: 1.507e+00\n",
            "update_bcs1: 1.493e+00\n",
            "mean_grad_bcs: 2.7224e-03\n",
            "mean_grad_res: 3.1321e-03\n",
            "Compute NTK...\n",
            "It: 13700, Loss: 8.279e-03, Loss_bcs: 3.209e-05, Loss_res: 8.246e-03 ,Time: 0.53\n",
            "update_res: 1.506e+00\n",
            "update_bcs1: 1.494e+00\n",
            "mean_grad_bcs: 2.5963e-03\n",
            "mean_grad_res: 2.7580e-03\n",
            "Compute NTK...\n",
            "It: 13800, Loss: 7.865e-03, Loss_bcs: 2.939e-05, Loss_res: 7.835e-03 ,Time: 0.53\n",
            "update_res: 1.506e+00\n",
            "update_bcs1: 1.494e+00\n",
            "mean_grad_bcs: 2.4604e-03\n",
            "mean_grad_res: 4.5708e-03\n",
            "Compute NTK...\n",
            "It: 13900, Loss: 7.472e-03, Loss_bcs: 2.647e-05, Loss_res: 7.446e-03 ,Time: 0.51\n",
            "update_res: 1.506e+00\n",
            "update_bcs1: 1.494e+00\n",
            "mean_grad_bcs: 2.3500e-03\n",
            "mean_grad_res: 2.8854e-03\n",
            "Compute NTK...\n",
            "It: 14000, Loss: 7.104e-03, Loss_bcs: 2.395e-05, Loss_res: 7.080e-03 ,Time: 0.51\n",
            "update_res: 1.505e+00\n",
            "update_bcs1: 1.495e+00\n",
            "mean_grad_bcs: 2.2382e-03\n",
            "mean_grad_res: 2.5643e-03\n",
            "Compute NTK...\n",
            "It: 14100, Loss: 6.756e-03, Loss_bcs: 2.167e-05, Loss_res: 6.734e-03 ,Time: 0.54\n",
            "update_res: 1.505e+00\n",
            "update_bcs1: 1.495e+00\n",
            "mean_grad_bcs: 2.1312e-03\n",
            "mean_grad_res: 2.4817e-03\n",
            "Compute NTK...\n",
            "It: 14200, Loss: 6.428e-03, Loss_bcs: 1.979e-05, Loss_res: 6.408e-03 ,Time: 0.50\n",
            "update_res: 1.505e+00\n",
            "update_bcs1: 1.495e+00\n",
            "mean_grad_bcs: 2.0206e-03\n",
            "mean_grad_res: 4.3222e-03\n",
            "Compute NTK...\n",
            "It: 14300, Loss: 6.115e-03, Loss_bcs: 1.781e-05, Loss_res: 6.098e-03 ,Time: 0.51\n",
            "update_res: 1.505e+00\n",
            "update_bcs1: 1.495e+00\n",
            "mean_grad_bcs: 1.9298e-03\n",
            "mean_grad_res: 2.3641e-03\n",
            "Compute NTK...\n",
            "It: 14400, Loss: 5.837e-03, Loss_bcs: 1.580e-05, Loss_res: 5.822e-03 ,Time: 0.50\n",
            "update_res: 1.504e+00\n",
            "update_bcs1: 1.496e+00\n",
            "mean_grad_bcs: 1.8546e-03\n",
            "mean_grad_res: 8.4803e-03\n",
            "Compute NTK...\n",
            "It: 14500, Loss: 5.544e-03, Loss_bcs: 1.458e-05, Loss_res: 5.530e-03 ,Time: 0.51\n",
            "update_res: 1.504e+00\n",
            "update_bcs1: 1.496e+00\n",
            "mean_grad_bcs: 1.7495e-03\n",
            "mean_grad_res: 1.9378e-03\n",
            "Compute NTK...\n",
            "It: 14600, Loss: 5.282e-03, Loss_bcs: 1.320e-05, Loss_res: 5.269e-03 ,Time: 0.51\n",
            "update_res: 1.504e+00\n",
            "update_bcs1: 1.496e+00\n",
            "mean_grad_bcs: 1.6699e-03\n",
            "mean_grad_res: 3.0536e-03\n",
            "Compute NTK...\n",
            "It: 14700, Loss: 5.034e-03, Loss_bcs: 1.194e-05, Loss_res: 5.022e-03 ,Time: 0.53\n",
            "update_res: 1.504e+00\n",
            "update_bcs1: 1.496e+00\n",
            "mean_grad_bcs: 1.5840e-03\n",
            "mean_grad_res: 1.7473e-03\n",
            "Compute NTK...\n",
            "It: 14800, Loss: 4.800e-03, Loss_bcs: 1.082e-05, Loss_res: 4.789e-03 ,Time: 0.53\n",
            "update_res: 1.504e+00\n",
            "update_bcs1: 1.496e+00\n",
            "mean_grad_bcs: 1.5129e-03\n",
            "mean_grad_res: 2.5372e-03\n",
            "Compute NTK...\n",
            "It: 14900, Loss: 4.578e-03, Loss_bcs: 9.796e-06, Loss_res: 4.568e-03 ,Time: 0.53\n",
            "update_res: 1.503e+00\n",
            "update_bcs1: 1.497e+00\n",
            "mean_grad_bcs: 1.4455e-03\n",
            "mean_grad_res: 4.7302e-03\n",
            "Compute NTK...\n",
            "It: 15000, Loss: 4.364e-03, Loss_bcs: 8.959e-06, Loss_res: 4.355e-03 ,Time: 0.51\n",
            "update_res: 1.503e+00\n",
            "update_bcs1: 1.497e+00\n",
            "mean_grad_bcs: 1.3701e-03\n",
            "mean_grad_res: 1.9548e-03\n",
            "Compute NTK...\n",
            "It: 15100, Loss: 4.164e-03, Loss_bcs: 8.117e-06, Loss_res: 4.156e-03 ,Time: 0.51\n",
            "update_res: 1.503e+00\n",
            "update_bcs1: 1.497e+00\n",
            "mean_grad_bcs: 1.3056e-03\n",
            "mean_grad_res: 1.5202e-03\n",
            "Compute NTK...\n",
            "It: 15200, Loss: 3.974e-03, Loss_bcs: 7.400e-06, Loss_res: 3.967e-03 ,Time: 0.50\n",
            "update_res: 1.503e+00\n",
            "update_bcs1: 1.497e+00\n",
            "mean_grad_bcs: 1.2424e-03\n",
            "mean_grad_res: 1.8704e-03\n",
            "Compute NTK...\n",
            "It: 15300, Loss: 3.799e-03, Loss_bcs: 6.746e-06, Loss_res: 3.792e-03 ,Time: 0.52\n",
            "update_res: 1.503e+00\n",
            "update_bcs1: 1.497e+00\n",
            "mean_grad_bcs: 1.1838e-03\n",
            "mean_grad_res: 3.5013e-03\n",
            "Compute NTK...\n",
            "It: 15400, Loss: 3.624e-03, Loss_bcs: 6.111e-06, Loss_res: 3.618e-03 ,Time: 0.51\n",
            "update_res: 1.503e+00\n",
            "update_bcs1: 1.497e+00\n",
            "mean_grad_bcs: 1.1314e-03\n",
            "mean_grad_res: 2.4918e-03\n",
            "Compute NTK...\n",
            "It: 15500, Loss: 3.462e-03, Loss_bcs: 5.552e-06, Loss_res: 3.456e-03 ,Time: 0.50\n",
            "update_res: 1.503e+00\n",
            "update_bcs1: 1.497e+00\n",
            "mean_grad_bcs: 1.0799e-03\n",
            "mean_grad_res: 1.3086e-03\n",
            "Compute NTK...\n",
            "It: 15600, Loss: 3.309e-03, Loss_bcs: 5.067e-06, Loss_res: 3.304e-03 ,Time: 0.55\n",
            "update_res: 1.503e+00\n",
            "update_bcs1: 1.497e+00\n",
            "mean_grad_bcs: 1.0299e-03\n",
            "mean_grad_res: 2.2669e-03\n",
            "Compute NTK...\n",
            "It: 15700, Loss: 3.166e-03, Loss_bcs: 4.631e-06, Loss_res: 3.162e-03 ,Time: 0.50\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 9.8234e-04\n",
            "mean_grad_res: 3.9663e-03\n",
            "Compute NTK...\n",
            "It: 15800, Loss: 3.027e-03, Loss_bcs: 4.222e-06, Loss_res: 3.023e-03 ,Time: 0.51\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 9.3905e-04\n",
            "mean_grad_res: 5.3336e-03\n",
            "Compute NTK...\n",
            "It: 15900, Loss: 2.894e-03, Loss_bcs: 3.844e-06, Loss_res: 2.890e-03 ,Time: 0.52\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 8.9921e-04\n",
            "mean_grad_res: 1.0580e-03\n",
            "Compute NTK...\n",
            "It: 16000, Loss: 2.770e-03, Loss_bcs: 3.512e-06, Loss_res: 2.766e-03 ,Time: 0.52\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 8.5991e-04\n",
            "mean_grad_res: 1.2593e-03\n",
            "Compute NTK...\n",
            "It: 16100, Loss: 2.714e-03, Loss_bcs: 3.258e-06, Loss_res: 2.711e-03 ,Time: 0.51\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 8.2075e-04\n",
            "mean_grad_res: 1.3798e-02\n",
            "Compute NTK...\n",
            "It: 16200, Loss: 2.541e-03, Loss_bcs: 2.943e-06, Loss_res: 2.538e-03 ,Time: 0.52\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 7.8688e-04\n",
            "mean_grad_res: 9.5386e-04\n",
            "Compute NTK...\n",
            "It: 16300, Loss: 2.435e-03, Loss_bcs: 2.697e-06, Loss_res: 2.432e-03 ,Time: 0.51\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 7.5411e-04\n",
            "mean_grad_res: 8.7483e-04\n",
            "Compute NTK...\n",
            "It: 16400, Loss: 2.334e-03, Loss_bcs: 2.490e-06, Loss_res: 2.332e-03 ,Time: 0.52\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 7.2199e-04\n",
            "mean_grad_res: 1.8949e-03\n",
            "Compute NTK...\n",
            "It: 16500, Loss: 2.253e-03, Loss_bcs: 2.244e-06, Loss_res: 2.251e-03 ,Time: 0.51\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 6.9991e-04\n",
            "mean_grad_res: 1.2897e-02\n",
            "Compute NTK...\n",
            "It: 16600, Loss: 2.147e-03, Loss_bcs: 2.103e-06, Loss_res: 2.145e-03 ,Time: 0.53\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 6.6463e-04\n",
            "mean_grad_res: 8.8344e-04\n",
            "Compute NTK...\n",
            "It: 16700, Loss: 2.060e-03, Loss_bcs: 1.939e-06, Loss_res: 2.058e-03 ,Time: 0.76\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 6.3799e-04\n",
            "mean_grad_res: 1.0950e-03\n",
            "Compute NTK...\n",
            "It: 16800, Loss: 1.981e-03, Loss_bcs: 1.775e-06, Loss_res: 1.979e-03 ,Time: 0.51\n",
            "update_res: 1.502e+00\n",
            "update_bcs1: 1.498e+00\n",
            "mean_grad_bcs: 6.1485e-04\n",
            "mean_grad_res: 5.9499e-03\n",
            "Compute NTK...\n",
            "It: 16900, Loss: 1.901e-03, Loss_bcs: 1.646e-06, Loss_res: 1.899e-03 ,Time: 0.50\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 5.8932e-04\n",
            "mean_grad_res: 7.0387e-04\n",
            "Compute NTK...\n",
            "It: 17000, Loss: 1.827e-03, Loss_bcs: 1.518e-06, Loss_res: 1.825e-03 ,Time: 0.50\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 5.6659e-04\n",
            "mean_grad_res: 1.4942e-03\n",
            "Compute NTK...\n",
            "It: 17100, Loss: 1.757e-03, Loss_bcs: 1.406e-06, Loss_res: 1.755e-03 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 5.4354e-04\n",
            "mean_grad_res: 7.6407e-04\n",
            "Compute NTK...\n",
            "It: 17200, Loss: 1.691e-03, Loss_bcs: 1.297e-06, Loss_res: 1.690e-03 ,Time: 0.55\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 5.2379e-04\n",
            "mean_grad_res: 2.0082e-03\n",
            "Compute NTK...\n",
            "It: 17300, Loss: 1.627e-03, Loss_bcs: 1.206e-06, Loss_res: 1.625e-03 ,Time: 0.50\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 5.0457e-04\n",
            "mean_grad_res: 7.3959e-04\n",
            "Compute NTK...\n",
            "It: 17400, Loss: 1.567e-03, Loss_bcs: 1.125e-06, Loss_res: 1.565e-03 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 4.8671e-04\n",
            "mean_grad_res: 6.0941e-04\n",
            "Compute NTK...\n",
            "It: 17500, Loss: 1.530e-03, Loss_bcs: 1.074e-06, Loss_res: 1.529e-03 ,Time: 0.50\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 4.6174e-04\n",
            "mean_grad_res: 1.8035e-02\n",
            "Compute NTK...\n",
            "It: 17600, Loss: 1.455e-03, Loss_bcs: 9.718e-07, Loss_res: 1.454e-03 ,Time: 0.52\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 4.5097e-04\n",
            "mean_grad_res: 1.7436e-03\n",
            "Compute NTK...\n",
            "It: 17700, Loss: 1.449e-03, Loss_bcs: 8.680e-07, Loss_res: 1.448e-03 ,Time: 0.52\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 4.4247e-04\n",
            "mean_grad_res: 1.6886e-02\n",
            "Compute NTK...\n",
            "It: 17800, Loss: 1.354e-03, Loss_bcs: 8.426e-07, Loss_res: 1.353e-03 ,Time: 0.54\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 4.2172e-04\n",
            "mean_grad_res: 5.5907e-04\n",
            "Compute NTK...\n",
            "It: 17900, Loss: 1.307e-03, Loss_bcs: 7.881e-07, Loss_res: 1.307e-03 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 4.0602e-04\n",
            "mean_grad_res: 2.7726e-03\n",
            "Compute NTK...\n",
            "It: 18000, Loss: 1.264e-03, Loss_bcs: 7.327e-07, Loss_res: 1.263e-03 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 3.9445e-04\n",
            "mean_grad_res: 2.3829e-03\n",
            "Compute NTK...\n",
            "It: 18100, Loss: 1.221e-03, Loss_bcs: 6.871e-07, Loss_res: 1.220e-03 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 3.8059e-04\n",
            "mean_grad_res: 4.9552e-04\n",
            "Compute NTK...\n",
            "It: 18200, Loss: 1.180e-03, Loss_bcs: 6.447e-07, Loss_res: 1.180e-03 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 3.6969e-04\n",
            "mean_grad_res: 1.7575e-03\n",
            "Compute NTK...\n",
            "It: 18300, Loss: 1.161e-03, Loss_bcs: 5.718e-07, Loss_res: 1.161e-03 ,Time: 0.50\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 3.6581e-04\n",
            "mean_grad_res: 1.6594e-02\n",
            "Compute NTK...\n",
            "It: 18400, Loss: 1.120e-03, Loss_bcs: 5.471e-07, Loss_res: 1.120e-03 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 3.4999e-04\n",
            "mean_grad_res: 8.6877e-03\n",
            "Compute NTK...\n",
            "It: 18500, Loss: 1.071e-03, Loss_bcs: 5.316e-07, Loss_res: 1.070e-03 ,Time: 0.52\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 3.3426e-04\n",
            "mean_grad_res: 5.3337e-04\n",
            "Compute NTK...\n",
            "It: 18600, Loss: 1.038e-03, Loss_bcs: 4.990e-07, Loss_res: 1.038e-03 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 3.2227e-04\n",
            "mean_grad_res: 1.8414e-03\n",
            "Compute NTK...\n",
            "It: 18700, Loss: 1.007e-03, Loss_bcs: 4.675e-07, Loss_res: 1.007e-03 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 3.1358e-04\n",
            "mean_grad_res: 4.7689e-04\n",
            "Compute NTK...\n",
            "It: 18800, Loss: 1.042e-03, Loss_bcs: 4.748e-07, Loss_res: 1.041e-03 ,Time: 0.54\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.9288e-04\n",
            "mean_grad_res: 2.6259e-02\n",
            "Compute NTK...\n",
            "It: 18900, Loss: 9.486e-04, Loss_bcs: 4.143e-07, Loss_res: 9.482e-04 ,Time: 0.50\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.9522e-04\n",
            "mean_grad_res: 5.5196e-04\n",
            "Compute NTK...\n",
            "It: 19000, Loss: 9.218e-04, Loss_bcs: 3.902e-07, Loss_res: 9.214e-04 ,Time: 0.53\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.8526e-04\n",
            "mean_grad_res: 1.2824e-03\n",
            "Compute NTK...\n",
            "It: 19100, Loss: 8.972e-04, Loss_bcs: 3.693e-07, Loss_res: 8.969e-04 ,Time: 0.50\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.7644e-04\n",
            "mean_grad_res: 4.0243e-03\n",
            "Compute NTK...\n",
            "It: 19200, Loss: 8.717e-04, Loss_bcs: 3.455e-07, Loss_res: 8.714e-04 ,Time: 0.58\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.6975e-04\n",
            "mean_grad_res: 3.4434e-04\n",
            "Compute NTK...\n",
            "It: 19300, Loss: 8.483e-04, Loss_bcs: 3.253e-07, Loss_res: 8.480e-04 ,Time: 0.53\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.6186e-04\n",
            "mean_grad_res: 1.0511e-03\n",
            "Compute NTK...\n",
            "It: 19400, Loss: 8.383e-04, Loss_bcs: 2.938e-07, Loss_res: 8.380e-04 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.6207e-04\n",
            "mean_grad_res: 1.3536e-02\n",
            "Compute NTK...\n",
            "It: 19500, Loss: 8.053e-04, Loss_bcs: 2.909e-07, Loss_res: 8.050e-04 ,Time: 0.52\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.4763e-04\n",
            "mean_grad_res: 3.3236e-04\n",
            "Compute NTK...\n",
            "It: 19600, Loss: 8.501e-04, Loss_bcs: 2.961e-07, Loss_res: 8.498e-04 ,Time: 0.52\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.3248e-04\n",
            "mean_grad_res: 2.4841e-02\n",
            "Compute NTK...\n",
            "It: 19700, Loss: 7.652e-04, Loss_bcs: 2.618e-07, Loss_res: 7.649e-04 ,Time: 0.52\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.3459e-04\n",
            "mean_grad_res: 5.8991e-04\n",
            "Compute NTK...\n",
            "It: 19800, Loss: 7.465e-04, Loss_bcs: 2.453e-07, Loss_res: 7.463e-04 ,Time: 0.53\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.2840e-04\n",
            "mean_grad_res: 5.6754e-04\n",
            "Compute NTK...\n",
            "It: 19900, Loss: 7.289e-04, Loss_bcs: 2.358e-07, Loss_res: 7.287e-04 ,Time: 0.55\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.2260e-04\n",
            "mean_grad_res: 1.2311e-03\n",
            "Compute NTK...\n",
            "It: 20000, Loss: 7.120e-04, Loss_bcs: 2.189e-07, Loss_res: 7.118e-04 ,Time: 0.50\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.1875e-04\n",
            "mean_grad_res: 3.3140e-03\n",
            "Compute NTK...\n",
            "It: 20100, Loss: 6.950e-04, Loss_bcs: 2.101e-07, Loss_res: 6.948e-04 ,Time: 0.51\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.1020e-04\n",
            "mean_grad_res: 2.9947e-04\n",
            "Compute NTK...\n",
            "It: 20200, Loss: 6.796e-04, Loss_bcs: 2.018e-07, Loss_res: 6.794e-04 ,Time: 0.55\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.0520e-04\n",
            "mean_grad_res: 1.6255e-03\n",
            "Compute NTK...\n",
            "It: 20300, Loss: 6.645e-04, Loss_bcs: 1.904e-07, Loss_res: 6.643e-04 ,Time: 0.50\n",
            "update_res: 1.501e+00\n",
            "update_bcs1: 1.499e+00\n",
            "mean_grad_bcs: 2.0075e-04\n",
            "mean_grad_res: 2.8840e-04\n",
            "Compute NTK...\n",
            "It: 20400, Loss: 6.504e-04, Loss_bcs: 1.777e-07, Loss_res: 6.502e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.9625e-04\n",
            "mean_grad_res: 2.4186e-03\n",
            "Compute NTK...\n",
            "It: 20500, Loss: 6.388e-04, Loss_bcs: 1.751e-07, Loss_res: 6.386e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.8928e-04\n",
            "mean_grad_res: 9.5959e-03\n",
            "Compute NTK...\n",
            "It: 20600, Loss: 6.228e-04, Loss_bcs: 1.636e-07, Loss_res: 6.226e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.8561e-04\n",
            "mean_grad_res: 2.5982e-04\n",
            "Compute NTK...\n",
            "It: 20700, Loss: 6.102e-04, Loss_bcs: 1.523e-07, Loss_res: 6.101e-04 ,Time: 0.55\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.8175e-04\n",
            "mean_grad_res: 2.3728e-03\n",
            "Compute NTK...\n",
            "It: 20800, Loss: 5.984e-04, Loss_bcs: 1.480e-07, Loss_res: 5.982e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.7703e-04\n",
            "mean_grad_res: 4.1134e-03\n",
            "Compute NTK...\n",
            "It: 20900, Loss: 5.900e-04, Loss_bcs: 1.314e-07, Loss_res: 5.899e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.7996e-04\n",
            "mean_grad_res: 1.0446e-02\n",
            "Compute NTK...\n",
            "It: 21000, Loss: 5.736e-04, Loss_bcs: 1.338e-07, Loss_res: 5.735e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.6877e-04\n",
            "mean_grad_res: 5.3552e-04\n",
            "Compute NTK...\n",
            "It: 21100, Loss: 5.627e-04, Loss_bcs: 1.287e-07, Loss_res: 5.626e-04 ,Time: 0.49\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.6482e-04\n",
            "mean_grad_res: 3.7124e-04\n",
            "Compute NTK...\n",
            "It: 21200, Loss: 5.520e-04, Loss_bcs: 1.243e-07, Loss_res: 5.519e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.5945e-04\n",
            "mean_grad_res: 1.7465e-03\n",
            "Compute NTK...\n",
            "It: 21300, Loss: 5.416e-04, Loss_bcs: 1.162e-07, Loss_res: 5.414e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.5709e-04\n",
            "mean_grad_res: 3.8009e-04\n",
            "Compute NTK...\n",
            "It: 21400, Loss: 5.315e-04, Loss_bcs: 1.111e-07, Loss_res: 5.314e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.5384e-04\n",
            "mean_grad_res: 6.3035e-04\n",
            "Compute NTK...\n",
            "It: 21500, Loss: 5.220e-04, Loss_bcs: 1.067e-07, Loss_res: 5.219e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.5039e-04\n",
            "mean_grad_res: 2.7007e-04\n",
            "Compute NTK...\n",
            "It: 21600, Loss: 5.126e-04, Loss_bcs: 9.915e-08, Loss_res: 5.126e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.4865e-04\n",
            "mean_grad_res: 1.6273e-03\n",
            "Compute NTK...\n",
            "It: 21700, Loss: 5.256e-04, Loss_bcs: 1.018e-07, Loss_res: 5.255e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.4226e-04\n",
            "mean_grad_res: 1.4253e-02\n",
            "Compute NTK...\n",
            "It: 21800, Loss: 4.950e-04, Loss_bcs: 9.412e-08, Loss_res: 4.949e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.4046e-04\n",
            "mean_grad_res: 2.4600e-04\n",
            "Compute NTK...\n",
            "It: 21900, Loss: 4.865e-04, Loss_bcs: 9.076e-08, Loss_res: 4.864e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.3739e-04\n",
            "mean_grad_res: 3.5282e-04\n",
            "Compute NTK...\n",
            "It: 22000, Loss: 4.781e-04, Loss_bcs: 8.755e-08, Loss_res: 4.780e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.3507e-04\n",
            "mean_grad_res: 8.2523e-04\n",
            "Compute NTK...\n",
            "It: 22100, Loss: 4.708e-04, Loss_bcs: 8.167e-08, Loss_res: 4.707e-04 ,Time: 0.54\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.3336e-04\n",
            "mean_grad_res: 2.0629e-03\n",
            "Compute NTK...\n",
            "It: 22200, Loss: 4.627e-04, Loss_bcs: 7.935e-08, Loss_res: 4.626e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.2949e-04\n",
            "mean_grad_res: 2.5886e-04\n",
            "Compute NTK...\n",
            "It: 22300, Loss: 4.551e-04, Loss_bcs: 7.616e-08, Loss_res: 4.550e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.2933e-04\n",
            "mean_grad_res: 2.8774e-03\n",
            "Compute NTK...\n",
            "It: 22400, Loss: 4.478e-04, Loss_bcs: 7.438e-08, Loss_res: 4.477e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.2426e-04\n",
            "mean_grad_res: 1.1504e-03\n",
            "Compute NTK...\n",
            "It: 22500, Loss: 4.406e-04, Loss_bcs: 7.110e-08, Loss_res: 4.405e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.2231e-04\n",
            "mean_grad_res: 2.8007e-04\n",
            "Compute NTK...\n",
            "It: 22600, Loss: 4.338e-04, Loss_bcs: 6.852e-08, Loss_res: 4.338e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.2110e-04\n",
            "mean_grad_res: 1.6154e-03\n",
            "Compute NTK...\n",
            "It: 22700, Loss: 4.270e-04, Loss_bcs: 6.513e-08, Loss_res: 4.269e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.1859e-04\n",
            "mean_grad_res: 7.7758e-04\n",
            "Compute NTK...\n",
            "It: 22800, Loss: 4.205e-04, Loss_bcs: 6.271e-08, Loss_res: 4.204e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.1565e-04\n",
            "mean_grad_res: 4.3514e-04\n",
            "Compute NTK...\n",
            "It: 22900, Loss: 4.143e-04, Loss_bcs: 6.374e-08, Loss_res: 4.143e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.1228e-04\n",
            "mean_grad_res: 3.2144e-03\n",
            "Compute NTK...\n",
            "It: 23000, Loss: 4.162e-04, Loss_bcs: 5.083e-08, Loss_res: 4.161e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.2104e-04\n",
            "mean_grad_res: 1.2497e-02\n",
            "Compute NTK...\n",
            "It: 23100, Loss: 4.020e-04, Loss_bcs: 5.659e-08, Loss_res: 4.019e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.0955e-04\n",
            "mean_grad_res: 3.2519e-04\n",
            "Compute NTK...\n",
            "It: 23200, Loss: 3.965e-04, Loss_bcs: 5.796e-08, Loss_res: 3.965e-04 ,Time: 0.54\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.0249e-04\n",
            "mean_grad_res: 3.2778e-03\n",
            "Compute NTK...\n",
            "It: 23300, Loss: 3.905e-04, Loss_bcs: 5.308e-08, Loss_res: 3.904e-04 ,Time: 0.55\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.0515e-04\n",
            "mean_grad_res: 7.7715e-04\n",
            "Compute NTK...\n",
            "It: 23400, Loss: 3.850e-04, Loss_bcs: 5.141e-08, Loss_res: 3.849e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.0395e-04\n",
            "mean_grad_res: 1.5875e-04\n",
            "Compute NTK...\n",
            "It: 23500, Loss: 3.856e-04, Loss_bcs: 4.331e-08, Loss_res: 3.855e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.0956e-04\n",
            "mean_grad_res: 8.9958e-03\n",
            "Compute NTK...\n",
            "It: 23600, Loss: 3.744e-04, Loss_bcs: 4.749e-08, Loss_res: 3.743e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.0055e-04\n",
            "mean_grad_res: 2.2734e-04\n",
            "Compute NTK...\n",
            "It: 23700, Loss: 4.192e-04, Loss_bcs: 3.172e-08, Loss_res: 4.191e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 1.1429e-04\n",
            "mean_grad_res: 2.2696e-02\n",
            "Compute NTK...\n",
            "It: 23800, Loss: 3.645e-04, Loss_bcs: 4.476e-08, Loss_res: 3.645e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 9.7276e-05\n",
            "mean_grad_res: 1.5211e-04\n",
            "Compute NTK...\n",
            "It: 23900, Loss: 3.598e-04, Loss_bcs: 4.283e-08, Loss_res: 3.598e-04 ,Time: 0.54\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 9.5715e-05\n",
            "mean_grad_res: 1.4315e-03\n",
            "Compute NTK...\n",
            "It: 24000, Loss: 3.551e-04, Loss_bcs: 4.218e-08, Loss_res: 3.550e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 9.4221e-05\n",
            "mean_grad_res: 2.0138e-04\n",
            "Compute NTK...\n",
            "It: 24100, Loss: 3.505e-04, Loss_bcs: 4.045e-08, Loss_res: 3.504e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 9.2712e-05\n",
            "mean_grad_res: 1.3899e-04\n",
            "Compute NTK...\n",
            "It: 24200, Loss: 3.460e-04, Loss_bcs: 3.908e-08, Loss_res: 3.459e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 9.1533e-05\n",
            "mean_grad_res: 4.9585e-04\n",
            "Compute NTK...\n",
            "It: 24300, Loss: 3.415e-04, Loss_bcs: 3.856e-08, Loss_res: 3.415e-04 ,Time: 0.55\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 9.0072e-05\n",
            "mean_grad_res: 1.6251e-04\n",
            "Compute NTK...\n",
            "It: 24400, Loss: 3.373e-04, Loss_bcs: 3.689e-08, Loss_res: 3.373e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 8.9313e-05\n",
            "mean_grad_res: 7.2284e-04\n",
            "Compute NTK...\n",
            "It: 24500, Loss: 3.332e-04, Loss_bcs: 3.606e-08, Loss_res: 3.332e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 8.7582e-05\n",
            "mean_grad_res: 2.0254e-04\n",
            "Compute NTK...\n",
            "It: 24600, Loss: 3.292e-04, Loss_bcs: 3.413e-08, Loss_res: 3.292e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 8.6623e-05\n",
            "mean_grad_res: 1.1539e-03\n",
            "Compute NTK...\n",
            "It: 24700, Loss: 3.252e-04, Loss_bcs: 3.434e-08, Loss_res: 3.251e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 8.4946e-05\n",
            "mean_grad_res: 1.6335e-04\n",
            "Compute NTK...\n",
            "It: 24800, Loss: 3.214e-04, Loss_bcs: 3.355e-08, Loss_res: 3.214e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 8.3125e-05\n",
            "mean_grad_res: 2.2539e-03\n",
            "Compute NTK...\n",
            "It: 24900, Loss: 3.174e-04, Loss_bcs: 3.247e-08, Loss_res: 3.173e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 8.2512e-05\n",
            "mean_grad_res: 1.4270e-04\n",
            "Compute NTK...\n",
            "It: 25000, Loss: 3.137e-04, Loss_bcs: 3.212e-08, Loss_res: 3.137e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 8.0958e-05\n",
            "mean_grad_res: 7.9828e-04\n",
            "Compute NTK...\n",
            "It: 25100, Loss: 3.100e-04, Loss_bcs: 3.078e-08, Loss_res: 3.100e-04 ,Time: 0.54\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 8.0349e-05\n",
            "mean_grad_res: 2.0378e-04\n",
            "Compute NTK...\n",
            "It: 25200, Loss: 3.065e-04, Loss_bcs: 2.930e-08, Loss_res: 3.065e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.9251e-05\n",
            "mean_grad_res: 1.2463e-03\n",
            "Compute NTK...\n",
            "It: 25300, Loss: 3.029e-04, Loss_bcs: 2.879e-08, Loss_res: 3.029e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.8232e-05\n",
            "mean_grad_res: 1.6095e-04\n",
            "Compute NTK...\n",
            "It: 25400, Loss: 3.001e-04, Loss_bcs: 3.161e-08, Loss_res: 3.001e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.5545e-05\n",
            "mean_grad_res: 2.3826e-03\n",
            "Compute NTK...\n",
            "It: 25500, Loss: 2.962e-04, Loss_bcs: 2.785e-08, Loss_res: 2.962e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.6636e-05\n",
            "mean_grad_res: 1.6329e-04\n",
            "Compute NTK...\n",
            "It: 25600, Loss: 2.929e-04, Loss_bcs: 2.698e-08, Loss_res: 2.929e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.5824e-05\n",
            "mean_grad_res: 1.3835e-04\n",
            "Compute NTK...\n",
            "It: 25700, Loss: 2.897e-04, Loss_bcs: 2.598e-08, Loss_res: 2.896e-04 ,Time: 0.56\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.4889e-05\n",
            "mean_grad_res: 2.3921e-04\n",
            "Compute NTK...\n",
            "It: 25800, Loss: 2.865e-04, Loss_bcs: 2.585e-08, Loss_res: 2.865e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.3673e-05\n",
            "mean_grad_res: 1.1545e-04\n",
            "Compute NTK...\n",
            "It: 25900, Loss: 2.836e-04, Loss_bcs: 2.518e-08, Loss_res: 2.836e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.2123e-05\n",
            "mean_grad_res: 1.4369e-03\n",
            "Compute NTK...\n",
            "It: 26000, Loss: 2.805e-04, Loss_bcs: 2.413e-08, Loss_res: 2.805e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.1996e-05\n",
            "mean_grad_res: 2.3349e-04\n",
            "Compute NTK...\n",
            "It: 26100, Loss: 2.776e-04, Loss_bcs: 2.293e-08, Loss_res: 2.776e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.1672e-05\n",
            "mean_grad_res: 1.5825e-03\n",
            "Compute NTK...\n",
            "It: 26200, Loss: 2.760e-04, Loss_bcs: 2.180e-08, Loss_res: 2.759e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.2564e-05\n",
            "mean_grad_res: 4.9214e-03\n",
            "Compute NTK...\n",
            "It: 26300, Loss: 2.717e-04, Loss_bcs: 2.306e-08, Loss_res: 2.717e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.9535e-05\n",
            "mean_grad_res: 1.3798e-04\n",
            "Compute NTK...\n",
            "It: 26400, Loss: 2.690e-04, Loss_bcs: 2.214e-08, Loss_res: 2.690e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.8689e-05\n",
            "mean_grad_res: 2.1923e-04\n",
            "Compute NTK...\n",
            "It: 26500, Loss: 2.662e-04, Loss_bcs: 2.223e-08, Loss_res: 2.662e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.7117e-05\n",
            "mean_grad_res: 8.2887e-04\n",
            "Compute NTK...\n",
            "It: 26600, Loss: 2.721e-04, Loss_bcs: 1.867e-08, Loss_res: 2.721e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 7.1875e-05\n",
            "mean_grad_res: 1.1314e-02\n",
            "Compute NTK...\n",
            "It: 26700, Loss: 2.607e-04, Loss_bcs: 2.066e-08, Loss_res: 2.607e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.6538e-05\n",
            "mean_grad_res: 1.3585e-04\n",
            "Compute NTK...\n",
            "It: 26800, Loss: 2.584e-04, Loss_bcs: 2.048e-08, Loss_res: 2.584e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.4406e-05\n",
            "mean_grad_res: 2.0961e-03\n",
            "Compute NTK...\n",
            "It: 26900, Loss: 2.556e-04, Loss_bcs: 1.953e-08, Loss_res: 2.556e-04 ,Time: 0.57\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.4764e-05\n",
            "mean_grad_res: 1.3012e-04\n",
            "Compute NTK...\n",
            "It: 27000, Loss: 2.531e-04, Loss_bcs: 1.971e-08, Loss_res: 2.530e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.3998e-05\n",
            "mean_grad_res: 2.9915e-04\n",
            "Compute NTK...\n",
            "It: 27100, Loss: 2.505e-04, Loss_bcs: 1.867e-08, Loss_res: 2.505e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.3227e-05\n",
            "mean_grad_res: 2.8787e-04\n",
            "Compute NTK...\n",
            "It: 27200, Loss: 2.482e-04, Loss_bcs: 1.861e-08, Loss_res: 2.481e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.3100e-05\n",
            "mean_grad_res: 1.2697e-04\n",
            "Compute NTK...\n",
            "It: 27300, Loss: 2.459e-04, Loss_bcs: 1.846e-08, Loss_res: 2.459e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.1608e-05\n",
            "mean_grad_res: 6.6464e-04\n",
            "Compute NTK...\n",
            "It: 27400, Loss: 2.436e-04, Loss_bcs: 1.794e-08, Loss_res: 2.436e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.1736e-05\n",
            "mean_grad_res: 1.5503e-04\n",
            "Compute NTK...\n",
            "It: 27500, Loss: 2.413e-04, Loss_bcs: 1.756e-08, Loss_res: 2.413e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.0981e-05\n",
            "mean_grad_res: 1.1134e-04\n",
            "Compute NTK...\n",
            "It: 27600, Loss: 2.408e-04, Loss_bcs: 1.388e-08, Loss_res: 2.408e-04 ,Time: 0.55\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.6793e-05\n",
            "mean_grad_res: 5.1976e-03\n",
            "Compute NTK...\n",
            "It: 27700, Loss: 2.368e-04, Loss_bcs: 1.708e-08, Loss_res: 2.368e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.0025e-05\n",
            "mean_grad_res: 1.1366e-04\n",
            "Compute NTK...\n",
            "It: 27800, Loss: 2.347e-04, Loss_bcs: 1.660e-08, Loss_res: 2.347e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.8954e-05\n",
            "mean_grad_res: 4.7355e-04\n",
            "Compute NTK...\n",
            "It: 27900, Loss: 2.325e-04, Loss_bcs: 1.570e-08, Loss_res: 2.325e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.8502e-05\n",
            "mean_grad_res: 1.3098e-04\n",
            "Compute NTK...\n",
            "It: 28000, Loss: 2.350e-04, Loss_bcs: 1.314e-08, Loss_res: 2.350e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.9107e-05\n",
            "mean_grad_res: 5.5445e-03\n",
            "Compute NTK...\n",
            "It: 28100, Loss: 2.283e-04, Loss_bcs: 1.553e-08, Loss_res: 2.283e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.7493e-05\n",
            "mean_grad_res: 1.7930e-04\n",
            "Compute NTK...\n",
            "It: 28200, Loss: 2.262e-04, Loss_bcs: 1.550e-08, Loss_res: 2.262e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.6837e-05\n",
            "mean_grad_res: 2.1324e-04\n",
            "Compute NTK...\n",
            "It: 28300, Loss: 2.249e-04, Loss_bcs: 1.422e-08, Loss_res: 2.249e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 6.1044e-05\n",
            "mean_grad_res: 3.4493e-03\n",
            "Compute NTK...\n",
            "It: 28400, Loss: 2.222e-04, Loss_bcs: 1.511e-08, Loss_res: 2.222e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.6074e-05\n",
            "mean_grad_res: 1.0486e-04\n",
            "Compute NTK...\n",
            "It: 28500, Loss: 2.203e-04, Loss_bcs: 1.441e-08, Loss_res: 2.203e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.5653e-05\n",
            "mean_grad_res: 1.1947e-04\n",
            "Compute NTK...\n",
            "It: 28600, Loss: 2.186e-04, Loss_bcs: 1.355e-08, Loss_res: 2.185e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.6090e-05\n",
            "mean_grad_res: 1.6297e-03\n",
            "Compute NTK...\n",
            "It: 28700, Loss: 2.166e-04, Loss_bcs: 1.423e-08, Loss_res: 2.166e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.5247e-05\n",
            "mean_grad_res: 1.1874e-04\n",
            "Compute NTK...\n",
            "It: 28800, Loss: 2.148e-04, Loss_bcs: 1.386e-08, Loss_res: 2.148e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.4193e-05\n",
            "mean_grad_res: 1.6237e-04\n",
            "Compute NTK...\n",
            "It: 28900, Loss: 2.137e-04, Loss_bcs: 1.389e-08, Loss_res: 2.137e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.2839e-05\n",
            "mean_grad_res: 3.9319e-03\n",
            "Compute NTK...\n",
            "It: 29000, Loss: 2.111e-04, Loss_bcs: 1.320e-08, Loss_res: 2.111e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.3230e-05\n",
            "mean_grad_res: 1.0182e-04\n",
            "Compute NTK...\n",
            "It: 29100, Loss: 2.092e-04, Loss_bcs: 1.312e-08, Loss_res: 2.092e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.2730e-05\n",
            "mean_grad_res: 1.4334e-04\n",
            "Compute NTK...\n",
            "It: 29200, Loss: 2.122e-04, Loss_bcs: 1.084e-08, Loss_res: 2.122e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.6404e-05\n",
            "mean_grad_res: 3.9819e-03\n",
            "Compute NTK...\n",
            "It: 29300, Loss: 2.059e-04, Loss_bcs: 1.271e-08, Loss_res: 2.059e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.1498e-05\n",
            "mean_grad_res: 9.6076e-05\n",
            "Compute NTK...\n",
            "It: 29400, Loss: 2.042e-04, Loss_bcs: 1.244e-08, Loss_res: 2.042e-04 ,Time: 0.55\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.1014e-05\n",
            "mean_grad_res: 1.0936e-04\n",
            "Compute NTK...\n",
            "It: 29500, Loss: 2.026e-04, Loss_bcs: 1.233e-08, Loss_res: 2.025e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.0863e-05\n",
            "mean_grad_res: 1.6582e-04\n",
            "Compute NTK...\n",
            "It: 29600, Loss: 2.010e-04, Loss_bcs: 1.214e-08, Loss_res: 2.010e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.0662e-05\n",
            "mean_grad_res: 3.4992e-04\n",
            "Compute NTK...\n",
            "It: 29700, Loss: 1.994e-04, Loss_bcs: 1.200e-08, Loss_res: 1.994e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.0470e-05\n",
            "mean_grad_res: 1.5891e-04\n",
            "Compute NTK...\n",
            "It: 29800, Loss: 1.978e-04, Loss_bcs: 1.174e-08, Loss_res: 1.978e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.9782e-05\n",
            "mean_grad_res: 1.6883e-04\n",
            "Compute NTK...\n",
            "It: 29900, Loss: 1.962e-04, Loss_bcs: 1.147e-08, Loss_res: 1.962e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.9837e-05\n",
            "mean_grad_res: 1.9499e-04\n",
            "Compute NTK...\n",
            "It: 30000, Loss: 1.947e-04, Loss_bcs: 1.139e-08, Loss_res: 1.947e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.8567e-05\n",
            "mean_grad_res: 3.6166e-04\n",
            "Compute NTK...\n",
            "It: 30100, Loss: 1.931e-04, Loss_bcs: 1.129e-08, Loss_res: 1.931e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.9047e-05\n",
            "mean_grad_res: 1.2523e-04\n",
            "Compute NTK...\n",
            "It: 30200, Loss: 1.917e-04, Loss_bcs: 1.108e-08, Loss_res: 1.917e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.8059e-05\n",
            "mean_grad_res: 8.9109e-05\n",
            "Compute NTK...\n",
            "It: 30300, Loss: 1.958e-04, Loss_bcs: 9.051e-09, Loss_res: 1.958e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.5810e-05\n",
            "mean_grad_res: 1.2088e-02\n",
            "Compute NTK...\n",
            "It: 30400, Loss: 1.888e-04, Loss_bcs: 1.068e-08, Loss_res: 1.888e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.7831e-05\n",
            "mean_grad_res: 1.1619e-04\n",
            "Compute NTK...\n",
            "It: 30500, Loss: 1.874e-04, Loss_bcs: 1.042e-08, Loss_res: 1.874e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.7400e-05\n",
            "mean_grad_res: 9.7405e-05\n",
            "Compute NTK...\n",
            "It: 30600, Loss: 1.861e-04, Loss_bcs: 1.048e-08, Loss_res: 1.861e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.7026e-05\n",
            "mean_grad_res: 1.2153e-04\n",
            "Compute NTK...\n",
            "It: 30700, Loss: 1.847e-04, Loss_bcs: 1.029e-08, Loss_res: 1.847e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.6787e-05\n",
            "mean_grad_res: 1.3742e-04\n",
            "Compute NTK...\n",
            "It: 30800, Loss: 1.852e-04, Loss_bcs: 1.043e-08, Loss_res: 1.852e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 5.2652e-05\n",
            "mean_grad_res: 6.7689e-03\n",
            "Compute NTK...\n",
            "It: 30900, Loss: 1.820e-04, Loss_bcs: 1.019e-08, Loss_res: 1.820e-04 ,Time: 0.49\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.6480e-05\n",
            "mean_grad_res: 6.6206e-05\n",
            "Compute NTK...\n",
            "It: 31000, Loss: 1.808e-04, Loss_bcs: 1.006e-08, Loss_res: 1.807e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.6199e-05\n",
            "mean_grad_res: 9.7090e-05\n",
            "Compute NTK...\n",
            "It: 31100, Loss: 1.794e-04, Loss_bcs: 9.996e-09, Loss_res: 1.794e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.5357e-05\n",
            "mean_grad_res: 1.0207e-04\n",
            "Compute NTK...\n",
            "It: 31200, Loss: 1.781e-04, Loss_bcs: 9.398e-09, Loss_res: 1.781e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.5366e-05\n",
            "mean_grad_res: 1.3340e-04\n",
            "Compute NTK...\n",
            "It: 31300, Loss: 1.768e-04, Loss_bcs: 9.477e-09, Loss_res: 1.768e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.4489e-05\n",
            "mean_grad_res: 2.7978e-04\n",
            "Compute NTK...\n",
            "It: 31400, Loss: 1.754e-04, Loss_bcs: 9.434e-09, Loss_res: 1.754e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.4542e-05\n",
            "mean_grad_res: 2.0311e-04\n",
            "Compute NTK...\n",
            "It: 31500, Loss: 1.741e-04, Loss_bcs: 9.617e-09, Loss_res: 1.741e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.4663e-05\n",
            "mean_grad_res: 5.4096e-04\n",
            "Compute NTK...\n",
            "It: 31600, Loss: 1.729e-04, Loss_bcs: 9.250e-09, Loss_res: 1.729e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.4282e-05\n",
            "mean_grad_res: 9.6223e-05\n",
            "Compute NTK...\n",
            "It: 31700, Loss: 1.717e-04, Loss_bcs: 9.116e-09, Loss_res: 1.717e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.4057e-05\n",
            "mean_grad_res: 1.4517e-04\n",
            "Compute NTK...\n",
            "It: 31800, Loss: 1.704e-04, Loss_bcs: 8.637e-09, Loss_res: 1.704e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.3591e-05\n",
            "mean_grad_res: 2.0543e-04\n",
            "Compute NTK...\n",
            "It: 31900, Loss: 1.706e-04, Loss_bcs: 9.406e-09, Loss_res: 1.706e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.2453e-05\n",
            "mean_grad_res: 3.5428e-03\n",
            "Compute NTK...\n",
            "It: 32000, Loss: 1.680e-04, Loss_bcs: 8.430e-09, Loss_res: 1.680e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.2785e-05\n",
            "mean_grad_res: 8.7954e-05\n",
            "Compute NTK...\n",
            "It: 32100, Loss: 1.667e-04, Loss_bcs: 8.614e-09, Loss_res: 1.667e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.2498e-05\n",
            "mean_grad_res: 4.0003e-04\n",
            "Compute NTK...\n",
            "It: 32200, Loss: 1.656e-04, Loss_bcs: 8.434e-09, Loss_res: 1.656e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.2746e-05\n",
            "mean_grad_res: 1.6665e-04\n",
            "Compute NTK...\n",
            "It: 32300, Loss: 1.644e-04, Loss_bcs: 8.345e-09, Loss_res: 1.644e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.2446e-05\n",
            "mean_grad_res: 8.6748e-05\n",
            "Compute NTK...\n",
            "It: 32400, Loss: 1.634e-04, Loss_bcs: 8.290e-09, Loss_res: 1.634e-04 ,Time: 0.49\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.2013e-05\n",
            "mean_grad_res: 1.2395e-04\n",
            "Compute NTK...\n",
            "It: 32500, Loss: 1.641e-04, Loss_bcs: 9.803e-09, Loss_res: 1.641e-04 ,Time: 0.56\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.0825e-05\n",
            "mean_grad_res: 4.1316e-03\n",
            "Compute NTK...\n",
            "It: 32600, Loss: 1.611e-04, Loss_bcs: 8.282e-09, Loss_res: 1.611e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.1438e-05\n",
            "mean_grad_res: 7.3695e-05\n",
            "Compute NTK...\n",
            "It: 32700, Loss: 1.601e-04, Loss_bcs: 7.917e-09, Loss_res: 1.601e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.1139e-05\n",
            "mean_grad_res: 7.9635e-05\n",
            "Compute NTK...\n",
            "It: 32800, Loss: 1.589e-04, Loss_bcs: 7.939e-09, Loss_res: 1.589e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.0562e-05\n",
            "mean_grad_res: 1.2943e-04\n",
            "Compute NTK...\n",
            "It: 32900, Loss: 1.579e-04, Loss_bcs: 7.575e-09, Loss_res: 1.579e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.0919e-05\n",
            "mean_grad_res: 5.1962e-04\n",
            "Compute NTK...\n",
            "It: 33000, Loss: 1.569e-04, Loss_bcs: 7.929e-09, Loss_res: 1.569e-04 ,Time: 0.58\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.0948e-05\n",
            "mean_grad_res: 7.3929e-05\n",
            "Compute NTK...\n",
            "It: 33100, Loss: 1.559e-04, Loss_bcs: 7.617e-09, Loss_res: 1.559e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.0411e-05\n",
            "mean_grad_res: 8.2974e-05\n",
            "Compute NTK...\n",
            "It: 33200, Loss: 1.549e-04, Loss_bcs: 7.710e-09, Loss_res: 1.549e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 4.0063e-05\n",
            "mean_grad_res: 1.3496e-04\n",
            "Compute NTK...\n",
            "It: 33300, Loss: 1.538e-04, Loss_bcs: 7.380e-09, Loss_res: 1.538e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.9510e-05\n",
            "mean_grad_res: 1.1081e-04\n",
            "Compute NTK...\n",
            "It: 33400, Loss: 1.529e-04, Loss_bcs: 7.320e-09, Loss_res: 1.529e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.9998e-05\n",
            "mean_grad_res: 6.4664e-04\n",
            "Compute NTK...\n",
            "It: 33500, Loss: 1.518e-04, Loss_bcs: 7.392e-09, Loss_res: 1.518e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.9642e-05\n",
            "mean_grad_res: 1.8855e-04\n",
            "Compute NTK...\n",
            "It: 33600, Loss: 1.509e-04, Loss_bcs: 7.308e-09, Loss_res: 1.509e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.9326e-05\n",
            "mean_grad_res: 6.9602e-05\n",
            "Compute NTK...\n",
            "It: 33700, Loss: 1.499e-04, Loss_bcs: 7.174e-09, Loss_res: 1.499e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.9176e-05\n",
            "mean_grad_res: 8.9654e-05\n",
            "Compute NTK...\n",
            "It: 33800, Loss: 1.489e-04, Loss_bcs: 7.101e-09, Loss_res: 1.489e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.8921e-05\n",
            "mean_grad_res: 8.2120e-05\n",
            "Compute NTK...\n",
            "It: 33900, Loss: 1.480e-04, Loss_bcs: 6.947e-09, Loss_res: 1.480e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.8140e-05\n",
            "mean_grad_res: 1.5949e-04\n",
            "Compute NTK...\n",
            "It: 34000, Loss: 1.472e-04, Loss_bcs: 6.961e-09, Loss_res: 1.472e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.8290e-05\n",
            "mean_grad_res: 8.4285e-05\n",
            "Compute NTK...\n",
            "It: 34100, Loss: 1.462e-04, Loss_bcs: 6.707e-09, Loss_res: 1.462e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.8099e-05\n",
            "mean_grad_res: 6.2495e-05\n",
            "Compute NTK...\n",
            "It: 34200, Loss: 1.454e-04, Loss_bcs: 6.677e-09, Loss_res: 1.454e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.8019e-05\n",
            "mean_grad_res: 1.3106e-04\n",
            "Compute NTK...\n",
            "It: 34300, Loss: 1.445e-04, Loss_bcs: 6.810e-09, Loss_res: 1.445e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.7533e-05\n",
            "mean_grad_res: 5.7465e-05\n",
            "Compute NTK...\n",
            "It: 34400, Loss: 1.436e-04, Loss_bcs: 6.698e-09, Loss_res: 1.436e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.7871e-05\n",
            "mean_grad_res: 5.5094e-04\n",
            "Compute NTK...\n",
            "It: 34500, Loss: 1.428e-04, Loss_bcs: 6.685e-09, Loss_res: 1.428e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.7058e-05\n",
            "mean_grad_res: 9.5989e-05\n",
            "Compute NTK...\n",
            "It: 34600, Loss: 1.419e-04, Loss_bcs: 6.414e-09, Loss_res: 1.419e-04 ,Time: 0.58\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.7118e-05\n",
            "mean_grad_res: 9.0034e-05\n",
            "Compute NTK...\n",
            "It: 34700, Loss: 1.411e-04, Loss_bcs: 6.479e-09, Loss_res: 1.411e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.7219e-05\n",
            "mean_grad_res: 7.0276e-05\n",
            "Compute NTK...\n",
            "It: 34800, Loss: 1.402e-04, Loss_bcs: 6.479e-09, Loss_res: 1.402e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.6503e-05\n",
            "mean_grad_res: 8.3460e-05\n",
            "Compute NTK...\n",
            "It: 34900, Loss: 1.394e-04, Loss_bcs: 6.157e-09, Loss_res: 1.394e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.6898e-05\n",
            "mean_grad_res: 1.7086e-04\n",
            "Compute NTK...\n",
            "It: 35000, Loss: 1.386e-04, Loss_bcs: 5.829e-09, Loss_res: 1.386e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.6987e-05\n",
            "mean_grad_res: 6.1372e-04\n",
            "Compute NTK...\n",
            "It: 35100, Loss: 1.377e-04, Loss_bcs: 6.306e-09, Loss_res: 1.377e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.6508e-05\n",
            "mean_grad_res: 8.5539e-05\n",
            "Compute NTK...\n",
            "It: 35200, Loss: 1.369e-04, Loss_bcs: 6.022e-09, Loss_res: 1.369e-04 ,Time: 0.55\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.6084e-05\n",
            "mean_grad_res: 9.5528e-05\n",
            "Compute NTK...\n",
            "It: 35300, Loss: 1.398e-04, Loss_bcs: 5.234e-09, Loss_res: 1.398e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.8392e-05\n",
            "mean_grad_res: 4.9487e-03\n",
            "Compute NTK...\n",
            "It: 35400, Loss: 1.353e-04, Loss_bcs: 6.026e-09, Loss_res: 1.353e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.5847e-05\n",
            "mean_grad_res: 5.6467e-05\n",
            "Compute NTK...\n",
            "It: 35500, Loss: 1.345e-04, Loss_bcs: 6.088e-09, Loss_res: 1.345e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.5509e-05\n",
            "mean_grad_res: 6.5449e-05\n",
            "Compute NTK...\n",
            "It: 35600, Loss: 1.338e-04, Loss_bcs: 5.880e-09, Loss_res: 1.337e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.5751e-05\n",
            "mean_grad_res: 7.6166e-05\n",
            "Compute NTK...\n",
            "It: 35700, Loss: 1.331e-04, Loss_bcs: 5.822e-09, Loss_res: 1.330e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.5270e-05\n",
            "mean_grad_res: 6.6182e-05\n",
            "Compute NTK...\n",
            "It: 35800, Loss: 1.322e-04, Loss_bcs: 5.756e-09, Loss_res: 1.322e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.4643e-05\n",
            "mean_grad_res: 1.0376e-04\n",
            "Compute NTK...\n",
            "It: 35900, Loss: 1.314e-04, Loss_bcs: 5.607e-09, Loss_res: 1.314e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.4881e-05\n",
            "mean_grad_res: 4.5011e-04\n",
            "Compute NTK...\n",
            "It: 36000, Loss: 1.307e-04, Loss_bcs: 5.724e-09, Loss_res: 1.307e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.5384e-05\n",
            "mean_grad_res: 1.3794e-04\n",
            "Compute NTK...\n",
            "It: 36100, Loss: 1.300e-04, Loss_bcs: 5.688e-09, Loss_res: 1.300e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.4894e-05\n",
            "mean_grad_res: 7.3111e-05\n",
            "Compute NTK...\n",
            "It: 36200, Loss: 1.293e-04, Loss_bcs: 5.619e-09, Loss_res: 1.293e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.4725e-05\n",
            "mean_grad_res: 6.6089e-05\n",
            "Compute NTK...\n",
            "It: 36300, Loss: 1.286e-04, Loss_bcs: 5.611e-09, Loss_res: 1.286e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.4359e-05\n",
            "mean_grad_res: 7.1632e-05\n",
            "Compute NTK...\n",
            "It: 36400, Loss: 1.278e-04, Loss_bcs: 5.452e-09, Loss_res: 1.278e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.3979e-05\n",
            "mean_grad_res: 1.4304e-04\n",
            "Compute NTK...\n",
            "It: 36500, Loss: 1.271e-04, Loss_bcs: 5.438e-09, Loss_res: 1.271e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.4024e-05\n",
            "mean_grad_res: 2.6453e-04\n",
            "Compute NTK...\n",
            "It: 36600, Loss: 1.265e-04, Loss_bcs: 5.662e-09, Loss_res: 1.265e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.4554e-05\n",
            "mean_grad_res: 6.1843e-05\n",
            "Compute NTK...\n",
            "It: 36700, Loss: 1.258e-04, Loss_bcs: 5.560e-09, Loss_res: 1.258e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.4161e-05\n",
            "mean_grad_res: 5.8112e-05\n",
            "Compute NTK...\n",
            "It: 36800, Loss: 1.252e-04, Loss_bcs: 5.423e-09, Loss_res: 1.252e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.4032e-05\n",
            "mean_grad_res: 7.1933e-05\n",
            "Compute NTK...\n",
            "It: 36900, Loss: 1.246e-04, Loss_bcs: 5.301e-09, Loss_res: 1.246e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.3376e-05\n",
            "mean_grad_res: 7.8408e-05\n",
            "Compute NTK...\n",
            "It: 37000, Loss: 1.238e-04, Loss_bcs: 5.355e-09, Loss_res: 1.238e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.3746e-05\n",
            "mean_grad_res: 1.1327e-04\n",
            "Compute NTK...\n",
            "It: 37100, Loss: 1.231e-04, Loss_bcs: 5.080e-09, Loss_res: 1.231e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.2856e-05\n",
            "mean_grad_res: 1.1813e-04\n",
            "Compute NTK...\n",
            "It: 37200, Loss: 1.268e-04, Loss_bcs: 6.652e-09, Loss_res: 1.268e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.0993e-05\n",
            "mean_grad_res: 1.8438e-03\n",
            "Compute NTK...\n",
            "It: 37300, Loss: 1.218e-04, Loss_bcs: 5.244e-09, Loss_res: 1.218e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.3694e-05\n",
            "mean_grad_res: 6.6138e-05\n",
            "Compute NTK...\n",
            "It: 37400, Loss: 1.212e-04, Loss_bcs: 5.186e-09, Loss_res: 1.212e-04 ,Time: 0.55\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.3209e-05\n",
            "mean_grad_res: 6.9518e-05\n",
            "Compute NTK...\n",
            "It: 37500, Loss: 1.206e-04, Loss_bcs: 5.039e-09, Loss_res: 1.206e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.2797e-05\n",
            "mean_grad_res: 7.2210e-05\n",
            "Compute NTK...\n",
            "It: 37600, Loss: 1.200e-04, Loss_bcs: 5.036e-09, Loss_res: 1.199e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.2978e-05\n",
            "mean_grad_res: 6.5972e-05\n",
            "Compute NTK...\n",
            "It: 37700, Loss: 1.193e-04, Loss_bcs: 4.973e-09, Loss_res: 1.193e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.2035e-05\n",
            "mean_grad_res: 1.1338e-04\n",
            "Compute NTK...\n",
            "It: 37800, Loss: 1.187e-04, Loss_bcs: 4.883e-09, Loss_res: 1.187e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1829e-05\n",
            "mean_grad_res: 4.8535e-04\n",
            "Compute NTK...\n",
            "It: 37900, Loss: 1.181e-04, Loss_bcs: 5.022e-09, Loss_res: 1.181e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.2772e-05\n",
            "mean_grad_res: 6.4778e-05\n",
            "Compute NTK...\n",
            "It: 38000, Loss: 1.174e-04, Loss_bcs: 4.904e-09, Loss_res: 1.174e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1917e-05\n",
            "mean_grad_res: 6.7381e-05\n",
            "Compute NTK...\n",
            "It: 38100, Loss: 1.168e-04, Loss_bcs: 4.809e-09, Loss_res: 1.168e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1710e-05\n",
            "mean_grad_res: 5.7230e-05\n",
            "Compute NTK...\n",
            "It: 38200, Loss: 1.161e-04, Loss_bcs: 4.682e-09, Loss_res: 1.161e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1695e-05\n",
            "mean_grad_res: 7.9352e-05\n",
            "Compute NTK...\n",
            "It: 38300, Loss: 1.154e-04, Loss_bcs: 4.622e-09, Loss_res: 1.154e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1327e-05\n",
            "mean_grad_res: 1.5677e-04\n",
            "Compute NTK...\n",
            "It: 38400, Loss: 1.149e-04, Loss_bcs: 5.060e-09, Loss_res: 1.149e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.0119e-05\n",
            "mean_grad_res: 4.4925e-04\n",
            "Compute NTK...\n",
            "It: 38500, Loss: 1.143e-04, Loss_bcs: 4.721e-09, Loss_res: 1.143e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1603e-05\n",
            "mean_grad_res: 5.6299e-05\n",
            "Compute NTK...\n",
            "It: 38600, Loss: 1.139e-04, Loss_bcs: 4.646e-09, Loss_res: 1.139e-04 ,Time: 0.57\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1451e-05\n",
            "mean_grad_res: 5.5459e-05\n",
            "Compute NTK...\n",
            "It: 38700, Loss: 1.133e-04, Loss_bcs: 4.482e-09, Loss_res: 1.133e-04 ,Time: 0.65\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.0923e-05\n",
            "mean_grad_res: 1.1359e-04\n",
            "Compute NTK...\n",
            "It: 38800, Loss: 1.128e-04, Loss_bcs: 4.477e-09, Loss_res: 1.127e-04 ,Time: 0.53\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1051e-05\n",
            "mean_grad_res: 7.3907e-05\n",
            "Compute NTK...\n",
            "It: 38900, Loss: 1.122e-04, Loss_bcs: 4.358e-09, Loss_res: 1.122e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.0768e-05\n",
            "mean_grad_res: 9.7989e-05\n",
            "Compute NTK...\n",
            "It: 39000, Loss: 1.116e-04, Loss_bcs: 4.404e-09, Loss_res: 1.116e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.0361e-05\n",
            "mean_grad_res: 5.6529e-05\n",
            "Compute NTK...\n",
            "It: 39100, Loss: 1.111e-04, Loss_bcs: 4.556e-09, Loss_res: 1.111e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1693e-05\n",
            "mean_grad_res: 2.3000e-03\n",
            "Compute NTK...\n",
            "It: 39200, Loss: 1.106e-04, Loss_bcs: 4.629e-09, Loss_res: 1.106e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1465e-05\n",
            "mean_grad_res: 4.2948e-05\n",
            "Compute NTK...\n",
            "It: 39300, Loss: 1.101e-04, Loss_bcs: 4.533e-09, Loss_res: 1.100e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.1132e-05\n",
            "mean_grad_res: 7.1551e-05\n",
            "Compute NTK...\n",
            "It: 39400, Loss: 1.095e-04, Loss_bcs: 4.497e-09, Loss_res: 1.095e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.0852e-05\n",
            "mean_grad_res: 5.6678e-05\n",
            "Compute NTK...\n",
            "It: 39500, Loss: 1.090e-04, Loss_bcs: 4.206e-09, Loss_res: 1.090e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.0178e-05\n",
            "mean_grad_res: 1.1865e-04\n",
            "Compute NTK...\n",
            "It: 39600, Loss: 1.085e-04, Loss_bcs: 4.259e-09, Loss_res: 1.085e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.0406e-05\n",
            "mean_grad_res: 5.0582e-05\n",
            "Compute NTK...\n",
            "It: 39700, Loss: 1.079e-04, Loss_bcs: 4.217e-09, Loss_res: 1.079e-04 ,Time: 0.51\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 2.9782e-05\n",
            "mean_grad_res: 1.1240e-04\n",
            "Compute NTK...\n",
            "It: 39800, Loss: 1.074e-04, Loss_bcs: 4.271e-09, Loss_res: 1.074e-04 ,Time: 0.52\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 3.0069e-05\n",
            "mean_grad_res: 7.7219e-05\n",
            "Compute NTK...\n",
            "It: 39900, Loss: 1.068e-04, Loss_bcs: 3.933e-09, Loss_res: 1.068e-04 ,Time: 0.50\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 2.9572e-05\n",
            "mean_grad_res: 2.0853e-04\n",
            "Compute NTK...\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 40000, Loss: 1.063e-04, Loss_bcs: 4.151e-09, Loss_res: 1.063e-04 ,Time: 1.98\n",
            "update_res: 1.500e+00\n",
            "update_bcs1: 1.500e+00\n",
            "mean_grad_bcs: 2.9644e-05\n",
            "mean_grad_res: 6.7418e-05\n",
            "Compute NTK...\n",
            "elapsed: 3.37e+02\n",
            "Relative L2 error_u: 5.22e-05\n",
            "Relative L2 error_r: 9.07e-05\n",
            "mean value of lambda_bc: 1.00e+00\n",
            "first value of lambda_bc: 1.00e+00\n",
            "Relative L2 error_u: 5.22e-05\n",
            "Relative L2 error_v: 9.07e-05\n",
            "Save uv NN parameters successfully in %s ...checkpoints/Dec-29-2023_22-59-52-461801_M1\n",
            "Final loss total loss: 1.063211e-04\n",
            "Final loss loss_res: 1.063169e-04\n",
            "Final loss loss_bcs: 4.150956e-09\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Method:  mini_batch\n",
            "\n",
            "average of time_list: 336.73049330711365\n",
            "average of error_u_list: 5.221516236998207e-05\n",
            "average of error_v_list: 9.070802330410866e-05\n"
          ]
        }
      ],
      "source": [
        "# Define computional domain\n",
        "bc1_coords = np.array([[0.0], [0.0]])\n",
        "bc2_coords = np.array([[1.0], [1.0]])\n",
        "dom_coords = np.array([[0.0], [1.0]])\n",
        "\n",
        "# Training data on u(x) -- Dirichlet boundary conditions\n",
        "\n",
        "nn  = 100\n",
        "\n",
        "X_bc1 = dom_coords[0, 0] * np.ones((nn // 2, 1))\n",
        "X_bc2 = dom_coords[1, 0] * np.ones((nn // 2, 1))\n",
        "X_u = np.vstack([X_bc1, X_bc2])\n",
        "Y_u = u(X_u, a)\n",
        "\n",
        "X_r = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "Y_r = u_xx(X_r, a)\n",
        "\n",
        "nn = 1000\n",
        "X_star = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "u_star = u(X_star, a)\n",
        "r_star = u_xx(X_star, a)\n",
        "\n",
        "nIter =40001\n",
        "bcbatch_size = 500\n",
        "ubatch_size = 5000\n",
        "mbbatch_size = 128\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "mode = 'M1'\n",
        "layers = [1, 500,500, 1]\n",
        "\n",
        "\n",
        "\n",
        "iterations = 1\n",
        "methods = [ \"mini_batch\"]\n",
        "\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "for mtd in methods:\n",
        "    print(\"Method: \", mtd)\n",
        "    time_list = []\n",
        "    error_u_list = []\n",
        "    error_r_list = []\n",
        "    \n",
        "    for index in range(iterations):\n",
        "\n",
        "        print(\"Epoch: \", str(index+1))\n",
        "\n",
        "        # Create residual sampler\n",
        "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "        tf.reset_default_graph()\n",
        "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "\n",
        "            model = PINN(layers, X_u, Y_u, X_r, Y_r , mode , sess)    \n",
        "\n",
        "            # Train model\n",
        "            start_time = time.time()\n",
        "\n",
        "            if mtd ==\"full_batch\":\n",
        "                print(\"full_batch method is used\")\n",
        "                model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "            elif mtd ==\"mini_batch\":\n",
        "                print(\"mini_batch method is used\")\n",
        "                model.trainmb(nIter, mbbatch_size)\n",
        "            else:\n",
        "                print(\"unknown method!\")\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            # Predictions\n",
        "            u_pred = model.predict_u(X_star)\n",
        "            r_pred = model.predict_r(X_star)\n",
        "            # Predictions\n",
        "\n",
        " \n",
        "\n",
        "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "            error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
        "\n",
        "            model.print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "            model.print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "\n",
        "            model.print('mean value of lambda_bc: {:.2e}'.format(np.average(model.adaptive_constant_bcs_log)))\n",
        "            model.print('first value of lambda_bc: {:.2e}'.format(model.adaptive_constant_bcs_log[0]))\n",
        "            model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "            model.print('Relative L2 error_v: {:.2e}'.format(error_r))\n",
        "            \n",
        "            model.save_NN()\n",
        "            model.plot_ntk()\n",
        "            model.plot_grad()\n",
        "            model.plot_lambda()\n",
        "            model.plt_prediction( X_star , u_star , u_pred)\n",
        "            sess.close()  \n",
        "            \n",
        "        time_list.append(elapsed)\n",
        "        error_u_list.append(error_u)\n",
        "        error_r_list.append(error_r)\n",
        "\n",
        "    print(\"\\n\\nMethod: \", mtd)\n",
        "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "    print(\"average of error_v_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "    result_dict[mtd] = [time_list ,error_u_list ,error_r_list ]\n",
        "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_model\"+mode+\"_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(bcbatch_size)+\"_n\"+str(iterations)+\"_nIter\"+str(nIter)+\".mat\") , result_dict)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "elapsed: 3.33e+02\n",
            "Relative L2 error_u: 8.37e-04\n",
            "Relative L2 error_r: 1.83e-03\n",
            "mean value of lambda_bc: 2.22e+05\n",
            "first value of lambda_bc: 5.10e+02\n",
            "Relative L2 error_u: 8.37e-04\n",
            "Relative L2 error_v: 1.83e-03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Method:  mini_batch\n",
            "\n",
            "average of time_list: 333.1971719264984\n",
            "average of error_u_list: 0.000837360383901785\n",
            "average of error_v_list: 0.0018271384381203183\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
        "\n",
        "model.print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "model.print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "\n",
        "model.print('mean value of lambda_bc: {:.2e}'.format(np.average(model.adaptive_constant_bcs_log)))\n",
        "model.print('first value of lambda_bc: {:.2e}'.format(model.adaptive_constant_bcs_log[0]))\n",
        "model.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "model.print('Relative L2 error_v: {:.2e}'.format(error_r))\n",
        "\n",
        "# model.save_NN()\n",
        "model.plot_ntk()\n",
        "model.plot_grad()\n",
        "model.plot_lambda()\n",
        "model.plt_prediction( X_star , u_star , u_pred)\n",
        "sess.close()  \n",
        "\n",
        "time_list.append(elapsed)\n",
        "error_u_list.append(error_u)\n",
        "error_r_list.append(error_r)\n",
        "\n",
        "print(\"\\n\\nMethod: \", mtd)\n",
        "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "print(\"average of error_v_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "result_dict[mtd] = [time_list ,error_u_list ,error_r_list ]\n",
        "# scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_model\"+mode+\"_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(bcbatch_size)+\"_n\"+str(iterations)+\"_nIter\"+str(nIter)+\".mat\") , result_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.mean_grad_bcs_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_grad(self):\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches([15,8])\n",
        "    ax.semilogy(self.mean_grad_res_list_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{phy}}}$')\n",
        "    ax.semilogy(self.mean_grad_bcs_list_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
        "\n",
        "    ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "    ax.set_ylabel(\"loss\", fontsize=15)\n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.legend()\n",
        "    path = os.path.join(self.dirname,'grad_history.png')\n",
        "    plt.savefig(path)\n",
        "\n",
        "plot_grad(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model\n",
        "layers = [1, 512, 1]  \n",
        "# layers = [1, 512, 512, 512, 1]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Fw807UNzhu5z",
        "outputId": "929ae89c-3e10-4c56-e349-051441e8ab32"
      },
      "outputs": [],
      "source": [
        "loss_bcs = model.loss_bcs_log\n",
        "loss_res = model.loss_res_log\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "plt.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
        "plt.plot(loss_bcs, label='$\\mathcal{L}_{b}$')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFLIBq5xjZ3v"
      },
      "source": [
        "**Model Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To0PDN17cc0v",
        "outputId": "7284b31e-f2fe-41ab-93a9-4c2c91f94cde"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "K428lOuXhdc8",
        "outputId": "b1e23055-178c-400c-8972-f3e7987e0892"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYdfKGLj6h0"
      },
      "source": [
        "**NTK Eigenvalues**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3dByeQjhBYj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "vSn3Q_1IhisN",
        "outputId": "4c713f42-11b2-4de9-8698-085eb54c164d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIS5UH81kOxT"
      },
      "source": [
        "**Change of NTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF4Q_iZshQ-0"
      },
      "outputs": [],
      "source": [
        "# Change of the NTK\n",
        "NTK_change_list = []\n",
        "K0 = K_list[0]\n",
        "for K in K_list:\n",
        "    diff = np.linalg.norm(K - K0) / np.linalg.norm(K0) \n",
        "    NTK_change_list.append(diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "E-_gPGpCkF4n",
        "outputId": "9893e038-907d-4425-bb6e-8ecdb2ad497d"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6,5))\n",
        "plt.plot(NTK_change_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg0ZGHbAkW6N"
      },
      "source": [
        "\n",
        "**Change of NN Params**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLGv9JUuioVZ"
      },
      "outputs": [],
      "source": [
        "# Change of the weights and biases\n",
        "def compute_weights_diff(weights_1, weights_2):\n",
        "    weights = []\n",
        "    N = len(weights_1)\n",
        "    for k in range(N):\n",
        "        weight = weights_1[k] - weights_2[k]\n",
        "        weights.append(weight)\n",
        "    return weights\n",
        "\n",
        "def compute_weights_norm(weights, biases):\n",
        "    norm = 0\n",
        "    for w in weights:\n",
        "        norm = norm + np.sum(np.square(w))\n",
        "    for b in biases:\n",
        "        norm = norm + np.sum(np.square(b))\n",
        "    norm = np.sqrt(norm)\n",
        "    return norm\n",
        "\n",
        "# Restore the list weights and biases\n",
        "weights_log = model.weights_log\n",
        "biases_log = model.biases_log\n",
        "\n",
        "weights_0 = weights_log[0]\n",
        "biases_0 = biases_log[0]\n",
        "\n",
        "# Norm of the weights at initialization\n",
        "weights_init_norm = compute_weights_norm(weights_0, biases_0)\n",
        "\n",
        "weights_change_list = []\n",
        "\n",
        "N = len(weights_log)\n",
        "for k in range(N):\n",
        "    weights_diff = compute_weights_diff(weights_log[k], weights_log[0])\n",
        "    biases_diff = compute_weights_diff(biases_log[k], biases_log[0])\n",
        "    \n",
        "    weights_diff_norm = compute_weights_norm(weights_diff, biases_diff)\n",
        "    weights_change = weights_diff_norm / weights_init_norm\n",
        "    weights_change_list.append(weights_change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "5NLsAxgzi4KH",
        "outputId": "74d92bf9-0dde-438e-e9b3-a551904f4e2f"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6,5))\n",
        "plt.plot(weights_change_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYbzkhfMjJ8k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINNsNTK_Poisson.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
