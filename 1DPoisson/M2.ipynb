{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OybZJApDYGsi",
        "outputId": "dc7cdffa-8613-42ee-86cc-0bc5000b2998"
      },
      "outputs": [],
      "source": [
        "# # Switch to tensorflow 1.x\n",
        "# %tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WkCgnRiYQSY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
        "import numpy as np\n",
        "import timeit\n",
        "from scipy.interpolate import griddata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
        "import timeit\n",
        "\n",
        "import sys\n",
        "\n",
        "import scipy\n",
        "import scipy.io\n",
        "import time\n",
        "import logging\n",
        "\n",
        "import os.path\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y7cHTcJfBTR"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "    # Initialize the class\n",
        "    def __init__(self, dim, coords, func, name=None):\n",
        "        self.dim = dim\n",
        "        self.coords = coords\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "\n",
        "    def sample(self, N):\n",
        "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
        "        y = self.func(x)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDqDWN3nfSAg"
      },
      "outputs": [],
      "source": [
        "class PINN:\n",
        "    def __init__(self, layers, X_u, Y_u, X_r, Y_r ,mode ,  sess):\n",
        "\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        self.dirname, logpath = self.make_output_dir()\n",
        "        self.logger = self.get_logger(logpath)     \n",
        "\n",
        "        self.mu_X, self.sigma_X = X_r.mean(0), X_r.std(0)\n",
        "        self.mu_x, self.sigma_x = self.mu_X[0], self.sigma_X[0]\n",
        "\n",
        "        # Normalize\n",
        "        self.X_u = (X_u - self.mu_X) / self.sigma_X\n",
        "        self.Y_u = Y_u\n",
        "        self.X_r = (X_r - self.mu_X) / self.sigma_X\n",
        "        self.Y_r = Y_r\n",
        "\n",
        "        self.beta = 0.9\n",
        "\n",
        "        self.lam_bc_var =np.array(1.0)\n",
        "        \n",
        "        self.lam_bc_tf = tf.placeholder(tf.float32, shape=self.lam_bc_var.shape)\n",
        "        # Initialize network weights and biases\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "            \n",
        "        # Define the size of the Kernel\n",
        "        self.kernel_size = X_u.shape[0]\n",
        "        # Define Tensorflow session\n",
        "        self.sess = sess# tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
        "\n",
        "        # Define placeholders and computational graph\n",
        "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.x_bc_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_bc_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        \n",
        "        self.x_u_ntk_tf = tf.placeholder(tf.float32, shape=(self.kernel_size, 1))\n",
        "        self.x_r_ntk_tf = tf.placeholder(tf.float32, shape=(self.kernel_size, 1))\n",
        "\n",
        "\n",
        "        # Evaluate predictions\n",
        "        self.u_bc_pred = self.net_u(self.x_bc_tf)\n",
        "\n",
        "        self.u_pred = self.net_u(self.x_u_tf)\n",
        "        self.r_pred = self.net_r(self.x_r_tf)\n",
        "        \n",
        "        self.u_ntk_pred = self.net_u(self.x_u_ntk_tf)\n",
        "        self.r_ntk_pred = self.net_r(self.x_r_ntk_tf)\n",
        "     \n",
        "        # Boundary loss\n",
        "        self.loss_bcs = tf.reduce_mean(tf.square(self.u_bc_pred - self.u_bc_tf))\n",
        "\n",
        "        # Residual loss        \n",
        "        self.loss_res =  tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
        "        \n",
        "        # Total loss\n",
        "        self.loss = self.loss_res + self.lam_bc_tf * self.loss_bcs\n",
        "\n",
        "        # Define optimizer with learning rate schedule\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 1e-5\n",
        "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
        "        # Passing global_step to minimize() will increment it at each step.\n",
        "        # To compute NTK, it is better to use SGD optimizer\n",
        "        # since the corresponding gradient flow is not exactly same.\n",
        "        self.train_op = tf.train.GradientDescentOptimizer(starter_learning_rate).minimize(self.loss)\n",
        "        # self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
        "\n",
        "\n",
        "        \n",
        "        # self.saver = tf.train.Saver()\n",
        "        \n",
        "        # Compute the Jacobian for weights and biases in each hidden layer  \n",
        "        self.J_u = self.compute_jacobian(self.u_ntk_pred) \n",
        "        self.J_r = self.compute_jacobian(self.r_ntk_pred)\n",
        "        \n",
        "        # The empirical NTK = J J^T, compute NTK of PINNs \n",
        "        self.K_uu = self.compute_ntk(self.J_u, self.x_u_ntk_tf, self.J_u, self.x_u_ntk_tf)\n",
        "        self.K_ur = self.compute_ntk(self.J_u, self.x_u_ntk_tf, self.J_r, self.x_r_ntk_tf)\n",
        "        self.K_rr = self.compute_ntk(self.J_r, self.x_r_ntk_tf, self.J_r, self.x_r_ntk_tf)\n",
        "        \n",
        "        # Logger\n",
        "        # Loss logger\n",
        "        self.loss_bcs_log = []\n",
        "        self.loss_res_log = []\n",
        "\n",
        "        # NTK logger \n",
        "        self.K_uu_log = []\n",
        "        self.K_rr_log = []\n",
        "        self.K_ur_log = []\n",
        "        \n",
        "        # Weights logger \n",
        "        self.weights_log = []\n",
        "        self.biases_log = []\n",
        "       # Gradients Storage\n",
        "\n",
        "\n",
        "        # Generate dicts for gradients storage\n",
        "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
        "        self.dict_gradients_bc_layers = self.generate_grad_dict()\n",
        "\n",
        "        self.grad_res = []\n",
        "        self.grad_bc = []\n",
        "        self.grad_bc_list = []\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
        "            self.grad_bc.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.grad_bc_list.append(tf.reduce_max(tf.abs(self.grad_res[i])) / tf.reduce_mean(tf.abs(self.grad_bc[i])))\n",
        "\n",
        "        self.adaptive_constant_bc = tf.reduce_max(tf.stack(self.grad_bc_list))\n",
        "\n",
        "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bcs] \n",
        "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs\"] \n",
        "\n",
        "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
        "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
        "        \n",
        "\n",
        "        # Initialize Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        self.sess.run(init)\n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "    def generate_grad_dict(self):\n",
        "        num = len(self.layers) - 1\n",
        "        grad_dict = {}\n",
        "        for i in range(num):\n",
        "            grad_dict['layer_{}'.format(i + 1)] = []\n",
        "        return grad_dict\n",
        "\n",
        "    # Xavier initialization\n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "        return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,  dtype=tf.float32)\n",
        "    \n",
        "    # NTK initialization\n",
        "    def NTK_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        std = 1. / np.sqrt(in_dim)\n",
        "        return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * std, dtype=tf.float32)\n",
        "\n",
        "     # Initialize network weights and biases using Xavier initialization\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.NTK_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.random.normal([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    # Evaluates the forward pass\n",
        "    def forward_pass(self, H):\n",
        "        num_layers = len(self.layers)\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = self.weights[l]\n",
        "            b = self.biases[l]\n",
        "            H = tf.nn.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = self.weights[-1]\n",
        "        b = self.biases[-1]\n",
        "        H = tf.add(tf.matmul(H, W), b)\n",
        "        return H\n",
        "\n",
        "    # Evaluates the PDE solution\n",
        "    def net_u(self, x):\n",
        "        u = self.forward_pass(x)\n",
        "        return u\n",
        "\n",
        "    # Forward pass for the residual\n",
        "    def net_r(self, x):\n",
        "        u = self.net_u(x)\n",
        "\n",
        "        u_x = tf.gradients(u, x)[0] / self.sigma_x\n",
        "        u_xx = tf.gradients(u_x, x)[0] / self.sigma_x\n",
        "\n",
        "        res_u = u_xx\n",
        "        return res_u\n",
        "    \n",
        "    # Compute Jacobian for each weights and biases in each layer and retrun a list \n",
        "    def compute_jacobian(self, f):\n",
        "        J_list =[]\n",
        "        L = len(self.weights)    \n",
        "        for i in range(L):\n",
        "            J_w = jacobian(f, self.weights[i])\n",
        "            J_list.append(J_w)\n",
        "     \n",
        "        for i in range(L):\n",
        "            J_b = jacobian(f, self.biases[i])\n",
        "            J_list.append(J_b)\n",
        "        return J_list\n",
        "    \n",
        "    # Compute the empirical NTK = J J^T\n",
        "    def compute_ntk(self, J1_list, x1, J2_list, x2):\n",
        "        D = x1.shape[0]\n",
        "        N = len(J1_list)\n",
        "        \n",
        "        Ker = tf.zeros((D,D))\n",
        "        for k in range(N):\n",
        "            J1 = tf.reshape(J1_list[k], shape=(D,-1))\n",
        "            J2 = tf.reshape(J2_list[k], shape=(D,-1))\n",
        "            \n",
        "            K = tf.matmul(J1, tf.transpose(J2))\n",
        "            Ker = Ker + K\n",
        "        return Ker\n",
        "            \n",
        "    # Trains the model by minimizing the MSE loss\n",
        "    def trainmb(self, nIter=10000, batch_size=128, log_NTK=True, log_weights=True):\n",
        "\n",
        "\n",
        "        itValues = [1,100,1000,39999]\n",
        "        start_time = timeit.default_timer()\n",
        "        # Fetch boundary mini-batches\n",
        "        # Define a dictionary for associating placeholders with data\n",
        "        tf_dict = {self.x_bc_tf: self.X_u,\n",
        "                    self.u_bc_tf: self.Y_u,\n",
        "                    self.x_u_tf: self.X_u,\n",
        "                    self.x_r_tf: self.X_r,\n",
        "                    self.r_tf: self.Y_r,\n",
        "                    self.lam_bc_tf:self.lam_bc_var\n",
        "                    }\n",
        "        \n",
        "        for it in range(nIter):\n",
        "\n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "            _, batch_losses = self.sess.run([self.train_op, self.loss_tensor_list] ,tf_dict)\n",
        "\n",
        "            # self.print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = timeit.default_timer() - start_time\n",
        "                loss_value , loss_bcs_value, loss_res_value = self.sess.run([self.loss , self.loss_bcs, self.loss_res], tf_dict)\n",
        "                # self.loss_bcs_log.append(loss_bcs_value)\n",
        "                # self.loss_res_log.append(loss_res_value)\n",
        "\n",
        "                self.print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e ,Time: %.2f' %  (it, loss_value, loss_bcs_value, loss_res_value, elapsed))\n",
        "                \n",
        "            if log_NTK:\n",
        "                # provide x, x' for NTK\n",
        "                if it % 1000 == 0:\n",
        "\n",
        "                    adaptive_constant_bc = self.sess.run([self.adaptive_constant_bc],  tf_dict)\n",
        "                    print(adaptive_constant_bc[0])\n",
        "\n",
        "                    self.lam_bc_var = self.lam_bc_var * (1.0 - self.beta)  +  self.beta * adaptive_constant_bc[0]\n",
        "\n",
        "                    # self.lam_bc_var = loss_bcs_value\n",
        "                    # self.lam_r_var = loss_res_value\n",
        "                    self.print(\"Compute NTK...\")\n",
        "\n",
        "                    tf_dict2 = {self.x_u_ntk_tf: self.X_u,   self.x_r_ntk_tf: self.X_r  }\n",
        "\n",
        "                    K_uu_value, K_ur_value, K_rr_value = self.sess.run([self.K_uu,  self.K_ur,  self.K_rr], tf_dict2)\n",
        "                    self.K_uu_log.append(K_uu_value)\n",
        "                    self.K_ur_log.append(K_ur_value)\n",
        "                    self.K_rr_log.append(K_rr_value)\n",
        "            start_time = timeit.default_timer()\n",
        "\n",
        "            if it in itValues:\n",
        "                    self.plot_layerLoss(tf_dict , it)\n",
        "                    self.print(\"Gradients information stored ...\")\n",
        "\n",
        "            sys.stdout.flush()\n",
        "            self.assign_batch_losses(batch_losses)\n",
        "            for key in self.loss_history:\n",
        "                self.loss_history[key].append(self.epoch_loss[key])\n",
        "            \n",
        "    # Evaluates predictions at test points\n",
        "    def predict_u(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.x_u_tf: X_star}\n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        return u_star\n",
        "\n",
        "    # Evaluates predictions at test points\n",
        "    def predict_r(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.x_r_tf: X_star}\n",
        "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
        "        return r_star\n",
        " ############################################################\n",
        "\n",
        "    def assign_batch_losses(self, batch_losses):\n",
        "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
        "            self.epoch_loss[key] = loss_values\n",
        "\n",
        "  ############################################################\n",
        "###################################################################################################################\n",
        "\n",
        "\n",
        "    def plot_layerLoss(self , tf_dict , epoch):\n",
        "        ## Gradients #\n",
        "        num_layers = len(self.layers)\n",
        "        for i in range(num_layers - 1):\n",
        "            grad_res, grad_bc  = self.sess.run([ self.grad_res[i],self.grad_bc[i]], feed_dict=tf_dict)\n",
        "\n",
        "            # save gradients of loss_r and loss_u\n",
        "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
        "            self.dict_gradients_bc_layers['layer_' + str(i + 1)].append(grad_bc.flatten())\n",
        "\n",
        "        num_hidden_layers = num_layers -1\n",
        "        cnt = 1\n",
        "        fig = plt.figure(4, figsize=(13, 4))\n",
        "        for j in range(num_hidden_layers):\n",
        "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
        "            ax.set_title('Layer {}'.format(j + 1))\n",
        "            ax.set_yscale('symlog')\n",
        "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
        "            gradients_bc = self.dict_gradients_bc_layers['layer_' + str(j + 1)][-1]\n",
        "\n",
        "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
        "\n",
        "            sns.distplot(gradients_bc, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc}}$')\n",
        "\n",
        "            #ax.get_legend().remove()\n",
        "            ax.set_xlim([-1.0, 1.0])\n",
        "            #ax.set_ylim([0, 150])\n",
        "            cnt += 1\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
        "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
        "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    # #########################\n",
        "    # def make_output_dir(self):\n",
        "        \n",
        "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
        "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
        "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "    #     os.mkdir(dirname)\n",
        "    #     text = 'output.log'\n",
        "    #     logpath = os.path.join(dirname, text)\n",
        "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
        "\n",
        "    #     return dirname, logpath\n",
        "    \n",
        "    # # ###########################################################\n",
        "    def make_output_dir(self):\n",
        "        \n",
        "        if not os.path.exists(\"checkpoints\"):\n",
        "            os.mkdir(\"checkpoints\")\n",
        "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "        os.mkdir(dirname)\n",
        "        text = 'output.log'\n",
        "        logpath = os.path.join(dirname, text)\n",
        "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M1.ipynb'))\n",
        "        return dirname, logpath\n",
        "    \n",
        "\n",
        "    def get_logger(self, logpath):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "        sh = logging.StreamHandler()\n",
        "        sh.setLevel(logging.DEBUG)        \n",
        "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
        "        fh = logging.FileHandler(logpath)\n",
        "        logger.addHandler(sh)\n",
        "        logger.addHandler(fh)\n",
        "        return logger\n",
        "    \n",
        "    def print(self, *args):\n",
        "        for word in args:\n",
        "            if len(args) == 1:\n",
        "                self.logger.info(word)\n",
        "            elif word != args[-1]:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "            else:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\\n\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "\n",
        "\n",
        "    def plot_loss_history(self , path):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        for key in self.loss_history:\n",
        "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
        "            ax.semilogy(self.loss_history[key], label=key)\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        plt.savefig(path)\n",
        "        #plt.show()\n",
        "       #######################\n",
        "    def save_NN(self):\n",
        "\n",
        "        uv_weights = self.sess.run(self.weights)\n",
        "        uv_biases = self.sess.run(self.biases)\n",
        "\n",
        "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
        "            pickle.dump([uv_weights, uv_biases], f)\n",
        "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
        "\n",
        "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
        "        #     pickle.dump(self.loss_rec, f)\n",
        "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
        "            self.plot_loss_history(f)\n",
        "\n",
        "        return self.dirname\n",
        "    \n",
        "    def plot_ntk(self):\n",
        "        # Create empty lists for storing the eigenvalues of NTK\n",
        "        lambda_K_log = []\n",
        "        lambda_K_uu_log = []\n",
        "        lambda_K_ur_log = []\n",
        "        lambda_K_rr_log = []\n",
        "\n",
        "        # Restore the NTK\n",
        "        K_uu_list = self.K_uu_log\n",
        "        K_ur_list = self.K_ur_log\n",
        "        K_rr_list = self.K_rr_log\n",
        "        K_list = []\n",
        "            \n",
        "        for k in range(len(K_uu_list)):\n",
        "            K_uu = K_uu_list[k]\n",
        "            K_ur = K_ur_list[k]\n",
        "            K_rr = K_rr_list[k]\n",
        "            \n",
        "            K = np.concatenate([np.concatenate([K_uu, K_ur], axis = 1), np.concatenate([K_ur.T, K_rr], axis = 1)], axis = 0)\n",
        "            K_list.append(K)\n",
        "\n",
        "            # Compute eigenvalues\n",
        "            lambda_K, _ = np.linalg.eig(K)\n",
        "            lambda_K_uu, _ = np.linalg.eig(K_uu)\n",
        "            lambda_K_rr, _ = np.linalg.eig(K_rr)\n",
        "            \n",
        "            # Sort in descresing order\n",
        "            lambda_K = np.sort(np.real(lambda_K))[::-1]\n",
        "            lambda_K_uu = np.sort(np.real(lambda_K_uu))[::-1]\n",
        "            lambda_K_rr = np.sort(np.real(lambda_K_rr))[::-1]\n",
        "            \n",
        "            # Store eigenvalues\n",
        "            lambda_K_log.append(lambda_K)\n",
        "            lambda_K_uu_log.append(lambda_K_uu)\n",
        "            lambda_K_rr_log.append(lambda_K_rr)\n",
        "        fig = plt.figure(figsize=(18, 5))\n",
        "        plt.subplot(1,3,1)\n",
        "        for i in range(1, len(lambda_K_log), 10):\n",
        "            plt.plot(lambda_K_log[i], '--')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.title(r'Eigenvalues of ${K}$')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(1,3,2)\n",
        "        for i in range(1, len(lambda_K_uu_log), 10):\n",
        "            plt.plot(lambda_K_uu_log[i], '--')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.title(r'Eigenvalues of ${K}_{uu}$')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(1,3,3)\n",
        "        for i in range(1, len(lambda_K_log), 10):\n",
        "            plt.plot(lambda_K_rr_log[i], '--')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.title(r'Eigenvalues of ${K}_{rr}$')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.dirname,\"ntk.png\"))\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    def plt_prediction(self , X_star , u_star , u_pred):\n",
        "        fig = plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(X_star, u_star, label='Exact')\n",
        "        plt.plot(X_star, u_pred, '--', label='Predicted')\n",
        "        plt.xlabel('$x$')\n",
        "        plt.ylabel('$y$')\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(X_star, np.abs(u_star - u_pred), label='Error')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('$x$')\n",
        "        plt.ylabel('Point-wise error')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
        "        plt.close(\"all\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN1jEdRwY90i"
      },
      "outputs": [],
      "source": [
        "# Define solution and its Laplace\n",
        "a = 4\n",
        "\n",
        "def u(x, a):\n",
        "  return np.sin(np.pi * a * x)\n",
        "\n",
        "def u_xx(x, a):\n",
        "  return -(np.pi * a)**2 * np.sin(np.pi * a * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
        "def test_method(method , layers,  X_u, Y_u, X_r, Y_r , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size , mode):\n",
        "\n",
        "\n",
        "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "        # sess.run(init)\n",
        "\n",
        "        model = PINN(layers, X_u, Y_u, X_r, Y_r , mode , sess)    \n",
        "\n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "\n",
        "        if method ==\"full_batch\":\n",
        "            print(\"full_batch method is used\")\n",
        "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "        elif method ==\"mini_batch\":\n",
        "            print(\"mini_batch method is used\")\n",
        "            model.trainmb(nIter, mbbatch_size)\n",
        "        else:\n",
        "            print(\"unknown method!\")\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Predictions\n",
        "        u_pred = model.predict_u(X_star)\n",
        "        r_pred = model.predict_r(X_star)\n",
        "        # Predictions\n",
        "\n",
        "        sess.close()   \n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "    error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
        "\n",
        "    print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "    print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "\n",
        "    return [elapsed, error_u , error_r ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define computional domain\n",
        "bc1_coords = np.array([[0.0], [0.0]])\n",
        "bc2_coords = np.array([[1.0], [1.0]])\n",
        "dom_coords = np.array([[0.0], [1.0]])\n",
        "\n",
        "# Training data on u(x) -- Dirichlet boundary conditions\n",
        "\n",
        "nn  = 100\n",
        "\n",
        "X_bc1 = dom_coords[0, 0] * np.ones((nn // 2, 1))\n",
        "X_bc2 = dom_coords[1, 0] * np.ones((nn // 2, 1))\n",
        "X_u = np.vstack([X_bc1, X_bc2])\n",
        "Y_u = u(X_u, a)\n",
        "\n",
        "X_r = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "Y_r = u_xx(X_r, a)\n",
        "\n",
        "nn = 1000\n",
        "X_star = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "u_star = u(X_star, a)\n",
        "r_star = u_xx(X_star, a)\n",
        "\n",
        "nIter =40001\n",
        "bcbatch_size = 500\n",
        "ubatch_size = 5000\n",
        "mbbatch_size = 128\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "mode = 'M2'\n",
        "layers = [1, 500, 1]\n",
        "\n",
        "\n",
        "\n",
        "iterations = 1\n",
        "methods = [ \"mini_batch\"]\n",
        "\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "for mtd in methods:\n",
        "    print(\"Method: \", mtd)\n",
        "    time_list = []\n",
        "    error_u_list = []\n",
        "    error_r_list = []\n",
        "    \n",
        "    for index in range(iterations):\n",
        "\n",
        "        print(\"Epoch: \", str(index+1))\n",
        "\n",
        "        # Create residual sampler\n",
        "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "        tf.reset_default_graph()\n",
        "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "\n",
        "            model = PINN(layers, X_u, Y_u, X_r, Y_r , mode , sess)    \n",
        "\n",
        "            # Train model\n",
        "            start_time = time.time()\n",
        "\n",
        "            if mtd ==\"full_batch\":\n",
        "                print(\"full_batch method is used\")\n",
        "                model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "            elif mtd ==\"mini_batch\":\n",
        "                print(\"mini_batch method is used\")\n",
        "                model.trainmb(nIter, mbbatch_size)\n",
        "            else:\n",
        "                print(\"unknown method!\")\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            # Predictions\n",
        "            u_pred = model.predict_u(X_star)\n",
        "            r_pred = model.predict_r(X_star)\n",
        "            # Predictions\n",
        "            model.save_NN()\n",
        "            model.plot_ntk()\n",
        " \n",
        "\n",
        "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "            error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
        "\n",
        "            print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "            print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "            print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "\n",
        "            print('elapsed: {:.2e}'.format(elapsed))\n",
        "            print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "            print('Relative L2 error_v: {:.2e}'.format(error_r))\n",
        "\n",
        "            model.plt_prediction( X_star , u_star , u_pred)\n",
        "            sess.close()  \n",
        "            \n",
        "        time_list.append(elapsed)\n",
        "        error_u_list.append(error_u)\n",
        "        error_r_list.append(error_r)\n",
        "\n",
        "    print(\"\\n\\nMethod: \", mtd)\n",
        "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "    print(\"average of error_v_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "    result_dict[mtd] = [time_list ,error_u_list ,error_r_list ]\n",
        "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_model\"+mode+\"_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(bcbatch_size)+\"_n\"+str(iterations)+\"_nIter\"+str(nIter)+\".mat\" ), result_dict)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model\n",
        "layers = [1, 512, 1]  \n",
        "# layers = [1, 512, 512, 512, 1]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Fw807UNzhu5z",
        "outputId": "929ae89c-3e10-4c56-e349-051441e8ab32"
      },
      "outputs": [],
      "source": [
        "loss_bcs = model.loss_bcs_log\n",
        "loss_res = model.loss_res_log\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "plt.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
        "plt.plot(loss_bcs, label='$\\mathcal{L}_{b}$')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFLIBq5xjZ3v"
      },
      "source": [
        "**Model Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To0PDN17cc0v",
        "outputId": "7284b31e-f2fe-41ab-93a9-4c2c91f94cde"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "K428lOuXhdc8",
        "outputId": "b1e23055-178c-400c-8972-f3e7987e0892"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYdfKGLj6h0"
      },
      "source": [
        "**NTK Eigenvalues**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3dByeQjhBYj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "vSn3Q_1IhisN",
        "outputId": "4c713f42-11b2-4de9-8698-085eb54c164d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIS5UH81kOxT"
      },
      "source": [
        "**Change of NTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF4Q_iZshQ-0"
      },
      "outputs": [],
      "source": [
        "# Change of the NTK\n",
        "NTK_change_list = []\n",
        "K0 = K_list[0]\n",
        "for K in K_list:\n",
        "    diff = np.linalg.norm(K - K0) / np.linalg.norm(K0) \n",
        "    NTK_change_list.append(diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "E-_gPGpCkF4n",
        "outputId": "9893e038-907d-4425-bb6e-8ecdb2ad497d"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6,5))\n",
        "plt.plot(NTK_change_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg0ZGHbAkW6N"
      },
      "source": [
        "\n",
        "**Change of NN Params**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLGv9JUuioVZ"
      },
      "outputs": [],
      "source": [
        "# Change of the weights and biases\n",
        "def compute_weights_diff(weights_1, weights_2):\n",
        "    weights = []\n",
        "    N = len(weights_1)\n",
        "    for k in range(N):\n",
        "        weight = weights_1[k] - weights_2[k]\n",
        "        weights.append(weight)\n",
        "    return weights\n",
        "\n",
        "def compute_weights_norm(weights, biases):\n",
        "    norm = 0\n",
        "    for w in weights:\n",
        "        norm = norm + np.sum(np.square(w))\n",
        "    for b in biases:\n",
        "        norm = norm + np.sum(np.square(b))\n",
        "    norm = np.sqrt(norm)\n",
        "    return norm\n",
        "\n",
        "# Restore the list weights and biases\n",
        "weights_log = model.weights_log\n",
        "biases_log = model.biases_log\n",
        "\n",
        "weights_0 = weights_log[0]\n",
        "biases_0 = biases_log[0]\n",
        "\n",
        "# Norm of the weights at initialization\n",
        "weights_init_norm = compute_weights_norm(weights_0, biases_0)\n",
        "\n",
        "weights_change_list = []\n",
        "\n",
        "N = len(weights_log)\n",
        "for k in range(N):\n",
        "    weights_diff = compute_weights_diff(weights_log[k], weights_log[0])\n",
        "    biases_diff = compute_weights_diff(biases_log[k], biases_log[0])\n",
        "    \n",
        "    weights_diff_norm = compute_weights_norm(weights_diff, biases_diff)\n",
        "    weights_change = weights_diff_norm / weights_init_norm\n",
        "    weights_change_list.append(weights_change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "5NLsAxgzi4KH",
        "outputId": "74d92bf9-0dde-438e-e9b3-a551904f4e2f"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6,5))\n",
        "plt.plot(weights_change_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYbzkhfMjJ8k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINNsNTK_Poisson.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
