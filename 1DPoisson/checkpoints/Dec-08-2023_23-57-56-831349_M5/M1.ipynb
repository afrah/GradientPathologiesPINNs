{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OybZJApDYGsi",
        "outputId": "dc7cdffa-8613-42ee-86cc-0bc5000b2998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "# # Switch to tensorflow 1.x\n",
        "# %tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkCgnRiYQSY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from Compute_Jacobian import jacobian # Please download 'Compute_Jacobian.py' in the repository \n",
        "import numpy as np\n",
        "import timeit\n",
        "from scipy.interpolate import griddata\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
        "import timeit\n",
        "\n",
        "import sys\n",
        "\n",
        "import scipy\n",
        "import scipy.io\n",
        "import time\n",
        "import logging\n",
        "\n",
        "import os.path\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-y7cHTcJfBTR"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "    # Initialize the class\n",
        "    def __init__(self, dim, coords, func, name=None):\n",
        "        self.dim = dim\n",
        "        self.coords = coords\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "\n",
        "    def sample(self, N):\n",
        "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
        "        y = self.func(x)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SDqDWN3nfSAg"
      },
      "outputs": [],
      "source": [
        "class PINN:\n",
        "    def __init__(self, layers, X_u, Y_u, X_r, Y_r ,mode ,  sess):\n",
        "\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        self.dirname, logpath = self.make_output_dir()\n",
        "        self.logger = self.get_logger(logpath)     \n",
        "\n",
        "        self.mu_X, self.sigma_X = X_r.mean(0), X_r.std(0)\n",
        "        self.mu_x, self.sigma_x = self.mu_X[0], self.sigma_X[0]\n",
        "\n",
        "        # Normalize\n",
        "        self.X_u = (X_u - self.mu_X) / self.sigma_X\n",
        "        self.Y_u = Y_u\n",
        "        self.X_r = (X_r - self.mu_X) / self.sigma_X\n",
        "        self.Y_r = Y_r\n",
        "\n",
        "        # Initialize network weights and biases\n",
        "        self.layers = layers\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "            \n",
        "        # Define the size of the Kernel\n",
        "        self.kernel_size = X_u.shape[0]\n",
        "        # Define Tensorflow session\n",
        "        self.sess = sess# tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
        "\n",
        "        # Define placeholders and computational graph\n",
        "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.x_bc_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.u_bc_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        self.r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        \n",
        "        self.x_u_ntk_tf = tf.placeholder(tf.float32, shape=(self.kernel_size, 1))\n",
        "        self.x_r_ntk_tf = tf.placeholder(tf.float32, shape=(self.kernel_size, 1))\n",
        "\n",
        "\n",
        "        # Evaluate predictions\n",
        "        self.u_bc_pred = self.net_u(self.x_bc_tf)\n",
        "\n",
        "        self.u_pred = self.net_u(self.x_u_tf)\n",
        "        self.r_pred = self.net_r(self.x_r_tf)\n",
        "        \n",
        "        self.u_ntk_pred = self.net_u(self.x_u_ntk_tf)\n",
        "        self.r_ntk_pred = self.net_r(self.x_r_ntk_tf)\n",
        "     \n",
        "        # Boundary loss\n",
        "        self.loss_bcs = tf.reduce_mean(tf.square(self.u_bc_pred - self.u_bc_tf))\n",
        "\n",
        "        # Residual loss        \n",
        "        self.loss_res =  tf.reduce_mean(tf.square(self.r_tf - self.r_pred))\n",
        "        \n",
        "        # Total loss\n",
        "        self.loss = self.loss_res + self.loss_bcs\n",
        "\n",
        "        # Define optimizer with learning rate schedule\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "        starter_learning_rate = 1e-5\n",
        "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
        "                                                        1000, 0.9, staircase=False)\n",
        "        # Passing global_step to minimize() will increment it at each step.\n",
        "        # To compute NTK, it is better to use SGD optimizer\n",
        "        # since the corresponding gradient flow is not exactly same.\n",
        "        self.train_op = tf.train.GradientDescentOptimizer(starter_learning_rate).minimize(self.loss)\n",
        "        # self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
        "\n",
        "\n",
        "        \n",
        "        # Compute the Jacobian for weights and biases in each hidden layer  \n",
        "        self.J_u = self.compute_jacobian(self.u_ntk_pred) \n",
        "        self.J_r = self.compute_jacobian(self.r_ntk_pred)\n",
        "        \n",
        "        # The empirical NTK = J J^T, compute NTK of PINNs \n",
        "        self.K_uu = self.compute_ntk(self.J_u, self.x_u_ntk_tf, self.J_u, self.x_u_ntk_tf)\n",
        "        self.K_ur = self.compute_ntk(self.J_u, self.x_u_ntk_tf, self.J_r, self.x_r_ntk_tf)\n",
        "        self.K_rr = self.compute_ntk(self.J_r, self.x_r_ntk_tf, self.J_r, self.x_r_ntk_tf)\n",
        "        \n",
        "        # Logger\n",
        "        # Loss logger\n",
        "        self.loss_bcs_log = []\n",
        "        self.loss_res_log = []\n",
        "\n",
        "        # NTK logger \n",
        "        self.K_uu_log = []\n",
        "        self.K_rr_log = []\n",
        "        self.K_ur_log = []\n",
        "        \n",
        "        # Weights logger \n",
        "        self.weights_log = []\n",
        "        self.biases_log = []\n",
        "       # Gradients Storage\n",
        "\n",
        "\n",
        "\n",
        "        # Generate dicts for gradients storage\n",
        "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
        "        self.dict_gradients_bc_layers = self.generate_grad_dict()\n",
        "\n",
        "        self.grad_res = []\n",
        "        self.grad_bc = []\n",
        "        self.grad_bc_list = []\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
        "            self.grad_bc.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.grad_bc_list.append(tf.reduce_max(tf.abs(self.grad_res[i])) / tf.reduce_mean(tf.abs(self.grad_bc[i])))\n",
        "\n",
        "        self.adaptive_constant_bc = tf.reduce_max(tf.stack(self.grad_bc_list))\n",
        "\n",
        "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bcs] \n",
        "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs\"] \n",
        "\n",
        "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
        "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
        "        \n",
        "\n",
        "        # Initialize Tensorflow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        self.sess.run(init)\n",
        "        \n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "    def generate_grad_dict(self):\n",
        "        num = len(self.layers) - 1\n",
        "        grad_dict = {}\n",
        "        for i in range(num):\n",
        "            grad_dict['layer_{}'.format(i + 1)] = []\n",
        "        return grad_dict\n",
        "\n",
        "    # Xavier initialization\n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "        return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
        "                           dtype=tf.float32)\n",
        "    \n",
        "    # NTK initialization\n",
        "    def NTK_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        std = 1. / np.sqrt(in_dim)\n",
        "        return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * std,\n",
        "                           dtype=tf.float32)\n",
        "\n",
        "     # Initialize network weights and biases using Xavier initialization\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.NTK_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.random.normal([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    # Evaluates the forward pass\n",
        "    def forward_pass(self, H):\n",
        "        num_layers = len(self.layers)\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = self.weights[l]\n",
        "            b = self.biases[l]\n",
        "            H = tf.nn.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = self.weights[-1]\n",
        "        b = self.biases[-1]\n",
        "        H = tf.add(tf.matmul(H, W), b)\n",
        "        return H\n",
        "\n",
        "    # Evaluates the PDE solution\n",
        "    def net_u(self, x):\n",
        "        u = self.forward_pass(x)\n",
        "        return u\n",
        "\n",
        "    # Forward pass for the residual\n",
        "    def net_r(self, x):\n",
        "        u = self.net_u(x)\n",
        "\n",
        "        u_x = tf.gradients(u, x)[0] / self.sigma_x\n",
        "        u_xx = tf.gradients(u_x, x)[0] / self.sigma_x\n",
        "\n",
        "        res_u = u_xx\n",
        "        return res_u\n",
        "    \n",
        "    # Compute Jacobian for each weights and biases in each layer and retrun a list \n",
        "    def compute_jacobian(self, f):\n",
        "        J_list =[]\n",
        "        L = len(self.weights)    \n",
        "        for i in range(L):\n",
        "            J_w = jacobian(f, self.weights[i])\n",
        "            J_list.append(J_w)\n",
        "     \n",
        "        for i in range(L):\n",
        "            J_b = jacobian(f, self.biases[i])\n",
        "            J_list.append(J_b)\n",
        "        return J_list\n",
        "    \n",
        "    # Compute the empirical NTK = J J^T\n",
        "    def compute_ntk(self, J1_list, x1, J2_list, x2):\n",
        "        D = x1.shape[0]\n",
        "        N = len(J1_list)\n",
        "        \n",
        "        Ker = tf.zeros((D,D))\n",
        "        for k in range(N):\n",
        "            J1 = tf.reshape(J1_list[k], shape=(D,-1))\n",
        "            J2 = tf.reshape(J2_list[k], shape=(D,-1))\n",
        "            \n",
        "            K = tf.matmul(J1, tf.transpose(J2))\n",
        "            Ker = Ker + K\n",
        "        return Ker\n",
        "            \n",
        "    # Trains the model by minimizing the MSE loss\n",
        "    def trainmb(self, nIter=10000, batch_size=128, log_NTK=True, log_weights=True):\n",
        "\n",
        "\n",
        "        itValues = [1,100,1000,39999]\n",
        "        start_time = timeit.default_timer()\n",
        "        for it in range(nIter):\n",
        "            # Fetch boundary mini-batches\n",
        "            # Define a dictionary for associating placeholders with data\n",
        "            tf_dict = {self.x_bc_tf: self.X_u, self.u_bc_tf: self.Y_u,\n",
        "                       self.x_u_tf: self.X_u, self.x_r_tf: self.X_r,\n",
        "                       self.r_tf: self.Y_r\n",
        "                       }\n",
        "        \n",
        "            # Run the Tensorflow session to minimize the loss\n",
        "            _, batch_losses = self.sess.run([self.train_op, self.loss_tensor_list] ,tf_dict)\n",
        "\n",
        "            # self.print\n",
        "            if it % 100 == 0:\n",
        "                elapsed = timeit.default_timer() - start_time\n",
        "                loss_value = self.sess.run(self.loss, tf_dict)\n",
        "                loss_bcs_value, loss_res_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
        "                self.loss_bcs_log.append(loss_bcs_value)\n",
        "                self.loss_res_log.append(loss_res_value)\n",
        "\n",
        "                self.print('It: %d, Loss: %.3e, Loss_bcs: %.3e, Loss_res: %.3e ,Time: %.2f' %  (it, loss_value, loss_bcs_value, loss_res_value, elapsed))\n",
        "                \n",
        "            if log_NTK:\n",
        "                # provide x, x' for NTK\n",
        "                if it % 100 == 0:\n",
        "\n",
        "                    adaptive_constant_bc = self.sess.run([self.adaptive_constant_bc],  tf_dict)\n",
        "                    print(adaptive_constant_bc[0])\n",
        "                   \n",
        "                    self.print(\"Compute NTK...\")\n",
        "                    tf_dict2 = {self.x_u_ntk_tf: self.X_u, \n",
        "                               self.x_r_ntk_tf: self.X_r\n",
        "                               }\n",
        "                    K_uu_value, K_ur_value, K_rr_value = self.sess.run([self.K_uu,  self.K_ur,  self.K_rr], tf_dict2)\n",
        "                    self.K_uu_log.append(K_uu_value)\n",
        "                    self.K_ur_log.append(K_ur_value)\n",
        "                    self.K_rr_log.append(K_rr_value)\n",
        "            start_time = timeit.default_timer()\n",
        "\n",
        "            if it in itValues:\n",
        "                    self.plot_layerLoss(tf_dict , it)\n",
        "                    self.print(\"Gradients information stored ...\")\n",
        "\n",
        "            sys.stdout.flush()\n",
        "            self.assign_batch_losses(batch_losses)\n",
        "            for key in self.loss_history:\n",
        "                self.loss_history[key].append(self.epoch_loss[key])\n",
        "            \n",
        "    # Evaluates predictions at test points\n",
        "    def predict_u(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.x_u_tf: X_star}\n",
        "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
        "        return u_star\n",
        "\n",
        "    # Evaluates predictions at test points\n",
        "    def predict_r(self, X_star):\n",
        "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
        "        tf_dict = {self.x_r_tf: X_star}\n",
        "        r_star = self.sess.run(self.r_pred, tf_dict)\n",
        "        return r_star\n",
        " ############################################################\n",
        "\n",
        "    def assign_batch_losses(self, batch_losses):\n",
        "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
        "            self.epoch_loss[key] = loss_values\n",
        "\n",
        "  ############################################################\n",
        "###################################################################################################################\n",
        "\n",
        "\n",
        "    def plot_layerLoss(self , tf_dict , epoch):\n",
        "        ## Gradients #\n",
        "        num_layers = len(self.layers)\n",
        "        for i in range(num_layers - 1):\n",
        "            grad_res, grad_bc  = self.sess.run([ self.grad_res[i],self.grad_bc[i]], feed_dict=tf_dict)\n",
        "\n",
        "            # save gradients of loss_r and loss_u\n",
        "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
        "            self.dict_gradients_bc_layers['layer_' + str(i + 1)].append(grad_bc.flatten())\n",
        "\n",
        "        num_hidden_layers = num_layers -1\n",
        "        cnt = 1\n",
        "        fig = plt.figure(4, figsize=(13, 4))\n",
        "        for j in range(num_hidden_layers):\n",
        "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
        "            ax.set_title('Layer {}'.format(j + 1))\n",
        "            ax.set_yscale('symlog')\n",
        "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
        "            gradients_bc = self.dict_gradients_bc_layers['layer_' + str(j + 1)][-1]\n",
        "\n",
        "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
        "\n",
        "            sns.distplot(gradients_bc, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc}}$')\n",
        "\n",
        "            #ax.get_legend().remove()\n",
        "            ax.set_xlim([-1.0, 1.0])\n",
        "            #ax.set_ylim([0, 150])\n",
        "            cnt += 1\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=2)\n",
        "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
        "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    # #########################\n",
        "    # def make_output_dir(self):\n",
        "        \n",
        "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
        "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
        "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "    #     os.mkdir(dirname)\n",
        "    #     text = 'output.log'\n",
        "    #     logpath = os.path.join(dirname, text)\n",
        "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
        "\n",
        "    #     return dirname, logpath\n",
        "    \n",
        "    # # ###########################################################\n",
        "    def make_output_dir(self):\n",
        "        \n",
        "        if not os.path.exists(\"checkpoints\"):\n",
        "            os.mkdir(\"checkpoints\")\n",
        "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
        "        os.mkdir(dirname)\n",
        "        text = 'output.log'\n",
        "        logpath = os.path.join(dirname, text)\n",
        "        shutil.copyfile('M1.ipynb', os.path.join(dirname, 'M1.ipynb'))\n",
        "        return dirname, logpath\n",
        "    \n",
        "\n",
        "    def get_logger(self, logpath):\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "        sh = logging.StreamHandler()\n",
        "        sh.setLevel(logging.DEBUG)        \n",
        "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
        "        fh = logging.FileHandler(logpath)\n",
        "        logger.addHandler(sh)\n",
        "        logger.addHandler(fh)\n",
        "        return logger\n",
        "    \n",
        "    def print(self, *args):\n",
        "        for word in args:\n",
        "            if len(args) == 1:\n",
        "                self.logger.info(word)\n",
        "            elif word != args[-1]:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "            else:\n",
        "                for handler in self.logger.handlers:\n",
        "                    handler.terminator = \"\\n\"\n",
        "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
        "                    self.logger.info(\"%.4e\" % (word))\n",
        "                else:\n",
        "                    self.logger.info(word)\n",
        "\n",
        "\n",
        "    def plot_loss_history(self , path):\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        fig.set_size_inches([15,8])\n",
        "        for key in self.loss_history:\n",
        "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
        "            ax.semilogy(self.loss_history[key], label=key)\n",
        "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
        "        ax.set_ylabel(\"loss\", fontsize=15)\n",
        "        ax.tick_params(labelsize=15)\n",
        "        ax.legend()\n",
        "        plt.savefig(path)\n",
        "        #plt.show()\n",
        "       #######################\n",
        "    def save_NN(self):\n",
        "\n",
        "        uv_weights = self.sess.run(self.weights)\n",
        "        uv_biases = self.sess.run(self.biases)\n",
        "\n",
        "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
        "            pickle.dump([uv_weights, uv_biases], f)\n",
        "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
        "\n",
        "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
        "        #     pickle.dump(self.loss_rec, f)\n",
        "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
        "            self.plot_loss_history(f)\n",
        "\n",
        "        return self.dirname\n",
        "    \n",
        "    def plot_ntk(self):\n",
        "        # Create empty lists for storing the eigenvalues of NTK\n",
        "        lambda_K_log = []\n",
        "        lambda_K_uu_log = []\n",
        "        lambda_K_ur_log = []\n",
        "        lambda_K_rr_log = []\n",
        "\n",
        "        # Restore the NTK\n",
        "        K_uu_list = self.K_uu_log\n",
        "        K_ur_list = self.K_ur_log\n",
        "        K_rr_list = self.K_rr_log\n",
        "        K_list = []\n",
        "            \n",
        "        for k in range(len(K_uu_list)):\n",
        "            K_uu = K_uu_list[k]\n",
        "            K_ur = K_ur_list[k]\n",
        "            K_rr = K_rr_list[k]\n",
        "            \n",
        "            K = np.concatenate([np.concatenate([K_uu, K_ur], axis = 1), np.concatenate([K_ur.T, K_rr], axis = 1)], axis = 0)\n",
        "            K_list.append(K)\n",
        "\n",
        "            # Compute eigenvalues\n",
        "            lambda_K, _ = np.linalg.eig(K)\n",
        "            lambda_K_uu, _ = np.linalg.eig(K_uu)\n",
        "            lambda_K_rr, _ = np.linalg.eig(K_rr)\n",
        "            \n",
        "            # Sort in descresing order\n",
        "            lambda_K = np.sort(np.real(lambda_K))[::-1]\n",
        "            lambda_K_uu = np.sort(np.real(lambda_K_uu))[::-1]\n",
        "            lambda_K_rr = np.sort(np.real(lambda_K_rr))[::-1]\n",
        "            \n",
        "            # Store eigenvalues\n",
        "            lambda_K_log.append(lambda_K)\n",
        "            lambda_K_uu_log.append(lambda_K_uu)\n",
        "            lambda_K_rr_log.append(lambda_K_rr)\n",
        "        fig = plt.figure(figsize=(18, 5))\n",
        "        plt.subplot(1,3,1)\n",
        "        for i in range(1, len(lambda_K_log), 10):\n",
        "            plt.plot(lambda_K_log[i], '--')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.title(r'Eigenvalues of ${K}$')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(1,3,2)\n",
        "        for i in range(1, len(lambda_K_uu_log), 10):\n",
        "            plt.plot(lambda_K_uu_log[i], '--')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.title(r'Eigenvalues of ${K}_{uu}$')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(1,3,3)\n",
        "        for i in range(1, len(lambda_K_log), 10):\n",
        "            plt.plot(lambda_K_rr_log[i], '--')\n",
        "        plt.xscale('log')\n",
        "        plt.yscale('log')\n",
        "        plt.title(r'Eigenvalues of ${K}_{rr}$')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.dirname,\"ntk.png\"))\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    def plt_prediction(self , X_star , u_star , u_pred):\n",
        "        fig = plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(X_star, u_star, label='Exact')\n",
        "        plt.plot(X_star, u_pred, '--', label='Predicted')\n",
        "        plt.xlabel('$x$')\n",
        "        plt.ylabel('$y$')\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(X_star, np.abs(u_star - u_pred), label='Error')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('$x$')\n",
        "        plt.ylabel('Point-wise error')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"))\n",
        "        plt.close(\"all\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FN1jEdRwY90i"
      },
      "outputs": [],
      "source": [
        "# Define solution and its Laplace\n",
        "a = 4\n",
        "\n",
        "def u(x, a):\n",
        "  return np.sin(np.pi * a * x)\n",
        "\n",
        "def u_xx(x, a):\n",
        "  return -(np.pi * a)**2 * np.sin(np.pi * a * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test_method(mtd , layers,  X_u, Y_u, X_r, Y_r ,  X_star , u_star , r_star  , nIter ,batch_size , bcbatch_size , ubatch_size)\n",
        "def test_method(method , layers,  X_u, Y_u, X_r, Y_r , X_star , u_star , r_star , nIter ,mbbatch_size , bcbatch_size , ubatch_size , mode):\n",
        "\n",
        "\n",
        "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "        # sess.run(init)\n",
        "\n",
        "        model = PINN(layers, X_u, Y_u, X_r, Y_r , mode , sess)    \n",
        "\n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "\n",
        "        if method ==\"full_batch\":\n",
        "            print(\"full_batch method is used\")\n",
        "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "        elif method ==\"mini_batch\":\n",
        "            print(\"mini_batch method is used\")\n",
        "            model.trainmb(nIter, mbbatch_size)\n",
        "        else:\n",
        "            print(\"unknown method!\")\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        # Predictions\n",
        "        u_pred = model.predict_u(X_star)\n",
        "        r_pred = model.predict_r(X_star)\n",
        "        # Predictions\n",
        "\n",
        "        sess.close()   \n",
        "\n",
        "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "    error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
        "\n",
        "    print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "    print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "\n",
        "    return [elapsed, error_u , error_r ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method:  mini_batch\n",
            "Epoch:  1\n",
            "WARNING:tensorflow:From /tmp/ipykernel_29025/657240385.py:52: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_29025/657240385.py:53: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_29025/657240385.py:54: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_29025/657240385.py:54: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_29025/1286129497.py:29: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_29025/1286129497.py:63: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_29025/1286129497.py:68: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_29025/1286129497.py:73: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-08 23:34:41.940912: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-08 23:34:41.961980: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
            "2023-12-08 23:34:41.962597: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555e53fd7030 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-12-08 23:34:41.962609: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2023-12-08 23:34:41.963388: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_29025/1286129497.py:76: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "mini_batch method is used\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "It: 0, Loss: 9.549e+03, Loss_bcs: 1.280e+00, Loss_res: 9.548e+03 ,Time: 0.33\n",
            "Compute NTK...\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 100, Loss: 3.962e+02, Loss_bcs: 8.037e+01, Loss_res: 3.158e+02 ,Time: 0.00\n",
            "Compute NTK...\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 200, Loss: 2.770e+02, Loss_bcs: 5.276e+01, Loss_res: 2.242e+02 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 300, Loss: 2.051e+02, Loss_bcs: 3.326e+01, Loss_res: 1.718e+02 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 400, Loss: 1.582e+02, Loss_bcs: 2.056e+01, Loss_res: 1.377e+02 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 500, Loss: 1.261e+02, Loss_bcs: 1.230e+01, Loss_res: 1.138e+02 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 600, Loss: 1.032e+02, Loss_bcs: 7.024e+00, Loss_res: 9.620e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 700, Loss: 8.635e+01, Loss_bcs: 3.747e+00, Loss_res: 8.261e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 800, Loss: 7.367e+01, Loss_bcs: 1.798e+00, Loss_res: 7.187e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 900, Loss: 6.396e+01, Loss_bcs: 7.173e-01, Loss_res: 6.324e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 1000, Loss: 5.639e+01, Loss_bcs: 1.921e-01, Loss_res: 5.619e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 1100, Loss: 5.039e+01, Loss_bcs: 1.119e-02, Loss_res: 5.038e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 1200, Loss: 4.556e+01, Loss_bcs: 3.390e-02, Loss_res: 4.553e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 1300, Loss: 4.162e+01, Loss_bcs: 1.682e-01, Loss_res: 4.145e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 1400, Loss: 3.835e+01, Loss_bcs: 3.552e-01, Loss_res: 3.800e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 1500, Loss: 3.561e+01, Loss_bcs: 5.582e-01, Loss_res: 3.505e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 1600, Loss: 3.327e+01, Loss_bcs: 7.555e-01, Loss_res: 3.252e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 1700, Loss: 3.126e+01, Loss_bcs: 9.348e-01, Loss_res: 3.033e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 1800, Loss: 2.951e+01, Loss_bcs: 1.090e+00, Loss_res: 2.842e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 1900, Loss: 2.797e+01, Loss_bcs: 1.220e+00, Loss_res: 2.675e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2000, Loss: 2.660e+01, Loss_bcs: 1.323e+00, Loss_res: 2.528e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2100, Loss: 2.538e+01, Loss_bcs: 1.403e+00, Loss_res: 2.397e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2200, Loss: 2.427e+01, Loss_bcs: 1.460e+00, Loss_res: 2.281e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2300, Loss: 2.327e+01, Loss_bcs: 1.499e+00, Loss_res: 2.178e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2400, Loss: 2.236e+01, Loss_bcs: 1.520e+00, Loss_res: 2.084e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2500, Loss: 2.153e+01, Loss_bcs: 1.528e+00, Loss_res: 2.000e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2600, Loss: 2.076e+01, Loss_bcs: 1.525e+00, Loss_res: 1.923e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2700, Loss: 2.005e+01, Loss_bcs: 1.511e+00, Loss_res: 1.854e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2800, Loss: 1.939e+01, Loss_bcs: 1.490e+00, Loss_res: 1.790e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 2900, Loss: 1.877e+01, Loss_bcs: 1.463e+00, Loss_res: 1.731e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3000, Loss: 1.819e+01, Loss_bcs: 1.432e+00, Loss_res: 1.676e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3100, Loss: 1.766e+01, Loss_bcs: 1.397e+00, Loss_res: 1.626e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3200, Loss: 1.715e+01, Loss_bcs: 1.359e+00, Loss_res: 1.579e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3300, Loss: 1.667e+01, Loss_bcs: 1.320e+00, Loss_res: 1.535e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3400, Loss: 1.622e+01, Loss_bcs: 1.280e+00, Loss_res: 1.494e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3500, Loss: 1.579e+01, Loss_bcs: 1.239e+00, Loss_res: 1.456e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3600, Loss: 1.539e+01, Loss_bcs: 1.198e+00, Loss_res: 1.419e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3700, Loss: 1.501e+01, Loss_bcs: 1.158e+00, Loss_res: 1.385e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3800, Loss: 1.464e+01, Loss_bcs: 1.118e+00, Loss_res: 1.352e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 3900, Loss: 1.429e+01, Loss_bcs: 1.079e+00, Loss_res: 1.321e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4000, Loss: 1.396e+01, Loss_bcs: 1.042e+00, Loss_res: 1.292e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4100, Loss: 1.364e+01, Loss_bcs: 1.005e+00, Loss_res: 1.263e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4200, Loss: 1.333e+01, Loss_bcs: 9.694e-01, Loss_res: 1.236e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4300, Loss: 1.304e+01, Loss_bcs: 9.351e-01, Loss_res: 1.210e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4400, Loss: 1.276e+01, Loss_bcs: 9.022e-01, Loss_res: 1.186e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4500, Loss: 1.249e+01, Loss_bcs: 8.705e-01, Loss_res: 1.162e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4600, Loss: 1.223e+01, Loss_bcs: 8.401e-01, Loss_res: 1.139e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4700, Loss: 1.198e+01, Loss_bcs: 8.109e-01, Loss_res: 1.117e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4800, Loss: 1.173e+01, Loss_bcs: 7.830e-01, Loss_res: 1.095e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 4900, Loss: 1.150e+01, Loss_bcs: 7.563e-01, Loss_res: 1.074e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5000, Loss: 1.128e+01, Loss_bcs: 7.308e-01, Loss_res: 1.054e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5100, Loss: 1.106e+01, Loss_bcs: 7.064e-01, Loss_res: 1.035e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5200, Loss: 1.085e+01, Loss_bcs: 6.831e-01, Loss_res: 1.016e+01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5300, Loss: 1.064e+01, Loss_bcs: 6.608e-01, Loss_res: 9.981e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5400, Loss: 1.044e+01, Loss_bcs: 6.395e-01, Loss_res: 9.805e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5500, Loss: 1.025e+01, Loss_bcs: 6.192e-01, Loss_res: 9.633e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5600, Loss: 1.007e+01, Loss_bcs: 5.997e-01, Loss_res: 9.467e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5700, Loss: 9.887e+00, Loss_bcs: 5.811e-01, Loss_res: 9.305e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5800, Loss: 9.712e+00, Loss_bcs: 5.634e-01, Loss_res: 9.148e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 5900, Loss: 9.542e+00, Loss_bcs: 5.464e-01, Loss_res: 8.995e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6000, Loss: 9.377e+00, Loss_bcs: 5.301e-01, Loss_res: 8.846e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6100, Loss: 9.216e+00, Loss_bcs: 5.146e-01, Loss_res: 8.702e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6200, Loss: 9.060e+00, Loss_bcs: 4.997e-01, Loss_res: 8.561e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6300, Loss: 8.909e+00, Loss_bcs: 4.854e-01, Loss_res: 8.423e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6400, Loss: 8.761e+00, Loss_bcs: 4.718e-01, Loss_res: 8.289e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6500, Loss: 8.618e+00, Loss_bcs: 4.586e-01, Loss_res: 8.159e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6600, Loss: 8.478e+00, Loss_bcs: 4.460e-01, Loss_res: 8.032e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6700, Loss: 8.342e+00, Loss_bcs: 4.340e-01, Loss_res: 7.908e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6800, Loss: 8.210e+00, Loss_bcs: 4.223e-01, Loss_res: 7.787e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 6900, Loss: 8.080e+00, Loss_bcs: 4.112e-01, Loss_res: 7.669e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7000, Loss: 7.955e+00, Loss_bcs: 4.005e-01, Loss_res: 7.554e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7100, Loss: 7.832e+00, Loss_bcs: 3.902e-01, Loss_res: 7.442e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7200, Loss: 7.713e+00, Loss_bcs: 3.803e-01, Loss_res: 7.332e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7300, Loss: 7.596e+00, Loss_bcs: 3.707e-01, Loss_res: 7.226e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7400, Loss: 7.483e+00, Loss_bcs: 3.615e-01, Loss_res: 7.121e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7500, Loss: 7.372e+00, Loss_bcs: 3.526e-01, Loss_res: 7.019e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7600, Loss: 7.264e+00, Loss_bcs: 3.441e-01, Loss_res: 6.920e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7700, Loss: 7.158e+00, Loss_bcs: 3.358e-01, Loss_res: 6.823e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7800, Loss: 7.055e+00, Loss_bcs: 3.278e-01, Loss_res: 6.728e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 7900, Loss: 6.955e+00, Loss_bcs: 3.201e-01, Loss_res: 6.635e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8000, Loss: 6.857e+00, Loss_bcs: 3.127e-01, Loss_res: 6.544e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8100, Loss: 6.761e+00, Loss_bcs: 3.055e-01, Loss_res: 6.455e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8200, Loss: 6.667e+00, Loss_bcs: 2.985e-01, Loss_res: 6.369e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8300, Loss: 6.576e+00, Loss_bcs: 2.918e-01, Loss_res: 6.284e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8400, Loss: 6.487e+00, Loss_bcs: 2.853e-01, Loss_res: 6.202e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8500, Loss: 6.400e+00, Loss_bcs: 2.789e-01, Loss_res: 6.121e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8600, Loss: 6.314e+00, Loss_bcs: 2.728e-01, Loss_res: 6.041e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8700, Loss: 6.231e+00, Loss_bcs: 2.669e-01, Loss_res: 5.964e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8800, Loss: 6.150e+00, Loss_bcs: 2.611e-01, Loss_res: 5.888e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 8900, Loss: 6.070e+00, Loss_bcs: 2.556e-01, Loss_res: 5.814e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9000, Loss: 5.992e+00, Loss_bcs: 2.502e-01, Loss_res: 5.742e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9100, Loss: 5.916e+00, Loss_bcs: 2.449e-01, Loss_res: 5.671e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9200, Loss: 5.841e+00, Loss_bcs: 2.398e-01, Loss_res: 5.602e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9300, Loss: 5.769e+00, Loss_bcs: 2.348e-01, Loss_res: 5.534e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9400, Loss: 5.697e+00, Loss_bcs: 2.300e-01, Loss_res: 5.467e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9500, Loss: 5.628e+00, Loss_bcs: 2.253e-01, Loss_res: 5.402e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9600, Loss: 5.559e+00, Loss_bcs: 2.207e-01, Loss_res: 5.339e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9700, Loss: 5.492e+00, Loss_bcs: 2.163e-01, Loss_res: 5.276e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9800, Loss: 5.427e+00, Loss_bcs: 2.120e-01, Loss_res: 5.215e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 9900, Loss: 5.363e+00, Loss_bcs: 2.078e-01, Loss_res: 5.155e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10000, Loss: 5.300e+00, Loss_bcs: 2.037e-01, Loss_res: 5.097e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10100, Loss: 5.239e+00, Loss_bcs: 1.997e-01, Loss_res: 5.039e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10200, Loss: 5.179e+00, Loss_bcs: 1.959e-01, Loss_res: 4.983e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10300, Loss: 5.120e+00, Loss_bcs: 1.921e-01, Loss_res: 4.928e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10400, Loss: 5.062e+00, Loss_bcs: 1.884e-01, Loss_res: 4.874e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10500, Loss: 5.006e+00, Loss_bcs: 1.848e-01, Loss_res: 4.821e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10600, Loss: 4.950e+00, Loss_bcs: 1.813e-01, Loss_res: 4.769e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10700, Loss: 4.896e+00, Loss_bcs: 1.779e-01, Loss_res: 4.718e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10800, Loss: 4.843e+00, Loss_bcs: 1.746e-01, Loss_res: 4.668e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 10900, Loss: 4.790e+00, Loss_bcs: 1.714e-01, Loss_res: 4.619e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11000, Loss: 4.739e+00, Loss_bcs: 1.682e-01, Loss_res: 4.571e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11100, Loss: 4.689e+00, Loss_bcs: 1.651e-01, Loss_res: 4.524e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11200, Loss: 4.640e+00, Loss_bcs: 1.621e-01, Loss_res: 4.478e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11300, Loss: 4.592e+00, Loss_bcs: 1.592e-01, Loss_res: 4.432e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11400, Loss: 4.544e+00, Loss_bcs: 1.563e-01, Loss_res: 4.388e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11500, Loss: 4.498e+00, Loss_bcs: 1.535e-01, Loss_res: 4.344e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11600, Loss: 4.452e+00, Loss_bcs: 1.508e-01, Loss_res: 4.301e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11700, Loss: 4.407e+00, Loss_bcs: 1.481e-01, Loss_res: 4.259e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11800, Loss: 4.364e+00, Loss_bcs: 1.454e-01, Loss_res: 4.218e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 11900, Loss: 4.320e+00, Loss_bcs: 1.429e-01, Loss_res: 4.178e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12000, Loss: 4.278e+00, Loss_bcs: 1.404e-01, Loss_res: 4.138e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12100, Loss: 4.237e+00, Loss_bcs: 1.379e-01, Loss_res: 4.099e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12200, Loss: 4.196e+00, Loss_bcs: 1.356e-01, Loss_res: 4.060e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12300, Loss: 4.156e+00, Loss_bcs: 1.333e-01, Loss_res: 4.022e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12400, Loss: 4.116e+00, Loss_bcs: 1.310e-01, Loss_res: 3.985e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12500, Loss: 4.078e+00, Loss_bcs: 1.287e-01, Loss_res: 3.949e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12600, Loss: 4.040e+00, Loss_bcs: 1.266e-01, Loss_res: 3.913e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12700, Loss: 4.002e+00, Loss_bcs: 1.244e-01, Loss_res: 3.878e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12800, Loss: 3.966e+00, Loss_bcs: 1.224e-01, Loss_res: 3.843e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 12900, Loss: 3.930e+00, Loss_bcs: 1.203e-01, Loss_res: 3.809e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13000, Loss: 3.894e+00, Loss_bcs: 1.183e-01, Loss_res: 3.776e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13100, Loss: 3.859e+00, Loss_bcs: 1.164e-01, Loss_res: 3.743e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13200, Loss: 3.825e+00, Loss_bcs: 1.145e-01, Loss_res: 3.711e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13300, Loss: 3.791e+00, Loss_bcs: 1.126e-01, Loss_res: 3.679e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13400, Loss: 3.758e+00, Loss_bcs: 1.108e-01, Loss_res: 3.647e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13500, Loss: 3.726e+00, Loss_bcs: 1.090e-01, Loss_res: 3.617e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13600, Loss: 3.694e+00, Loss_bcs: 1.072e-01, Loss_res: 3.586e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13700, Loss: 3.662e+00, Loss_bcs: 1.055e-01, Loss_res: 3.557e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13800, Loss: 3.631e+00, Loss_bcs: 1.038e-01, Loss_res: 3.527e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 13900, Loss: 3.600e+00, Loss_bcs: 1.022e-01, Loss_res: 3.498e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14000, Loss: 3.570e+00, Loss_bcs: 1.005e-01, Loss_res: 3.470e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14100, Loss: 3.541e+00, Loss_bcs: 9.897e-02, Loss_res: 3.442e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14200, Loss: 3.512e+00, Loss_bcs: 9.742e-02, Loss_res: 3.414e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14300, Loss: 3.483e+00, Loss_bcs: 9.590e-02, Loss_res: 3.387e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14400, Loss: 3.455e+00, Loss_bcs: 9.443e-02, Loss_res: 3.360e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14500, Loss: 3.427e+00, Loss_bcs: 9.298e-02, Loss_res: 3.334e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14600, Loss: 3.399e+00, Loss_bcs: 9.156e-02, Loss_res: 3.308e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14700, Loss: 3.372e+00, Loss_bcs: 9.017e-02, Loss_res: 3.282e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14800, Loss: 3.346e+00, Loss_bcs: 8.880e-02, Loss_res: 3.257e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 14900, Loss: 3.320e+00, Loss_bcs: 8.744e-02, Loss_res: 3.232e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15000, Loss: 3.294e+00, Loss_bcs: 8.612e-02, Loss_res: 3.208e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15100, Loss: 3.268e+00, Loss_bcs: 8.483e-02, Loss_res: 3.184e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15200, Loss: 3.243e+00, Loss_bcs: 8.357e-02, Loss_res: 3.160e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15300, Loss: 3.219e+00, Loss_bcs: 8.234e-02, Loss_res: 3.136e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15400, Loss: 3.194e+00, Loss_bcs: 8.113e-02, Loss_res: 3.113e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15500, Loss: 3.170e+00, Loss_bcs: 7.993e-02, Loss_res: 3.090e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15600, Loss: 3.147e+00, Loss_bcs: 7.877e-02, Loss_res: 3.068e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15700, Loss: 3.123e+00, Loss_bcs: 7.763e-02, Loss_res: 3.046e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15800, Loss: 3.100e+00, Loss_bcs: 7.651e-02, Loss_res: 3.024e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 15900, Loss: 3.078e+00, Loss_bcs: 7.540e-02, Loss_res: 3.002e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16000, Loss: 3.055e+00, Loss_bcs: 7.431e-02, Loss_res: 2.981e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16100, Loss: 3.033e+00, Loss_bcs: 7.326e-02, Loss_res: 2.960e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16200, Loss: 3.011e+00, Loss_bcs: 7.223e-02, Loss_res: 2.939e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16300, Loss: 2.990e+00, Loss_bcs: 7.120e-02, Loss_res: 2.918e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16400, Loss: 2.968e+00, Loss_bcs: 7.019e-02, Loss_res: 2.898e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16500, Loss: 2.947e+00, Loss_bcs: 6.920e-02, Loss_res: 2.878e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16600, Loss: 2.927e+00, Loss_bcs: 6.823e-02, Loss_res: 2.859e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16700, Loss: 2.906e+00, Loss_bcs: 6.729e-02, Loss_res: 2.839e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16800, Loss: 2.886e+00, Loss_bcs: 6.637e-02, Loss_res: 2.820e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 16900, Loss: 2.866e+00, Loss_bcs: 6.547e-02, Loss_res: 2.801e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17000, Loss: 2.847e+00, Loss_bcs: 6.458e-02, Loss_res: 2.782e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17100, Loss: 2.827e+00, Loss_bcs: 6.369e-02, Loss_res: 2.764e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17200, Loss: 2.808e+00, Loss_bcs: 6.281e-02, Loss_res: 2.745e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17300, Loss: 2.789e+00, Loss_bcs: 6.197e-02, Loss_res: 2.727e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17400, Loss: 2.770e+00, Loss_bcs: 6.114e-02, Loss_res: 2.709e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17500, Loss: 2.752e+00, Loss_bcs: 6.032e-02, Loss_res: 2.692e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17600, Loss: 2.734e+00, Loss_bcs: 5.952e-02, Loss_res: 2.674e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17700, Loss: 2.716e+00, Loss_bcs: 5.874e-02, Loss_res: 2.657e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17800, Loss: 2.698e+00, Loss_bcs: 5.796e-02, Loss_res: 2.640e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 17900, Loss: 2.680e+00, Loss_bcs: 5.720e-02, Loss_res: 2.623e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18000, Loss: 2.663e+00, Loss_bcs: 5.646e-02, Loss_res: 2.606e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18100, Loss: 2.646e+00, Loss_bcs: 5.573e-02, Loss_res: 2.590e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18200, Loss: 2.629e+00, Loss_bcs: 5.501e-02, Loss_res: 2.574e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18300, Loss: 2.612e+00, Loss_bcs: 5.431e-02, Loss_res: 2.557e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18400, Loss: 2.595e+00, Loss_bcs: 5.363e-02, Loss_res: 2.541e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18500, Loss: 2.579e+00, Loss_bcs: 5.295e-02, Loss_res: 2.526e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18600, Loss: 2.562e+00, Loss_bcs: 5.228e-02, Loss_res: 2.510e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18700, Loss: 2.546e+00, Loss_bcs: 5.162e-02, Loss_res: 2.495e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18800, Loss: 2.530e+00, Loss_bcs: 5.098e-02, Loss_res: 2.479e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 18900, Loss: 2.515e+00, Loss_bcs: 5.034e-02, Loss_res: 2.464e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19000, Loss: 2.499e+00, Loss_bcs: 4.970e-02, Loss_res: 2.449e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19100, Loss: 2.484e+00, Loss_bcs: 4.908e-02, Loss_res: 2.435e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19200, Loss: 2.468e+00, Loss_bcs: 4.847e-02, Loss_res: 2.420e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19300, Loss: 2.453e+00, Loss_bcs: 4.787e-02, Loss_res: 2.405e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19400, Loss: 2.438e+00, Loss_bcs: 4.728e-02, Loss_res: 2.391e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19500, Loss: 2.424e+00, Loss_bcs: 4.670e-02, Loss_res: 2.377e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19600, Loss: 2.409e+00, Loss_bcs: 4.615e-02, Loss_res: 2.363e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19700, Loss: 2.395e+00, Loss_bcs: 4.561e-02, Loss_res: 2.349e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19800, Loss: 2.380e+00, Loss_bcs: 4.507e-02, Loss_res: 2.335e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 19900, Loss: 2.366e+00, Loss_bcs: 4.453e-02, Loss_res: 2.322e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20000, Loss: 2.352e+00, Loss_bcs: 4.400e-02, Loss_res: 2.308e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20100, Loss: 2.338e+00, Loss_bcs: 4.349e-02, Loss_res: 2.295e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20200, Loss: 2.324e+00, Loss_bcs: 4.299e-02, Loss_res: 2.282e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20300, Loss: 2.311e+00, Loss_bcs: 4.248e-02, Loss_res: 2.268e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20400, Loss: 2.297e+00, Loss_bcs: 4.198e-02, Loss_res: 2.255e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20500, Loss: 2.284e+00, Loss_bcs: 4.149e-02, Loss_res: 2.243e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20600, Loss: 2.271e+00, Loss_bcs: 4.101e-02, Loss_res: 2.230e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20700, Loss: 2.258e+00, Loss_bcs: 4.054e-02, Loss_res: 2.217e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20800, Loss: 2.245e+00, Loss_bcs: 4.008e-02, Loss_res: 2.205e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 20900, Loss: 2.232e+00, Loss_bcs: 3.963e-02, Loss_res: 2.192e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21000, Loss: 2.219e+00, Loss_bcs: 3.918e-02, Loss_res: 2.180e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21100, Loss: 2.207e+00, Loss_bcs: 3.874e-02, Loss_res: 2.168e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21200, Loss: 2.194e+00, Loss_bcs: 3.832e-02, Loss_res: 2.156e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21300, Loss: 2.182e+00, Loss_bcs: 3.789e-02, Loss_res: 2.144e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21400, Loss: 2.170e+00, Loss_bcs: 3.747e-02, Loss_res: 2.132e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21500, Loss: 2.158e+00, Loss_bcs: 3.706e-02, Loss_res: 2.121e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21600, Loss: 2.146e+00, Loss_bcs: 3.665e-02, Loss_res: 2.109e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21700, Loss: 2.134e+00, Loss_bcs: 3.624e-02, Loss_res: 2.097e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21800, Loss: 2.122e+00, Loss_bcs: 3.584e-02, Loss_res: 2.086e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 21900, Loss: 2.110e+00, Loss_bcs: 3.544e-02, Loss_res: 2.075e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22000, Loss: 2.099e+00, Loss_bcs: 3.505e-02, Loss_res: 2.064e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22100, Loss: 2.087e+00, Loss_bcs: 3.467e-02, Loss_res: 2.053e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22200, Loss: 2.076e+00, Loss_bcs: 3.431e-02, Loss_res: 2.042e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22300, Loss: 2.065e+00, Loss_bcs: 3.395e-02, Loss_res: 2.031e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22400, Loss: 2.053e+00, Loss_bcs: 3.360e-02, Loss_res: 2.020e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22500, Loss: 2.042e+00, Loss_bcs: 3.325e-02, Loss_res: 2.009e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22600, Loss: 2.031e+00, Loss_bcs: 3.289e-02, Loss_res: 1.999e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22700, Loss: 2.021e+00, Loss_bcs: 3.254e-02, Loss_res: 1.988e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22800, Loss: 2.010e+00, Loss_bcs: 3.220e-02, Loss_res: 1.978e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 22900, Loss: 1.999e+00, Loss_bcs: 3.186e-02, Loss_res: 1.967e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 23000, Loss: 1.988e+00, Loss_bcs: 3.152e-02, Loss_res: 1.957e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 23100, Loss: 1.978e+00, Loss_bcs: 3.120e-02, Loss_res: 1.947e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 23200, Loss: 1.968e+00, Loss_bcs: 3.088e-02, Loss_res: 1.937e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 23300, Loss: 1.957e+00, Loss_bcs: 3.056e-02, Loss_res: 1.927e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 23400, Loss: 1.947e+00, Loss_bcs: 3.026e-02, Loss_res: 1.917e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 23500, Loss: 1.937e+00, Loss_bcs: 2.996e-02, Loss_res: 1.907e+00 ,Time: 0.01\n",
            "Compute NTK...\n",
            "It: 23600, Loss: 1.927e+00, Loss_bcs: 2.967e-02, Loss_res: 1.897e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 23700, Loss: 1.917e+00, Loss_bcs: 2.938e-02, Loss_res: 1.887e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 23800, Loss: 1.907e+00, Loss_bcs: 2.909e-02, Loss_res: 1.878e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 23900, Loss: 1.897e+00, Loss_bcs: 2.880e-02, Loss_res: 1.868e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24000, Loss: 1.887e+00, Loss_bcs: 2.851e-02, Loss_res: 1.859e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24100, Loss: 1.877e+00, Loss_bcs: 2.823e-02, Loss_res: 1.849e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24200, Loss: 1.868e+00, Loss_bcs: 2.795e-02, Loss_res: 1.840e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24300, Loss: 1.858e+00, Loss_bcs: 2.768e-02, Loss_res: 1.831e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24400, Loss: 1.849e+00, Loss_bcs: 2.741e-02, Loss_res: 1.822e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24500, Loss: 1.840e+00, Loss_bcs: 2.714e-02, Loss_res: 1.812e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24600, Loss: 1.830e+00, Loss_bcs: 2.688e-02, Loss_res: 1.803e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24700, Loss: 1.821e+00, Loss_bcs: 2.663e-02, Loss_res: 1.794e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24800, Loss: 1.812e+00, Loss_bcs: 2.638e-02, Loss_res: 1.786e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 24900, Loss: 1.803e+00, Loss_bcs: 2.613e-02, Loss_res: 1.777e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25000, Loss: 1.794e+00, Loss_bcs: 2.588e-02, Loss_res: 1.768e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25100, Loss: 1.785e+00, Loss_bcs: 2.565e-02, Loss_res: 1.759e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25200, Loss: 1.776e+00, Loss_bcs: 2.541e-02, Loss_res: 1.751e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25300, Loss: 1.767e+00, Loss_bcs: 2.517e-02, Loss_res: 1.742e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25400, Loss: 1.759e+00, Loss_bcs: 2.493e-02, Loss_res: 1.734e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25500, Loss: 1.750e+00, Loss_bcs: 2.471e-02, Loss_res: 1.725e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25600, Loss: 1.741e+00, Loss_bcs: 2.449e-02, Loss_res: 1.717e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25700, Loss: 1.733e+00, Loss_bcs: 2.427e-02, Loss_res: 1.709e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25800, Loss: 1.724e+00, Loss_bcs: 2.406e-02, Loss_res: 1.700e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 25900, Loss: 1.716e+00, Loss_bcs: 2.384e-02, Loss_res: 1.692e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26000, Loss: 1.708e+00, Loss_bcs: 2.362e-02, Loss_res: 1.684e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26100, Loss: 1.699e+00, Loss_bcs: 2.340e-02, Loss_res: 1.676e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26200, Loss: 1.691e+00, Loss_bcs: 2.319e-02, Loss_res: 1.668e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26300, Loss: 1.683e+00, Loss_bcs: 2.297e-02, Loss_res: 1.660e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26400, Loss: 1.675e+00, Loss_bcs: 2.276e-02, Loss_res: 1.652e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26500, Loss: 1.667e+00, Loss_bcs: 2.256e-02, Loss_res: 1.644e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26600, Loss: 1.659e+00, Loss_bcs: 2.236e-02, Loss_res: 1.636e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26700, Loss: 1.651e+00, Loss_bcs: 2.216e-02, Loss_res: 1.629e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26800, Loss: 1.643e+00, Loss_bcs: 2.197e-02, Loss_res: 1.621e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 26900, Loss: 1.635e+00, Loss_bcs: 2.178e-02, Loss_res: 1.614e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27000, Loss: 1.628e+00, Loss_bcs: 2.159e-02, Loss_res: 1.606e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27100, Loss: 1.620e+00, Loss_bcs: 2.141e-02, Loss_res: 1.598e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27200, Loss: 1.612e+00, Loss_bcs: 2.123e-02, Loss_res: 1.591e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27300, Loss: 1.605e+00, Loss_bcs: 2.106e-02, Loss_res: 1.584e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27400, Loss: 1.597e+00, Loss_bcs: 2.088e-02, Loss_res: 1.576e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27500, Loss: 1.590e+00, Loss_bcs: 2.071e-02, Loss_res: 1.569e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27600, Loss: 1.582e+00, Loss_bcs: 2.053e-02, Loss_res: 1.562e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27700, Loss: 1.575e+00, Loss_bcs: 2.035e-02, Loss_res: 1.555e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27800, Loss: 1.568e+00, Loss_bcs: 2.018e-02, Loss_res: 1.547e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 27900, Loss: 1.560e+00, Loss_bcs: 2.000e-02, Loss_res: 1.540e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28000, Loss: 1.553e+00, Loss_bcs: 1.983e-02, Loss_res: 1.533e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28100, Loss: 1.546e+00, Loss_bcs: 1.966e-02, Loss_res: 1.526e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28200, Loss: 1.539e+00, Loss_bcs: 1.949e-02, Loss_res: 1.519e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28300, Loss: 1.532e+00, Loss_bcs: 1.932e-02, Loss_res: 1.513e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28400, Loss: 1.525e+00, Loss_bcs: 1.916e-02, Loss_res: 1.506e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28500, Loss: 1.518e+00, Loss_bcs: 1.900e-02, Loss_res: 1.499e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28600, Loss: 1.511e+00, Loss_bcs: 1.885e-02, Loss_res: 1.492e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28700, Loss: 1.504e+00, Loss_bcs: 1.870e-02, Loss_res: 1.485e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28800, Loss: 1.497e+00, Loss_bcs: 1.855e-02, Loss_res: 1.479e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 28900, Loss: 1.491e+00, Loss_bcs: 1.840e-02, Loss_res: 1.472e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29000, Loss: 1.484e+00, Loss_bcs: 1.826e-02, Loss_res: 1.466e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29100, Loss: 1.477e+00, Loss_bcs: 1.811e-02, Loss_res: 1.459e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29200, Loss: 1.471e+00, Loss_bcs: 1.796e-02, Loss_res: 1.453e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29300, Loss: 1.464e+00, Loss_bcs: 1.782e-02, Loss_res: 1.446e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29400, Loss: 1.458e+00, Loss_bcs: 1.767e-02, Loss_res: 1.440e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29500, Loss: 1.451e+00, Loss_bcs: 1.753e-02, Loss_res: 1.433e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29600, Loss: 1.445e+00, Loss_bcs: 1.740e-02, Loss_res: 1.427e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29700, Loss: 1.438e+00, Loss_bcs: 1.727e-02, Loss_res: 1.421e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29800, Loss: 1.432e+00, Loss_bcs: 1.713e-02, Loss_res: 1.415e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 29900, Loss: 1.426e+00, Loss_bcs: 1.700e-02, Loss_res: 1.409e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30000, Loss: 1.419e+00, Loss_bcs: 1.687e-02, Loss_res: 1.402e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30100, Loss: 1.413e+00, Loss_bcs: 1.674e-02, Loss_res: 1.396e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30200, Loss: 1.407e+00, Loss_bcs: 1.661e-02, Loss_res: 1.390e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30300, Loss: 1.401e+00, Loss_bcs: 1.648e-02, Loss_res: 1.384e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30400, Loss: 1.395e+00, Loss_bcs: 1.636e-02, Loss_res: 1.378e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30500, Loss: 1.389e+00, Loss_bcs: 1.623e-02, Loss_res: 1.372e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30600, Loss: 1.382e+00, Loss_bcs: 1.611e-02, Loss_res: 1.366e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30700, Loss: 1.376e+00, Loss_bcs: 1.600e-02, Loss_res: 1.361e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30800, Loss: 1.371e+00, Loss_bcs: 1.588e-02, Loss_res: 1.355e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 30900, Loss: 1.365e+00, Loss_bcs: 1.576e-02, Loss_res: 1.349e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31000, Loss: 1.359e+00, Loss_bcs: 1.563e-02, Loss_res: 1.343e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31100, Loss: 1.353e+00, Loss_bcs: 1.551e-02, Loss_res: 1.337e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31200, Loss: 1.347e+00, Loss_bcs: 1.538e-02, Loss_res: 1.332e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31300, Loss: 1.341e+00, Loss_bcs: 1.527e-02, Loss_res: 1.326e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31400, Loss: 1.336e+00, Loss_bcs: 1.515e-02, Loss_res: 1.321e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31500, Loss: 1.330e+00, Loss_bcs: 1.504e-02, Loss_res: 1.315e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31600, Loss: 1.324e+00, Loss_bcs: 1.493e-02, Loss_res: 1.309e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31700, Loss: 1.319e+00, Loss_bcs: 1.482e-02, Loss_res: 1.304e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31800, Loss: 1.313e+00, Loss_bcs: 1.471e-02, Loss_res: 1.298e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 31900, Loss: 1.308e+00, Loss_bcs: 1.460e-02, Loss_res: 1.293e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32000, Loss: 1.302e+00, Loss_bcs: 1.450e-02, Loss_res: 1.288e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32100, Loss: 1.297e+00, Loss_bcs: 1.440e-02, Loss_res: 1.282e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32200, Loss: 1.291e+00, Loss_bcs: 1.429e-02, Loss_res: 1.277e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32300, Loss: 1.286e+00, Loss_bcs: 1.420e-02, Loss_res: 1.272e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32400, Loss: 1.281e+00, Loss_bcs: 1.410e-02, Loss_res: 1.266e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32500, Loss: 1.275e+00, Loss_bcs: 1.400e-02, Loss_res: 1.261e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32600, Loss: 1.270e+00, Loss_bcs: 1.391e-02, Loss_res: 1.256e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32700, Loss: 1.265e+00, Loss_bcs: 1.381e-02, Loss_res: 1.251e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32800, Loss: 1.259e+00, Loss_bcs: 1.372e-02, Loss_res: 1.246e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 32900, Loss: 1.254e+00, Loss_bcs: 1.362e-02, Loss_res: 1.241e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33000, Loss: 1.249e+00, Loss_bcs: 1.352e-02, Loss_res: 1.236e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33100, Loss: 1.244e+00, Loss_bcs: 1.343e-02, Loss_res: 1.230e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33200, Loss: 1.239e+00, Loss_bcs: 1.334e-02, Loss_res: 1.226e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33300, Loss: 1.234e+00, Loss_bcs: 1.324e-02, Loss_res: 1.221e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33400, Loss: 1.229e+00, Loss_bcs: 1.315e-02, Loss_res: 1.216e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33500, Loss: 1.224e+00, Loss_bcs: 1.306e-02, Loss_res: 1.211e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33600, Loss: 1.219e+00, Loss_bcs: 1.296e-02, Loss_res: 1.206e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33700, Loss: 1.214e+00, Loss_bcs: 1.286e-02, Loss_res: 1.201e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33800, Loss: 1.209e+00, Loss_bcs: 1.277e-02, Loss_res: 1.196e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 33900, Loss: 1.204e+00, Loss_bcs: 1.269e-02, Loss_res: 1.192e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34000, Loss: 1.199e+00, Loss_bcs: 1.260e-02, Loss_res: 1.187e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34100, Loss: 1.195e+00, Loss_bcs: 1.252e-02, Loss_res: 1.182e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34200, Loss: 1.190e+00, Loss_bcs: 1.244e-02, Loss_res: 1.177e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34300, Loss: 1.185e+00, Loss_bcs: 1.236e-02, Loss_res: 1.173e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34400, Loss: 1.180e+00, Loss_bcs: 1.228e-02, Loss_res: 1.168e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34500, Loss: 1.176e+00, Loss_bcs: 1.220e-02, Loss_res: 1.163e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34600, Loss: 1.171e+00, Loss_bcs: 1.212e-02, Loss_res: 1.159e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34700, Loss: 1.166e+00, Loss_bcs: 1.205e-02, Loss_res: 1.154e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34800, Loss: 1.162e+00, Loss_bcs: 1.197e-02, Loss_res: 1.150e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 34900, Loss: 1.157e+00, Loss_bcs: 1.189e-02, Loss_res: 1.145e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35000, Loss: 1.153e+00, Loss_bcs: 1.181e-02, Loss_res: 1.141e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35100, Loss: 1.148e+00, Loss_bcs: 1.173e-02, Loss_res: 1.136e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35200, Loss: 1.144e+00, Loss_bcs: 1.165e-02, Loss_res: 1.132e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35300, Loss: 1.139e+00, Loss_bcs: 1.157e-02, Loss_res: 1.128e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35400, Loss: 1.135e+00, Loss_bcs: 1.150e-02, Loss_res: 1.123e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35500, Loss: 1.130e+00, Loss_bcs: 1.142e-02, Loss_res: 1.119e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35600, Loss: 1.126e+00, Loss_bcs: 1.134e-02, Loss_res: 1.115e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35700, Loss: 1.122e+00, Loss_bcs: 1.126e-02, Loss_res: 1.110e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35800, Loss: 1.117e+00, Loss_bcs: 1.119e-02, Loss_res: 1.106e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 35900, Loss: 1.113e+00, Loss_bcs: 1.111e-02, Loss_res: 1.102e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36000, Loss: 1.109e+00, Loss_bcs: 1.103e-02, Loss_res: 1.098e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36100, Loss: 1.104e+00, Loss_bcs: 1.096e-02, Loss_res: 1.093e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36200, Loss: 1.100e+00, Loss_bcs: 1.088e-02, Loss_res: 1.089e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36300, Loss: 1.096e+00, Loss_bcs: 1.081e-02, Loss_res: 1.085e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36400, Loss: 1.092e+00, Loss_bcs: 1.075e-02, Loss_res: 1.081e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36500, Loss: 1.088e+00, Loss_bcs: 1.068e-02, Loss_res: 1.077e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36600, Loss: 1.083e+00, Loss_bcs: 1.061e-02, Loss_res: 1.073e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36700, Loss: 1.079e+00, Loss_bcs: 1.054e-02, Loss_res: 1.069e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36800, Loss: 1.075e+00, Loss_bcs: 1.048e-02, Loss_res: 1.065e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 36900, Loss: 1.071e+00, Loss_bcs: 1.042e-02, Loss_res: 1.061e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37000, Loss: 1.067e+00, Loss_bcs: 1.035e-02, Loss_res: 1.057e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37100, Loss: 1.063e+00, Loss_bcs: 1.029e-02, Loss_res: 1.053e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37200, Loss: 1.059e+00, Loss_bcs: 1.022e-02, Loss_res: 1.049e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37300, Loss: 1.055e+00, Loss_bcs: 1.016e-02, Loss_res: 1.045e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37400, Loss: 1.051e+00, Loss_bcs: 1.009e-02, Loss_res: 1.041e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37500, Loss: 1.047e+00, Loss_bcs: 1.003e-02, Loss_res: 1.037e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37600, Loss: 1.044e+00, Loss_bcs: 9.971e-03, Loss_res: 1.034e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37700, Loss: 1.040e+00, Loss_bcs: 9.913e-03, Loss_res: 1.030e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37800, Loss: 1.036e+00, Loss_bcs: 9.853e-03, Loss_res: 1.026e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 37900, Loss: 1.032e+00, Loss_bcs: 9.794e-03, Loss_res: 1.022e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38000, Loss: 1.028e+00, Loss_bcs: 9.735e-03, Loss_res: 1.018e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38100, Loss: 1.024e+00, Loss_bcs: 9.672e-03, Loss_res: 1.015e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38200, Loss: 1.021e+00, Loss_bcs: 9.610e-03, Loss_res: 1.011e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38300, Loss: 1.017e+00, Loss_bcs: 9.551e-03, Loss_res: 1.007e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38400, Loss: 1.013e+00, Loss_bcs: 9.493e-03, Loss_res: 1.004e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38500, Loss: 1.010e+00, Loss_bcs: 9.436e-03, Loss_res: 1.000e+00 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38600, Loss: 1.006e+00, Loss_bcs: 9.380e-03, Loss_res: 9.965e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38700, Loss: 1.002e+00, Loss_bcs: 9.325e-03, Loss_res: 9.929e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38800, Loss: 9.986e-01, Loss_bcs: 9.272e-03, Loss_res: 9.893e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 38900, Loss: 9.950e-01, Loss_bcs: 9.219e-03, Loss_res: 9.858e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39000, Loss: 9.914e-01, Loss_bcs: 9.167e-03, Loss_res: 9.822e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39100, Loss: 9.878e-01, Loss_bcs: 9.116e-03, Loss_res: 9.787e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39200, Loss: 9.843e-01, Loss_bcs: 9.065e-03, Loss_res: 9.752e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39300, Loss: 9.807e-01, Loss_bcs: 9.014e-03, Loss_res: 9.717e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39400, Loss: 9.772e-01, Loss_bcs: 8.963e-03, Loss_res: 9.683e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39500, Loss: 9.737e-01, Loss_bcs: 8.913e-03, Loss_res: 9.648e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39600, Loss: 9.702e-01, Loss_bcs: 8.861e-03, Loss_res: 9.614e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39700, Loss: 9.668e-01, Loss_bcs: 8.811e-03, Loss_res: 9.580e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39800, Loss: 9.634e-01, Loss_bcs: 8.760e-03, Loss_res: 9.546e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "It: 39900, Loss: 9.600e-01, Loss_bcs: 8.710e-03, Loss_res: 9.512e-01 ,Time: 0.00\n",
            "Compute NTK...\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "Gradients information stored ...\n",
            "It: 40000, Loss: 9.566e-01, Loss_bcs: 8.661e-03, Loss_res: 9.479e-01 ,Time: 0.26\n",
            "Compute NTK...\n",
            "Save uv NN parameters successfully in %s ...checkpoints/Dec-08-2023_23-34-41-963687_M1\n",
            "Final loss total loss: 9.566028e-01\n",
            "Final loss loss_res: 9.479414e-01\n",
            "Final loss loss_bcs: 8.661379e-03\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "elapsed: 6.89e+01\n",
            "Relative L2 error_u: 7.65e-02\n",
            "Relative L2 error_r: 7.60e-03\n",
            "elapsed: 6.89e+01\n",
            "Relative L2 error_u: 7.65e-02\n",
            "Relative L2 error_v: 7.60e-03\n",
            "\n",
            "\n",
            "Method:  mini_batch\n",
            "\n",
            "average of time_list: 68.88730430603027\n",
            "average of error_u_list: 0.07648353358277864\n",
            "average of error_v_list: 0.0075999168203618435\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "join() argument must be str or bytes, not 'dict'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_29025/657240385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavemat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmtd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_model\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_mb\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmbbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_fb\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mubatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_bc\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbcbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_nIter\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mresult_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBytesWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mgenericpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_arg_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'join'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/twoPhase/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36m_check_arg_types\u001b[0;34m(funcname, *args)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             raise TypeError('%s() argument must be str or bytes, not %r' %\n\u001b[0;32m--> 153\u001b[0;31m                             (funcname, s.__class__.__name__)) from None\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasstr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't mix strings and bytes in path components\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: join() argument must be str or bytes, not 'dict'"
          ]
        }
      ],
      "source": [
        "# Define computional domain\n",
        "bc1_coords = np.array([[0.0], [0.0]])\n",
        "bc2_coords = np.array([[1.0], [1.0]])\n",
        "dom_coords = np.array([[0.0], [1.0]])\n",
        "\n",
        "# Training data on u(x) -- Dirichlet boundary conditions\n",
        "\n",
        "nn  = 100\n",
        "\n",
        "X_bc1 = dom_coords[0, 0] * np.ones((nn // 2, 1))\n",
        "X_bc2 = dom_coords[1, 0] * np.ones((nn // 2, 1))\n",
        "X_u = np.vstack([X_bc1, X_bc2])\n",
        "Y_u = u(X_u, a)\n",
        "\n",
        "X_r = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "Y_r = u_xx(X_r, a)\n",
        "\n",
        "nn = 1000\n",
        "X_star = np.linspace(dom_coords[0, 0], dom_coords[1, 0], nn)[:, None]\n",
        "u_star = u(X_star, a)\n",
        "r_star = u_xx(X_star, a)\n",
        "\n",
        "nIter =40001\n",
        "bcbatch_size = 500\n",
        "ubatch_size = 5000\n",
        "mbbatch_size = 128\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "mode = 'M1'\n",
        "layers = [1, 500, 1]\n",
        "\n",
        "\n",
        "\n",
        "iterations = 1\n",
        "methods = [ \"mini_batch\"]\n",
        "\n",
        "result_dict =  dict((mtd, []) for mtd in methods)\n",
        "\n",
        "for mtd in methods:\n",
        "    print(\"Method: \", mtd)\n",
        "    time_list = []\n",
        "    error_u_list = []\n",
        "    error_r_list = []\n",
        "    \n",
        "    for index in range(iterations):\n",
        "\n",
        "        print(\"Epoch: \", str(index+1))\n",
        "\n",
        "        # Create residual sampler\n",
        "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
        "        tf.reset_default_graph()\n",
        "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
        "\n",
        "            model = PINN(layers, X_u, Y_u, X_r, Y_r , mode , sess)    \n",
        "\n",
        "            # Train model\n",
        "            start_time = time.time()\n",
        "\n",
        "            if mtd ==\"full_batch\":\n",
        "                print(\"full_batch method is used\")\n",
        "                model.train(nIter  , bcbatch_size , ubatch_size  )\n",
        "            elif mtd ==\"mini_batch\":\n",
        "                print(\"mini_batch method is used\")\n",
        "                model.trainmb(nIter, mbbatch_size)\n",
        "            else:\n",
        "                print(\"unknown method!\")\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            # Predictions\n",
        "            u_pred = model.predict_u(X_star)\n",
        "            r_pred = model.predict_r(X_star)\n",
        "            # Predictions\n",
        "            model.save_NN()\n",
        "            model.plot_ntk()\n",
        " \n",
        "\n",
        "            error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "            error_r = np.linalg.norm(r_star - r_pred, 2) / np.linalg.norm(r_star, 2)\n",
        "\n",
        "            print('elapsed: {:.2e}'.format(elapsed))\n",
        "\n",
        "            print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "            print('Relative L2 error_r: {:.2e}'.format(error_r))\n",
        "\n",
        "\n",
        "            print('elapsed: {:.2e}'.format(elapsed))\n",
        "            print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
        "            print('Relative L2 error_v: {:.2e}'.format(error_r))\n",
        "\n",
        "            model.plt_prediction( X_star , u_star , u_pred)\n",
        "            sess.close()  \n",
        "            \n",
        "        time_list.append(elapsed)\n",
        "        error_u_list.append(error_u)\n",
        "        error_r_list.append(error_r)\n",
        "\n",
        "    print(\"\\n\\nMethod: \", mtd)\n",
        "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
        "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
        "    print(\"average of error_v_list:\" , sum(error_r_list) / len(error_r_list) )\n",
        "\n",
        "    result_dict[mtd] = [time_list ,error_u_list ,error_r_list ]\n",
        "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
        "\n",
        "    scipy.io.savemat(os.path.join(model.dirname,\"\"+mtd+\"_model\"+mode+\"_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(bcbatch_size)+\"_n\"+str(iterations)+\"_nIter\"+str(nIter)+\".mat\" , result_dict))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model\n",
        "layers = [1, 512, 1]  \n",
        "# layers = [1, 512, 512, 512, 1]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Fw807UNzhu5z",
        "outputId": "929ae89c-3e10-4c56-e349-051441e8ab32"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "loss_bcs = model.loss_bcs_log\n",
        "loss_res = model.loss_res_log\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "plt.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
        "plt.plot(loss_bcs, label='$\\mathcal{L}_{b}$')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFLIBq5xjZ3v"
      },
      "source": [
        "**Model Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To0PDN17cc0v",
        "outputId": "7284b31e-f2fe-41ab-93a9-4c2c91f94cde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative L2 error_u: 3.05e-02\n",
            "Relative L2 error_r: 4.16e-03\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "K428lOuXhdc8",
        "outputId": "b1e23055-178c-400c-8972-f3e7987e0892"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYdfKGLj6h0"
      },
      "source": [
        "**NTK Eigenvalues**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e3dByeQjhBYj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "vSn3Q_1IhisN",
        "outputId": "4c713f42-11b2-4de9-8698-085eb54c164d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIS5UH81kOxT"
      },
      "source": [
        "**Change of NTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF4Q_iZshQ-0"
      },
      "outputs": [],
      "source": [
        "# Change of the NTK\n",
        "NTK_change_list = []\n",
        "K0 = K_list[0]\n",
        "for K in K_list:\n",
        "    diff = np.linalg.norm(K - K0) / np.linalg.norm(K0) \n",
        "    NTK_change_list.append(diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "E-_gPGpCkF4n",
        "outputId": "9893e038-907d-4425-bb6e-8ecdb2ad497d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1bd39ddd50>]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = plt.figure(figsize=(6,5))\n",
        "plt.plot(NTK_change_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg0ZGHbAkW6N"
      },
      "source": [
        "\n",
        "**Change of NN Params**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLGv9JUuioVZ"
      },
      "outputs": [],
      "source": [
        "# Change of the weights and biases\n",
        "def compute_weights_diff(weights_1, weights_2):\n",
        "    weights = []\n",
        "    N = len(weights_1)\n",
        "    for k in range(N):\n",
        "        weight = weights_1[k] - weights_2[k]\n",
        "        weights.append(weight)\n",
        "    return weights\n",
        "\n",
        "def compute_weights_norm(weights, biases):\n",
        "    norm = 0\n",
        "    for w in weights:\n",
        "        norm = norm + np.sum(np.square(w))\n",
        "    for b in biases:\n",
        "        norm = norm + np.sum(np.square(b))\n",
        "    norm = np.sqrt(norm)\n",
        "    return norm\n",
        "\n",
        "# Restore the list weights and biases\n",
        "weights_log = model.weights_log\n",
        "biases_log = model.biases_log\n",
        "\n",
        "weights_0 = weights_log[0]\n",
        "biases_0 = biases_log[0]\n",
        "\n",
        "# Norm of the weights at initialization\n",
        "weights_init_norm = compute_weights_norm(weights_0, biases_0)\n",
        "\n",
        "weights_change_list = []\n",
        "\n",
        "N = len(weights_log)\n",
        "for k in range(N):\n",
        "    weights_diff = compute_weights_diff(weights_log[k], weights_log[0])\n",
        "    biases_diff = compute_weights_diff(biases_log[k], biases_log[0])\n",
        "    \n",
        "    weights_diff_norm = compute_weights_norm(weights_diff, biases_diff)\n",
        "    weights_change = weights_diff_norm / weights_init_norm\n",
        "    weights_change_list.append(weights_change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "5NLsAxgzi4KH",
        "outputId": "74d92bf9-0dde-438e-e9b3-a551904f4e2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1bd213a090>]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = plt.figure(figsize=(6,5))\n",
        "plt.plot(weights_change_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYbzkhfMjJ8k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINNsNTK_Poisson.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
