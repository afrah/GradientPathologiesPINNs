{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "# from NS_model_tf import Sampler, Navier_Stokes2D\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "import timeit\n",
    "\n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "def U_gamma_1(x):\n",
    "    num = x.shape[0]\n",
    "    return np.tile(np.array([1.0, 0.0]), (num, 1))\n",
    "\n",
    "\n",
    "def U_gamma_2(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "def operator(psi, p, x, y, Re, sigma_x=1.0, sigma_y=1.0):\n",
    "    u = tf.gradients(psi, y)[0] / sigma_y\n",
    "    v = - tf.gradients(psi, x)[0] / sigma_x\n",
    "\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_y = tf.gradients(u, y)[0] / sigma_y\n",
    "\n",
    "    v_x = tf.gradients(v, x)[0] / sigma_x\n",
    "    v_y = tf.gradients(v, y)[0] / sigma_y\n",
    "\n",
    "    p_x = tf.gradients(p, x)[0] / sigma_x\n",
    "    p_y = tf.gradients(p, y)[0] / sigma_y\n",
    "\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    u_yy = tf.gradients(u_y, y)[0] / sigma_y\n",
    "\n",
    "    v_xx = tf.gradients(v_x, x)[0] / sigma_x\n",
    "    v_yy = tf.gradients(v_y, y)[0] / sigma_y\n",
    "\n",
    "    Ru_momentum = u * u_x + v * u_y + p_x - (u_xx + u_yy) / Re\n",
    "    Rv_momentum = u * v_x + v * v_y + p_y - (v_xx + v_yy) / Re\n",
    "\n",
    "    return Ru_momentum, Rv_momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "class Navier_Stokes2D:\n",
    "    def __init__(self, layers, operator, bcs_sampler, res_sampler, Re, model , sess):\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x, self.sigma_x = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_y, self.sigma_y = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "        # Choose model\n",
    "        self.model = model\n",
    "\n",
    "        # Navier Stokes constant\n",
    "        self.Re = tf.constant(Re, dtype=tf.float32)\n",
    "        self.sess = sess\n",
    "        # Adaptive re-weighting constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_bcs1_u_val = np.array(2.0)\n",
    "        self.adaptive_constant_bcs1_v_val = np.array(2.0)\n",
    "        self.adaptive_constant_bcs2_u_val = np.array(2.0)\n",
    "        self.adaptive_constant_bcs2_v_val = np.array(2.0)\n",
    "        self.adaptive_constant_bcs3_u_val = np.array(2.0)\n",
    "        self.adaptive_constant_bcs3_v_val = np.array(2.0)\n",
    "        self.adaptive_constant_bcs4_u_val = np.array(2.0)\n",
    "        self.adaptive_constant_bcs4_v_val = np.array(2.0)\n",
    "\n",
    "        # self.adaptive_constant_bcs2_val = np.array(2.0)\n",
    "        # self.adaptive_constant_bcs3_val = np.array(2.0)\n",
    "        # self.adaptive_constant_bcs4_val = np.array(2.0)\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "        if model in ['M3', 'M4']:\n",
    "            # Initialize encoder weights and biases\n",
    "            self.encoder_weights_1 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_1 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "            self.encoder_weights_2 = self.xavier_init([2, layers[1]])\n",
    "            self.encoder_biases_2 = self.xavier_init([1, layers[1]])\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.u_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.v_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        # self.U_bc2_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        # self.U_bc3_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        # self.U_bc4_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.adaptive_constant_bcs1_u_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs1_u_val.shape)\n",
    "        self.adaptive_constant_bcs1_v_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs1_v_val.shape)\n",
    "\n",
    "        self.adaptive_constant_bcs2_u_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs2_u_val.shape)\n",
    "        self.adaptive_constant_bcs2_v_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs2_v_val.shape)\n",
    "\n",
    "        self.adaptive_constant_bcs3_u_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs3_u_val.shape)\n",
    "        self.adaptive_constant_bcs3_v_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs3_v_val.shape)\n",
    "\n",
    "        self.adaptive_constant_bcs4_u_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs4_u_val.shape)\n",
    "        self.adaptive_constant_bcs4_v_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs4_v_val.shape)\n",
    "\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred, self.v_bc1_pred = self.net_uv(self.x_bc1_tf, self.y_bc1_tf)\n",
    "        self.u_bc2_pred, self.v_bc2_pred = self.net_uv(self.x_bc2_tf, self.y_bc2_tf)\n",
    "        self.u_bc3_pred, self.v_bc3_pred = self.net_uv(self.x_bc3_tf, self.y_bc3_tf)\n",
    "        self.u_bc4_pred, self.v_bc4_pred = self.net_uv(self.x_bc4_tf, self.y_bc4_tf)\n",
    "\n",
    "\n",
    "        self.psi_pred, self.p_pred = self.net_psi_p(self.x_u_tf, self.y_u_tf)\n",
    "        self.u_pred, self.v_pred = self.net_uv(self.x_u_tf, self.y_u_tf)\n",
    "        self.u_momentum_pred, self.v_momentum_pred = self.net_r(self.x_r_tf, self.y_r_tf)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_u_momentum = tf.reduce_mean(tf.square(self.u_momentum_pred))\n",
    "        self.loss_v_momentum = tf.reduce_mean(tf.square(self.v_momentum_pred))\n",
    "\n",
    "        self.loss_res = self.loss_u_momentum + self.loss_v_momentum\n",
    "        \n",
    "        # Boundary loss\n",
    "        self.loss_bc1_u = tf.reduce_mean(tf.square(self.u_bc1_pred - self.u_bc1_tf)) \n",
    "        self.loss_bc1_v = tf.reduce_mean(tf.square(self.v_bc1_pred - self.v_bc1_tf))\n",
    "\n",
    "        self.loss_bc1 =  self.loss_bc1_u  + self.loss_bc1_v\n",
    "\n",
    "        self.loss_bc2_u = tf.reduce_mean(tf.square(self.u_bc2_pred)) \n",
    "        self.loss_bc2_v = tf.reduce_mean(tf.square(self.v_bc2_pred))\n",
    "\n",
    "        self.loss_bc2 =  self.loss_bc2_u  + self.loss_bc2_v\n",
    "\n",
    "        self.loss_bc3_u = tf.reduce_mean(tf.square(self.u_bc3_pred)) \n",
    "        self.loss_bc3_v = tf.reduce_mean(tf.square(self.v_bc3_pred))\n",
    "\n",
    "        self.loss_bc3 =  self.loss_bc3_u  + self.loss_bc3_v\n",
    "\n",
    "        self.loss_bc4_u = tf.reduce_mean(tf.square(self.u_bc4_pred)) \n",
    "        self.loss_bc4_v = tf.reduce_mean(tf.square(self.v_bc4_pred))\n",
    "        \n",
    "        self.loss_bc4 =  self.loss_bc4_u  + self.loss_bc4_v\n",
    "\n",
    "        self.loss_bcs = self.loss_bc1 + self.loss_bc2  + self.loss_bc3 + self.loss_bc4\n",
    "        \n",
    "        # Total loss\n",
    "        self.loss = self.loss_res +  \\\n",
    "        self.adaptive_constant_bcs1_u_tf * tf.reduce_mean(tf.square(self.u_bc1_pred - self.u_bc1_tf)) +  self.adaptive_constant_bcs1_v_tf *  tf.reduce_mean(tf.square(self.v_bc1_pred - self.v_bc1_tf)) +\\\n",
    "        self.adaptive_constant_bcs2_u_tf * tf.reduce_mean(tf.square(self.u_bc2_pred)) + self.adaptive_constant_bcs2_v_tf *tf.reduce_mean(tf.square(self.v_bc2_pred)) + \\\n",
    "        self.adaptive_constant_bcs3_u_tf * tf.reduce_mean(tf.square(self.u_bc3_pred)) + self.adaptive_constant_bcs3_v_tf *tf.reduce_mean(tf.square(self.v_bc3_pred)) +\\\n",
    "        self.adaptive_constant_bcs4_u_tf * tf.reduce_mean(tf.square(self.u_bc4_pred)) + self.adaptive_constant_bcs4_v_tf *tf.reduce_mean(tf.square(self.v_bc4_pred)) \n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                        1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        # Initialize Tensorflow variables\n",
    "     \n",
    "\n",
    "        # tf session\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def generate_grad_dict(self, layers):\n",
    "        num = len(layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    # Save gradients during training\n",
    "    def save_gradients(self, tf_dict):\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res_value, grad_bcs_value = self.sess.run([self.grad_res[i], self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res_value.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bcs_value.flatten())\n",
    "        return None\n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
    "                           dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        if self.model in ['M1', 'M2']:\n",
    "            num_layers = len(self.layers)\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "        if self.model in ['M3', 'M4']:\n",
    "            num_layers = len(self.layers)\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for l in range(0, num_layers - 2):\n",
    "                W = self.weights[l]\n",
    "                b = self.biases[l]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
    "                    tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            H = tf.add(tf.matmul(H, W), b)\n",
    "            return H\n",
    "\n",
    "    # Forward pass for stream-pressure formulation\n",
    "    def net_psi_p(self, x, y):\n",
    "        psi_p = self.forward_pass(tf.concat([x, y], 1))\n",
    "        psi = psi_p[:, 0:1]\n",
    "        p = psi_p[:, 1:2]\n",
    "        return psi, p\n",
    "\n",
    "    # Forward pass for velocities\n",
    "    def net_uv(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u = tf.gradients(psi, y)[0] / self.sigma_y\n",
    "        v = - tf.gradients(psi, x)[0] / self.sigma_x\n",
    "        return u, v\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u_momentum_pred, v_momentum_pred = self.operator(psi, p, x, y,\n",
    "                                                         self.Re,\n",
    "                                                         self.sigma_x, self.sigma_y)\n",
    "\n",
    "        return u_momentum_pred, v_momentum_pred\n",
    "\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter ,  batch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(1,nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.u_bc1_tf: u_bc1_batch[:, 0:1], self.v_bc1_tf: u_bc1_batch[:, 1:2],\n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.adaptive_constant_bcs1_u_tf: self.adaptive_constant_bcs1_u_val,\n",
    "                       self.adaptive_constant_bcs1_v_tf: self.adaptive_constant_bcs1_v_val,\n",
    "                      self.adaptive_constant_bcs2_u_tf: self.adaptive_constant_bcs2_u_val,\n",
    "                       self.adaptive_constant_bcs2_v_tf: self.adaptive_constant_bcs2_v_val,\n",
    "                      self.adaptive_constant_bcs3_u_tf: self.adaptive_constant_bcs3_u_val,\n",
    "                       self.adaptive_constant_bcs3_v_tf: self.adaptive_constant_bcs3_v_val,\n",
    "                      self.adaptive_constant_bcs4_u_tf: self.adaptive_constant_bcs4_u_val,\n",
    "                       self.adaptive_constant_bcs4_v_tf: self.adaptive_constant_bcs4_v_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss, loss_res , loss_bcs ,  loss_bc1_u, loss_bc1_v,loss_bc2_u,  loss_bc2_v ,  loss_bc3_u,  loss_bc3_v , loss_bc4_u,  loss_bc4_v , = self.sess.run([self.loss, self.loss_res,\n",
    "                                                                                                                                                                    self.loss_bcs,\n",
    "                                                                                                                                                                    self.loss_bc1_u, self.loss_bc1_v, \n",
    "                                                                                                                                                                    self.loss_bc2_u,  self.loss_bc2_v,\n",
    "                                                                                                                                                                    self.loss_bc3_u,  self.loss_bc3_v,\n",
    "                                                                                                                                                                    self.loss_bc4_u,  self.loss_bc4_v\n",
    "                                                                                                                                                                      ], tf_dict)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_r: %.3e, loss_bc: %.3e, Time: %.2f' % (it, loss, loss_res, loss_bcs , elapsed))\n",
    "\n",
    "                if it % 1000 == 0:\n",
    "                        \n",
    "                        alpha = 1000\n",
    "                        # adaptive_constant_bcs_val = self.sess.run(self.adaptive_constant_bcs, tf_dict)\n",
    "\n",
    "                        # self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * (1.0 - self.beta) + self.beta * self.adaptive_constant_bcs_val\n",
    "                        self.adaptive_constant_bcs1_val_u = alpha * loss_bc1_u\n",
    "                        self.adaptive_constant_bcs1_val_v = alpha * loss_bc1_v\n",
    "                        self.adaptive_constant_bcs2_val_u = alpha * loss_bc2_u\n",
    "                        self.adaptive_constant_bcs2_val_v = alpha * loss_bc2_v\n",
    "                        self.adaptive_constant_bcs3_val_u = alpha * loss_bc3_u\n",
    "                        self.adaptive_constant_bcs3_val_v = alpha * loss_bc3_v\n",
    "                        self.adaptive_constant_bcs4_val_u = alpha * loss_bc4_u\n",
    "                        self.adaptive_constant_bcs4_val_v = alpha * loss_bc4_v\n",
    "\n",
    "                        print(\"adaptive_constant_bcs1_val_u: {:.3f}\".format(self.adaptive_constant_bcs1_val_u))\n",
    "                        print(\"adaptive_constant_bcs1_val_v: {:.3f}\".format(self.adaptive_constant_bcs1_val_v))\n",
    "                        print(\"adaptive_constant_bcs2_val_u: {:.3f}\".format(self.adaptive_constant_bcs2_val_u))\n",
    "                        print(\"adaptive_constant_bcs2_val_v: {:.3f}\".format(self.adaptive_constant_bcs2_val_v))\n",
    "                        print(\"adaptive_constant_bcs3_val_u: {:.3f}\".format(self.adaptive_constant_bcs3_val_u))\n",
    "                        print(\"adaptive_constant_bcs3_val_v: {:.3f}\".format(self.adaptive_constant_bcs3_val_v))\n",
    "                        print(\"adaptive_constant_bcs4_val_u: {:.3f}\".format(self.adaptive_constant_bcs4_val_u))\n",
    "                        print(\"adaptive_constant_bcs4_val_v: {:.3f}\".format(self.adaptive_constant_bcs4_val_v))\n",
    "\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter ,  bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "            # Fetch boundary mini-batches\n",
    "\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], bcbatch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], bcbatch_size)\n",
    "        X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], bcbatch_size)\n",
    "        X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], bcbatch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, ubatch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.u_bc1_tf: u_bc1_batch[:, 0:1], self.v_bc1_tf: u_bc1_batch[:, 1:2],\n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs1_u_tf: self.adaptive_constant_bcs1_u_val,\n",
    "                    self.adaptive_constant_bcs1_v_tf: self.adaptive_constant_bcs1_v_val,\n",
    "                    self.adaptive_constant_bcs2_u_tf: self.adaptive_constant_bcs2_u_val,\n",
    "                    self.adaptive_constant_bcs2_v_tf: self.adaptive_constant_bcs2_v_val,\n",
    "                    self.adaptive_constant_bcs3_u_tf: self.adaptive_constant_bcs3_u_val,\n",
    "                    self.adaptive_constant_bcs3_v_tf: self.adaptive_constant_bcs3_v_val,\n",
    "                    self.adaptive_constant_bcs4_u_tf: self.adaptive_constant_bcs4_u_val,\n",
    "                    self.adaptive_constant_bcs4_v_tf: self.adaptive_constant_bcs4_v_val\n",
    "                    }\n",
    "        \n",
    "        for it in range(1,nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            self.sess.run(self.train_op, tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 100 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss, loss_res , loss_bcs ,  loss_bc1_u, loss_bc1_v,loss_bc2_u,  loss_bc2_v ,  loss_bc3_u,  loss_bc3_v , loss_bc4_u,  loss_bc4_v , = self.sess.run([self.loss, self.loss_res,\n",
    "                                                                                                                                                                    self.loss_bcs,\n",
    "                                                                                                                                                                    self.loss_bc1_u, self.loss_bc1_v, \n",
    "                                                                                                                                                                    self.loss_bc2_u,  self.loss_bc2_v,\n",
    "                                                                                                                                                                    self.loss_bc3_u,  self.loss_bc3_v,\n",
    "                                                                                                                                                                    self.loss_bc4_u,  self.loss_bc4_v\n",
    "                                                                                                                                                                      ], tf_dict)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_r: %.3e, loss_bc: %.3e, Time: %.2f' % (it, loss, loss_res, loss_bcs , elapsed))\n",
    "\n",
    "                if it % 1000 == 0:\n",
    "                        # Compute the adaptive constant\n",
    "\n",
    "                        alpha = 1000\n",
    "                        # adaptive_constant_bcs_val = self.sess.run(self.adaptive_constant_bcs, tf_dict)\n",
    "\n",
    "                        # self.adaptive_constant_bcs_val = adaptive_constant_bcs_val * (1.0 - self.beta) + self.beta * self.adaptive_constant_bcs_val\n",
    "                        self.adaptive_constant_bcs1_val_u = alpha * loss_bc1_u\n",
    "                        self.adaptive_constant_bcs1_val_v = alpha * loss_bc1_v\n",
    "                        self.adaptive_constant_bcs2_val_u = alpha * loss_bc2_u\n",
    "                        self.adaptive_constant_bcs2_val_v = alpha * loss_bc2_v\n",
    "                        self.adaptive_constant_bcs3_val_u = alpha * loss_bc3_u\n",
    "                        self.adaptive_constant_bcs3_val_v = alpha * loss_bc3_v\n",
    "                        self.adaptive_constant_bcs4_val_u = alpha * loss_bc4_u\n",
    "                        self.adaptive_constant_bcs4_val_v = alpha * loss_bc4_v\n",
    "\n",
    "                        print(\"adaptive_constant_bcs1_val_u: {:.3f}\".format(self.adaptive_constant_bcs1_val_u))\n",
    "                        print(\"adaptive_constant_bcs1_val_v: {:.3f}\".format(self.adaptive_constant_bcs1_val_v))\n",
    "                        print(\"adaptive_constant_bcs2_val_u: {:.3f}\".format(self.adaptive_constant_bcs2_val_u))\n",
    "                        print(\"adaptive_constant_bcs2_val_v: {:.3f}\".format(self.adaptive_constant_bcs2_val_v))\n",
    "                        print(\"adaptive_constant_bcs3_val_u: {:.3f}\".format(self.adaptive_constant_bcs3_val_u))\n",
    "                        print(\"adaptive_constant_bcs3_val_v: {:.3f}\".format(self.adaptive_constant_bcs3_val_v))\n",
    "                        print(\"adaptive_constant_bcs4_val_u: {:.3f}\".format(self.adaptive_constant_bcs4_val_u))\n",
    "                        print(\"adaptive_constant_bcs4_val_v: {:.3f}\".format(self.adaptive_constant_bcs4_val_v))\n",
    "\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "#################################################################################################################################################################\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_psi_p(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        psi_star = self.sess.run(self.psi_pred, tf_dict)\n",
    "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
    "        return psi_star, p_star\n",
    "\n",
    "    def predict_uv(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
    "        return u_star, v_star\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        [elapsed, error_u , error_v ,  error_p] = test_method(mtd , layers, operator, bcs_sampler, res_sampler ,Re , stiff_ratio ,  X_star , X , Y)\n",
    "\n",
    "def test_method(method , layers, operator, bcs_sampler, res_sampler, Re ,  mode , X_star , X , Y , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "        # sess.run(init)\n",
    " \n",
    "\n",
    "        model = Navier_Stokes2D(layers, operator, bcs_sampler, res_sampler, Re, mode , sess)\n",
    "\n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "\n",
    "        if method ==\"full_batch\":\n",
    "            model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "        elif method ==\"mini_batch\":\n",
    "            model.trainmb(nIter, mbbatch_size)\n",
    "        else:\n",
    "            print(\"unknown method!\")\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # Predictions\n",
    "        _, p_pred = model.predict_psi_p(X_star)\n",
    "        u_pred, v_pred = model.predict_uv(X_star)\n",
    "\n",
    "        sess.close()   \n",
    "    # psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "    p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "    # u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "    # v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "    # velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # Reference\n",
    "    u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "    v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "    # velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "    u_pred = u_pred.reshape(100,100)\n",
    "    v_pred = v_pred.reshape(100,100)\n",
    "    p_pred = p_pred.reshape(100,100)\n",
    "\n",
    "    # Relative error\n",
    "\n",
    "    error_u = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "    error_v = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "    error_p = np.linalg.norm(p_pred - p_star.T, 2) / np.linalg.norm(p_star, 2)\n",
    "\n",
    "    print('elapsed: {:.2e}'.format(elapsed))\n",
    "\n",
    "    print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "    print('Relative L2 error_v: {:.2e}'.format(error_v))\n",
    "    print('Relative L2 error_p: {:.2e}'.format(error_p))\n",
    "\n",
    "    return [elapsed, error_u , error_v ,error_p ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16962/3940482384.py:5: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16962/3940482384.py:6: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16962/3940482384.py:7: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16962/3940482384.py:7: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16962/4023773752.py:186: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16962/4023773752.py:64: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 02:33:35.277227: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-24 02:33:35.299977: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-11-24 02:33:35.300470: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562d59434310 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-24 02:33:35.300483: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-11-24 02:33:35.301009: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_16962/4023773752.py:150: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16962/4023773752.py:153: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_16962/4023773752.py:160: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "It: 100, Loss: 1.306e+00, Loss_r: 4.696e-02, loss_bc: 6.295e-01, Time: 20.03\n",
      "It: 200, Loss: 6.537e-01, Loss_r: 1.991e-02, loss_bc: 3.169e-01, Time: 3.71\n",
      "It: 300, Loss: 5.035e-01, Loss_r: 1.616e-02, loss_bc: 2.437e-01, Time: 3.85\n",
      "It: 400, Loss: 3.274e-01, Loss_r: 1.926e-02, loss_bc: 1.541e-01, Time: 3.79\n",
      "It: 500, Loss: 3.182e-01, Loss_r: 1.917e-02, loss_bc: 1.495e-01, Time: 3.81\n",
      "It: 600, Loss: 2.990e-01, Loss_r: 1.890e-02, loss_bc: 1.400e-01, Time: 4.03\n",
      "It: 700, Loss: 2.589e-01, Loss_r: 1.436e-02, loss_bc: 1.223e-01, Time: 3.85\n",
      "It: 800, Loss: 2.667e-01, Loss_r: 1.161e-02, loss_bc: 1.275e-01, Time: 3.89\n",
      "It: 900, Loss: 2.502e-01, Loss_r: 1.445e-02, loss_bc: 1.179e-01, Time: 3.88\n",
      "It: 1000, Loss: 2.568e-01, Loss_r: 1.162e-02, loss_bc: 1.226e-01, Time: 3.84\n",
      "adaptive_constant_bcs1_val_u: 27.583\n",
      "adaptive_constant_bcs1_val_v: 5.383\n",
      "adaptive_constant_bcs2_val_u: 46.265\n",
      "adaptive_constant_bcs2_val_v: 1.229\n",
      "adaptive_constant_bcs3_val_u: 40.023\n",
      "adaptive_constant_bcs3_val_v: 1.791\n",
      "adaptive_constant_bcs4_val_u: 0.149\n",
      "adaptive_constant_bcs4_val_v: 0.154\n",
      "elapsed: 5.84e+01\n",
      "Relative L2 error_u: 6.25e-01\n",
      "Relative L2 error_v: 8.37e-01\n",
      "Relative L2 error_p: 7.47e-01\n",
      "\n",
      "\n",
      "Method:  mini_batch\n",
      "\n",
      "average of time_list: 58.42804574966431\n",
      "average of error_u_list: 0.6248645219582811\n",
      "average of error_v_list: 0.8374562674122427\n",
      "average of error_p_list: 0.7466893626996584\n",
      "Method:  full_batch\n",
      "Epoch:  1\n",
      "It: 100, Loss: 1.479e+00, Loss_r: 3.739e-02, loss_bc: 7.209e-01, Time: 32.76\n",
      "It: 200, Loss: 7.743e-01, Loss_r: 2.818e-02, loss_bc: 3.730e-01, Time: 17.10\n",
      "It: 300, Loss: 4.548e-01, Loss_r: 2.713e-02, loss_bc: 2.138e-01, Time: 17.78\n",
      "It: 400, Loss: 3.434e-01, Loss_r: 2.190e-02, loss_bc: 1.608e-01, Time: 15.59\n",
      "It: 500, Loss: 3.049e-01, Loss_r: 1.753e-02, loss_bc: 1.437e-01, Time: 15.81\n",
      "It: 600, Loss: 2.826e-01, Loss_r: 1.458e-02, loss_bc: 1.340e-01, Time: 16.12\n",
      "It: 700, Loss: 2.650e-01, Loss_r: 1.275e-02, loss_bc: 1.261e-01, Time: 15.78\n",
      "It: 800, Loss: 2.505e-01, Loss_r: 1.182e-02, loss_bc: 1.193e-01, Time: 15.63\n",
      "It: 900, Loss: 2.387e-01, Loss_r: 1.149e-02, loss_bc: 1.136e-01, Time: 16.90\n",
      "It: 1000, Loss: 2.292e-01, Loss_r: 1.153e-02, loss_bc: 1.088e-01, Time: 17.73\n",
      "adaptive_constant_bcs1_val_u: 29.284\n",
      "adaptive_constant_bcs1_val_v: 3.687\n",
      "adaptive_constant_bcs2_val_u: 34.833\n",
      "adaptive_constant_bcs2_val_v: 2.020\n",
      "adaptive_constant_bcs3_val_u: 37.155\n",
      "adaptive_constant_bcs3_val_v: 0.893\n",
      "adaptive_constant_bcs4_val_u: 0.605\n",
      "adaptive_constant_bcs4_val_v: 0.367\n",
      "elapsed: 1.85e+02\n",
      "Relative L2 error_u: 5.78e-01\n",
      "Relative L2 error_v: 8.36e-01\n",
      "Relative L2 error_p: 2.16e-01\n",
      "\n",
      "\n",
      "Method:  full_batch\n",
      "\n",
      "average of time_list: 185.15453815460205\n",
      "average of error_u_list: 0.5784952979567056\n",
      "average of error_v_list: 0.8358140404172113\n",
      "average of error_p_list: 0.21573757860018836\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "nIter =1001\n",
    "bcbatch_size = 500\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "# Parameters of equations\n",
    "Re = 100.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[0.0, 1.0], [1.0, 1.0]])\n",
    "bc2_coords = np.array([[0.0, 0.0], [0.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 0.0], [1.0, 1.0]])\n",
    "bc4_coords = np.array([[0.0, 0.0], [1.0, 0.0]])\n",
    "dom_coords = np.array([[0.0, 0.0], [1.0, 1.0]])\n",
    "\n",
    "# Define model\n",
    "mode = 'M4'\n",
    "layers = [2, 50 , 2] #, 50, 50, 50 , 50 , 2]\n",
    "\n",
    "stiff_ratio = False  # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_v_list = []\n",
    "    error_p_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: U_gamma_1(x), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: U_gamma_2(x), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: U_gamma_2(x), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: U_gamma_2(x), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x), name='Forcing')\n",
    "\n",
    "        [elapsed, error_u , error_v ,  error_p] = test_method(mtd , layers, operator, bcs_sampler, res_sampler ,Re , mode ,  X_star , X , Y  , nIter ,mbbatch_size , bcbatch_size , ubatch_size)\n",
    "\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_v_list.append(error_v)\n",
    "        error_p_list.append(error_p)\n",
    "\n",
    "    print(\"\\n\\nMethod: \", mtd)\n",
    "    print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "    print(\"average of error_p_list:\" , sum(error_p_list) / len(error_p_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "scipy.io.savemat(\"./dataset/NS_model_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Method:  full_batch\n",
      "\n",
      "average of time_list: 7459.62825\n",
      "average of error_u_list: 0.037450000000000004\n",
      "average of error_v_list: 0.0524\n",
      "average of error_p_list: 0.32332500000000003\n",
      "\n",
      "\n",
      "Method:  full_batch\n",
      "\n",
      "average of time_list: 23525.738400000002\n",
      "average of error_u_list: 0.030434\n",
      "average of error_v_list: 0.04129\n",
      "average of error_p_list: 0.252614\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import scipy.io\n",
    "\n",
    "mode = 'M4'\n",
    "mbbatch_size = 128\n",
    "ubatch_size = 5000\n",
    "bcbatch_size = 500\n",
    "iterations = 40000\n",
    "\n",
    "time_list = []\n",
    "error_u_list = []\n",
    "error_v_list = []\n",
    "error_p_list = []\n",
    "    \n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "##Mini Batch\n",
    "time_list = [8090 , 7220 , 7232.24 , 7296.273]\n",
    "error_u_list = [0.0366 , 0.0392 , 0.035 ,  0.039]\n",
    "error_v_list = [0.0511 , 0.0555 , 0.044 ,  0.059]\n",
    "error_p_list = [0.511 , 0.446 , 0.214 ,  0.1223]\n",
    "\n",
    "\n",
    "result_dict[\"mini_batch\"] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "print(\"\\n\\nMethod: \", mtd)\n",
    "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "print(\"average of error_p_list:\" , sum(error_p_list) / len(error_p_list) )\n",
    "\n",
    "##Full Batch\n",
    "time_list = [23482.04 , 23500.01  , 23477.035 , 23513.68 , 23655.927]\n",
    "error_u_list = [ 0.03107 ,0.0364 , 0.024  ,  0.0274 , 0.0333]\n",
    "error_v_list = [0.03975 , 0.0574 , 0.039 , 0.0394 , 0.0309]\n",
    "error_p_list = [0.09167 , 0.139 , 0.369 , 0.2268 , 0.4366]\n",
    "\n",
    "result_dict[\"full_batch\"] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "print(\"\\n\\nMethod: \", mtd)\n",
    "print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "print(\"average of error_p_list:\" , sum(error_p_list) / len(error_p_list) )\n",
    "\n",
    "\n",
    "scipy.io.savemat(\"./dataset/NS_model_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mini_batch': [[8090, 7220, 7232.24, 7296.273],\n",
       "  [0.0366, 0.0392, 0.035, 0.039],\n",
       "  [0.0511, 0.0555, 0.044, 0.059],\n",
       "  [0.511, 0.446, 0.214, 0.1223]],\n",
       " 'full_batch': [[23482.04, 23500.01, 23477.035, 23513.68, 23655.927],\n",
       "  [0.03107, 0.0364, 0.024, 0.0274, 0.0333],\n",
       "  [0.03975, 0.0574, 0.039, 0.0394, 0.0309],\n",
       "  [0.09167, 0.139, 0.369, 0.2268, 0.4366]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "# Predictions\n",
    "psi_pred, p_pred = model.predict_psi_p(X_star)\n",
    "u_pred, v_pred = model.predict_uv(X_star)\n",
    "\n",
    "psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "# Reference\n",
    "u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "u_pred = u_pred.reshape(100,100)\n",
    "v_pred = v_pred.reshape(100,100)\n",
    "p_pred = p_pred.reshape(100,100)\n",
    "\n",
    "# Relative error\n",
    "error = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "error = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "error = np.linalg.norm(p_pred - p_star.T, 2) / np.linalg.norm(p_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "\n",
    "### Plot ###\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "# Predictions\n",
    "psi_pred, p_pred = model.predict_psi_p(X_star)\n",
    "u_pred, v_pred = model.predict_uv(X_star)\n",
    "\n",
    "psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "# Reference\n",
    "u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "# Relative error\n",
    "error = np.linalg.norm(velocity_star - velocity_ref.T, 2) / np.linalg.norm(velocity_ref, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "\n",
    "### Plot ###\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference solution & Prediceted solution\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "fig_1.add_subplot(1, 3, 1)\n",
    "plt.pcolor(X.T, Y.T, velocity_ref, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Reference Velocity')\n",
    "\n",
    "fig_1.add_subplot(1, 3, 2)\n",
    "plt.pcolor(X, Y, velocity_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predicted Velocity')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_1.add_subplot(1, 3, 3)\n",
    "plt.pcolor(X, Y, np.abs(velocity_star - velocity_ref.T), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Absolute Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference solution & Prediceted solution\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "fig_1.add_subplot(1, 3, 1)\n",
    "plt.pcolor(X, Y, p_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Reference Velocity')\n",
    "\n",
    "fig_1.add_subplot(1, 3, 2)\n",
    "plt.pcolor(x, Y, p_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predicted Velocity')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_1.add_subplot(1, 3, 3)\n",
    "plt.pcolor(X, Y, np.abs(p_star - p_pred.T), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Absolute Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ## Loss ##\n",
    "loss_res = model.loss_res_log\n",
    "loss_bcs = model.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Adaptive Constant\n",
    "adaptive_constant = model.adpative_constant_bcs_log\n",
    "    \n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradients #\n",
    "data_gradients_res = model.dict_gradients_res_layers\n",
    "data_gradients_bcs = model.dict_gradients_bcs_layers\n",
    "\n",
    "num_hidden_layers = len(layers) -1\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    \n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_{u_b}$')\n",
    "\n",
    "    # ax.get_legend().remove()\n",
    "    ax.set_xlim([-1.0, 1.0])\n",
    "    ax.set_ylim([0, 100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
