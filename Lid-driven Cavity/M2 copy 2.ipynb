{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "# from NS_model_tf import Sampler, Navier_Stokes2D\n",
    "import os\n",
    "os.environ[\"KMP_WARNINGS\"] = \"FALSE\" \n",
    "import timeit\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "def U_gamma_1(x):\n",
    "    num = x.shape[0]\n",
    "    return np.tile(np.array([1.0, 0.0]), (num, 1))\n",
    "\n",
    "\n",
    "def U_gamma_2(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    num = x.shape[0]\n",
    "    return np.zeros((num, 2))\n",
    "\n",
    "def operator(psi, p, x, y, Re, sigma_x=1.0, sigma_y=1.0):\n",
    "    u = tf.gradients(psi, y)[0] / sigma_y\n",
    "    v = - tf.gradients(psi, x)[0] / sigma_x\n",
    "\n",
    "    u_x = tf.gradients(u, x)[0] / sigma_x\n",
    "    u_y = tf.gradients(u, y)[0] / sigma_y\n",
    "\n",
    "    v_x = tf.gradients(v, x)[0] / sigma_x\n",
    "    v_y = tf.gradients(v, y)[0] / sigma_y\n",
    "\n",
    "    p_x = tf.gradients(p, x)[0] / sigma_x\n",
    "    p_y = tf.gradients(p, y)[0] / sigma_y\n",
    "\n",
    "    u_xx = tf.gradients(u_x, x)[0] / sigma_x\n",
    "    u_yy = tf.gradients(u_y, y)[0] / sigma_y\n",
    "\n",
    "    v_xx = tf.gradients(v_x, x)[0] / sigma_x\n",
    "    v_yy = tf.gradients(v_y, y)[0] / sigma_y\n",
    "\n",
    "    Ru_momentum = u * u_x + v * u_y + p_x - (u_xx + u_yy) / Re\n",
    "    Rv_momentum = u * v_x + v * v_y + p_y - (v_xx + v_yy) / Re\n",
    "\n",
    "    return Ru_momentum, Rv_momentum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampler:\n",
    "    # Initialize the class\n",
    "    def __init__(self, dim, coords, func, name=None):\n",
    "        self.dim = dim\n",
    "        self.coords = coords\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "\n",
    "    def sample(self, N):\n",
    "        x = self.coords[0:1, :] + (self.coords[1:2, :] - self.coords[0:1, :]) * np.random.rand(N, self.dim)\n",
    "        y = self.func(x)\n",
    "        return x, y\n",
    "\n",
    "class Navier_Stokes2D:\n",
    "    def __init__(self, layers, operator, bcs_sampler, res_sampler, Re, mode , sess):\n",
    "\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.dirname, logpath = self.make_output_dir()\n",
    "        self.logger = self.get_logger(logpath)     \n",
    "\n",
    "        # Normalization constants\n",
    "        X, _ = res_sampler.sample(np.int32(1e5))\n",
    "        self.mu_X, self.sigma_X = X.mean(0), X.std(0)\n",
    "        self.mu_x, self.sigma_x = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_y, self.sigma_y = self.mu_X[1], self.sigma_X[1]\n",
    "\n",
    "        # Samplers\n",
    "        self.operator = operator\n",
    "        self.bcs_sampler = bcs_sampler\n",
    "        self.res_sampler = res_sampler\n",
    "\n",
    "\n",
    "        # Navier Stokes constant\n",
    "        self.Re = tf.constant(Re, dtype=tf.float32)\n",
    "\n",
    "        # Adaptive re-weighting constant\n",
    "        self.beta = 0.9\n",
    "        self.adaptive_constant_bcs_val = np.array(1.0)\n",
    "\n",
    "        # Define Tensorflow session\n",
    "        self.sess = sess\n",
    "\n",
    "        # Initialize network weights and biases\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "\n",
    "\n",
    "        # Define placeholders and computational graph\n",
    "        self.x_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_u_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.x_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc1_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc1_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc2_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc2_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc3_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc3_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_bc4_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.U_bc4_tf = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "\n",
    "        self.x_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        self.y_r_tf = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.adaptive_constant_bcs_tf = tf.placeholder(tf.float32, shape=self.adaptive_constant_bcs_val.shape)\n",
    "\n",
    "        # Evaluate predictions\n",
    "        self.u_bc1_pred, self.v_bc1_pred = self.net_uv(self.x_bc1_tf, self.y_bc1_tf)\n",
    "        self.u_bc2_pred, self.v_bc2_pred = self.net_uv(self.x_bc2_tf, self.y_bc2_tf)\n",
    "        self.u_bc3_pred, self.v_bc3_pred = self.net_uv(self.x_bc3_tf, self.y_bc3_tf)\n",
    "        self.u_bc4_pred, self.v_bc4_pred = self.net_uv(self.x_bc4_tf, self.y_bc4_tf)\n",
    "\n",
    "        self.U_bc1_pred = tf.concat([self.u_bc1_pred, self.v_bc1_pred], axis=1)\n",
    "        self.U_bc2_pred = tf.concat([self.u_bc2_pred, self.v_bc2_pred], axis=1)\n",
    "        self.U_bc3_pred = tf.concat([self.u_bc3_pred, self.v_bc3_pred], axis=1)\n",
    "        self.U_bc4_pred = tf.concat([self.u_bc4_pred, self.v_bc4_pred], axis=1)\n",
    "\n",
    "        self.psi_pred, self.p_pred = self.net_psi_p(self.x_u_tf, self.y_u_tf)\n",
    "        self.u_pred, self.v_pred = self.net_uv(self.x_u_tf, self.y_u_tf)\n",
    "        self.u_momentum_pred, self.v_momentum_pred = self.net_r(self.x_r_tf, self.y_r_tf)\n",
    "\n",
    "        # Residual loss\n",
    "        self.loss_u_momentum = tf.reduce_mean(tf.square(self.u_momentum_pred))\n",
    "        self.loss_v_momentum = tf.reduce_mean(tf.square(self.v_momentum_pred))\n",
    "\n",
    "        self.loss_res = self.loss_u_momentum + self.loss_v_momentum\n",
    "        \n",
    "        # Boundary loss\n",
    "        self.loss_bc1 = tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf))\n",
    "        self.loss_bc2 = tf.reduce_mean(tf.square(self.U_bc2_pred))\n",
    "        self.loss_bc3 = tf.reduce_mean(tf.square(self.U_bc3_pred))\n",
    "        self.loss_bc4 = tf.reduce_mean(tf.square(self.U_bc4_pred))\n",
    "        \n",
    "        self.loss_bcs = self.adaptive_constant_bcs_tf * tf.reduce_mean(tf.square(self.U_bc1_pred - self.U_bc1_tf) +tf.square(self.U_bc2_pred) + tf.square(self.U_bc3_pred) + tf.square(self.U_bc4_pred))\n",
    "        \n",
    "        # Total loss\n",
    "        self.loss = self.loss_res + self.loss_bcs\n",
    "\n",
    "        # Define optimizer with learning rate schedule\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 1e-3\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.9, staircase=False)\n",
    "        # Passing global_step to minimize() will increment it at each step.\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 , self.loss_bc2 , self.loss_bc3, self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bc1\", \"loss_bc2\", \"loss_bc3\", \"loss_bc4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "        # Logger\n",
    "        self.loss_res_log = []\n",
    "        self.loss_bcs_log = []\n",
    "        # self.saver = tf.train.Saver()\n",
    "\n",
    "        # Generate dicts for gradients storage\n",
    "        self.dict_gradients_res_layers = self.generate_grad_dict()\n",
    "        self.dict_gradients_bcs_layers = self.generate_grad_dict()\n",
    "\n",
    "        # Gradients Storage\n",
    "        self.grad_res = []\n",
    "        self.grad_bcs = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.grad_res.append(tf.gradients(self.loss_res, self.weights[i])[0])\n",
    "            self.grad_bcs.append(tf.gradients(self.loss_bcs, self.weights[i])[0])\n",
    "\n",
    "        self.adpative_constant_bcs_list = []\n",
    "        self.adpative_constant_res_list = []\n",
    "        self.adpative_constant_bcs_log = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.adpative_constant_res_list.append( tf.reduce_max(tf.abs(self.grad_res[i])))\n",
    "            self.adpative_constant_bcs_list.append(  tf.reduce_mean(tf.abs(self.grad_bcs[i])))\n",
    "\n",
    "        self.adaptive_constant_bcs = tf.reduce_max(tf.stack(self.adpative_constant_res_list))/  tf.reduce_mean(tf.stack(self.adpative_constant_bcs_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.loss_tensor_list = [self.loss ,  self.loss_res,  self.loss_bc1 ,  self.loss_bc2 ,  self.loss_bc3  ,  self.loss_bc4] \n",
    "        self.loss_list = [\"total loss\" , \"loss_res\" , \"loss_bcs1\" , \"loss_bcs2\", \"loss_bcs3\" ,  \"loss_bcs4\"] \n",
    "\n",
    "        self.epoch_loss = dict.fromkeys(self.loss_list, 0)\n",
    "        self.loss_history = dict((loss, []) for loss in self.loss_list)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "\n",
    "    # Xavier initialization\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
    "        return tf.Variable(tf.random_normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev, dtype=tf.float32)\n",
    "\n",
    "    # Initialize network weights and biases using Xavier initialization\n",
    "    def initialize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    # Evaluates the forward pass\n",
    "    def forward_pass(self, H):\n",
    "        num_layers = len(self.layers)\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = self.weights[l]\n",
    "            b = self.biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        H = tf.add(tf.matmul(H, W), b)\n",
    "        return H\n",
    "\n",
    "\n",
    "    # Forward pass for stream-pressure formulation\n",
    "    def net_psi_p(self, x, y):\n",
    "        psi_p = self.forward_pass(tf.concat([x, y], 1))\n",
    "        psi = psi_p[:, 0:1]\n",
    "        p = psi_p[:, 1:2]\n",
    "        return psi, p\n",
    "\n",
    "    # Forward pass for velocities\n",
    "    def net_uv(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u = tf.gradients(psi, y)[0] / self.sigma_y\n",
    "        v = - tf.gradients(psi, x)[0] / self.sigma_x\n",
    "        return u, v\n",
    "\n",
    "    # Forward pass for residual\n",
    "    def net_r(self, x, y):\n",
    "        psi, p = self.net_psi_p(x, y)\n",
    "        u_momentum_pred, v_momentum_pred = self.operator(psi, p, x, y,  self.Re,  self.sigma_x, self.sigma_y)\n",
    "\n",
    "        return u_momentum_pred, v_momentum_pred\n",
    "\n",
    "    def fetch_minibatch(self, sampler, N):\n",
    "        X, Y = sampler.sample(N)\n",
    "        X = (X - self.mu_X) / self.sigma_X\n",
    "        return X, Y\n",
    "\n",
    "    # Trains the model by minimizing the MSE loss\n",
    "    def train(self, nIter ,  bcbatch_size , ubatch_size):\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        # Fetch boundary mini-batches\n",
    "        batch_size = bcbatch_size\n",
    "        X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "        X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "        X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "        X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "        # Fetch residual mini-batch\n",
    "        batch_size = ubatch_size\n",
    "\n",
    "        X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "        # Define a dictionary for associating placeholders with data\n",
    "        tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                    self.U_bc1_tf: u_bc1_batch,\n",
    "                    self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                    self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                    self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                    self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                    self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val\n",
    "                    }\n",
    "        for it in range(nIter):\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _, batch_losses = self.sess.run([self.train_op, self.loss_tensor_list] ,tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "\n",
    "                # Compute the adaptive constant\n",
    "                adaptive_constant_bcs_val = self.sess.run(self.adaptive_constant_bcs, tf_dict)\n",
    "\n",
    "                self.adaptive_constant_bcs_val = adaptive_constant_bcs_val *  (1.0 - self.beta) + self.beta * self.adaptive_constant_bcs_val\n",
    "\n",
    "                # self.adpative_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "                # self.loss_bcs_log.append(loss_u_value)\n",
    "                # self.loss_res_log.append(loss_r_value)\n",
    "\n",
    "                print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' % (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "\n",
    "                print(\"constant_bcs_val: {:.3f}\".format(self.adaptive_constant_bcs_val))\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "            # # Store gradients\n",
    "            # if it % 10000 == 0:\n",
    "            #     self.save_gradients(tf_dict)\n",
    "            #     print(\"Gradients information stored ...\")\n",
    "\n",
    "   # Trains the model by minimizing the MSE loss\n",
    "    def trainmb(self, nIter, batch_size):\n",
    "        itValues = [1,100,1000,39999]\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        for it in range(nIter):\n",
    "            # Fetch boundary mini-batches\n",
    "            X_bc1_batch, u_bc1_batch = self.fetch_minibatch(self.bcs_sampler[0], batch_size)\n",
    "            X_bc2_batch, _ = self.fetch_minibatch(self.bcs_sampler[1], batch_size)\n",
    "            X_bc3_batch, _ = self.fetch_minibatch(self.bcs_sampler[2], batch_size)\n",
    "            X_bc4_batch, _ = self.fetch_minibatch(self.bcs_sampler[3], batch_size)\n",
    "\n",
    "            # Fetch residual mini-batch\n",
    "            X_res_batch, _ = self.fetch_minibatch(self.res_sampler, batch_size)\n",
    "\n",
    "            # Define a dictionary for associating placeholders with data\n",
    "            tf_dict = {self.x_bc1_tf: X_bc1_batch[:, 0:1], self.y_bc1_tf: X_bc1_batch[:, 1:2],\n",
    "                       self.U_bc1_tf: u_bc1_batch,\n",
    "                       self.x_bc2_tf: X_bc2_batch[:, 0:1], self.y_bc2_tf: X_bc2_batch[:, 1:2],\n",
    "                       self.x_bc3_tf: X_bc3_batch[:, 0:1], self.y_bc3_tf: X_bc3_batch[:, 1:2],\n",
    "                       self.x_bc4_tf: X_bc4_batch[:, 0:1], self.y_bc4_tf: X_bc4_batch[:, 1:2],\n",
    "                       self.x_r_tf: X_res_batch[:, 0:1], self.y_r_tf: X_res_batch[:, 1:2],\n",
    "                       self.adaptive_constant_bcs_tf: self.adaptive_constant_bcs_val\n",
    "                       }\n",
    "\n",
    "            # Run the Tensorflow session to minimize the loss\n",
    "            _, batch_losses = self.sess.run([self.train_op, self.loss_tensor_list] ,tf_dict)\n",
    "\n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = timeit.default_timer() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                loss_u_value, loss_r_value = self.sess.run([self.loss_bcs, self.loss_res], tf_dict)\n",
    "                self.print('It: %d, Loss: %.3e, Loss_u: %.3e, Loss_r: %.3e, Time: %.2f' % (it, loss_value, loss_u_value, loss_r_value, elapsed))\n",
    "\n",
    "                if it % 1000 == 0:\n",
    "\n",
    "                    # Compute the adaptive constant\n",
    "                    adaptive_constant_bcs_val = self.sess.run(self.adaptive_constant_bcs, tf_dict)\n",
    "\n",
    "                    self.adaptive_constant_bcs_val = adaptive_constant_bcs_val *  (1.0 - self.beta) + self.beta * self.adaptive_constant_bcs_val\n",
    "\n",
    "                    self.adpative_constant_bcs_log.append(self.adaptive_constant_bcs_val)\n",
    "                    # self.loss_bcs_log.append(loss_u_value)\n",
    "                    # self.loss_res_log.append(loss_r_value)\n",
    "\n",
    "\n",
    "                    self.print(\"constant_bcs_val: {:.3f}\".format(self.adaptive_constant_bcs_val))\n",
    "\n",
    "  \n",
    "            sys.stdout.flush()\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "            if it in itValues:\n",
    "                    self.plot_layerLoss(tf_dict , it)\n",
    "                    self.print(\"Gradients information stored ...\")\n",
    "\n",
    "            self.assign_batch_losses(batch_losses)\n",
    "            for key in self.loss_history:\n",
    "                self.loss_history[key].append(self.epoch_loss[key])\n",
    "\n",
    "\n",
    "    # Evaluates predictions at test points\n",
    "    def predict_psi_p(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        psi_star = self.sess.run(self.psi_pred, tf_dict)\n",
    "        p_star = self.sess.run(self.p_pred, tf_dict)\n",
    "        return psi_star, p_star\n",
    "\n",
    "    def predict_uv(self, X_star):\n",
    "        X_star = (X_star - self.mu_X) / self.sigma_X\n",
    "        tf_dict = {self.x_u_tf: X_star[:, 0:1], self.y_u_tf: X_star[:, 1:2]}\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        v_star = self.sess.run(self.v_pred, tf_dict)\n",
    "        return u_star, v_star\n",
    "\n",
    "\n",
    "\n",
    " ############################################################\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "    def plot_loss_history(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    ############################################################\n",
    "  \n",
    "  # ###############################################################################################################################################\n",
    "   # \n",
    "   #  \n",
    "    def plot_layerLoss(self , tf_dict , epoch):\n",
    "        ## Gradients #\n",
    "        num_layers = len(self.layers)\n",
    "        for i in range(num_layers - 1):\n",
    "            grad_res, grad_bc1    = self.sess.run([ self.grad_res[i],self.grad_bcs[i]], feed_dict=tf_dict)\n",
    "\n",
    "            # save gradients of loss_r and loss_u\n",
    "            self.dict_gradients_res_layers['layer_' + str(i + 1)].append(grad_res.flatten())\n",
    "            self.dict_gradients_bcs_layers['layer_' + str(i + 1)].append(grad_bc1.flatten())\n",
    "\n",
    "        num_hidden_layers = num_layers -1\n",
    "        cnt = 1\n",
    "        fig = plt.figure(4, figsize=(13, 4))\n",
    "        for j in range(num_hidden_layers):\n",
    "            ax = plt.subplot(1, num_hidden_layers, cnt)\n",
    "            ax.set_title('Layer {}'.format(j + 1))\n",
    "            ax.set_yscale('symlog')\n",
    "            gradients_res = self.dict_gradients_res_layers['layer_' + str(j + 1)][-1]\n",
    "            gradients_bc1 = self.dict_gradients_bcs_layers['layer_' + str(j + 1)][-1]\n",
    "\n",
    "            sns.distplot(gradients_res, hist=False,kde_kws={\"shade\": False},norm_hist=True,  label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "\n",
    "            sns.distplot(gradients_bc1, hist=False,kde_kws={\"shade\": False},norm_hist=True,   label=r'$\\nabla_\\theta \\mathcal{L}_{u_{bc1}}$')\n",
    "\n",
    "            #ax.get_legend().remove()\n",
    "            ax.set_xlim([-1.0, 1.0])\n",
    "            #ax.set_ylim([0, 150])\n",
    "            cnt += 1\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        fig.legend(handles, labels, loc=\"center\",  bbox_to_anchor=(0.5, -0.03),borderaxespad=0,bbox_transform=fig.transFigure, ncol=3)\n",
    "        text = 'layerLoss_epoch' + str(epoch) +'.png'\n",
    "        plt.savefig(os.path.join(self.dirname,text) , bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "    # #########################\n",
    "    # def make_output_dir(self):\n",
    "        \n",
    "    #     if not os.path.exists(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\"):\n",
    "    #         os.mkdir(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\")\n",
    "    #     dirname = os.path.join(\"/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "    #     os.mkdir(dirname)\n",
    "    #     text = 'output.log'\n",
    "    #     logpath = os.path.join(dirname, text)\n",
    "    #     shutil.copyfile('/okyanus/users/afarea/PINN/Adaptive_PINN/IB_PINN/M2.py', os.path.join(dirname, 'M2.py'))\n",
    "\n",
    "    #     return dirname, logpath\n",
    "    \n",
    "    # # ###########################################################\n",
    "    def make_output_dir(self):\n",
    "        \n",
    "        if not os.path.exists(\"checkpoints\"):\n",
    "            os.mkdir(\"checkpoints\")\n",
    "        dirname = os.path.join(\"checkpoints\", datetime.now().strftime(\"%b-%d-%Y_%H-%M-%S-%f_\") + self.mode)\n",
    "        os.mkdir(dirname)\n",
    "        text = 'output.log'\n",
    "        logpath = os.path.join(dirname, text)\n",
    "        shutil.copyfile('M2.ipynb', os.path.join(dirname, 'M2.ipynb'))\n",
    "        return dirname, logpath\n",
    "    \n",
    "\n",
    "    def get_logger(self, logpath):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(logging.DEBUG)        \n",
    "        sh.setFormatter(logging.Formatter('%(message)s'))\n",
    "        fh = logging.FileHandler(logpath)\n",
    "        logger.addHandler(sh)\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "   \n",
    "    def print(self, *args):\n",
    "        for word in args:\n",
    "            if len(args) == 1:\n",
    "                self.logger.info(word)\n",
    "            elif word != args[-1]:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32: \n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "            else:\n",
    "                for handler in self.logger.handlers:\n",
    "                    handler.terminator = \"\\n\"\n",
    "                if type(word) == float or type(word) == np.float64 or type(word) == np.float32:\n",
    "                    self.logger.info(\"%.4e\" % (word))\n",
    "                else:\n",
    "                    self.logger.info(word)\n",
    "\n",
    "\n",
    "    def plot_loss_history(self , path):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        for key in self.loss_history:\n",
    "            self.print(\"Final loss %s: %e\" % (key, self.loss_history[key][-1]))\n",
    "            ax.semilogy(self.loss_history[key], label=key)\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "       #######################\n",
    "    def save_NN(self):\n",
    "\n",
    "        uv_weights = self.sess.run(self.weights)\n",
    "        uv_biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(os.path.join(self.dirname,'model.pickle'), 'wb') as f:\n",
    "            pickle.dump([uv_weights, uv_biases], f)\n",
    "            self.print(\"Save uv NN parameters successfully in %s ...\" , self.dirname)\n",
    "\n",
    "        # with open(os.path.join(self.dirname,'loss_history_BFS.pickle'), 'wb') as f:\n",
    "        #     pickle.dump(self.loss_rec, f)\n",
    "        with open(os.path.join(self.dirname,'loss_history_BFS.png'), 'wb') as f:\n",
    "            self.plot_loss_history(f)\n",
    "\n",
    "\n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "\n",
    "\n",
    "    def generate_grad_dict(self):\n",
    "        num = len(self.layers) - 1\n",
    "        grad_dict = {}\n",
    "        for i in range(num):\n",
    "            grad_dict['layer_{}'.format(i + 1)] = []\n",
    "        return grad_dict\n",
    "    \n",
    "    def assign_batch_losses(self, batch_losses):\n",
    "        for loss_values, key in zip(batch_losses, self.epoch_loss):\n",
    "            self.epoch_loss[key] = loss_values\n",
    "            \n",
    "    def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "        from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "        ### Plot ###\n",
    "\n",
    "        # Exact solution & Predicted solution\n",
    "        # Exact soluton\n",
    "        U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "        F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        # Predicted solution\n",
    "        U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "        F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "        titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $v(x)$' , 'Predicted $v(x)$' , 'Absolute error']\n",
    "        data = [U_star.T , U_pred ,  np.abs(U_star.T - U_pred) , F_star.T , F_pred ,  np.abs(F_star.T - F_pred) ]\n",
    "\n",
    "        fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "        grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                        label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                        cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                        cbar_size=\"5%\", cbar_pad=0.0)\n",
    "    # CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "        minmax_list = []\n",
    "        for d in data:\n",
    "            # if(local):\n",
    "            #     minmax_list.append([np.min(d), np.max(d)])\n",
    "            # else:\n",
    "            minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "            # kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60), cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "        for ax, z, minmax, title in zip(grid, data, minmax_list, titles):\n",
    "        #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "            #pcfsets.append(pcf)\n",
    "            # if (timeStp == 0):\n",
    "            pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "            cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "            ax.cax.tick_params(labelsize=14.5)\n",
    "            ax.set_title(title, fontsize=14.5, pad=7)\n",
    "            ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "            ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "            ax.tick_params(labelsize=14.5)\n",
    "            ax.set_xlim(x1.min(), x1.max())\n",
    "            ax.set_ylim(x2.min(), x2.max())\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "        fig_1.set_size_inches(15, 10, True)\n",
    "        fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "        plt.close(\"all\" , )\n",
    "\n",
    "\n",
    "    def plot_grad(self ):\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches([15,8])\n",
    "        ax.semilogy(self.adpative_constant_bcs_log, label=r'$\\bar{\\nabla_\\theta \\mathcal{L}_{u_{bc}}}$')\n",
    "        ax.set_xlabel(\"epochs\", fontsize=15)\n",
    "        ax.set_ylabel(\"loss\", fontsize=15)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.legend()\n",
    "        path = os.path.join(self.dirname,'grad_history.png')\n",
    "        plt.savefig(path)\n",
    "        plt.close(\"all\" , )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        [elapsed, error_u , error_v ,  error_p] = test_method(mtd , layers, operator, bcs_sampler, res_sampler ,Re , stiff_ratio ,  X_star , X , Y)\n",
    "\n",
    "def test_method(method , layers, operator, bcs_sampler, res_sampler, Re ,  mode , X_star , X , Y , nIter ,mbbatch_size , bcbatch_size , ubatch_size ):\n",
    "\n",
    "\n",
    "    model = Navier_Stokes2D(layers, operator, bcs_sampler, res_sampler, Re, mode)\n",
    "\n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if method ==\"full_batch\":\n",
    "        model.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "    elif method ==\"mini_batch\":\n",
    "        model.trainmb(nIter, mbbatch_size)\n",
    "    else:\n",
    "        print(\"unknown method!\")\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Predictions\n",
    "    _, p_pred = model.predict_psi_p(X_star)\n",
    "    u_pred, v_pred = model.predict_uv(X_star)\n",
    "\n",
    "    # psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "    p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "    # u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "    # v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "    # velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "    # Reference\n",
    "    u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "    v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "    # velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "    u_pred = u_pred.reshape(100,100)\n",
    "    v_pred = v_pred.reshape(100,100)\n",
    "    p_pred = p_pred.reshape(100,100)\n",
    "\n",
    "    # Relative error\n",
    "    error_u = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_u))\n",
    "    error_v = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_v))\n",
    "    error_p = np.linalg.norm(p_pred - p_star.T, 2) / np.linalg.norm(p_star, 2)\n",
    "    print('l2 error: {:.2e}'.format(error_p))\n",
    "\n",
    "    return [elapsed, error_u , error_v ,error_p ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  mini_batch\n",
      "Epoch:  1\n",
      "WARNING:tensorflow:From /tmp/ipykernel_68957/2981062063.py:59: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_68957/2981062063.py:60: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_68957/2981062063.py:61: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_68957/2981062063.py:61: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 05:45:57.686930: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 05:45:57.717467: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "2023-12-13 05:45:57.718042: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ff4b95d9d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-13 05:45:57.718061: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-12-13 05:45:57.721816: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_68957/2460533952.py:163: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_68957/2460533952.py:51: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_68957/2460533952.py:110: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_68957/2460533952.py:112: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_68957/2460533952.py:154: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 1.528e+00, Loss_u: 5.635e-01, Loss_r: 9.644e-01, Time: 15.42\n",
      "constant_bcs_val: 6.999\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 10, Loss: 2.315e+00, Loss_u: 2.146e+00, Loss_r: 1.693e-01, Time: 0.06\n",
      "It: 20, Loss: 1.338e+00, Loss_u: 1.281e+00, Loss_r: 5.671e-02, Time: 0.11\n",
      "It: 30, Loss: 9.870e-01, Loss_u: 8.286e-01, Loss_r: 1.584e-01, Time: 0.05\n",
      "It: 40, Loss: 8.728e-01, Loss_u: 8.264e-01, Loss_r: 4.635e-02, Time: 0.09\n",
      "It: 50, Loss: 7.863e-01, Loss_u: 7.676e-01, Loss_r: 1.865e-02, Time: 0.07\n",
      "It: 60, Loss: 6.776e-01, Loss_u: 6.527e-01, Loss_r: 2.493e-02, Time: 0.06\n",
      "It: 70, Loss: 6.957e-01, Loss_u: 6.652e-01, Loss_r: 3.052e-02, Time: 0.07\n",
      "It: 80, Loss: 7.073e-01, Loss_u: 6.775e-01, Loss_r: 2.985e-02, Time: 0.10\n",
      "It: 90, Loss: 6.342e-01, Loss_u: 6.065e-01, Loss_r: 2.774e-02, Time: 0.06\n",
      "It: 100, Loss: 5.701e-01, Loss_u: 5.451e-01, Loss_r: 2.495e-02, Time: 0.06\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 110, Loss: 5.871e-01, Loss_u: 5.643e-01, Loss_r: 2.284e-02, Time: 0.06\n",
      "It: 120, Loss: 5.268e-01, Loss_u: 5.027e-01, Loss_r: 2.405e-02, Time: 0.10\n",
      "It: 130, Loss: 5.912e-01, Loss_u: 5.629e-01, Loss_r: 2.831e-02, Time: 0.07\n",
      "It: 140, Loss: 5.946e-01, Loss_u: 5.736e-01, Loss_r: 2.097e-02, Time: 0.08\n",
      "It: 150, Loss: 4.822e-01, Loss_u: 4.592e-01, Loss_r: 2.304e-02, Time: 0.08\n",
      "It: 160, Loss: 5.873e-01, Loss_u: 5.670e-01, Loss_r: 2.034e-02, Time: 0.08\n",
      "It: 170, Loss: 4.551e-01, Loss_u: 4.345e-01, Loss_r: 2.055e-02, Time: 0.06\n",
      "It: 180, Loss: 4.380e-01, Loss_u: 4.162e-01, Loss_r: 2.183e-02, Time: 0.07\n",
      "It: 190, Loss: 4.280e-01, Loss_u: 3.959e-01, Loss_r: 3.211e-02, Time: 0.07\n",
      "It: 200, Loss: 4.161e-01, Loss_u: 3.894e-01, Loss_r: 2.667e-02, Time: 0.08\n",
      "It: 210, Loss: 4.111e-01, Loss_u: 3.795e-01, Loss_r: 3.164e-02, Time: 0.06\n",
      "It: 220, Loss: 3.985e-01, Loss_u: 3.725e-01, Loss_r: 2.603e-02, Time: 0.05\n",
      "It: 230, Loss: 3.477e-01, Loss_u: 3.218e-01, Loss_r: 2.595e-02, Time: 0.07\n",
      "It: 240, Loss: 3.922e-01, Loss_u: 3.664e-01, Loss_r: 2.573e-02, Time: 0.04\n",
      "It: 250, Loss: 4.542e-01, Loss_u: 4.378e-01, Loss_r: 1.641e-02, Time: 0.06\n",
      "It: 260, Loss: 3.255e-01, Loss_u: 3.003e-01, Loss_r: 2.519e-02, Time: 0.05\n",
      "It: 270, Loss: 3.641e-01, Loss_u: 3.370e-01, Loss_r: 2.709e-02, Time: 0.07\n",
      "It: 280, Loss: 3.102e-01, Loss_u: 2.832e-01, Loss_r: 2.707e-02, Time: 0.06\n",
      "It: 290, Loss: 2.940e-01, Loss_u: 2.686e-01, Loss_r: 2.542e-02, Time: 0.07\n",
      "It: 300, Loss: 3.238e-01, Loss_u: 3.075e-01, Loss_r: 1.626e-02, Time: 0.09\n",
      "It: 310, Loss: 2.072e-01, Loss_u: 1.802e-01, Loss_r: 2.702e-02, Time: 0.06\n",
      "It: 320, Loss: 2.711e-01, Loss_u: 2.391e-01, Loss_r: 3.206e-02, Time: 0.07\n",
      "It: 330, Loss: 3.259e-01, Loss_u: 2.979e-01, Loss_r: 2.805e-02, Time: 0.07\n",
      "It: 340, Loss: 2.650e-01, Loss_u: 2.372e-01, Loss_r: 2.782e-02, Time: 0.06\n",
      "It: 350, Loss: 2.439e-01, Loss_u: 2.198e-01, Loss_r: 2.412e-02, Time: 0.07\n",
      "It: 360, Loss: 2.656e-01, Loss_u: 2.400e-01, Loss_r: 2.555e-02, Time: 0.10\n",
      "It: 370, Loss: 2.993e-01, Loss_u: 2.753e-01, Loss_r: 2.405e-02, Time: 0.07\n",
      "It: 380, Loss: 2.674e-01, Loss_u: 2.402e-01, Loss_r: 2.718e-02, Time: 0.05\n",
      "It: 390, Loss: 2.024e-01, Loss_u: 1.750e-01, Loss_r: 2.738e-02, Time: 0.05\n",
      "It: 400, Loss: 2.444e-01, Loss_u: 2.174e-01, Loss_r: 2.699e-02, Time: 0.06\n",
      "It: 410, Loss: 2.421e-01, Loss_u: 2.129e-01, Loss_r: 2.915e-02, Time: 0.06\n",
      "It: 420, Loss: 3.592e-01, Loss_u: 3.184e-01, Loss_r: 4.081e-02, Time: 0.06\n",
      "It: 430, Loss: 2.094e-01, Loss_u: 1.796e-01, Loss_r: 2.986e-02, Time: 0.06\n",
      "It: 440, Loss: 2.120e-01, Loss_u: 1.886e-01, Loss_r: 2.335e-02, Time: 0.05\n",
      "It: 450, Loss: 3.114e-01, Loss_u: 2.673e-01, Loss_r: 4.411e-02, Time: 0.06\n",
      "It: 460, Loss: 1.872e-01, Loss_u: 1.651e-01, Loss_r: 2.210e-02, Time: 0.05\n",
      "It: 470, Loss: 2.115e-01, Loss_u: 1.944e-01, Loss_r: 1.711e-02, Time: 0.05\n",
      "It: 480, Loss: 2.244e-01, Loss_u: 1.902e-01, Loss_r: 3.415e-02, Time: 0.09\n",
      "It: 490, Loss: 2.360e-01, Loss_u: 1.989e-01, Loss_r: 3.713e-02, Time: 0.09\n",
      "It: 500, Loss: 2.655e-01, Loss_u: 2.499e-01, Loss_r: 1.565e-02, Time: 0.07\n",
      "It: 510, Loss: 1.832e-01, Loss_u: 1.694e-01, Loss_r: 1.374e-02, Time: 0.07\n",
      "It: 520, Loss: 2.124e-01, Loss_u: 1.957e-01, Loss_r: 1.673e-02, Time: 0.07\n",
      "It: 530, Loss: 1.624e-01, Loss_u: 1.459e-01, Loss_r: 1.654e-02, Time: 0.07\n",
      "It: 540, Loss: 1.809e-01, Loss_u: 1.502e-01, Loss_r: 3.064e-02, Time: 0.07\n",
      "It: 550, Loss: 1.986e-01, Loss_u: 1.671e-01, Loss_r: 3.155e-02, Time: 0.06\n",
      "It: 560, Loss: 1.467e-01, Loss_u: 1.281e-01, Loss_r: 1.863e-02, Time: 0.05\n",
      "It: 570, Loss: 2.248e-01, Loss_u: 1.895e-01, Loss_r: 3.526e-02, Time: 0.05\n",
      "It: 580, Loss: 1.768e-01, Loss_u: 1.544e-01, Loss_r: 2.243e-02, Time: 0.05\n",
      "It: 590, Loss: 2.402e-01, Loss_u: 2.092e-01, Loss_r: 3.101e-02, Time: 0.05\n",
      "It: 600, Loss: 1.952e-01, Loss_u: 1.783e-01, Loss_r: 1.698e-02, Time: 0.06\n",
      "It: 610, Loss: 1.462e-01, Loss_u: 1.310e-01, Loss_r: 1.516e-02, Time: 0.06\n",
      "It: 620, Loss: 1.747e-01, Loss_u: 1.599e-01, Loss_r: 1.476e-02, Time: 0.06\n",
      "It: 630, Loss: 2.065e-01, Loss_u: 1.842e-01, Loss_r: 2.228e-02, Time: 0.06\n",
      "It: 640, Loss: 1.813e-01, Loss_u: 1.636e-01, Loss_r: 1.772e-02, Time: 0.06\n",
      "It: 650, Loss: 2.146e-01, Loss_u: 1.974e-01, Loss_r: 1.724e-02, Time: 0.05\n",
      "It: 660, Loss: 1.995e-01, Loss_u: 1.833e-01, Loss_r: 1.622e-02, Time: 0.07\n",
      "It: 670, Loss: 1.472e-01, Loss_u: 1.339e-01, Loss_r: 1.335e-02, Time: 0.05\n",
      "It: 680, Loss: 1.729e-01, Loss_u: 1.567e-01, Loss_r: 1.621e-02, Time: 0.06\n",
      "It: 690, Loss: 1.897e-01, Loss_u: 1.748e-01, Loss_r: 1.490e-02, Time: 0.06\n",
      "It: 700, Loss: 1.447e-01, Loss_u: 1.234e-01, Loss_r: 2.126e-02, Time: 0.05\n",
      "It: 710, Loss: 1.756e-01, Loss_u: 1.639e-01, Loss_r: 1.172e-02, Time: 0.05\n",
      "It: 720, Loss: 1.970e-01, Loss_u: 1.767e-01, Loss_r: 2.033e-02, Time: 0.06\n",
      "It: 730, Loss: 1.872e-01, Loss_u: 1.559e-01, Loss_r: 3.126e-02, Time: 0.06\n",
      "It: 740, Loss: 2.105e-01, Loss_u: 1.848e-01, Loss_r: 2.571e-02, Time: 0.06\n",
      "It: 750, Loss: 1.750e-01, Loss_u: 1.529e-01, Loss_r: 2.206e-02, Time: 0.06\n",
      "It: 760, Loss: 1.808e-01, Loss_u: 1.584e-01, Loss_r: 2.241e-02, Time: 0.05\n",
      "It: 770, Loss: 1.379e-01, Loss_u: 1.257e-01, Loss_r: 1.215e-02, Time: 0.05\n",
      "It: 780, Loss: 1.322e-01, Loss_u: 1.114e-01, Loss_r: 2.085e-02, Time: 0.06\n",
      "It: 790, Loss: 2.082e-01, Loss_u: 1.892e-01, Loss_r: 1.905e-02, Time: 0.07\n",
      "It: 800, Loss: 1.905e-01, Loss_u: 1.698e-01, Loss_r: 2.075e-02, Time: 0.10\n",
      "It: 810, Loss: 2.378e-01, Loss_u: 2.198e-01, Loss_r: 1.799e-02, Time: 0.05\n",
      "It: 820, Loss: 1.779e-01, Loss_u: 1.573e-01, Loss_r: 2.055e-02, Time: 0.05\n",
      "It: 830, Loss: 1.718e-01, Loss_u: 1.526e-01, Loss_r: 1.926e-02, Time: 0.06\n",
      "It: 840, Loss: 1.944e-01, Loss_u: 1.705e-01, Loss_r: 2.390e-02, Time: 0.09\n",
      "It: 850, Loss: 1.928e-01, Loss_u: 1.669e-01, Loss_r: 2.591e-02, Time: 0.05\n",
      "It: 860, Loss: 1.945e-01, Loss_u: 1.680e-01, Loss_r: 2.657e-02, Time: 0.06\n",
      "It: 870, Loss: 1.731e-01, Loss_u: 1.438e-01, Loss_r: 2.932e-02, Time: 0.06\n",
      "It: 880, Loss: 1.848e-01, Loss_u: 1.548e-01, Loss_r: 2.998e-02, Time: 0.06\n",
      "It: 890, Loss: 1.660e-01, Loss_u: 1.464e-01, Loss_r: 1.962e-02, Time: 0.06\n",
      "It: 900, Loss: 1.732e-01, Loss_u: 1.533e-01, Loss_r: 1.990e-02, Time: 0.05\n",
      "It: 910, Loss: 2.188e-01, Loss_u: 1.976e-01, Loss_r: 2.128e-02, Time: 0.05\n",
      "It: 920, Loss: 1.628e-01, Loss_u: 1.479e-01, Loss_r: 1.487e-02, Time: 0.05\n",
      "It: 930, Loss: 2.362e-01, Loss_u: 2.061e-01, Loss_r: 3.011e-02, Time: 0.06\n",
      "It: 940, Loss: 2.591e-01, Loss_u: 2.401e-01, Loss_r: 1.900e-02, Time: 0.06\n",
      "It: 950, Loss: 1.751e-01, Loss_u: 1.494e-01, Loss_r: 2.564e-02, Time: 0.06\n",
      "It: 960, Loss: 2.331e-01, Loss_u: 2.156e-01, Loss_r: 1.753e-02, Time: 0.08\n",
      "It: 970, Loss: 1.598e-01, Loss_u: 1.376e-01, Loss_r: 2.226e-02, Time: 0.07\n",
      "It: 980, Loss: 1.213e-01, Loss_u: 9.590e-02, Loss_r: 2.539e-02, Time: 0.06\n",
      "It: 990, Loss: 1.624e-01, Loss_u: 1.473e-01, Loss_r: 1.512e-02, Time: 0.05\n",
      "It: 1000, Loss: 1.569e-01, Loss_u: 1.425e-01, Loss_r: 1.440e-02, Time: 0.40\n",
      "constant_bcs_val: 6.713\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/afrah2/anaconda3/envs/twoPhase/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "Gradients information stored ...\n",
      "It: 1010, Loss: 1.630e-01, Loss_u: 1.374e-01, Loss_r: 2.562e-02, Time: 0.05\n",
      "It: 1020, Loss: 1.780e-01, Loss_u: 1.610e-01, Loss_r: 1.696e-02, Time: 0.06\n",
      "It: 1030, Loss: 1.719e-01, Loss_u: 1.559e-01, Loss_r: 1.593e-02, Time: 0.05\n",
      "It: 1040, Loss: 1.688e-01, Loss_u: 1.514e-01, Loss_r: 1.745e-02, Time: 0.14\n",
      "It: 1050, Loss: 1.453e-01, Loss_u: 1.312e-01, Loss_r: 1.409e-02, Time: 0.05\n",
      "It: 1060, Loss: 1.755e-01, Loss_u: 1.581e-01, Loss_r: 1.742e-02, Time: 0.06\n",
      "It: 1070, Loss: 1.837e-01, Loss_u: 1.603e-01, Loss_r: 2.342e-02, Time: 0.05\n",
      "It: 1080, Loss: 1.747e-01, Loss_u: 1.605e-01, Loss_r: 1.420e-02, Time: 0.05\n",
      "It: 1090, Loss: 1.242e-01, Loss_u: 1.069e-01, Loss_r: 1.730e-02, Time: 0.07\n",
      "It: 1100, Loss: 1.401e-01, Loss_u: 1.141e-01, Loss_r: 2.603e-02, Time: 0.08\n",
      "It: 1110, Loss: 1.463e-01, Loss_u: 1.218e-01, Loss_r: 2.451e-02, Time: 0.05\n",
      "It: 1120, Loss: 1.670e-01, Loss_u: 1.433e-01, Loss_r: 2.378e-02, Time: 0.05\n",
      "It: 1130, Loss: 1.840e-01, Loss_u: 1.666e-01, Loss_r: 1.738e-02, Time: 0.08\n",
      "It: 1140, Loss: 1.320e-01, Loss_u: 1.121e-01, Loss_r: 1.988e-02, Time: 0.09\n",
      "It: 1150, Loss: 2.328e-01, Loss_u: 2.047e-01, Loss_r: 2.807e-02, Time: 0.36\n",
      "It: 1160, Loss: 2.044e-01, Loss_u: 1.834e-01, Loss_r: 2.108e-02, Time: 0.06\n",
      "It: 1170, Loss: 1.437e-01, Loss_u: 1.223e-01, Loss_r: 2.144e-02, Time: 0.42\n",
      "It: 1180, Loss: 2.008e-01, Loss_u: 1.795e-01, Loss_r: 2.128e-02, Time: 0.08\n",
      "It: 1190, Loss: 2.114e-01, Loss_u: 1.804e-01, Loss_r: 3.098e-02, Time: 0.05\n",
      "It: 1200, Loss: 2.313e-01, Loss_u: 2.080e-01, Loss_r: 2.329e-02, Time: 0.19\n",
      "It: 1210, Loss: 1.616e-01, Loss_u: 1.394e-01, Loss_r: 2.218e-02, Time: 0.05\n",
      "It: 1220, Loss: 1.888e-01, Loss_u: 1.591e-01, Loss_r: 2.963e-02, Time: 0.05\n",
      "It: 1230, Loss: 1.790e-01, Loss_u: 1.589e-01, Loss_r: 2.011e-02, Time: 0.07\n",
      "It: 1240, Loss: 1.597e-01, Loss_u: 1.359e-01, Loss_r: 2.379e-02, Time: 0.05\n",
      "It: 1250, Loss: 1.496e-01, Loss_u: 1.251e-01, Loss_r: 2.450e-02, Time: 0.07\n",
      "It: 1260, Loss: 1.056e-01, Loss_u: 8.716e-02, Loss_r: 1.844e-02, Time: 0.06\n",
      "It: 1270, Loss: 1.879e-01, Loss_u: 1.704e-01, Loss_r: 1.742e-02, Time: 0.05\n",
      "It: 1280, Loss: 1.300e-01, Loss_u: 1.132e-01, Loss_r: 1.679e-02, Time: 0.05\n",
      "It: 1290, Loss: 1.508e-01, Loss_u: 1.138e-01, Loss_r: 3.697e-02, Time: 0.06\n",
      "It: 1300, Loss: 1.206e-01, Loss_u: 1.050e-01, Loss_r: 1.561e-02, Time: 0.09\n",
      "It: 1310, Loss: 2.103e-01, Loss_u: 1.741e-01, Loss_r: 3.620e-02, Time: 0.05\n",
      "It: 1320, Loss: 1.921e-01, Loss_u: 1.722e-01, Loss_r: 1.993e-02, Time: 0.09\n",
      "It: 1330, Loss: 1.349e-01, Loss_u: 1.036e-01, Loss_r: 3.133e-02, Time: 0.06\n",
      "It: 1340, Loss: 2.483e-01, Loss_u: 2.199e-01, Loss_r: 2.841e-02, Time: 0.07\n",
      "It: 1350, Loss: 1.416e-01, Loss_u: 1.227e-01, Loss_r: 1.894e-02, Time: 0.08\n",
      "It: 1360, Loss: 2.025e-01, Loss_u: 1.900e-01, Loss_r: 1.247e-02, Time: 0.09\n",
      "It: 1370, Loss: 1.992e-01, Loss_u: 1.849e-01, Loss_r: 1.425e-02, Time: 0.07\n",
      "It: 1380, Loss: 1.687e-01, Loss_u: 1.514e-01, Loss_r: 1.735e-02, Time: 0.22\n",
      "It: 1390, Loss: 1.722e-01, Loss_u: 1.539e-01, Loss_r: 1.833e-02, Time: 0.32\n",
      "It: 1400, Loss: 2.109e-01, Loss_u: 1.986e-01, Loss_r: 1.230e-02, Time: 0.07\n",
      "It: 1410, Loss: 1.693e-01, Loss_u: 1.468e-01, Loss_r: 2.259e-02, Time: 0.26\n",
      "It: 1420, Loss: 1.684e-01, Loss_u: 1.477e-01, Loss_r: 2.073e-02, Time: 0.05\n",
      "It: 1430, Loss: 1.949e-01, Loss_u: 1.621e-01, Loss_r: 3.282e-02, Time: 0.05\n",
      "It: 1440, Loss: 1.569e-01, Loss_u: 1.257e-01, Loss_r: 3.121e-02, Time: 0.05\n",
      "It: 1450, Loss: 1.486e-01, Loss_u: 1.226e-01, Loss_r: 2.598e-02, Time: 0.06\n",
      "It: 1460, Loss: 2.110e-01, Loss_u: 1.894e-01, Loss_r: 2.159e-02, Time: 0.06\n",
      "It: 1470, Loss: 2.090e-01, Loss_u: 1.890e-01, Loss_r: 2.001e-02, Time: 0.05\n",
      "It: 1480, Loss: 1.282e-01, Loss_u: 1.168e-01, Loss_r: 1.136e-02, Time: 0.10\n",
      "It: 1490, Loss: 1.304e-01, Loss_u: 1.019e-01, Loss_r: 2.851e-02, Time: 0.09\n",
      "It: 1500, Loss: 1.760e-01, Loss_u: 1.557e-01, Loss_r: 2.020e-02, Time: 0.05\n",
      "It: 1510, Loss: 1.290e-01, Loss_u: 1.178e-01, Loss_r: 1.121e-02, Time: 0.05\n",
      "It: 1520, Loss: 1.553e-01, Loss_u: 1.412e-01, Loss_r: 1.410e-02, Time: 0.05\n",
      "It: 1530, Loss: 1.705e-01, Loss_u: 1.448e-01, Loss_r: 2.565e-02, Time: 0.05\n",
      "It: 1540, Loss: 1.513e-01, Loss_u: 1.289e-01, Loss_r: 2.232e-02, Time: 0.05\n",
      "It: 1550, Loss: 1.275e-01, Loss_u: 1.046e-01, Loss_r: 2.289e-02, Time: 0.06\n",
      "It: 1560, Loss: 1.642e-01, Loss_u: 1.451e-01, Loss_r: 1.912e-02, Time: 0.06\n",
      "It: 1570, Loss: 1.305e-01, Loss_u: 1.192e-01, Loss_r: 1.128e-02, Time: 0.38\n",
      "It: 1580, Loss: 1.488e-01, Loss_u: 1.282e-01, Loss_r: 2.059e-02, Time: 0.06\n",
      "It: 1590, Loss: 1.725e-01, Loss_u: 1.516e-01, Loss_r: 2.088e-02, Time: 0.11\n",
      "It: 1600, Loss: 1.463e-01, Loss_u: 1.275e-01, Loss_r: 1.872e-02, Time: 0.05\n",
      "It: 1610, Loss: 1.978e-01, Loss_u: 1.856e-01, Loss_r: 1.218e-02, Time: 0.07\n",
      "It: 1620, Loss: 1.627e-01, Loss_u: 1.431e-01, Loss_r: 1.953e-02, Time: 0.07\n",
      "It: 1630, Loss: 2.172e-01, Loss_u: 2.033e-01, Loss_r: 1.386e-02, Time: 0.08\n",
      "It: 1640, Loss: 1.721e-01, Loss_u: 1.598e-01, Loss_r: 1.233e-02, Time: 0.14\n",
      "It: 1650, Loss: 1.296e-01, Loss_u: 1.137e-01, Loss_r: 1.596e-02, Time: 0.07\n",
      "It: 1660, Loss: 1.506e-01, Loss_u: 1.327e-01, Loss_r: 1.792e-02, Time: 0.07\n",
      "It: 1670, Loss: 1.609e-01, Loss_u: 1.432e-01, Loss_r: 1.766e-02, Time: 0.06\n",
      "It: 1680, Loss: 1.323e-01, Loss_u: 1.136e-01, Loss_r: 1.876e-02, Time: 0.05\n",
      "It: 1690, Loss: 1.683e-01, Loss_u: 1.528e-01, Loss_r: 1.547e-02, Time: 0.05\n",
      "It: 1700, Loss: 1.638e-01, Loss_u: 1.388e-01, Loss_r: 2.499e-02, Time: 0.44\n",
      "It: 1710, Loss: 1.317e-01, Loss_u: 1.167e-01, Loss_r: 1.501e-02, Time: 0.09\n",
      "It: 1720, Loss: 2.049e-01, Loss_u: 1.909e-01, Loss_r: 1.405e-02, Time: 0.08\n",
      "It: 1730, Loss: 1.809e-01, Loss_u: 1.649e-01, Loss_r: 1.601e-02, Time: 0.07\n",
      "It: 1740, Loss: 1.694e-01, Loss_u: 1.389e-01, Loss_r: 3.048e-02, Time: 0.06\n",
      "It: 1750, Loss: 2.393e-01, Loss_u: 2.146e-01, Loss_r: 2.468e-02, Time: 0.05\n",
      "It: 1760, Loss: 1.610e-01, Loss_u: 1.414e-01, Loss_r: 1.959e-02, Time: 0.07\n",
      "It: 1770, Loss: 1.393e-01, Loss_u: 1.211e-01, Loss_r: 1.817e-02, Time: 0.05\n",
      "It: 1780, Loss: 1.741e-01, Loss_u: 1.605e-01, Loss_r: 1.358e-02, Time: 0.06\n",
      "It: 1790, Loss: 1.419e-01, Loss_u: 1.171e-01, Loss_r: 2.488e-02, Time: 0.06\n",
      "It: 1800, Loss: 1.973e-01, Loss_u: 1.820e-01, Loss_r: 1.528e-02, Time: 0.07\n",
      "It: 1810, Loss: 1.369e-01, Loss_u: 1.219e-01, Loss_r: 1.499e-02, Time: 0.06\n",
      "It: 1820, Loss: 1.440e-01, Loss_u: 1.236e-01, Loss_r: 2.046e-02, Time: 0.07\n",
      "It: 1830, Loss: 1.759e-01, Loss_u: 1.569e-01, Loss_r: 1.900e-02, Time: 0.06\n",
      "It: 1840, Loss: 1.435e-01, Loss_u: 1.208e-01, Loss_r: 2.266e-02, Time: 0.05\n",
      "It: 1850, Loss: 1.825e-01, Loss_u: 1.628e-01, Loss_r: 1.964e-02, Time: 0.08\n",
      "It: 1860, Loss: 1.918e-01, Loss_u: 1.770e-01, Loss_r: 1.481e-02, Time: 0.07\n",
      "It: 1870, Loss: 1.305e-01, Loss_u: 1.150e-01, Loss_r: 1.557e-02, Time: 0.05\n",
      "It: 1880, Loss: 1.366e-01, Loss_u: 1.207e-01, Loss_r: 1.595e-02, Time: 0.35\n",
      "It: 1890, Loss: 1.351e-01, Loss_u: 1.171e-01, Loss_r: 1.792e-02, Time: 0.06\n",
      "It: 1900, Loss: 1.925e-01, Loss_u: 1.692e-01, Loss_r: 2.327e-02, Time: 0.36\n",
      "It: 1910, Loss: 1.853e-01, Loss_u: 1.639e-01, Loss_r: 2.142e-02, Time: 0.06\n",
      "It: 1920, Loss: 1.401e-01, Loss_u: 1.257e-01, Loss_r: 1.438e-02, Time: 0.06\n",
      "It: 1930, Loss: 1.466e-01, Loss_u: 1.294e-01, Loss_r: 1.720e-02, Time: 0.07\n",
      "It: 1940, Loss: 1.305e-01, Loss_u: 1.138e-01, Loss_r: 1.667e-02, Time: 0.08\n",
      "It: 1950, Loss: 2.099e-01, Loss_u: 1.987e-01, Loss_r: 1.118e-02, Time: 0.06\n",
      "It: 1960, Loss: 1.413e-01, Loss_u: 1.215e-01, Loss_r: 1.975e-02, Time: 0.05\n",
      "It: 1970, Loss: 1.558e-01, Loss_u: 1.396e-01, Loss_r: 1.624e-02, Time: 0.07\n",
      "It: 1980, Loss: 1.436e-01, Loss_u: 1.246e-01, Loss_r: 1.892e-02, Time: 0.06\n",
      "It: 1990, Loss: 1.689e-01, Loss_u: 1.510e-01, Loss_r: 1.795e-02, Time: 0.06\n",
      "It: 2000, Loss: 1.681e-01, Loss_u: 1.538e-01, Loss_r: 1.429e-02, Time: 0.06\n",
      "constant_bcs_val: 6.280\n",
      "It: 2010, Loss: 1.596e-01, Loss_u: 1.418e-01, Loss_r: 1.783e-02, Time: 0.06\n",
      "It: 2020, Loss: 1.150e-01, Loss_u: 9.609e-02, Loss_r: 1.888e-02, Time: 0.50\n",
      "It: 2030, Loss: 1.407e-01, Loss_u: 1.224e-01, Loss_r: 1.833e-02, Time: 0.05\n",
      "It: 2040, Loss: 1.670e-01, Loss_u: 1.457e-01, Loss_r: 2.125e-02, Time: 0.05\n",
      "It: 2050, Loss: 1.105e-01, Loss_u: 8.833e-02, Loss_r: 2.219e-02, Time: 0.05\n",
      "It: 2060, Loss: 1.295e-01, Loss_u: 1.064e-01, Loss_r: 2.309e-02, Time: 0.05\n",
      "It: 2070, Loss: 8.948e-02, Loss_u: 7.181e-02, Loss_r: 1.767e-02, Time: 0.05\n",
      "It: 2080, Loss: 1.498e-01, Loss_u: 1.372e-01, Loss_r: 1.261e-02, Time: 0.06\n",
      "It: 2090, Loss: 1.313e-01, Loss_u: 1.159e-01, Loss_r: 1.538e-02, Time: 0.06\n",
      "It: 2100, Loss: 1.832e-01, Loss_u: 1.719e-01, Loss_r: 1.126e-02, Time: 0.05\n",
      "It: 2110, Loss: 9.665e-02, Loss_u: 8.191e-02, Loss_r: 1.474e-02, Time: 0.06\n",
      "It: 2120, Loss: 1.640e-01, Loss_u: 1.467e-01, Loss_r: 1.727e-02, Time: 0.08\n",
      "It: 2130, Loss: 1.577e-01, Loss_u: 1.429e-01, Loss_r: 1.482e-02, Time: 0.07\n",
      "It: 2140, Loss: 1.235e-01, Loss_u: 9.675e-02, Loss_r: 2.677e-02, Time: 0.06\n",
      "It: 2150, Loss: 1.727e-01, Loss_u: 1.597e-01, Loss_r: 1.301e-02, Time: 0.05\n",
      "It: 2160, Loss: 1.447e-01, Loss_u: 1.279e-01, Loss_r: 1.687e-02, Time: 0.08\n",
      "It: 2170, Loss: 1.057e-01, Loss_u: 8.760e-02, Loss_r: 1.809e-02, Time: 0.06\n",
      "It: 2180, Loss: 1.587e-01, Loss_u: 1.445e-01, Loss_r: 1.417e-02, Time: 0.06\n",
      "It: 2190, Loss: 1.310e-01, Loss_u: 1.082e-01, Loss_r: 2.278e-02, Time: 0.05\n",
      "It: 2200, Loss: 1.663e-01, Loss_u: 1.413e-01, Loss_r: 2.496e-02, Time: 0.07\n",
      "It: 2210, Loss: 1.412e-01, Loss_u: 1.146e-01, Loss_r: 2.660e-02, Time: 0.05\n",
      "It: 2220, Loss: 1.205e-01, Loss_u: 1.044e-01, Loss_r: 1.609e-02, Time: 0.08\n",
      "It: 2230, Loss: 1.493e-01, Loss_u: 1.367e-01, Loss_r: 1.260e-02, Time: 0.08\n",
      "It: 2240, Loss: 1.148e-01, Loss_u: 1.074e-01, Loss_r: 7.443e-03, Time: 0.06\n",
      "It: 2250, Loss: 1.412e-01, Loss_u: 1.202e-01, Loss_r: 2.097e-02, Time: 0.09\n",
      "It: 2260, Loss: 1.330e-01, Loss_u: 1.162e-01, Loss_r: 1.681e-02, Time: 0.07\n",
      "It: 2270, Loss: 1.168e-01, Loss_u: 9.634e-02, Loss_r: 2.051e-02, Time: 0.06\n",
      "It: 2280, Loss: 1.694e-01, Loss_u: 1.524e-01, Loss_r: 1.702e-02, Time: 0.05\n",
      "It: 2290, Loss: 1.610e-01, Loss_u: 1.416e-01, Loss_r: 1.948e-02, Time: 0.06\n",
      "It: 2300, Loss: 2.056e-01, Loss_u: 1.856e-01, Loss_r: 1.997e-02, Time: 0.05\n",
      "It: 2310, Loss: 1.350e-01, Loss_u: 1.171e-01, Loss_r: 1.794e-02, Time: 0.09\n",
      "It: 2320, Loss: 1.136e-01, Loss_u: 1.004e-01, Loss_r: 1.323e-02, Time: 1.47\n",
      "It: 2330, Loss: 1.527e-01, Loss_u: 1.428e-01, Loss_r: 9.965e-03, Time: 0.21\n",
      "It: 2340, Loss: 1.550e-01, Loss_u: 1.410e-01, Loss_r: 1.396e-02, Time: 0.13\n",
      "It: 2350, Loss: 1.159e-01, Loss_u: 9.601e-02, Loss_r: 1.991e-02, Time: 0.06\n",
      "It: 2360, Loss: 1.213e-01, Loss_u: 1.115e-01, Loss_r: 9.776e-03, Time: 0.05\n",
      "It: 2370, Loss: 1.621e-01, Loss_u: 1.394e-01, Loss_r: 2.267e-02, Time: 0.05\n",
      "It: 2380, Loss: 1.064e-01, Loss_u: 8.723e-02, Loss_r: 1.919e-02, Time: 0.09\n",
      "It: 2390, Loss: 1.291e-01, Loss_u: 1.059e-01, Loss_r: 2.317e-02, Time: 0.05\n",
      "It: 2400, Loss: 1.418e-01, Loss_u: 1.207e-01, Loss_r: 2.106e-02, Time: 0.05\n",
      "It: 2410, Loss: 1.321e-01, Loss_u: 1.146e-01, Loss_r: 1.741e-02, Time: 0.05\n",
      "It: 2420, Loss: 1.189e-01, Loss_u: 9.870e-02, Loss_r: 2.022e-02, Time: 0.05\n",
      "It: 2430, Loss: 1.003e-01, Loss_u: 8.600e-02, Loss_r: 1.426e-02, Time: 0.06\n",
      "It: 2440, Loss: 1.292e-01, Loss_u: 1.148e-01, Loss_r: 1.440e-02, Time: 0.05\n",
      "It: 2450, Loss: 9.682e-02, Loss_u: 8.124e-02, Loss_r: 1.558e-02, Time: 0.06\n",
      "It: 2460, Loss: 1.415e-01, Loss_u: 1.215e-01, Loss_r: 2.002e-02, Time: 0.06\n",
      "It: 2470, Loss: 2.214e-01, Loss_u: 1.767e-01, Loss_r: 4.467e-02, Time: 0.05\n",
      "It: 2480, Loss: 1.576e-01, Loss_u: 1.404e-01, Loss_r: 1.726e-02, Time: 0.07\n",
      "It: 2490, Loss: 1.223e-01, Loss_u: 1.049e-01, Loss_r: 1.738e-02, Time: 0.06\n",
      "It: 2500, Loss: 1.386e-01, Loss_u: 1.174e-01, Loss_r: 2.121e-02, Time: 0.06\n",
      "It: 2510, Loss: 1.138e-01, Loss_u: 9.589e-02, Loss_r: 1.787e-02, Time: 0.05\n",
      "It: 2520, Loss: 1.173e-01, Loss_u: 9.503e-02, Loss_r: 2.228e-02, Time: 0.05\n",
      "It: 2530, Loss: 1.352e-01, Loss_u: 1.224e-01, Loss_r: 1.281e-02, Time: 0.06\n",
      "It: 2540, Loss: 1.199e-01, Loss_u: 9.863e-02, Loss_r: 2.124e-02, Time: 0.09\n",
      "It: 2550, Loss: 1.415e-01, Loss_u: 1.153e-01, Loss_r: 2.628e-02, Time: 0.05\n",
      "It: 2560, Loss: 1.215e-01, Loss_u: 1.063e-01, Loss_r: 1.523e-02, Time: 0.06\n",
      "It: 2570, Loss: 1.406e-01, Loss_u: 1.247e-01, Loss_r: 1.588e-02, Time: 0.06\n",
      "It: 2580, Loss: 1.257e-01, Loss_u: 1.095e-01, Loss_r: 1.624e-02, Time: 0.06\n",
      "It: 2590, Loss: 1.530e-01, Loss_u: 1.193e-01, Loss_r: 3.364e-02, Time: 0.07\n",
      "It: 2600, Loss: 1.512e-01, Loss_u: 1.290e-01, Loss_r: 2.221e-02, Time: 0.07\n",
      "It: 2610, Loss: 1.145e-01, Loss_u: 9.968e-02, Loss_r: 1.477e-02, Time: 0.05\n",
      "It: 2620, Loss: 9.436e-02, Loss_u: 7.882e-02, Loss_r: 1.554e-02, Time: 0.05\n",
      "It: 2630, Loss: 1.469e-01, Loss_u: 1.238e-01, Loss_r: 2.307e-02, Time: 0.07\n",
      "It: 2640, Loss: 1.119e-01, Loss_u: 8.494e-02, Loss_r: 2.700e-02, Time: 0.07\n",
      "It: 2650, Loss: 1.348e-01, Loss_u: 1.033e-01, Loss_r: 3.143e-02, Time: 0.05\n",
      "It: 2660, Loss: 1.222e-01, Loss_u: 1.033e-01, Loss_r: 1.891e-02, Time: 0.07\n",
      "It: 2670, Loss: 1.040e-01, Loss_u: 9.276e-02, Loss_r: 1.126e-02, Time: 0.10\n",
      "It: 2680, Loss: 1.049e-01, Loss_u: 9.180e-02, Loss_r: 1.308e-02, Time: 0.09\n",
      "It: 2690, Loss: 1.347e-01, Loss_u: 1.213e-01, Loss_r: 1.342e-02, Time: 0.05\n",
      "It: 2700, Loss: 1.161e-01, Loss_u: 1.057e-01, Loss_r: 1.035e-02, Time: 0.05\n",
      "It: 2710, Loss: 1.680e-01, Loss_u: 1.532e-01, Loss_r: 1.479e-02, Time: 0.04\n",
      "It: 2720, Loss: 9.422e-02, Loss_u: 8.441e-02, Loss_r: 9.808e-03, Time: 0.05\n",
      "It: 2730, Loss: 1.570e-01, Loss_u: 1.333e-01, Loss_r: 2.373e-02, Time: 0.05\n",
      "It: 2740, Loss: 1.345e-01, Loss_u: 1.109e-01, Loss_r: 2.355e-02, Time: 0.05\n",
      "It: 2750, Loss: 1.213e-01, Loss_u: 1.121e-01, Loss_r: 9.277e-03, Time: 0.06\n",
      "It: 2760, Loss: 1.213e-01, Loss_u: 1.117e-01, Loss_r: 9.644e-03, Time: 0.05\n",
      "It: 2770, Loss: 1.485e-01, Loss_u: 1.240e-01, Loss_r: 2.446e-02, Time: 0.05\n",
      "It: 2780, Loss: 1.192e-01, Loss_u: 8.822e-02, Loss_r: 3.096e-02, Time: 0.06\n",
      "It: 2790, Loss: 1.759e-01, Loss_u: 1.517e-01, Loss_r: 2.423e-02, Time: 0.06\n",
      "It: 2800, Loss: 1.102e-01, Loss_u: 9.605e-02, Loss_r: 1.418e-02, Time: 0.07\n",
      "It: 2810, Loss: 1.248e-01, Loss_u: 1.027e-01, Loss_r: 2.208e-02, Time: 0.06\n",
      "It: 2820, Loss: 9.281e-02, Loss_u: 6.915e-02, Loss_r: 2.367e-02, Time: 0.05\n",
      "It: 2830, Loss: 1.115e-01, Loss_u: 9.404e-02, Loss_r: 1.745e-02, Time: 0.05\n",
      "It: 2840, Loss: 1.443e-01, Loss_u: 1.226e-01, Loss_r: 2.172e-02, Time: 0.06\n",
      "It: 2850, Loss: 1.254e-01, Loss_u: 1.025e-01, Loss_r: 2.288e-02, Time: 0.05\n",
      "It: 2860, Loss: 7.955e-02, Loss_u: 6.517e-02, Loss_r: 1.437e-02, Time: 0.05\n",
      "It: 2870, Loss: 1.073e-01, Loss_u: 8.903e-02, Loss_r: 1.823e-02, Time: 0.05\n",
      "It: 2880, Loss: 1.339e-01, Loss_u: 1.212e-01, Loss_r: 1.269e-02, Time: 0.05\n",
      "It: 2890, Loss: 1.439e-01, Loss_u: 1.306e-01, Loss_r: 1.328e-02, Time: 0.06\n",
      "It: 2900, Loss: 9.433e-02, Loss_u: 6.787e-02, Loss_r: 2.646e-02, Time: 0.06\n",
      "It: 2910, Loss: 1.118e-01, Loss_u: 9.651e-02, Loss_r: 1.525e-02, Time: 0.05\n",
      "It: 2920, Loss: 1.258e-01, Loss_u: 1.181e-01, Loss_r: 7.699e-03, Time: 0.05\n",
      "It: 2930, Loss: 1.728e-01, Loss_u: 1.609e-01, Loss_r: 1.188e-02, Time: 0.05\n",
      "It: 2940, Loss: 1.031e-01, Loss_u: 8.531e-02, Loss_r: 1.780e-02, Time: 0.06\n",
      "It: 2950, Loss: 1.366e-01, Loss_u: 1.241e-01, Loss_r: 1.252e-02, Time: 0.05\n",
      "It: 2960, Loss: 1.447e-01, Loss_u: 1.271e-01, Loss_r: 1.757e-02, Time: 0.06\n",
      "It: 2970, Loss: 1.670e-01, Loss_u: 1.520e-01, Loss_r: 1.495e-02, Time: 0.10\n",
      "It: 2980, Loss: 1.284e-01, Loss_u: 1.135e-01, Loss_r: 1.489e-02, Time: 0.05\n",
      "It: 2990, Loss: 1.184e-01, Loss_u: 9.713e-02, Loss_r: 2.123e-02, Time: 0.06\n",
      "It: 3000, Loss: 1.263e-01, Loss_u: 1.125e-01, Loss_r: 1.375e-02, Time: 0.06\n",
      "constant_bcs_val: 6.189\n",
      "It: 3010, Loss: 1.136e-01, Loss_u: 8.942e-02, Loss_r: 2.421e-02, Time: 0.06\n",
      "It: 3020, Loss: 9.564e-02, Loss_u: 6.718e-02, Loss_r: 2.846e-02, Time: 0.05\n",
      "It: 3030, Loss: 1.981e-01, Loss_u: 1.709e-01, Loss_r: 2.724e-02, Time: 0.05\n",
      "It: 3040, Loss: 1.089e-01, Loss_u: 8.771e-02, Loss_r: 2.124e-02, Time: 0.05\n",
      "It: 3050, Loss: 1.433e-01, Loss_u: 1.209e-01, Loss_r: 2.246e-02, Time: 0.09\n",
      "It: 3060, Loss: 1.378e-01, Loss_u: 1.252e-01, Loss_r: 1.265e-02, Time: 0.06\n",
      "It: 3070, Loss: 1.141e-01, Loss_u: 1.013e-01, Loss_r: 1.278e-02, Time: 0.06\n",
      "It: 3080, Loss: 1.198e-01, Loss_u: 1.006e-01, Loss_r: 1.921e-02, Time: 0.05\n",
      "It: 3090, Loss: 1.048e-01, Loss_u: 8.436e-02, Loss_r: 2.045e-02, Time: 0.06\n",
      "It: 3100, Loss: 1.377e-01, Loss_u: 1.118e-01, Loss_r: 2.591e-02, Time: 0.05\n",
      "It: 3110, Loss: 1.250e-01, Loss_u: 1.105e-01, Loss_r: 1.443e-02, Time: 0.05\n",
      "It: 3120, Loss: 1.840e-01, Loss_u: 1.589e-01, Loss_r: 2.514e-02, Time: 0.05\n",
      "It: 3130, Loss: 1.625e-01, Loss_u: 1.342e-01, Loss_r: 2.831e-02, Time: 0.05\n",
      "It: 3140, Loss: 9.442e-02, Loss_u: 7.932e-02, Loss_r: 1.509e-02, Time: 0.06\n",
      "It: 3150, Loss: 9.807e-02, Loss_u: 6.820e-02, Loss_r: 2.987e-02, Time: 0.05\n",
      "It: 3160, Loss: 1.193e-01, Loss_u: 1.069e-01, Loss_r: 1.239e-02, Time: 0.06\n",
      "It: 3170, Loss: 1.407e-01, Loss_u: 1.236e-01, Loss_r: 1.717e-02, Time: 0.07\n",
      "It: 3180, Loss: 1.325e-01, Loss_u: 1.148e-01, Loss_r: 1.771e-02, Time: 0.07\n",
      "It: 3190, Loss: 1.532e-01, Loss_u: 1.372e-01, Loss_r: 1.596e-02, Time: 0.06\n",
      "It: 3200, Loss: 1.097e-01, Loss_u: 9.385e-02, Loss_r: 1.588e-02, Time: 0.05\n",
      "It: 3210, Loss: 1.228e-01, Loss_u: 9.069e-02, Loss_r: 3.213e-02, Time: 0.06\n",
      "It: 3220, Loss: 1.199e-01, Loss_u: 1.031e-01, Loss_r: 1.682e-02, Time: 0.05\n",
      "It: 3230, Loss: 1.077e-01, Loss_u: 9.159e-02, Loss_r: 1.615e-02, Time: 0.07\n",
      "It: 3240, Loss: 1.049e-01, Loss_u: 8.897e-02, Loss_r: 1.589e-02, Time: 0.05\n",
      "It: 3250, Loss: 9.688e-02, Loss_u: 7.806e-02, Loss_r: 1.882e-02, Time: 0.06\n",
      "It: 3260, Loss: 1.108e-01, Loss_u: 9.231e-02, Loss_r: 1.846e-02, Time: 0.05\n",
      "It: 3270, Loss: 1.049e-01, Loss_u: 8.240e-02, Loss_r: 2.251e-02, Time: 0.05\n",
      "It: 3280, Loss: 1.193e-01, Loss_u: 1.012e-01, Loss_r: 1.807e-02, Time: 0.06\n",
      "It: 3290, Loss: 1.244e-01, Loss_u: 1.085e-01, Loss_r: 1.582e-02, Time: 0.07\n",
      "It: 3300, Loss: 1.047e-01, Loss_u: 8.825e-02, Loss_r: 1.643e-02, Time: 0.07\n",
      "It: 3310, Loss: 8.535e-02, Loss_u: 6.715e-02, Loss_r: 1.820e-02, Time: 0.05\n",
      "It: 3320, Loss: 1.147e-01, Loss_u: 9.398e-02, Loss_r: 2.072e-02, Time: 0.05\n",
      "It: 3330, Loss: 1.200e-01, Loss_u: 1.098e-01, Loss_r: 1.028e-02, Time: 0.05\n",
      "It: 3340, Loss: 1.173e-01, Loss_u: 9.388e-02, Loss_r: 2.340e-02, Time: 0.05\n",
      "It: 3350, Loss: 1.405e-01, Loss_u: 1.191e-01, Loss_r: 2.135e-02, Time: 0.07\n",
      "It: 3360, Loss: 1.324e-01, Loss_u: 1.181e-01, Loss_r: 1.432e-02, Time: 0.05\n",
      "It: 3370, Loss: 1.001e-01, Loss_u: 8.703e-02, Loss_r: 1.311e-02, Time: 0.05\n",
      "It: 3380, Loss: 8.087e-02, Loss_u: 5.661e-02, Loss_r: 2.427e-02, Time: 0.05\n",
      "It: 3390, Loss: 9.115e-02, Loss_u: 7.658e-02, Loss_r: 1.457e-02, Time: 0.07\n",
      "It: 3400, Loss: 1.611e-01, Loss_u: 1.374e-01, Loss_r: 2.370e-02, Time: 0.05\n",
      "It: 3410, Loss: 1.304e-01, Loss_u: 1.106e-01, Loss_r: 1.978e-02, Time: 0.05\n",
      "It: 3420, Loss: 1.079e-01, Loss_u: 9.581e-02, Loss_r: 1.207e-02, Time: 0.05\n",
      "It: 3430, Loss: 9.809e-02, Loss_u: 7.832e-02, Loss_r: 1.977e-02, Time: 0.05\n",
      "It: 3440, Loss: 1.372e-01, Loss_u: 1.243e-01, Loss_r: 1.290e-02, Time: 0.05\n",
      "It: 3450, Loss: 1.167e-01, Loss_u: 9.456e-02, Loss_r: 2.219e-02, Time: 0.06\n",
      "It: 3460, Loss: 1.235e-01, Loss_u: 9.299e-02, Loss_r: 3.047e-02, Time: 0.05\n",
      "It: 3470, Loss: 1.304e-01, Loss_u: 1.150e-01, Loss_r: 1.545e-02, Time: 0.05\n",
      "It: 3480, Loss: 9.709e-02, Loss_u: 7.852e-02, Loss_r: 1.857e-02, Time: 0.05\n",
      "It: 3490, Loss: 1.237e-01, Loss_u: 1.038e-01, Loss_r: 1.986e-02, Time: 0.05\n",
      "It: 3500, Loss: 8.604e-02, Loss_u: 7.136e-02, Loss_r: 1.468e-02, Time: 0.06\n",
      "It: 3510, Loss: 1.290e-01, Loss_u: 1.083e-01, Loss_r: 2.066e-02, Time: 0.06\n",
      "It: 3520, Loss: 1.014e-01, Loss_u: 9.104e-02, Loss_r: 1.033e-02, Time: 0.05\n",
      "It: 3530, Loss: 1.260e-01, Loss_u: 1.090e-01, Loss_r: 1.700e-02, Time: 0.05\n",
      "It: 3540, Loss: 1.219e-01, Loss_u: 9.720e-02, Loss_r: 2.470e-02, Time: 0.05\n",
      "It: 3550, Loss: 1.865e-01, Loss_u: 1.649e-01, Loss_r: 2.159e-02, Time: 0.05\n",
      "It: 3560, Loss: 1.108e-01, Loss_u: 9.313e-02, Loss_r: 1.769e-02, Time: 0.05\n",
      "It: 3570, Loss: 8.380e-02, Loss_u: 6.260e-02, Loss_r: 2.120e-02, Time: 0.05\n",
      "It: 3580, Loss: 1.300e-01, Loss_u: 1.153e-01, Loss_r: 1.472e-02, Time: 0.05\n",
      "It: 3590, Loss: 1.390e-01, Loss_u: 1.177e-01, Loss_r: 2.130e-02, Time: 0.05\n",
      "It: 3600, Loss: 1.488e-01, Loss_u: 1.151e-01, Loss_r: 3.364e-02, Time: 0.06\n",
      "It: 3610, Loss: 1.221e-01, Loss_u: 9.988e-02, Loss_r: 2.222e-02, Time: 0.05\n",
      "It: 3620, Loss: 1.257e-01, Loss_u: 1.123e-01, Loss_r: 1.335e-02, Time: 0.05\n",
      "It: 3630, Loss: 1.621e-01, Loss_u: 1.289e-01, Loss_r: 3.318e-02, Time: 0.07\n",
      "It: 3640, Loss: 1.063e-01, Loss_u: 7.792e-02, Loss_r: 2.833e-02, Time: 0.05\n",
      "It: 3650, Loss: 1.461e-01, Loss_u: 1.303e-01, Loss_r: 1.574e-02, Time: 0.05\n",
      "It: 3660, Loss: 1.346e-01, Loss_u: 1.179e-01, Loss_r: 1.671e-02, Time: 0.05\n",
      "It: 3670, Loss: 1.236e-01, Loss_u: 9.578e-02, Loss_r: 2.778e-02, Time: 0.05\n",
      "It: 3680, Loss: 1.526e-01, Loss_u: 1.353e-01, Loss_r: 1.737e-02, Time: 0.05\n",
      "It: 3690, Loss: 9.891e-02, Loss_u: 8.328e-02, Loss_r: 1.563e-02, Time: 0.05\n",
      "It: 3700, Loss: 1.363e-01, Loss_u: 1.041e-01, Loss_r: 3.221e-02, Time: 0.07\n",
      "It: 3710, Loss: 1.226e-01, Loss_u: 9.367e-02, Loss_r: 2.890e-02, Time: 0.06\n",
      "It: 3720, Loss: 1.484e-01, Loss_u: 1.331e-01, Loss_r: 1.525e-02, Time: 0.06\n",
      "It: 3730, Loss: 6.957e-02, Loss_u: 5.714e-02, Loss_r: 1.244e-02, Time: 0.06\n",
      "It: 3740, Loss: 1.309e-01, Loss_u: 1.124e-01, Loss_r: 1.848e-02, Time: 0.06\n",
      "It: 3750, Loss: 9.586e-02, Loss_u: 7.858e-02, Loss_r: 1.727e-02, Time: 0.06\n",
      "It: 3760, Loss: 9.578e-02, Loss_u: 7.917e-02, Loss_r: 1.662e-02, Time: 0.06\n",
      "It: 3770, Loss: 1.226e-01, Loss_u: 9.903e-02, Loss_r: 2.354e-02, Time: 0.08\n",
      "It: 3780, Loss: 1.158e-01, Loss_u: 8.512e-02, Loss_r: 3.065e-02, Time: 0.05\n",
      "It: 3790, Loss: 1.018e-01, Loss_u: 9.045e-02, Loss_r: 1.134e-02, Time: 0.05\n",
      "It: 3800, Loss: 1.204e-01, Loss_u: 9.754e-02, Loss_r: 2.287e-02, Time: 0.06\n",
      "It: 3810, Loss: 9.461e-02, Loss_u: 7.465e-02, Loss_r: 1.996e-02, Time: 0.06\n",
      "It: 3820, Loss: 1.325e-01, Loss_u: 1.151e-01, Loss_r: 1.735e-02, Time: 0.07\n",
      "It: 3830, Loss: 9.068e-02, Loss_u: 6.891e-02, Loss_r: 2.176e-02, Time: 0.07\n",
      "It: 3840, Loss: 1.206e-01, Loss_u: 1.093e-01, Loss_r: 1.122e-02, Time: 0.06\n",
      "It: 3850, Loss: 1.048e-01, Loss_u: 9.139e-02, Loss_r: 1.341e-02, Time: 0.06\n",
      "It: 3860, Loss: 1.140e-01, Loss_u: 9.903e-02, Loss_r: 1.500e-02, Time: 0.05\n",
      "It: 3870, Loss: 1.453e-01, Loss_u: 1.258e-01, Loss_r: 1.952e-02, Time: 0.05\n",
      "It: 3880, Loss: 1.132e-01, Loss_u: 8.986e-02, Loss_r: 2.335e-02, Time: 0.05\n",
      "It: 3890, Loss: 1.286e-01, Loss_u: 1.088e-01, Loss_r: 1.981e-02, Time: 0.05\n",
      "It: 3900, Loss: 1.653e-01, Loss_u: 1.440e-01, Loss_r: 2.130e-02, Time: 0.05\n",
      "It: 3910, Loss: 1.150e-01, Loss_u: 1.042e-01, Loss_r: 1.080e-02, Time: 0.05\n",
      "It: 3920, Loss: 1.083e-01, Loss_u: 8.245e-02, Loss_r: 2.588e-02, Time: 0.06\n",
      "It: 3930, Loss: 9.342e-02, Loss_u: 7.848e-02, Loss_r: 1.493e-02, Time: 0.05\n",
      "It: 3940, Loss: 9.945e-02, Loss_u: 6.767e-02, Loss_r: 3.178e-02, Time: 0.05\n",
      "It: 3950, Loss: 1.234e-01, Loss_u: 1.073e-01, Loss_r: 1.613e-02, Time: 0.05\n",
      "It: 3960, Loss: 9.565e-02, Loss_u: 7.558e-02, Loss_r: 2.007e-02, Time: 0.06\n",
      "It: 3970, Loss: 1.357e-01, Loss_u: 1.124e-01, Loss_r: 2.339e-02, Time: 0.05\n",
      "It: 3980, Loss: 1.057e-01, Loss_u: 8.870e-02, Loss_r: 1.695e-02, Time: 0.05\n",
      "It: 3990, Loss: 1.155e-01, Loss_u: 9.519e-02, Loss_r: 2.027e-02, Time: 0.06\n",
      "It: 4000, Loss: 8.666e-02, Loss_u: 7.296e-02, Loss_r: 1.370e-02, Time: 0.05\n",
      "constant_bcs_val: 5.836\n",
      "It: 4010, Loss: 1.003e-01, Loss_u: 8.959e-02, Loss_r: 1.068e-02, Time: 0.05\n",
      "It: 4020, Loss: 1.108e-01, Loss_u: 9.165e-02, Loss_r: 1.913e-02, Time: 0.05\n",
      "It: 4030, Loss: 1.288e-01, Loss_u: 1.021e-01, Loss_r: 2.678e-02, Time: 0.05\n",
      "It: 4040, Loss: 1.245e-01, Loss_u: 1.003e-01, Loss_r: 2.415e-02, Time: 0.05\n",
      "It: 4050, Loss: 9.421e-02, Loss_u: 7.250e-02, Loss_r: 2.171e-02, Time: 0.05\n",
      "It: 4060, Loss: 1.037e-01, Loss_u: 9.397e-02, Loss_r: 9.766e-03, Time: 0.05\n",
      "It: 4070, Loss: 9.294e-02, Loss_u: 8.339e-02, Loss_r: 9.553e-03, Time: 0.07\n",
      "It: 4080, Loss: 7.690e-02, Loss_u: 6.746e-02, Loss_r: 9.442e-03, Time: 0.06\n",
      "It: 4090, Loss: 8.970e-02, Loss_u: 6.742e-02, Loss_r: 2.228e-02, Time: 0.05\n",
      "It: 4100, Loss: 7.800e-02, Loss_u: 5.780e-02, Loss_r: 2.020e-02, Time: 0.05\n",
      "It: 4110, Loss: 8.070e-02, Loss_u: 5.170e-02, Loss_r: 2.899e-02, Time: 0.05\n",
      "It: 4120, Loss: 1.078e-01, Loss_u: 7.508e-02, Loss_r: 3.276e-02, Time: 0.05\n",
      "It: 4130, Loss: 1.052e-01, Loss_u: 9.186e-02, Loss_r: 1.333e-02, Time: 0.06\n",
      "It: 4140, Loss: 1.039e-01, Loss_u: 7.949e-02, Loss_r: 2.439e-02, Time: 0.05\n",
      "It: 4150, Loss: 9.157e-02, Loss_u: 7.956e-02, Loss_r: 1.200e-02, Time: 0.05\n",
      "It: 4160, Loss: 1.105e-01, Loss_u: 9.142e-02, Loss_r: 1.906e-02, Time: 0.07\n",
      "It: 4170, Loss: 9.824e-02, Loss_u: 8.049e-02, Loss_r: 1.775e-02, Time: 0.06\n",
      "It: 4180, Loss: 1.419e-01, Loss_u: 1.335e-01, Loss_r: 8.451e-03, Time: 0.05\n",
      "It: 4190, Loss: 1.178e-01, Loss_u: 9.030e-02, Loss_r: 2.747e-02, Time: 0.05\n",
      "It: 4200, Loss: 8.550e-02, Loss_u: 6.188e-02, Loss_r: 2.363e-02, Time: 0.06\n",
      "It: 4210, Loss: 1.055e-01, Loss_u: 7.260e-02, Loss_r: 3.290e-02, Time: 0.05\n",
      "It: 4220, Loss: 8.683e-02, Loss_u: 6.777e-02, Loss_r: 1.907e-02, Time: 0.07\n",
      "It: 4230, Loss: 7.805e-02, Loss_u: 5.752e-02, Loss_r: 2.053e-02, Time: 0.05\n",
      "It: 4240, Loss: 1.027e-01, Loss_u: 8.718e-02, Loss_r: 1.553e-02, Time: 0.06\n",
      "It: 4250, Loss: 1.227e-01, Loss_u: 9.155e-02, Loss_r: 3.110e-02, Time: 0.05\n",
      "It: 4260, Loss: 1.101e-01, Loss_u: 9.527e-02, Loss_r: 1.488e-02, Time: 0.08\n",
      "It: 4270, Loss: 1.495e-01, Loss_u: 1.226e-01, Loss_r: 2.698e-02, Time: 0.05\n",
      "It: 4280, Loss: 1.126e-01, Loss_u: 9.988e-02, Loss_r: 1.277e-02, Time: 0.05\n",
      "It: 4290, Loss: 8.001e-02, Loss_u: 6.837e-02, Loss_r: 1.164e-02, Time: 0.05\n",
      "It: 4300, Loss: 7.686e-02, Loss_u: 5.439e-02, Loss_r: 2.247e-02, Time: 0.05\n",
      "It: 4310, Loss: 1.363e-01, Loss_u: 1.185e-01, Loss_r: 1.774e-02, Time: 0.05\n",
      "It: 4320, Loss: 1.305e-01, Loss_u: 1.184e-01, Loss_r: 1.205e-02, Time: 0.05\n",
      "It: 4330, Loss: 1.465e-01, Loss_u: 1.220e-01, Loss_r: 2.449e-02, Time: 0.07\n",
      "It: 4340, Loss: 1.645e-01, Loss_u: 1.462e-01, Loss_r: 1.833e-02, Time: 0.05\n",
      "It: 4350, Loss: 1.032e-01, Loss_u: 8.978e-02, Loss_r: 1.344e-02, Time: 0.05\n",
      "It: 4360, Loss: 1.004e-01, Loss_u: 8.573e-02, Loss_r: 1.470e-02, Time: 0.06\n",
      "It: 4370, Loss: 1.375e-01, Loss_u: 1.086e-01, Loss_r: 2.890e-02, Time: 0.05\n",
      "It: 4380, Loss: 8.881e-02, Loss_u: 6.641e-02, Loss_r: 2.239e-02, Time: 0.05\n",
      "It: 4390, Loss: 7.055e-02, Loss_u: 5.461e-02, Loss_r: 1.595e-02, Time: 0.06\n",
      "It: 4400, Loss: 1.050e-01, Loss_u: 8.805e-02, Loss_r: 1.696e-02, Time: 0.05\n",
      "It: 4410, Loss: 7.508e-02, Loss_u: 6.338e-02, Loss_r: 1.170e-02, Time: 0.06\n",
      "It: 4420, Loss: 9.310e-02, Loss_u: 7.811e-02, Loss_r: 1.499e-02, Time: 0.05\n",
      "It: 4430, Loss: 1.464e-01, Loss_u: 9.832e-02, Loss_r: 4.805e-02, Time: 0.05\n",
      "It: 4440, Loss: 1.179e-01, Loss_u: 9.856e-02, Loss_r: 1.931e-02, Time: 0.05\n",
      "It: 4450, Loss: 1.276e-01, Loss_u: 1.035e-01, Loss_r: 2.407e-02, Time: 0.05\n",
      "It: 4460, Loss: 1.131e-01, Loss_u: 9.501e-02, Loss_r: 1.806e-02, Time: 0.05\n",
      "It: 4470, Loss: 7.904e-02, Loss_u: 5.431e-02, Loss_r: 2.473e-02, Time: 0.05\n",
      "It: 4480, Loss: 1.043e-01, Loss_u: 9.134e-02, Loss_r: 1.292e-02, Time: 0.08\n",
      "It: 4490, Loss: 7.872e-02, Loss_u: 5.692e-02, Loss_r: 2.180e-02, Time: 0.05\n",
      "It: 4500, Loss: 9.196e-02, Loss_u: 7.274e-02, Loss_r: 1.922e-02, Time: 0.05\n",
      "It: 4510, Loss: 1.445e-01, Loss_u: 1.266e-01, Loss_r: 1.793e-02, Time: 0.05\n",
      "It: 4520, Loss: 8.810e-02, Loss_u: 6.725e-02, Loss_r: 2.085e-02, Time: 0.06\n",
      "It: 4530, Loss: 1.340e-01, Loss_u: 1.154e-01, Loss_r: 1.857e-02, Time: 0.05\n",
      "It: 4540, Loss: 8.169e-02, Loss_u: 6.352e-02, Loss_r: 1.817e-02, Time: 0.05\n",
      "It: 4550, Loss: 1.123e-01, Loss_u: 9.180e-02, Loss_r: 2.049e-02, Time: 0.05\n",
      "It: 4560, Loss: 1.159e-01, Loss_u: 9.907e-02, Loss_r: 1.680e-02, Time: 0.05\n",
      "It: 4570, Loss: 1.432e-01, Loss_u: 1.309e-01, Loss_r: 1.236e-02, Time: 0.05\n",
      "It: 4580, Loss: 1.115e-01, Loss_u: 9.018e-02, Loss_r: 2.129e-02, Time: 0.05\n",
      "It: 4590, Loss: 1.362e-01, Loss_u: 1.146e-01, Loss_r: 2.163e-02, Time: 0.05\n",
      "It: 4600, Loss: 1.117e-01, Loss_u: 9.186e-02, Loss_r: 1.985e-02, Time: 0.06\n",
      "It: 4610, Loss: 1.373e-01, Loss_u: 1.182e-01, Loss_r: 1.910e-02, Time: 0.06\n",
      "It: 4620, Loss: 9.212e-02, Loss_u: 7.882e-02, Loss_r: 1.330e-02, Time: 0.06\n",
      "It: 4630, Loss: 8.511e-02, Loss_u: 7.322e-02, Loss_r: 1.189e-02, Time: 0.05\n",
      "It: 4640, Loss: 8.495e-02, Loss_u: 6.541e-02, Loss_r: 1.954e-02, Time: 0.05\n",
      "It: 4650, Loss: 1.259e-01, Loss_u: 1.119e-01, Loss_r: 1.398e-02, Time: 0.05\n",
      "It: 4660, Loss: 9.182e-02, Loss_u: 7.421e-02, Loss_r: 1.761e-02, Time: 0.06\n",
      "It: 4670, Loss: 9.932e-02, Loss_u: 7.651e-02, Loss_r: 2.281e-02, Time: 0.05\n",
      "It: 4680, Loss: 8.677e-02, Loss_u: 7.842e-02, Loss_r: 8.353e-03, Time: 0.05\n",
      "It: 4690, Loss: 9.438e-02, Loss_u: 6.700e-02, Loss_r: 2.737e-02, Time: 0.05\n",
      "It: 4700, Loss: 1.530e-01, Loss_u: 1.360e-01, Loss_r: 1.710e-02, Time: 0.06\n",
      "It: 4710, Loss: 1.129e-01, Loss_u: 9.986e-02, Loss_r: 1.301e-02, Time: 0.05\n",
      "It: 4720, Loss: 1.386e-01, Loss_u: 1.225e-01, Loss_r: 1.608e-02, Time: 0.05\n",
      "It: 4730, Loss: 1.036e-01, Loss_u: 7.133e-02, Loss_r: 3.230e-02, Time: 0.05\n",
      "It: 4740, Loss: 1.238e-01, Loss_u: 1.092e-01, Loss_r: 1.455e-02, Time: 0.05\n",
      "It: 4750, Loss: 9.559e-02, Loss_u: 6.925e-02, Loss_r: 2.634e-02, Time: 0.07\n",
      "It: 4760, Loss: 9.482e-02, Loss_u: 8.006e-02, Loss_r: 1.476e-02, Time: 0.05\n",
      "It: 4770, Loss: 1.141e-01, Loss_u: 8.920e-02, Loss_r: 2.493e-02, Time: 0.05\n",
      "It: 4780, Loss: 9.089e-02, Loss_u: 7.504e-02, Loss_r: 1.585e-02, Time: 0.05\n",
      "It: 4790, Loss: 8.144e-02, Loss_u: 6.527e-02, Loss_r: 1.617e-02, Time: 0.05\n",
      "It: 4800, Loss: 8.579e-02, Loss_u: 7.308e-02, Loss_r: 1.271e-02, Time: 0.05\n",
      "It: 4810, Loss: 9.257e-02, Loss_u: 7.084e-02, Loss_r: 2.174e-02, Time: 0.05\n",
      "It: 4820, Loss: 6.911e-02, Loss_u: 4.815e-02, Loss_r: 2.096e-02, Time: 0.05\n",
      "It: 4830, Loss: 1.189e-01, Loss_u: 9.560e-02, Loss_r: 2.330e-02, Time: 0.05\n",
      "It: 4840, Loss: 7.586e-02, Loss_u: 6.585e-02, Loss_r: 1.001e-02, Time: 0.07\n",
      "It: 4850, Loss: 1.069e-01, Loss_u: 9.325e-02, Loss_r: 1.364e-02, Time: 0.07\n",
      "It: 4860, Loss: 8.243e-02, Loss_u: 6.734e-02, Loss_r: 1.509e-02, Time: 0.06\n",
      "It: 4870, Loss: 1.284e-01, Loss_u: 1.141e-01, Loss_r: 1.435e-02, Time: 0.05\n",
      "It: 4880, Loss: 1.101e-01, Loss_u: 8.504e-02, Loss_r: 2.501e-02, Time: 0.05\n",
      "It: 4890, Loss: 8.829e-02, Loss_u: 6.529e-02, Loss_r: 2.300e-02, Time: 0.05\n",
      "It: 4900, Loss: 1.033e-01, Loss_u: 7.832e-02, Loss_r: 2.495e-02, Time: 0.05\n",
      "It: 4910, Loss: 8.025e-02, Loss_u: 6.331e-02, Loss_r: 1.694e-02, Time: 0.05\n",
      "It: 4920, Loss: 9.685e-02, Loss_u: 7.694e-02, Loss_r: 1.991e-02, Time: 0.06\n",
      "It: 4930, Loss: 1.008e-01, Loss_u: 7.781e-02, Loss_r: 2.297e-02, Time: 0.05\n",
      "It: 4940, Loss: 1.168e-01, Loss_u: 8.066e-02, Loss_r: 3.618e-02, Time: 0.05\n",
      "It: 4950, Loss: 8.182e-02, Loss_u: 6.102e-02, Loss_r: 2.081e-02, Time: 0.05\n",
      "It: 4960, Loss: 1.054e-01, Loss_u: 9.615e-02, Loss_r: 9.215e-03, Time: 0.05\n",
      "It: 4970, Loss: 9.835e-02, Loss_u: 7.087e-02, Loss_r: 2.748e-02, Time: 0.05\n",
      "It: 4980, Loss: 1.060e-01, Loss_u: 8.355e-02, Loss_r: 2.244e-02, Time: 0.06\n",
      "It: 4990, Loss: 1.493e-01, Loss_u: 1.313e-01, Loss_r: 1.799e-02, Time: 0.07\n",
      "It: 5000, Loss: 1.198e-01, Loss_u: 1.018e-01, Loss_r: 1.799e-02, Time: 0.05\n",
      "constant_bcs_val: 5.811\n",
      "It: 5010, Loss: 9.431e-02, Loss_u: 7.907e-02, Loss_r: 1.524e-02, Time: 0.06\n",
      "It: 5020, Loss: 1.034e-01, Loss_u: 8.571e-02, Loss_r: 1.769e-02, Time: 0.05\n",
      "It: 5030, Loss: 1.152e-01, Loss_u: 9.756e-02, Loss_r: 1.769e-02, Time: 0.05\n",
      "It: 5040, Loss: 8.418e-02, Loss_u: 7.268e-02, Loss_r: 1.151e-02, Time: 0.05\n",
      "It: 5050, Loss: 1.387e-01, Loss_u: 1.130e-01, Loss_r: 2.568e-02, Time: 0.05\n",
      "It: 5060, Loss: 1.467e-01, Loss_u: 1.263e-01, Loss_r: 2.045e-02, Time: 0.05\n",
      "It: 5070, Loss: 1.009e-01, Loss_u: 8.410e-02, Loss_r: 1.684e-02, Time: 0.05\n",
      "It: 5080, Loss: 1.026e-01, Loss_u: 7.799e-02, Loss_r: 2.456e-02, Time: 0.05\n",
      "It: 5090, Loss: 9.022e-02, Loss_u: 8.197e-02, Loss_r: 8.250e-03, Time: 0.05\n",
      "It: 5100, Loss: 1.111e-01, Loss_u: 8.792e-02, Loss_r: 2.317e-02, Time: 0.05\n",
      "It: 5110, Loss: 1.286e-01, Loss_u: 1.110e-01, Loss_r: 1.766e-02, Time: 0.05\n",
      "It: 5120, Loss: 8.255e-02, Loss_u: 6.534e-02, Loss_r: 1.720e-02, Time: 0.05\n",
      "It: 5130, Loss: 1.542e-01, Loss_u: 1.364e-01, Loss_r: 1.777e-02, Time: 0.08\n",
      "It: 5140, Loss: 9.840e-02, Loss_u: 7.414e-02, Loss_r: 2.426e-02, Time: 0.06\n",
      "It: 5150, Loss: 8.444e-02, Loss_u: 5.969e-02, Loss_r: 2.475e-02, Time: 0.05\n",
      "It: 5160, Loss: 1.313e-01, Loss_u: 1.112e-01, Loss_r: 2.008e-02, Time: 0.05\n",
      "It: 5170, Loss: 9.444e-02, Loss_u: 8.146e-02, Loss_r: 1.298e-02, Time: 0.06\n",
      "It: 5180, Loss: 6.691e-02, Loss_u: 4.741e-02, Loss_r: 1.950e-02, Time: 0.06\n",
      "It: 5190, Loss: 8.484e-02, Loss_u: 5.375e-02, Loss_r: 3.109e-02, Time: 0.05\n",
      "It: 5200, Loss: 8.938e-02, Loss_u: 7.835e-02, Loss_r: 1.103e-02, Time: 0.06\n",
      "It: 5210, Loss: 1.001e-01, Loss_u: 8.338e-02, Loss_r: 1.670e-02, Time: 0.05\n",
      "It: 5220, Loss: 1.473e-01, Loss_u: 1.314e-01, Loss_r: 1.589e-02, Time: 0.06\n",
      "It: 5230, Loss: 9.930e-02, Loss_u: 7.908e-02, Loss_r: 2.022e-02, Time: 0.05\n",
      "It: 5240, Loss: 9.786e-02, Loss_u: 8.238e-02, Loss_r: 1.548e-02, Time: 0.05\n",
      "It: 5250, Loss: 1.291e-01, Loss_u: 8.989e-02, Loss_r: 3.917e-02, Time: 0.05\n",
      "It: 5260, Loss: 1.111e-01, Loss_u: 8.450e-02, Loss_r: 2.658e-02, Time: 0.05\n",
      "It: 5270, Loss: 1.494e-01, Loss_u: 1.379e-01, Loss_r: 1.152e-02, Time: 0.05\n",
      "It: 5280, Loss: 1.204e-01, Loss_u: 9.952e-02, Loss_r: 2.084e-02, Time: 0.06\n",
      "It: 5290, Loss: 8.858e-02, Loss_u: 7.643e-02, Loss_r: 1.214e-02, Time: 0.05\n",
      "It: 5300, Loss: 1.202e-01, Loss_u: 9.335e-02, Loss_r: 2.687e-02, Time: 0.06\n",
      "It: 5310, Loss: 1.111e-01, Loss_u: 7.877e-02, Loss_r: 3.228e-02, Time: 0.06\n",
      "It: 5320, Loss: 1.292e-01, Loss_u: 1.145e-01, Loss_r: 1.472e-02, Time: 0.05\n",
      "It: 5330, Loss: 1.269e-01, Loss_u: 1.036e-01, Loss_r: 2.336e-02, Time: 0.05\n",
      "It: 5340, Loss: 1.232e-01, Loss_u: 1.100e-01, Loss_r: 1.320e-02, Time: 0.04\n",
      "It: 5350, Loss: 1.010e-01, Loss_u: 8.704e-02, Loss_r: 1.392e-02, Time: 0.05\n",
      "It: 5360, Loss: 9.599e-02, Loss_u: 7.896e-02, Loss_r: 1.703e-02, Time: 0.05\n",
      "It: 5370, Loss: 1.491e-01, Loss_u: 1.230e-01, Loss_r: 2.610e-02, Time: 0.05\n",
      "It: 5380, Loss: 1.205e-01, Loss_u: 9.919e-02, Loss_r: 2.127e-02, Time: 0.06\n",
      "It: 5390, Loss: 1.062e-01, Loss_u: 8.219e-02, Loss_r: 2.404e-02, Time: 0.05\n",
      "It: 5400, Loss: 7.793e-02, Loss_u: 5.301e-02, Loss_r: 2.492e-02, Time: 0.05\n",
      "It: 5410, Loss: 1.005e-01, Loss_u: 9.058e-02, Loss_r: 9.892e-03, Time: 0.05\n",
      "It: 5420, Loss: 9.474e-02, Loss_u: 7.365e-02, Loss_r: 2.109e-02, Time: 0.07\n",
      "It: 5430, Loss: 8.408e-02, Loss_u: 6.786e-02, Loss_r: 1.622e-02, Time: 0.06\n",
      "It: 5440, Loss: 9.625e-02, Loss_u: 7.692e-02, Loss_r: 1.933e-02, Time: 0.06\n",
      "It: 5450, Loss: 8.229e-02, Loss_u: 6.836e-02, Loss_r: 1.393e-02, Time: 0.05\n",
      "It: 5460, Loss: 1.127e-01, Loss_u: 8.123e-02, Loss_r: 3.148e-02, Time: 0.05\n",
      "It: 5470, Loss: 9.632e-02, Loss_u: 8.065e-02, Loss_r: 1.567e-02, Time: 0.10\n",
      "It: 5480, Loss: 1.124e-01, Loss_u: 8.528e-02, Loss_r: 2.710e-02, Time: 0.05\n",
      "It: 5490, Loss: 5.772e-02, Loss_u: 4.200e-02, Loss_r: 1.572e-02, Time: 0.05\n",
      "It: 5500, Loss: 8.756e-02, Loss_u: 7.155e-02, Loss_r: 1.601e-02, Time: 0.05\n",
      "It: 5510, Loss: 1.432e-01, Loss_u: 1.293e-01, Loss_r: 1.387e-02, Time: 0.05\n",
      "It: 5520, Loss: 1.206e-01, Loss_u: 9.607e-02, Loss_r: 2.450e-02, Time: 0.06\n",
      "It: 5530, Loss: 1.311e-01, Loss_u: 1.174e-01, Loss_r: 1.367e-02, Time: 0.06\n",
      "It: 5540, Loss: 1.096e-01, Loss_u: 9.933e-02, Loss_r: 1.022e-02, Time: 0.09\n",
      "It: 5550, Loss: 9.897e-02, Loss_u: 7.913e-02, Loss_r: 1.984e-02, Time: 0.05\n",
      "It: 5560, Loss: 8.777e-02, Loss_u: 7.879e-02, Loss_r: 8.980e-03, Time: 0.05\n",
      "It: 5570, Loss: 1.236e-01, Loss_u: 1.043e-01, Loss_r: 1.933e-02, Time: 0.08\n",
      "It: 5580, Loss: 9.468e-02, Loss_u: 7.597e-02, Loss_r: 1.871e-02, Time: 0.05\n",
      "It: 5590, Loss: 1.545e-01, Loss_u: 1.340e-01, Loss_r: 2.048e-02, Time: 0.05\n",
      "It: 5600, Loss: 1.241e-01, Loss_u: 9.153e-02, Loss_r: 3.255e-02, Time: 0.05\n",
      "It: 5610, Loss: 9.144e-02, Loss_u: 7.116e-02, Loss_r: 2.027e-02, Time: 0.06\n",
      "It: 5620, Loss: 1.069e-01, Loss_u: 8.078e-02, Loss_r: 2.609e-02, Time: 0.05\n",
      "It: 5630, Loss: 1.079e-01, Loss_u: 8.564e-02, Loss_r: 2.228e-02, Time: 0.05\n",
      "It: 5640, Loss: 1.051e-01, Loss_u: 8.942e-02, Loss_r: 1.570e-02, Time: 0.06\n",
      "It: 5650, Loss: 1.205e-01, Loss_u: 1.103e-01, Loss_r: 1.021e-02, Time: 0.05\n",
      "It: 5660, Loss: 8.852e-02, Loss_u: 7.315e-02, Loss_r: 1.537e-02, Time: 0.05\n",
      "It: 5670, Loss: 1.143e-01, Loss_u: 1.004e-01, Loss_r: 1.394e-02, Time: 0.06\n",
      "It: 5680, Loss: 9.628e-02, Loss_u: 7.269e-02, Loss_r: 2.359e-02, Time: 0.05\n",
      "It: 5690, Loss: 1.199e-01, Loss_u: 1.011e-01, Loss_r: 1.882e-02, Time: 0.06\n",
      "It: 5700, Loss: 9.409e-02, Loss_u: 7.355e-02, Loss_r: 2.054e-02, Time: 0.05\n",
      "It: 5710, Loss: 1.238e-01, Loss_u: 1.048e-01, Loss_r: 1.907e-02, Time: 0.05\n",
      "It: 5720, Loss: 1.118e-01, Loss_u: 9.778e-02, Loss_r: 1.400e-02, Time: 0.07\n",
      "It: 5730, Loss: 1.245e-01, Loss_u: 9.676e-02, Loss_r: 2.775e-02, Time: 0.05\n",
      "It: 5740, Loss: 1.475e-01, Loss_u: 1.224e-01, Loss_r: 2.508e-02, Time: 0.05\n",
      "It: 5750, Loss: 9.535e-02, Loss_u: 8.340e-02, Loss_r: 1.195e-02, Time: 0.05\n",
      "It: 5760, Loss: 1.008e-01, Loss_u: 8.691e-02, Loss_r: 1.385e-02, Time: 0.07\n",
      "It: 5770, Loss: 1.278e-01, Loss_u: 1.091e-01, Loss_r: 1.865e-02, Time: 0.06\n",
      "It: 5780, Loss: 9.210e-02, Loss_u: 6.604e-02, Loss_r: 2.606e-02, Time: 0.05\n",
      "It: 5790, Loss: 1.564e-01, Loss_u: 1.357e-01, Loss_r: 2.066e-02, Time: 0.07\n",
      "It: 5800, Loss: 7.018e-02, Loss_u: 5.498e-02, Loss_r: 1.520e-02, Time: 0.06\n",
      "It: 5810, Loss: 1.274e-01, Loss_u: 1.172e-01, Loss_r: 1.028e-02, Time: 0.06\n",
      "It: 5820, Loss: 1.209e-01, Loss_u: 1.058e-01, Loss_r: 1.508e-02, Time: 0.06\n",
      "It: 5830, Loss: 1.054e-01, Loss_u: 9.431e-02, Loss_r: 1.112e-02, Time: 0.05\n",
      "It: 5840, Loss: 1.385e-01, Loss_u: 1.240e-01, Loss_r: 1.444e-02, Time: 0.05\n",
      "It: 5850, Loss: 7.940e-02, Loss_u: 6.513e-02, Loss_r: 1.427e-02, Time: 0.05\n",
      "It: 5860, Loss: 1.242e-01, Loss_u: 1.007e-01, Loss_r: 2.346e-02, Time: 0.05\n",
      "It: 5870, Loss: 1.140e-01, Loss_u: 8.904e-02, Loss_r: 2.496e-02, Time: 0.06\n",
      "It: 5880, Loss: 1.099e-01, Loss_u: 1.022e-01, Loss_r: 7.646e-03, Time: 0.09\n",
      "It: 5890, Loss: 8.209e-02, Loss_u: 6.626e-02, Loss_r: 1.583e-02, Time: 0.05\n",
      "It: 5900, Loss: 7.145e-02, Loss_u: 5.238e-02, Loss_r: 1.906e-02, Time: 0.05\n",
      "It: 5910, Loss: 8.837e-02, Loss_u: 7.262e-02, Loss_r: 1.574e-02, Time: 0.05\n",
      "It: 5920, Loss: 9.065e-02, Loss_u: 7.114e-02, Loss_r: 1.951e-02, Time: 0.05\n",
      "It: 5930, Loss: 7.529e-02, Loss_u: 5.789e-02, Loss_r: 1.740e-02, Time: 0.05\n",
      "It: 5940, Loss: 1.203e-01, Loss_u: 9.833e-02, Loss_r: 2.195e-02, Time: 0.06\n",
      "It: 5950, Loss: 8.758e-02, Loss_u: 6.981e-02, Loss_r: 1.777e-02, Time: 0.06\n",
      "It: 5960, Loss: 9.935e-02, Loss_u: 9.010e-02, Loss_r: 9.259e-03, Time: 0.07\n",
      "It: 5970, Loss: 1.045e-01, Loss_u: 8.798e-02, Loss_r: 1.657e-02, Time: 0.07\n",
      "It: 5980, Loss: 1.435e-01, Loss_u: 1.111e-01, Loss_r: 3.231e-02, Time: 0.05\n",
      "It: 5990, Loss: 9.632e-02, Loss_u: 7.789e-02, Loss_r: 1.843e-02, Time: 0.05\n",
      "It: 6000, Loss: 1.005e-01, Loss_u: 7.992e-02, Loss_r: 2.060e-02, Time: 0.06\n",
      "constant_bcs_val: 5.578\n",
      "It: 6010, Loss: 1.168e-01, Loss_u: 9.168e-02, Loss_r: 2.515e-02, Time: 0.06\n",
      "It: 6020, Loss: 8.231e-02, Loss_u: 6.699e-02, Loss_r: 1.532e-02, Time: 0.05\n",
      "It: 6030, Loss: 9.051e-02, Loss_u: 6.898e-02, Loss_r: 2.154e-02, Time: 0.05\n",
      "It: 6040, Loss: 1.015e-01, Loss_u: 8.212e-02, Loss_r: 1.938e-02, Time: 0.05\n",
      "It: 6050, Loss: 1.066e-01, Loss_u: 8.962e-02, Loss_r: 1.698e-02, Time: 0.06\n",
      "It: 6060, Loss: 8.512e-02, Loss_u: 6.293e-02, Loss_r: 2.219e-02, Time: 0.05\n",
      "It: 6070, Loss: 8.299e-02, Loss_u: 7.275e-02, Loss_r: 1.024e-02, Time: 0.05\n",
      "It: 6080, Loss: 1.207e-01, Loss_u: 1.050e-01, Loss_r: 1.567e-02, Time: 0.05\n",
      "It: 6090, Loss: 8.976e-02, Loss_u: 6.772e-02, Loss_r: 2.204e-02, Time: 0.05\n",
      "It: 6100, Loss: 7.693e-02, Loss_u: 6.119e-02, Loss_r: 1.574e-02, Time: 0.06\n",
      "It: 6110, Loss: 8.464e-02, Loss_u: 6.446e-02, Loss_r: 2.018e-02, Time: 0.05\n",
      "It: 6120, Loss: 1.013e-01, Loss_u: 8.796e-02, Loss_r: 1.336e-02, Time: 0.05\n",
      "It: 6130, Loss: 8.473e-02, Loss_u: 7.181e-02, Loss_r: 1.292e-02, Time: 0.05\n",
      "It: 6140, Loss: 8.710e-02, Loss_u: 6.158e-02, Loss_r: 2.552e-02, Time: 0.05\n",
      "It: 6150, Loss: 1.052e-01, Loss_u: 8.660e-02, Loss_r: 1.858e-02, Time: 0.05\n",
      "It: 6160, Loss: 1.182e-01, Loss_u: 7.400e-02, Loss_r: 4.418e-02, Time: 0.06\n",
      "It: 6170, Loss: 1.481e-01, Loss_u: 1.012e-01, Loss_r: 4.692e-02, Time: 0.06\n",
      "It: 6180, Loss: 7.367e-02, Loss_u: 5.529e-02, Loss_r: 1.838e-02, Time: 0.08\n",
      "It: 6190, Loss: 8.136e-02, Loss_u: 6.100e-02, Loss_r: 2.037e-02, Time: 0.06\n",
      "It: 6200, Loss: 9.177e-02, Loss_u: 7.921e-02, Loss_r: 1.257e-02, Time: 0.06\n",
      "It: 6210, Loss: 9.605e-02, Loss_u: 8.627e-02, Loss_r: 9.782e-03, Time: 0.05\n",
      "It: 6220, Loss: 8.878e-02, Loss_u: 7.538e-02, Loss_r: 1.340e-02, Time: 0.05\n",
      "It: 6230, Loss: 8.560e-02, Loss_u: 6.291e-02, Loss_r: 2.269e-02, Time: 0.05\n",
      "It: 6240, Loss: 1.409e-01, Loss_u: 1.243e-01, Loss_r: 1.652e-02, Time: 0.05\n",
      "It: 6250, Loss: 1.220e-01, Loss_u: 1.057e-01, Loss_r: 1.631e-02, Time: 0.05\n",
      "It: 6260, Loss: 6.524e-02, Loss_u: 5.728e-02, Loss_r: 7.959e-03, Time: 0.06\n",
      "It: 6270, Loss: 1.233e-01, Loss_u: 1.096e-01, Loss_r: 1.371e-02, Time: 0.05\n",
      "It: 6280, Loss: 1.485e-01, Loss_u: 1.202e-01, Loss_r: 2.833e-02, Time: 0.05\n",
      "It: 6290, Loss: 8.223e-02, Loss_u: 6.415e-02, Loss_r: 1.809e-02, Time: 0.05\n",
      "It: 6300, Loss: 7.499e-02, Loss_u: 6.329e-02, Loss_r: 1.171e-02, Time: 0.07\n",
      "It: 6310, Loss: 6.539e-02, Loss_u: 4.684e-02, Loss_r: 1.855e-02, Time: 0.07\n",
      "It: 6320, Loss: 9.186e-02, Loss_u: 7.756e-02, Loss_r: 1.431e-02, Time: 0.05\n",
      "It: 6330, Loss: 6.298e-02, Loss_u: 5.552e-02, Loss_r: 7.461e-03, Time: 0.05\n",
      "It: 6340, Loss: 9.400e-02, Loss_u: 7.803e-02, Loss_r: 1.596e-02, Time: 0.06\n",
      "It: 6350, Loss: 1.069e-01, Loss_u: 8.798e-02, Loss_r: 1.894e-02, Time: 0.06\n",
      "It: 6360, Loss: 5.678e-02, Loss_u: 4.030e-02, Loss_r: 1.648e-02, Time: 0.06\n",
      "It: 6370, Loss: 9.086e-02, Loss_u: 7.403e-02, Loss_r: 1.683e-02, Time: 0.13\n",
      "It: 6380, Loss: 1.045e-01, Loss_u: 8.933e-02, Loss_r: 1.520e-02, Time: 0.06\n",
      "It: 6390, Loss: 1.075e-01, Loss_u: 9.484e-02, Loss_r: 1.269e-02, Time: 0.05\n",
      "It: 6400, Loss: 5.250e-02, Loss_u: 3.741e-02, Loss_r: 1.509e-02, Time: 0.05\n",
      "It: 6410, Loss: 8.506e-02, Loss_u: 6.003e-02, Loss_r: 2.504e-02, Time: 0.06\n",
      "It: 6420, Loss: 7.996e-02, Loss_u: 7.161e-02, Loss_r: 8.349e-03, Time: 0.05\n",
      "It: 6430, Loss: 1.031e-01, Loss_u: 8.457e-02, Loss_r: 1.849e-02, Time: 0.06\n",
      "It: 6440, Loss: 9.038e-02, Loss_u: 6.806e-02, Loss_r: 2.232e-02, Time: 0.06\n",
      "It: 6450, Loss: 1.284e-01, Loss_u: 1.189e-01, Loss_r: 9.498e-03, Time: 0.05\n",
      "It: 6460, Loss: 8.894e-02, Loss_u: 8.004e-02, Loss_r: 8.898e-03, Time: 0.06\n",
      "It: 6470, Loss: 9.536e-02, Loss_u: 8.489e-02, Loss_r: 1.047e-02, Time: 0.05\n",
      "It: 6480, Loss: 8.149e-02, Loss_u: 7.504e-02, Loss_r: 6.451e-03, Time: 0.05\n",
      "It: 6490, Loss: 1.082e-01, Loss_u: 8.548e-02, Loss_r: 2.273e-02, Time: 0.05\n",
      "It: 6500, Loss: 8.909e-02, Loss_u: 6.722e-02, Loss_r: 2.187e-02, Time: 0.06\n",
      "It: 6510, Loss: 1.045e-01, Loss_u: 6.815e-02, Loss_r: 3.635e-02, Time: 0.05\n",
      "It: 6520, Loss: 1.014e-01, Loss_u: 7.619e-02, Loss_r: 2.519e-02, Time: 0.06\n",
      "It: 6530, Loss: 9.068e-02, Loss_u: 8.656e-02, Loss_r: 4.114e-03, Time: 0.05\n",
      "It: 6540, Loss: 6.523e-02, Loss_u: 5.067e-02, Loss_r: 1.456e-02, Time: 0.05\n",
      "It: 6550, Loss: 8.778e-02, Loss_u: 6.776e-02, Loss_r: 2.002e-02, Time: 0.05\n",
      "It: 6560, Loss: 1.223e-01, Loss_u: 1.124e-01, Loss_r: 9.890e-03, Time: 0.05\n",
      "It: 6570, Loss: 1.152e-01, Loss_u: 9.908e-02, Loss_r: 1.615e-02, Time: 0.05\n",
      "It: 6580, Loss: 7.996e-02, Loss_u: 6.146e-02, Loss_r: 1.850e-02, Time: 0.05\n",
      "It: 6590, Loss: 7.958e-02, Loss_u: 5.587e-02, Loss_r: 2.372e-02, Time: 0.05\n",
      "It: 6600, Loss: 4.335e-02, Loss_u: 2.730e-02, Loss_r: 1.605e-02, Time: 0.06\n",
      "It: 6610, Loss: 8.074e-02, Loss_u: 5.774e-02, Loss_r: 2.299e-02, Time: 0.05\n",
      "It: 6620, Loss: 9.700e-02, Loss_u: 7.711e-02, Loss_r: 1.989e-02, Time: 0.05\n",
      "It: 6630, Loss: 1.231e-01, Loss_u: 1.129e-01, Loss_r: 1.017e-02, Time: 0.05\n",
      "It: 6640, Loss: 1.206e-01, Loss_u: 1.055e-01, Loss_r: 1.517e-02, Time: 0.06\n",
      "It: 6650, Loss: 9.784e-02, Loss_u: 8.357e-02, Loss_r: 1.426e-02, Time: 0.05\n",
      "It: 6660, Loss: 1.065e-01, Loss_u: 9.173e-02, Loss_r: 1.479e-02, Time: 0.05\n",
      "It: 6670, Loss: 9.275e-02, Loss_u: 6.270e-02, Loss_r: 3.005e-02, Time: 0.05\n",
      "It: 6680, Loss: 1.003e-01, Loss_u: 7.560e-02, Loss_r: 2.472e-02, Time: 0.07\n",
      "It: 6690, Loss: 8.117e-02, Loss_u: 6.518e-02, Loss_r: 1.599e-02, Time: 0.05\n",
      "It: 6700, Loss: 1.160e-01, Loss_u: 1.002e-01, Loss_r: 1.573e-02, Time: 0.06\n",
      "It: 6710, Loss: 9.742e-02, Loss_u: 8.507e-02, Loss_r: 1.235e-02, Time: 0.06\n",
      "It: 6720, Loss: 1.178e-01, Loss_u: 1.044e-01, Loss_r: 1.339e-02, Time: 0.05\n",
      "It: 6730, Loss: 5.462e-02, Loss_u: 4.781e-02, Loss_r: 6.812e-03, Time: 0.05\n",
      "It: 6740, Loss: 1.260e-01, Loss_u: 1.087e-01, Loss_r: 1.725e-02, Time: 0.07\n",
      "It: 6750, Loss: 6.262e-02, Loss_u: 5.586e-02, Loss_r: 6.756e-03, Time: 0.05\n",
      "It: 6760, Loss: 7.023e-02, Loss_u: 5.705e-02, Loss_r: 1.318e-02, Time: 0.07\n",
      "It: 6770, Loss: 1.162e-01, Loss_u: 1.054e-01, Loss_r: 1.082e-02, Time: 0.05\n",
      "It: 6780, Loss: 9.906e-02, Loss_u: 8.553e-02, Loss_r: 1.352e-02, Time: 0.05\n",
      "It: 6790, Loss: 8.285e-02, Loss_u: 7.480e-02, Loss_r: 8.048e-03, Time: 0.06\n",
      "It: 6800, Loss: 7.555e-02, Loss_u: 6.624e-02, Loss_r: 9.305e-03, Time: 0.05\n",
      "It: 6810, Loss: 1.406e-01, Loss_u: 1.105e-01, Loss_r: 3.017e-02, Time: 0.05\n",
      "It: 6820, Loss: 7.121e-02, Loss_u: 5.862e-02, Loss_r: 1.259e-02, Time: 0.06\n",
      "It: 6830, Loss: 1.052e-01, Loss_u: 8.446e-02, Loss_r: 2.073e-02, Time: 0.06\n",
      "It: 6840, Loss: 1.244e-01, Loss_u: 1.001e-01, Loss_r: 2.428e-02, Time: 0.05\n",
      "It: 6850, Loss: 1.089e-01, Loss_u: 9.610e-02, Loss_r: 1.280e-02, Time: 0.05\n",
      "It: 6860, Loss: 8.659e-02, Loss_u: 7.645e-02, Loss_r: 1.014e-02, Time: 0.05\n",
      "It: 6870, Loss: 8.628e-02, Loss_u: 7.227e-02, Loss_r: 1.401e-02, Time: 0.05\n",
      "It: 6880, Loss: 8.206e-02, Loss_u: 6.601e-02, Loss_r: 1.605e-02, Time: 0.05\n",
      "It: 6890, Loss: 7.083e-02, Loss_u: 5.879e-02, Loss_r: 1.204e-02, Time: 0.07\n",
      "It: 6900, Loss: 1.082e-01, Loss_u: 9.428e-02, Loss_r: 1.388e-02, Time: 0.05\n",
      "It: 6910, Loss: 1.162e-01, Loss_u: 1.004e-01, Loss_r: 1.579e-02, Time: 0.05\n",
      "It: 6920, Loss: 9.264e-02, Loss_u: 7.572e-02, Loss_r: 1.692e-02, Time: 0.05\n",
      "It: 6930, Loss: 7.680e-02, Loss_u: 6.642e-02, Loss_r: 1.038e-02, Time: 0.05\n",
      "It: 6940, Loss: 8.126e-02, Loss_u: 5.439e-02, Loss_r: 2.687e-02, Time: 0.06\n",
      "It: 6950, Loss: 7.504e-02, Loss_u: 5.953e-02, Loss_r: 1.550e-02, Time: 0.05\n",
      "It: 6960, Loss: 8.971e-02, Loss_u: 6.877e-02, Loss_r: 2.093e-02, Time: 0.05\n",
      "It: 6970, Loss: 1.187e-01, Loss_u: 1.076e-01, Loss_r: 1.110e-02, Time: 0.06\n",
      "It: 6980, Loss: 9.694e-02, Loss_u: 8.731e-02, Loss_r: 9.624e-03, Time: 0.08\n",
      "It: 6990, Loss: 9.825e-02, Loss_u: 7.649e-02, Loss_r: 2.175e-02, Time: 0.05\n",
      "It: 7000, Loss: 1.011e-01, Loss_u: 8.670e-02, Loss_r: 1.435e-02, Time: 0.05\n",
      "constant_bcs_val: 6.965\n",
      "It: 7010, Loss: 8.848e-02, Loss_u: 8.456e-02, Loss_r: 3.920e-03, Time: 0.06\n",
      "It: 7020, Loss: 1.360e-01, Loss_u: 1.186e-01, Loss_r: 1.742e-02, Time: 0.05\n",
      "It: 7030, Loss: 7.715e-02, Loss_u: 6.860e-02, Loss_r: 8.549e-03, Time: 0.05\n",
      "It: 7040, Loss: 1.082e-01, Loss_u: 8.557e-02, Loss_r: 2.267e-02, Time: 0.05\n",
      "It: 7050, Loss: 1.197e-01, Loss_u: 1.025e-01, Loss_r: 1.711e-02, Time: 0.06\n",
      "It: 7060, Loss: 6.397e-02, Loss_u: 5.073e-02, Loss_r: 1.324e-02, Time: 0.08\n",
      "It: 7070, Loss: 1.234e-01, Loss_u: 1.018e-01, Loss_r: 2.161e-02, Time: 0.06\n",
      "It: 7080, Loss: 9.392e-02, Loss_u: 8.810e-02, Loss_r: 5.822e-03, Time: 0.05\n",
      "It: 7090, Loss: 8.785e-02, Loss_u: 7.344e-02, Loss_r: 1.441e-02, Time: 0.05\n",
      "It: 7100, Loss: 8.853e-02, Loss_u: 6.706e-02, Loss_r: 2.147e-02, Time: 0.11\n",
      "It: 7110, Loss: 9.747e-02, Loss_u: 7.933e-02, Loss_r: 1.814e-02, Time: 0.06\n",
      "It: 7120, Loss: 1.284e-01, Loss_u: 1.116e-01, Loss_r: 1.685e-02, Time: 0.05\n",
      "It: 7130, Loss: 1.158e-01, Loss_u: 9.786e-02, Loss_r: 1.798e-02, Time: 0.05\n",
      "It: 7140, Loss: 1.088e-01, Loss_u: 9.572e-02, Loss_r: 1.307e-02, Time: 0.05\n",
      "It: 7150, Loss: 1.259e-01, Loss_u: 9.747e-02, Loss_r: 2.838e-02, Time: 0.06\n",
      "It: 7160, Loss: 1.312e-01, Loss_u: 1.149e-01, Loss_r: 1.633e-02, Time: 0.06\n",
      "It: 7170, Loss: 1.165e-01, Loss_u: 9.215e-02, Loss_r: 2.437e-02, Time: 0.05\n",
      "It: 7180, Loss: 1.658e-01, Loss_u: 1.563e-01, Loss_r: 9.509e-03, Time: 0.05\n",
      "It: 7190, Loss: 1.503e-01, Loss_u: 1.328e-01, Loss_r: 1.751e-02, Time: 0.07\n",
      "It: 7200, Loss: 1.228e-01, Loss_u: 1.098e-01, Loss_r: 1.302e-02, Time: 0.05\n",
      "It: 7210, Loss: 1.279e-01, Loss_u: 1.079e-01, Loss_r: 1.991e-02, Time: 0.05\n",
      "It: 7220, Loss: 1.195e-01, Loss_u: 9.529e-02, Loss_r: 2.419e-02, Time: 0.05\n",
      "It: 7230, Loss: 1.174e-01, Loss_u: 9.547e-02, Loss_r: 2.191e-02, Time: 0.05\n",
      "It: 7240, Loss: 1.022e-01, Loss_u: 9.184e-02, Loss_r: 1.036e-02, Time: 0.07\n",
      "It: 7250, Loss: 1.146e-01, Loss_u: 1.078e-01, Loss_r: 6.737e-03, Time: 0.06\n",
      "It: 7260, Loss: 8.291e-02, Loss_u: 5.630e-02, Loss_r: 2.661e-02, Time: 0.05\n",
      "It: 7270, Loss: 1.024e-01, Loss_u: 7.919e-02, Loss_r: 2.324e-02, Time: 0.05\n",
      "It: 7280, Loss: 9.552e-02, Loss_u: 7.657e-02, Loss_r: 1.895e-02, Time: 0.08\n",
      "It: 7290, Loss: 1.254e-01, Loss_u: 1.066e-01, Loss_r: 1.884e-02, Time: 0.08\n",
      "It: 7300, Loss: 1.020e-01, Loss_u: 8.943e-02, Loss_r: 1.256e-02, Time: 0.05\n",
      "It: 7310, Loss: 1.334e-01, Loss_u: 1.156e-01, Loss_r: 1.784e-02, Time: 0.06\n",
      "It: 7320, Loss: 7.765e-02, Loss_u: 6.646e-02, Loss_r: 1.119e-02, Time: 0.05\n",
      "It: 7330, Loss: 1.045e-01, Loss_u: 8.496e-02, Loss_r: 1.957e-02, Time: 0.05\n",
      "It: 7340, Loss: 7.580e-02, Loss_u: 5.711e-02, Loss_r: 1.869e-02, Time: 0.05\n",
      "It: 7350, Loss: 1.237e-01, Loss_u: 1.181e-01, Loss_r: 5.624e-03, Time: 0.07\n",
      "It: 7360, Loss: 1.084e-01, Loss_u: 9.336e-02, Loss_r: 1.499e-02, Time: 0.05\n",
      "It: 7370, Loss: 8.927e-02, Loss_u: 7.289e-02, Loss_r: 1.638e-02, Time: 0.05\n",
      "It: 7380, Loss: 1.066e-01, Loss_u: 9.489e-02, Loss_r: 1.176e-02, Time: 0.05\n",
      "It: 7390, Loss: 1.228e-01, Loss_u: 9.000e-02, Loss_r: 3.281e-02, Time: 0.05\n",
      "It: 7400, Loss: 1.011e-01, Loss_u: 7.521e-02, Loss_r: 2.587e-02, Time: 0.05\n",
      "It: 7410, Loss: 7.966e-02, Loss_u: 6.063e-02, Loss_r: 1.903e-02, Time: 0.06\n",
      "It: 7420, Loss: 1.257e-01, Loss_u: 1.080e-01, Loss_r: 1.767e-02, Time: 0.05\n",
      "It: 7430, Loss: 9.008e-02, Loss_u: 7.812e-02, Loss_r: 1.196e-02, Time: 0.09\n",
      "It: 7440, Loss: 1.167e-01, Loss_u: 8.929e-02, Loss_r: 2.738e-02, Time: 0.05\n",
      "It: 7450, Loss: 9.229e-02, Loss_u: 8.072e-02, Loss_r: 1.157e-02, Time: 0.05\n",
      "It: 7460, Loss: 9.113e-02, Loss_u: 8.509e-02, Loss_r: 6.040e-03, Time: 0.06\n",
      "It: 7470, Loss: 7.487e-02, Loss_u: 6.541e-02, Loss_r: 9.462e-03, Time: 0.05\n",
      "It: 7480, Loss: 1.175e-01, Loss_u: 8.177e-02, Loss_r: 3.576e-02, Time: 0.06\n",
      "It: 7490, Loss: 1.165e-01, Loss_u: 1.086e-01, Loss_r: 7.918e-03, Time: 0.05\n",
      "It: 7500, Loss: 6.909e-02, Loss_u: 5.566e-02, Loss_r: 1.343e-02, Time: 0.06\n",
      "It: 7510, Loss: 8.537e-02, Loss_u: 6.569e-02, Loss_r: 1.968e-02, Time: 0.05\n",
      "It: 7520, Loss: 5.883e-02, Loss_u: 5.002e-02, Loss_r: 8.805e-03, Time: 0.07\n",
      "It: 7530, Loss: 1.502e-01, Loss_u: 1.295e-01, Loss_r: 2.065e-02, Time: 0.05\n",
      "It: 7540, Loss: 1.165e-01, Loss_u: 9.855e-02, Loss_r: 1.799e-02, Time: 0.06\n",
      "It: 7550, Loss: 1.036e-01, Loss_u: 7.688e-02, Loss_r: 2.671e-02, Time: 0.06\n",
      "It: 7560, Loss: 1.340e-01, Loss_u: 1.241e-01, Loss_r: 9.846e-03, Time: 0.05\n",
      "It: 7570, Loss: 1.239e-01, Loss_u: 1.100e-01, Loss_r: 1.389e-02, Time: 0.05\n",
      "It: 7580, Loss: 8.689e-02, Loss_u: 6.580e-02, Loss_r: 2.109e-02, Time: 0.07\n",
      "It: 7590, Loss: 1.137e-01, Loss_u: 9.394e-02, Loss_r: 1.979e-02, Time: 0.05\n",
      "It: 7600, Loss: 8.969e-02, Loss_u: 7.127e-02, Loss_r: 1.842e-02, Time: 0.07\n",
      "It: 7610, Loss: 6.393e-02, Loss_u: 5.623e-02, Loss_r: 7.704e-03, Time: 0.05\n",
      "It: 7620, Loss: 8.892e-02, Loss_u: 8.051e-02, Loss_r: 8.412e-03, Time: 0.05\n",
      "It: 7630, Loss: 7.998e-02, Loss_u: 6.898e-02, Loss_r: 1.101e-02, Time: 0.05\n",
      "It: 7640, Loss: 8.041e-02, Loss_u: 6.434e-02, Loss_r: 1.607e-02, Time: 0.05\n",
      "It: 7650, Loss: 1.004e-01, Loss_u: 8.806e-02, Loss_r: 1.232e-02, Time: 0.06\n",
      "It: 7660, Loss: 1.162e-01, Loss_u: 1.004e-01, Loss_r: 1.581e-02, Time: 0.05\n",
      "It: 7670, Loss: 8.515e-02, Loss_u: 7.308e-02, Loss_r: 1.207e-02, Time: 0.05\n",
      "It: 7680, Loss: 1.065e-01, Loss_u: 8.940e-02, Loss_r: 1.710e-02, Time: 0.05\n",
      "It: 7690, Loss: 8.967e-02, Loss_u: 7.169e-02, Loss_r: 1.797e-02, Time: 0.05\n",
      "It: 7700, Loss: 7.790e-02, Loss_u: 6.818e-02, Loss_r: 9.716e-03, Time: 0.05\n",
      "It: 7710, Loss: 6.861e-02, Loss_u: 5.285e-02, Loss_r: 1.576e-02, Time: 0.05\n",
      "It: 7720, Loss: 1.601e-01, Loss_u: 1.317e-01, Loss_r: 2.837e-02, Time: 0.05\n",
      "It: 7730, Loss: 9.047e-02, Loss_u: 6.879e-02, Loss_r: 2.168e-02, Time: 0.05\n",
      "It: 7740, Loss: 1.358e-01, Loss_u: 1.248e-01, Loss_r: 1.100e-02, Time: 0.06\n",
      "It: 7750, Loss: 9.031e-02, Loss_u: 7.982e-02, Loss_r: 1.048e-02, Time: 0.06\n",
      "It: 7760, Loss: 1.312e-01, Loss_u: 1.128e-01, Loss_r: 1.841e-02, Time: 0.05\n",
      "It: 7770, Loss: 9.633e-02, Loss_u: 8.486e-02, Loss_r: 1.147e-02, Time: 0.05\n",
      "It: 7780, Loss: 1.015e-01, Loss_u: 7.821e-02, Loss_r: 2.331e-02, Time: 0.05\n",
      "It: 7790, Loss: 1.325e-01, Loss_u: 1.130e-01, Loss_r: 1.956e-02, Time: 0.05\n",
      "It: 7800, Loss: 1.195e-01, Loss_u: 1.052e-01, Loss_r: 1.427e-02, Time: 0.05\n",
      "It: 7810, Loss: 6.624e-02, Loss_u: 6.066e-02, Loss_r: 5.578e-03, Time: 0.05\n",
      "It: 7820, Loss: 1.006e-01, Loss_u: 7.590e-02, Loss_r: 2.468e-02, Time: 0.05\n",
      "It: 7830, Loss: 1.034e-01, Loss_u: 9.054e-02, Loss_r: 1.285e-02, Time: 0.05\n",
      "It: 7840, Loss: 1.107e-01, Loss_u: 9.357e-02, Loss_r: 1.711e-02, Time: 0.05\n",
      "It: 7850, Loss: 8.812e-02, Loss_u: 7.382e-02, Loss_r: 1.430e-02, Time: 0.06\n",
      "It: 7860, Loss: 8.221e-02, Loss_u: 7.101e-02, Loss_r: 1.121e-02, Time: 0.09\n",
      "It: 7870, Loss: 7.843e-02, Loss_u: 7.118e-02, Loss_r: 7.255e-03, Time: 0.05\n",
      "It: 7880, Loss: 1.361e-01, Loss_u: 1.163e-01, Loss_r: 1.981e-02, Time: 0.05\n",
      "It: 7890, Loss: 1.073e-01, Loss_u: 9.094e-02, Loss_r: 1.632e-02, Time: 0.05\n",
      "It: 7900, Loss: 1.492e-01, Loss_u: 1.332e-01, Loss_r: 1.601e-02, Time: 0.05\n",
      "It: 7910, Loss: 9.294e-02, Loss_u: 8.451e-02, Loss_r: 8.432e-03, Time: 0.05\n",
      "It: 7920, Loss: 8.565e-02, Loss_u: 7.314e-02, Loss_r: 1.251e-02, Time: 0.05\n",
      "It: 7930, Loss: 9.450e-02, Loss_u: 7.782e-02, Loss_r: 1.667e-02, Time: 0.05\n",
      "It: 7940, Loss: 7.031e-02, Loss_u: 5.892e-02, Loss_r: 1.139e-02, Time: 0.05\n",
      "It: 7950, Loss: 1.237e-01, Loss_u: 1.073e-01, Loss_r: 1.641e-02, Time: 0.06\n",
      "It: 7960, Loss: 9.174e-02, Loss_u: 8.293e-02, Loss_r: 8.816e-03, Time: 0.06\n",
      "It: 7970, Loss: 7.707e-02, Loss_u: 6.745e-02, Loss_r: 9.620e-03, Time: 0.05\n",
      "It: 7980, Loss: 1.069e-01, Loss_u: 7.037e-02, Loss_r: 3.648e-02, Time: 0.07\n",
      "It: 7990, Loss: 8.729e-02, Loss_u: 7.289e-02, Loss_r: 1.441e-02, Time: 0.05\n",
      "It: 8000, Loss: 1.033e-01, Loss_u: 8.600e-02, Loss_r: 1.731e-02, Time: 0.05\n",
      "constant_bcs_val: 6.979\n",
      "It: 8010, Loss: 1.074e-01, Loss_u: 9.800e-02, Loss_r: 9.370e-03, Time: 0.05\n",
      "It: 8020, Loss: 1.185e-01, Loss_u: 1.094e-01, Loss_r: 9.131e-03, Time: 0.05\n",
      "It: 8030, Loss: 1.168e-01, Loss_u: 1.027e-01, Loss_r: 1.414e-02, Time: 0.06\n",
      "It: 8040, Loss: 5.395e-02, Loss_u: 4.560e-02, Loss_r: 8.346e-03, Time: 0.06\n",
      "It: 8050, Loss: 6.911e-02, Loss_u: 5.445e-02, Loss_r: 1.466e-02, Time: 0.07\n",
      "It: 8060, Loss: 1.030e-01, Loss_u: 7.521e-02, Loss_r: 2.776e-02, Time: 0.05\n",
      "It: 8070, Loss: 7.656e-02, Loss_u: 6.065e-02, Loss_r: 1.592e-02, Time: 0.06\n",
      "It: 8080, Loss: 8.005e-02, Loss_u: 6.574e-02, Loss_r: 1.431e-02, Time: 0.05\n",
      "It: 8090, Loss: 1.000e-01, Loss_u: 7.765e-02, Loss_r: 2.238e-02, Time: 0.05\n",
      "It: 8100, Loss: 8.776e-02, Loss_u: 7.339e-02, Loss_r: 1.437e-02, Time: 0.05\n",
      "It: 8110, Loss: 1.692e-01, Loss_u: 1.411e-01, Loss_r: 2.818e-02, Time: 0.05\n",
      "It: 8120, Loss: 1.022e-01, Loss_u: 8.740e-02, Loss_r: 1.483e-02, Time: 0.05\n",
      "It: 8130, Loss: 1.115e-01, Loss_u: 8.808e-02, Loss_r: 2.344e-02, Time: 0.05\n",
      "It: 8140, Loss: 1.193e-01, Loss_u: 1.021e-01, Loss_r: 1.715e-02, Time: 0.05\n",
      "It: 8150, Loss: 1.111e-01, Loss_u: 7.985e-02, Loss_r: 3.126e-02, Time: 0.06\n",
      "It: 8160, Loss: 1.363e-01, Loss_u: 1.194e-01, Loss_r: 1.685e-02, Time: 0.05\n",
      "It: 8170, Loss: 1.208e-01, Loss_u: 1.128e-01, Loss_r: 7.968e-03, Time: 0.05\n",
      "It: 8180, Loss: 7.589e-02, Loss_u: 5.939e-02, Loss_r: 1.650e-02, Time: 0.05\n",
      "It: 8190, Loss: 1.712e-01, Loss_u: 1.576e-01, Loss_r: 1.366e-02, Time: 0.05\n",
      "It: 8200, Loss: 1.188e-01, Loss_u: 1.050e-01, Loss_r: 1.374e-02, Time: 0.05\n",
      "It: 8210, Loss: 1.032e-01, Loss_u: 9.768e-02, Loss_r: 5.536e-03, Time: 0.07\n",
      "It: 8220, Loss: 1.235e-01, Loss_u: 1.055e-01, Loss_r: 1.808e-02, Time: 0.05\n",
      "It: 8230, Loss: 1.388e-01, Loss_u: 1.209e-01, Loss_r: 1.789e-02, Time: 0.06\n",
      "It: 8240, Loss: 1.350e-01, Loss_u: 1.203e-01, Loss_r: 1.465e-02, Time: 0.05\n",
      "It: 8250, Loss: 1.065e-01, Loss_u: 9.652e-02, Loss_r: 9.974e-03, Time: 0.07\n",
      "It: 8260, Loss: 8.567e-02, Loss_u: 6.892e-02, Loss_r: 1.675e-02, Time: 0.05\n",
      "It: 8270, Loss: 9.042e-02, Loss_u: 6.922e-02, Loss_r: 2.121e-02, Time: 0.05\n",
      "It: 8280, Loss: 1.029e-01, Loss_u: 9.451e-02, Loss_r: 8.399e-03, Time: 0.11\n",
      "It: 8290, Loss: 8.565e-02, Loss_u: 6.859e-02, Loss_r: 1.706e-02, Time: 0.05\n",
      "It: 8300, Loss: 8.803e-02, Loss_u: 8.271e-02, Loss_r: 5.325e-03, Time: 0.06\n",
      "It: 8310, Loss: 8.823e-02, Loss_u: 7.537e-02, Loss_r: 1.286e-02, Time: 0.06\n",
      "It: 8320, Loss: 8.619e-02, Loss_u: 7.101e-02, Loss_r: 1.518e-02, Time: 0.06\n",
      "It: 8330, Loss: 1.374e-01, Loss_u: 1.252e-01, Loss_r: 1.219e-02, Time: 0.05\n",
      "It: 8340, Loss: 9.837e-02, Loss_u: 9.127e-02, Loss_r: 7.104e-03, Time: 0.06\n",
      "It: 8350, Loss: 9.363e-02, Loss_u: 7.842e-02, Loss_r: 1.521e-02, Time: 0.20\n",
      "It: 8360, Loss: 1.108e-01, Loss_u: 1.010e-01, Loss_r: 9.853e-03, Time: 0.07\n",
      "It: 8370, Loss: 7.706e-02, Loss_u: 6.814e-02, Loss_r: 8.912e-03, Time: 0.05\n",
      "It: 8380, Loss: 7.580e-02, Loss_u: 5.946e-02, Loss_r: 1.634e-02, Time: 0.07\n",
      "It: 8390, Loss: 4.409e-02, Loss_u: 3.526e-02, Loss_r: 8.821e-03, Time: 0.07\n",
      "It: 8400, Loss: 9.274e-02, Loss_u: 7.773e-02, Loss_r: 1.501e-02, Time: 0.05\n",
      "It: 8410, Loss: 7.073e-02, Loss_u: 6.671e-02, Loss_r: 4.017e-03, Time: 0.05\n",
      "It: 8420, Loss: 1.305e-01, Loss_u: 1.101e-01, Loss_r: 2.038e-02, Time: 0.06\n",
      "It: 8430, Loss: 8.975e-02, Loss_u: 8.228e-02, Loss_r: 7.466e-03, Time: 0.07\n",
      "It: 8440, Loss: 9.133e-02, Loss_u: 8.167e-02, Loss_r: 9.660e-03, Time: 0.05\n",
      "It: 8450, Loss: 1.419e-01, Loss_u: 1.337e-01, Loss_r: 8.226e-03, Time: 0.06\n",
      "It: 8460, Loss: 1.236e-01, Loss_u: 9.882e-02, Loss_r: 2.475e-02, Time: 0.06\n",
      "It: 8470, Loss: 7.744e-02, Loss_u: 6.867e-02, Loss_r: 8.766e-03, Time: 0.09\n",
      "It: 8480, Loss: 1.063e-01, Loss_u: 9.168e-02, Loss_r: 1.463e-02, Time: 0.05\n",
      "It: 8490, Loss: 9.019e-02, Loss_u: 8.185e-02, Loss_r: 8.344e-03, Time: 0.05\n",
      "It: 8500, Loss: 7.520e-02, Loss_u: 6.604e-02, Loss_r: 9.162e-03, Time: 0.05\n",
      "It: 8510, Loss: 9.672e-02, Loss_u: 8.207e-02, Loss_r: 1.465e-02, Time: 0.05\n",
      "It: 8520, Loss: 1.030e-01, Loss_u: 9.546e-02, Loss_r: 7.533e-03, Time: 0.06\n",
      "It: 8530, Loss: 5.381e-02, Loss_u: 4.547e-02, Loss_r: 8.342e-03, Time: 0.06\n",
      "It: 8540, Loss: 8.069e-02, Loss_u: 6.784e-02, Loss_r: 1.285e-02, Time: 0.05\n",
      "It: 8550, Loss: 8.824e-02, Loss_u: 7.264e-02, Loss_r: 1.560e-02, Time: 0.05\n",
      "It: 8560, Loss: 8.312e-02, Loss_u: 7.526e-02, Loss_r: 7.854e-03, Time: 0.05\n",
      "It: 8570, Loss: 5.484e-02, Loss_u: 4.544e-02, Loss_r: 9.398e-03, Time: 0.06\n",
      "It: 8580, Loss: 1.230e-01, Loss_u: 1.028e-01, Loss_r: 2.018e-02, Time: 0.05\n",
      "It: 8590, Loss: 1.162e-01, Loss_u: 9.570e-02, Loss_r: 2.051e-02, Time: 0.05\n",
      "It: 8600, Loss: 9.465e-02, Loss_u: 8.263e-02, Loss_r: 1.202e-02, Time: 0.24\n",
      "It: 8610, Loss: 1.555e-01, Loss_u: 1.398e-01, Loss_r: 1.573e-02, Time: 0.12\n",
      "It: 8620, Loss: 8.761e-02, Loss_u: 7.187e-02, Loss_r: 1.574e-02, Time: 0.40\n",
      "It: 8630, Loss: 9.021e-02, Loss_u: 7.900e-02, Loss_r: 1.121e-02, Time: 0.05\n",
      "It: 8640, Loss: 1.125e-01, Loss_u: 9.833e-02, Loss_r: 1.415e-02, Time: 0.05\n",
      "It: 8650, Loss: 1.254e-01, Loss_u: 1.138e-01, Loss_r: 1.159e-02, Time: 0.05\n",
      "It: 8660, Loss: 1.183e-01, Loss_u: 1.073e-01, Loss_r: 1.097e-02, Time: 0.05\n",
      "It: 8670, Loss: 9.391e-02, Loss_u: 8.679e-02, Loss_r: 7.128e-03, Time: 0.06\n",
      "It: 8680, Loss: 1.110e-01, Loss_u: 1.012e-01, Loss_r: 9.760e-03, Time: 0.05\n",
      "It: 8690, Loss: 7.208e-02, Loss_u: 6.321e-02, Loss_r: 8.875e-03, Time: 0.05\n",
      "It: 8700, Loss: 9.011e-02, Loss_u: 7.803e-02, Loss_r: 1.208e-02, Time: 0.08\n",
      "It: 8710, Loss: 7.934e-02, Loss_u: 7.448e-02, Loss_r: 4.853e-03, Time: 0.05\n",
      "It: 8720, Loss: 8.489e-02, Loss_u: 7.777e-02, Loss_r: 7.127e-03, Time: 0.06\n",
      "It: 8730, Loss: 8.555e-02, Loss_u: 6.747e-02, Loss_r: 1.808e-02, Time: 0.05\n",
      "It: 8740, Loss: 9.228e-02, Loss_u: 7.918e-02, Loss_r: 1.310e-02, Time: 0.06\n",
      "It: 8750, Loss: 8.672e-02, Loss_u: 7.899e-02, Loss_r: 7.736e-03, Time: 0.05\n",
      "It: 8760, Loss: 6.722e-02, Loss_u: 5.562e-02, Loss_r: 1.160e-02, Time: 0.05\n",
      "It: 8770, Loss: 1.241e-01, Loss_u: 1.100e-01, Loss_r: 1.413e-02, Time: 0.07\n",
      "It: 8780, Loss: 9.281e-02, Loss_u: 7.849e-02, Loss_r: 1.432e-02, Time: 0.07\n",
      "It: 8790, Loss: 1.115e-01, Loss_u: 9.846e-02, Loss_r: 1.305e-02, Time: 0.06\n",
      "It: 8800, Loss: 1.040e-01, Loss_u: 8.963e-02, Loss_r: 1.441e-02, Time: 0.05\n",
      "It: 8810, Loss: 8.870e-02, Loss_u: 8.365e-02, Loss_r: 5.043e-03, Time: 0.26\n",
      "It: 8820, Loss: 1.046e-01, Loss_u: 9.244e-02, Loss_r: 1.213e-02, Time: 0.05\n",
      "It: 8830, Loss: 6.177e-02, Loss_u: 5.668e-02, Loss_r: 5.088e-03, Time: 0.05\n",
      "It: 8840, Loss: 8.167e-02, Loss_u: 6.415e-02, Loss_r: 1.752e-02, Time: 0.06\n",
      "It: 8850, Loss: 9.831e-02, Loss_u: 9.205e-02, Loss_r: 6.258e-03, Time: 0.19\n",
      "It: 8860, Loss: 8.602e-02, Loss_u: 7.728e-02, Loss_r: 8.736e-03, Time: 0.05\n",
      "It: 8870, Loss: 9.499e-02, Loss_u: 8.950e-02, Loss_r: 5.489e-03, Time: 0.08\n",
      "It: 8880, Loss: 7.870e-02, Loss_u: 7.478e-02, Loss_r: 3.913e-03, Time: 0.05\n",
      "It: 8890, Loss: 8.921e-02, Loss_u: 7.193e-02, Loss_r: 1.728e-02, Time: 0.08\n",
      "It: 8900, Loss: 6.959e-02, Loss_u: 5.558e-02, Loss_r: 1.402e-02, Time: 0.07\n",
      "It: 8910, Loss: 1.003e-01, Loss_u: 9.443e-02, Loss_r: 5.824e-03, Time: 0.05\n",
      "It: 8920, Loss: 7.719e-02, Loss_u: 6.577e-02, Loss_r: 1.141e-02, Time: 0.05\n",
      "It: 8930, Loss: 8.593e-02, Loss_u: 7.301e-02, Loss_r: 1.292e-02, Time: 0.06\n",
      "It: 8940, Loss: 8.791e-02, Loss_u: 7.812e-02, Loss_r: 9.794e-03, Time: 0.04\n",
      "It: 8950, Loss: 5.647e-02, Loss_u: 4.793e-02, Loss_r: 8.542e-03, Time: 0.05\n",
      "It: 8960, Loss: 7.444e-02, Loss_u: 6.822e-02, Loss_r: 6.225e-03, Time: 0.06\n",
      "It: 8970, Loss: 8.856e-02, Loss_u: 6.723e-02, Loss_r: 2.133e-02, Time: 0.06\n",
      "It: 8980, Loss: 8.951e-02, Loss_u: 8.420e-02, Loss_r: 5.308e-03, Time: 0.04\n",
      "It: 8990, Loss: 7.125e-02, Loss_u: 6.535e-02, Loss_r: 5.907e-03, Time: 0.04\n",
      "It: 9000, Loss: 6.608e-02, Loss_u: 5.739e-02, Loss_r: 8.684e-03, Time: 0.04\n",
      "constant_bcs_val: 7.368\n",
      "It: 9010, Loss: 1.045e-01, Loss_u: 8.835e-02, Loss_r: 1.613e-02, Time: 0.04\n",
      "It: 9020, Loss: 5.942e-02, Loss_u: 4.932e-02, Loss_r: 1.010e-02, Time: 0.05\n",
      "It: 9030, Loss: 1.017e-01, Loss_u: 9.164e-02, Loss_r: 1.008e-02, Time: 0.05\n",
      "It: 9040, Loss: 9.642e-02, Loss_u: 8.283e-02, Loss_r: 1.359e-02, Time: 0.07\n",
      "It: 9050, Loss: 8.825e-02, Loss_u: 7.795e-02, Loss_r: 1.029e-02, Time: 0.06\n",
      "It: 9060, Loss: 1.014e-01, Loss_u: 7.939e-02, Loss_r: 2.202e-02, Time: 0.04\n",
      "It: 9070, Loss: 9.153e-02, Loss_u: 7.386e-02, Loss_r: 1.767e-02, Time: 0.07\n",
      "It: 9080, Loss: 1.076e-01, Loss_u: 1.037e-01, Loss_r: 3.878e-03, Time: 0.04\n",
      "It: 9090, Loss: 9.959e-02, Loss_u: 8.219e-02, Loss_r: 1.739e-02, Time: 0.04\n",
      "It: 9100, Loss: 9.396e-02, Loss_u: 8.833e-02, Loss_r: 5.625e-03, Time: 0.04\n",
      "It: 9110, Loss: 9.221e-02, Loss_u: 8.120e-02, Loss_r: 1.101e-02, Time: 0.04\n",
      "It: 9120, Loss: 1.015e-01, Loss_u: 9.719e-02, Loss_r: 4.341e-03, Time: 0.05\n",
      "It: 9130, Loss: 9.255e-02, Loss_u: 8.626e-02, Loss_r: 6.296e-03, Time: 0.04\n",
      "It: 9140, Loss: 1.062e-01, Loss_u: 9.049e-02, Loss_r: 1.569e-02, Time: 0.04\n",
      "It: 9150, Loss: 1.085e-01, Loss_u: 9.568e-02, Loss_r: 1.277e-02, Time: 0.05\n",
      "It: 9160, Loss: 6.662e-02, Loss_u: 5.311e-02, Loss_r: 1.351e-02, Time: 0.04\n",
      "It: 9170, Loss: 9.953e-02, Loss_u: 9.178e-02, Loss_r: 7.753e-03, Time: 0.04\n",
      "It: 9180, Loss: 7.941e-02, Loss_u: 6.427e-02, Loss_r: 1.515e-02, Time: 0.04\n",
      "It: 9190, Loss: 1.501e-01, Loss_u: 1.087e-01, Loss_r: 4.142e-02, Time: 0.05\n",
      "It: 9200, Loss: 9.783e-02, Loss_u: 8.465e-02, Loss_r: 1.319e-02, Time: 0.04\n",
      "It: 9210, Loss: 8.099e-02, Loss_u: 6.901e-02, Loss_r: 1.198e-02, Time: 0.04\n",
      "It: 9220, Loss: 1.067e-01, Loss_u: 7.047e-02, Loss_r: 3.622e-02, Time: 0.04\n",
      "It: 9230, Loss: 1.194e-01, Loss_u: 1.110e-01, Loss_r: 8.333e-03, Time: 0.04\n",
      "It: 9240, Loss: 7.308e-02, Loss_u: 6.563e-02, Loss_r: 7.452e-03, Time: 0.04\n",
      "It: 9250, Loss: 9.542e-02, Loss_u: 7.556e-02, Loss_r: 1.986e-02, Time: 0.04\n",
      "It: 9260, Loss: 5.748e-02, Loss_u: 4.372e-02, Loss_r: 1.376e-02, Time: 0.04\n",
      "It: 9270, Loss: 8.272e-02, Loss_u: 7.492e-02, Loss_r: 7.800e-03, Time: 0.04\n",
      "It: 9280, Loss: 6.582e-02, Loss_u: 5.628e-02, Loss_r: 9.534e-03, Time: 0.04\n",
      "It: 9290, Loss: 8.055e-02, Loss_u: 6.109e-02, Loss_r: 1.946e-02, Time: 0.05\n",
      "It: 9300, Loss: 6.264e-02, Loss_u: 5.593e-02, Loss_r: 6.710e-03, Time: 0.05\n",
      "It: 9310, Loss: 8.526e-02, Loss_u: 6.806e-02, Loss_r: 1.720e-02, Time: 0.05\n",
      "It: 9320, Loss: 8.252e-02, Loss_u: 6.793e-02, Loss_r: 1.459e-02, Time: 0.06\n",
      "It: 9330, Loss: 9.455e-02, Loss_u: 8.389e-02, Loss_r: 1.066e-02, Time: 0.04\n",
      "It: 9340, Loss: 1.193e-01, Loss_u: 1.121e-01, Loss_r: 7.186e-03, Time: 0.05\n",
      "It: 9350, Loss: 7.048e-02, Loss_u: 5.871e-02, Loss_r: 1.177e-02, Time: 0.04\n",
      "It: 9360, Loss: 6.726e-02, Loss_u: 5.855e-02, Loss_r: 8.703e-03, Time: 0.04\n",
      "It: 9370, Loss: 8.038e-02, Loss_u: 7.360e-02, Loss_r: 6.772e-03, Time: 0.05\n",
      "It: 9380, Loss: 5.938e-02, Loss_u: 4.873e-02, Loss_r: 1.065e-02, Time: 0.07\n",
      "It: 9390, Loss: 4.473e-02, Loss_u: 3.316e-02, Loss_r: 1.157e-02, Time: 0.05\n",
      "It: 9400, Loss: 7.500e-02, Loss_u: 6.448e-02, Loss_r: 1.052e-02, Time: 0.04\n",
      "It: 9410, Loss: 7.545e-02, Loss_u: 6.525e-02, Loss_r: 1.020e-02, Time: 0.04\n",
      "It: 9420, Loss: 7.232e-02, Loss_u: 5.924e-02, Loss_r: 1.308e-02, Time: 0.04\n",
      "It: 9430, Loss: 7.930e-02, Loss_u: 6.596e-02, Loss_r: 1.334e-02, Time: 0.10\n",
      "It: 9440, Loss: 6.850e-02, Loss_u: 5.226e-02, Loss_r: 1.624e-02, Time: 0.04\n",
      "It: 9450, Loss: 8.066e-02, Loss_u: 6.353e-02, Loss_r: 1.713e-02, Time: 0.05\n",
      "It: 9460, Loss: 7.023e-02, Loss_u: 5.783e-02, Loss_r: 1.240e-02, Time: 0.04\n",
      "It: 9470, Loss: 6.977e-02, Loss_u: 6.590e-02, Loss_r: 3.874e-03, Time: 0.05\n",
      "It: 9480, Loss: 8.402e-02, Loss_u: 7.941e-02, Loss_r: 4.613e-03, Time: 0.04\n",
      "It: 9490, Loss: 1.195e-01, Loss_u: 1.087e-01, Loss_r: 1.072e-02, Time: 0.05\n",
      "It: 9500, Loss: 9.136e-02, Loss_u: 6.453e-02, Loss_r: 2.683e-02, Time: 0.06\n",
      "It: 9510, Loss: 1.149e-01, Loss_u: 9.827e-02, Loss_r: 1.665e-02, Time: 0.05\n",
      "It: 9520, Loss: 1.041e-01, Loss_u: 9.393e-02, Loss_r: 1.019e-02, Time: 0.06\n",
      "It: 9530, Loss: 1.255e-01, Loss_u: 8.829e-02, Loss_r: 3.726e-02, Time: 0.04\n",
      "It: 9540, Loss: 1.122e-01, Loss_u: 1.029e-01, Loss_r: 9.348e-03, Time: 0.05\n",
      "It: 9550, Loss: 5.145e-02, Loss_u: 3.555e-02, Loss_r: 1.591e-02, Time: 0.04\n",
      "It: 9560, Loss: 1.166e-01, Loss_u: 1.071e-01, Loss_r: 9.537e-03, Time: 0.05\n",
      "It: 9570, Loss: 9.255e-02, Loss_u: 8.481e-02, Loss_r: 7.740e-03, Time: 0.04\n",
      "It: 9580, Loss: 8.172e-02, Loss_u: 6.911e-02, Loss_r: 1.261e-02, Time: 0.05\n",
      "It: 9590, Loss: 9.139e-02, Loss_u: 8.291e-02, Loss_r: 8.478e-03, Time: 0.05\n",
      "It: 9600, Loss: 7.695e-02, Loss_u: 5.785e-02, Loss_r: 1.909e-02, Time: 0.07\n",
      "It: 9610, Loss: 1.157e-01, Loss_u: 1.074e-01, Loss_r: 8.310e-03, Time: 0.06\n",
      "It: 9620, Loss: 6.027e-02, Loss_u: 5.219e-02, Loss_r: 8.086e-03, Time: 0.04\n",
      "It: 9630, Loss: 9.612e-02, Loss_u: 8.663e-02, Loss_r: 9.490e-03, Time: 0.04\n",
      "It: 9640, Loss: 9.906e-02, Loss_u: 8.815e-02, Loss_r: 1.090e-02, Time: 0.04\n",
      "It: 9650, Loss: 8.016e-02, Loss_u: 6.270e-02, Loss_r: 1.746e-02, Time: 0.05\n",
      "It: 9660, Loss: 1.077e-01, Loss_u: 9.318e-02, Loss_r: 1.448e-02, Time: 0.04\n",
      "It: 9670, Loss: 8.991e-02, Loss_u: 8.339e-02, Loss_r: 6.512e-03, Time: 0.04\n",
      "It: 9680, Loss: 1.219e-01, Loss_u: 1.135e-01, Loss_r: 8.377e-03, Time: 0.10\n",
      "It: 9690, Loss: 1.066e-01, Loss_u: 9.039e-02, Loss_r: 1.625e-02, Time: 0.06\n",
      "It: 9700, Loss: 1.006e-01, Loss_u: 9.194e-02, Loss_r: 8.705e-03, Time: 0.07\n",
      "It: 9710, Loss: 1.392e-01, Loss_u: 1.205e-01, Loss_r: 1.876e-02, Time: 0.04\n",
      "It: 9720, Loss: 8.502e-02, Loss_u: 7.545e-02, Loss_r: 9.571e-03, Time: 0.04\n",
      "It: 9730, Loss: 8.228e-02, Loss_u: 7.079e-02, Loss_r: 1.149e-02, Time: 0.05\n",
      "It: 9740, Loss: 1.071e-01, Loss_u: 9.711e-02, Loss_r: 9.988e-03, Time: 0.04\n",
      "It: 9750, Loss: 8.762e-02, Loss_u: 7.189e-02, Loss_r: 1.573e-02, Time: 0.04\n",
      "It: 9760, Loss: 5.827e-02, Loss_u: 4.974e-02, Loss_r: 8.531e-03, Time: 0.05\n",
      "It: 9770, Loss: 1.124e-01, Loss_u: 9.873e-02, Loss_r: 1.365e-02, Time: 0.06\n",
      "It: 9780, Loss: 8.236e-02, Loss_u: 6.600e-02, Loss_r: 1.636e-02, Time: 0.05\n",
      "It: 9790, Loss: 8.736e-02, Loss_u: 7.415e-02, Loss_r: 1.321e-02, Time: 0.04\n",
      "It: 9800, Loss: 9.150e-02, Loss_u: 8.133e-02, Loss_r: 1.017e-02, Time: 0.05\n",
      "It: 9810, Loss: 5.678e-02, Loss_u: 4.988e-02, Loss_r: 6.894e-03, Time: 0.05\n",
      "It: 9820, Loss: 5.730e-02, Loss_u: 4.468e-02, Loss_r: 1.262e-02, Time: 0.04\n",
      "It: 9830, Loss: 7.969e-02, Loss_u: 6.036e-02, Loss_r: 1.933e-02, Time: 0.05\n",
      "It: 9840, Loss: 1.047e-01, Loss_u: 8.873e-02, Loss_r: 1.596e-02, Time: 0.05\n",
      "It: 9850, Loss: 1.005e-01, Loss_u: 9.655e-02, Loss_r: 3.977e-03, Time: 0.05\n",
      "It: 9860, Loss: 7.368e-02, Loss_u: 6.250e-02, Loss_r: 1.118e-02, Time: 0.08\n",
      "It: 9870, Loss: 7.159e-02, Loss_u: 6.316e-02, Loss_r: 8.431e-03, Time: 0.04\n",
      "It: 9880, Loss: 1.276e-01, Loss_u: 1.191e-01, Loss_r: 8.496e-03, Time: 0.05\n",
      "It: 9890, Loss: 9.409e-02, Loss_u: 8.252e-02, Loss_r: 1.157e-02, Time: 0.04\n",
      "It: 9900, Loss: 8.113e-02, Loss_u: 5.822e-02, Loss_r: 2.292e-02, Time: 0.04\n",
      "It: 9910, Loss: 1.012e-01, Loss_u: 9.060e-02, Loss_r: 1.065e-02, Time: 0.06\n",
      "It: 9920, Loss: 1.179e-01, Loss_u: 1.078e-01, Loss_r: 1.004e-02, Time: 0.06\n",
      "It: 9930, Loss: 9.496e-02, Loss_u: 7.998e-02, Loss_r: 1.498e-02, Time: 0.08\n",
      "It: 9940, Loss: 1.083e-01, Loss_u: 1.013e-01, Loss_r: 7.039e-03, Time: 0.07\n",
      "It: 9950, Loss: 9.997e-02, Loss_u: 7.744e-02, Loss_r: 2.253e-02, Time: 0.04\n",
      "It: 9960, Loss: 7.611e-02, Loss_u: 6.727e-02, Loss_r: 8.832e-03, Time: 0.05\n",
      "It: 9970, Loss: 1.118e-01, Loss_u: 1.059e-01, Loss_r: 5.937e-03, Time: 0.05\n",
      "It: 9980, Loss: 1.151e-01, Loss_u: 9.362e-02, Loss_r: 2.150e-02, Time: 0.05\n",
      "It: 9990, Loss: 9.148e-02, Loss_u: 8.501e-02, Loss_r: 6.473e-03, Time: 0.05\n",
      "It: 10000, Loss: 8.955e-02, Loss_u: 7.879e-02, Loss_r: 1.076e-02, Time: 0.04\n",
      "constant_bcs_val: 7.105\n",
      "It: 10010, Loss: 1.013e-01, Loss_u: 8.695e-02, Loss_r: 1.432e-02, Time: 0.04\n",
      "It: 10020, Loss: 5.048e-02, Loss_u: 3.921e-02, Loss_r: 1.127e-02, Time: 0.08\n",
      "It: 10030, Loss: 8.850e-02, Loss_u: 7.656e-02, Loss_r: 1.194e-02, Time: 0.04\n",
      "It: 10040, Loss: 4.328e-02, Loss_u: 3.018e-02, Loss_r: 1.309e-02, Time: 0.04\n",
      "It: 10050, Loss: 1.165e-01, Loss_u: 1.056e-01, Loss_r: 1.089e-02, Time: 0.05\n",
      "It: 10060, Loss: 7.988e-02, Loss_u: 6.218e-02, Loss_r: 1.770e-02, Time: 0.05\n",
      "It: 10070, Loss: 7.709e-02, Loss_u: 7.360e-02, Loss_r: 3.490e-03, Time: 0.05\n",
      "It: 10080, Loss: 5.817e-02, Loss_u: 5.255e-02, Loss_r: 5.622e-03, Time: 0.07\n",
      "It: 10090, Loss: 6.582e-02, Loss_u: 5.801e-02, Loss_r: 7.810e-03, Time: 0.04\n",
      "It: 10100, Loss: 7.938e-02, Loss_u: 7.482e-02, Loss_r: 4.564e-03, Time: 0.05\n",
      "It: 10110, Loss: 6.795e-02, Loss_u: 5.377e-02, Loss_r: 1.418e-02, Time: 0.05\n",
      "It: 10120, Loss: 8.976e-02, Loss_u: 7.615e-02, Loss_r: 1.361e-02, Time: 0.05\n",
      "It: 10130, Loss: 6.089e-02, Loss_u: 4.144e-02, Loss_r: 1.945e-02, Time: 0.04\n",
      "It: 10140, Loss: 1.184e-01, Loss_u: 1.050e-01, Loss_r: 1.334e-02, Time: 0.05\n",
      "It: 10150, Loss: 7.304e-02, Loss_u: 5.900e-02, Loss_r: 1.404e-02, Time: 0.04\n",
      "It: 10160, Loss: 7.653e-02, Loss_u: 6.980e-02, Loss_r: 6.735e-03, Time: 0.05\n",
      "It: 10170, Loss: 6.105e-02, Loss_u: 5.582e-02, Loss_r: 5.233e-03, Time: 0.07\n",
      "It: 10180, Loss: 9.171e-02, Loss_u: 6.839e-02, Loss_r: 2.331e-02, Time: 0.06\n",
      "It: 10190, Loss: 7.438e-02, Loss_u: 5.780e-02, Loss_r: 1.659e-02, Time: 0.08\n",
      "It: 10200, Loss: 1.080e-01, Loss_u: 1.003e-01, Loss_r: 7.735e-03, Time: 0.05\n",
      "It: 10210, Loss: 9.024e-02, Loss_u: 8.072e-02, Loss_r: 9.523e-03, Time: 0.06\n",
      "It: 10220, Loss: 9.535e-02, Loss_u: 8.793e-02, Loss_r: 7.419e-03, Time: 0.05\n",
      "It: 10230, Loss: 8.498e-02, Loss_u: 7.564e-02, Loss_r: 9.343e-03, Time: 0.05\n",
      "It: 10240, Loss: 8.880e-02, Loss_u: 7.174e-02, Loss_r: 1.706e-02, Time: 0.06\n",
      "It: 10250, Loss: 8.121e-02, Loss_u: 7.293e-02, Loss_r: 8.281e-03, Time: 0.06\n",
      "It: 10260, Loss: 8.524e-02, Loss_u: 7.358e-02, Loss_r: 1.166e-02, Time: 0.05\n",
      "It: 10270, Loss: 9.440e-02, Loss_u: 8.696e-02, Loss_r: 7.446e-03, Time: 0.05\n",
      "It: 10280, Loss: 8.688e-02, Loss_u: 7.695e-02, Loss_r: 9.936e-03, Time: 0.04\n",
      "It: 10290, Loss: 5.883e-02, Loss_u: 5.160e-02, Loss_r: 7.229e-03, Time: 0.20\n",
      "It: 10300, Loss: 7.539e-02, Loss_u: 6.859e-02, Loss_r: 6.797e-03, Time: 0.05\n",
      "It: 10310, Loss: 9.992e-02, Loss_u: 8.760e-02, Loss_r: 1.232e-02, Time: 0.05\n",
      "It: 10320, Loss: 9.963e-02, Loss_u: 8.930e-02, Loss_r: 1.033e-02, Time: 0.06\n",
      "It: 10330, Loss: 9.227e-02, Loss_u: 8.780e-02, Loss_r: 4.470e-03, Time: 0.06\n",
      "It: 10340, Loss: 7.684e-02, Loss_u: 6.753e-02, Loss_r: 9.305e-03, Time: 0.04\n",
      "It: 10350, Loss: 5.768e-02, Loss_u: 5.047e-02, Loss_r: 7.212e-03, Time: 0.05\n",
      "It: 10360, Loss: 8.170e-02, Loss_u: 6.974e-02, Loss_r: 1.196e-02, Time: 0.04\n",
      "It: 10370, Loss: 9.471e-02, Loss_u: 6.867e-02, Loss_r: 2.603e-02, Time: 0.04\n",
      "It: 10380, Loss: 1.048e-01, Loss_u: 1.004e-01, Loss_r: 4.398e-03, Time: 0.07\n",
      "It: 10390, Loss: 1.286e-01, Loss_u: 1.169e-01, Loss_r: 1.167e-02, Time: 0.05\n",
      "It: 10400, Loss: 1.083e-01, Loss_u: 1.016e-01, Loss_r: 6.687e-03, Time: 0.06\n",
      "It: 10410, Loss: 7.495e-02, Loss_u: 6.972e-02, Loss_r: 5.233e-03, Time: 0.29\n",
      "It: 10420, Loss: 7.456e-02, Loss_u: 6.480e-02, Loss_r: 9.763e-03, Time: 0.08\n",
      "It: 10430, Loss: 6.223e-02, Loss_u: 5.464e-02, Loss_r: 7.590e-03, Time: 0.15\n",
      "It: 10440, Loss: 7.324e-02, Loss_u: 6.867e-02, Loss_r: 4.570e-03, Time: 0.05\n",
      "It: 10450, Loss: 5.625e-02, Loss_u: 3.510e-02, Loss_r: 2.115e-02, Time: 0.05\n",
      "It: 10460, Loss: 7.109e-02, Loss_u: 6.144e-02, Loss_r: 9.648e-03, Time: 0.04\n",
      "It: 10470, Loss: 7.632e-02, Loss_u: 6.351e-02, Loss_r: 1.281e-02, Time: 0.06\n",
      "It: 10480, Loss: 7.498e-02, Loss_u: 6.665e-02, Loss_r: 8.330e-03, Time: 0.04\n",
      "It: 10490, Loss: 5.161e-02, Loss_u: 4.758e-02, Loss_r: 4.034e-03, Time: 0.05\n",
      "It: 10500, Loss: 8.906e-02, Loss_u: 8.087e-02, Loss_r: 8.189e-03, Time: 0.05\n",
      "It: 10510, Loss: 1.069e-01, Loss_u: 9.649e-02, Loss_r: 1.045e-02, Time: 0.04\n",
      "It: 10520, Loss: 9.525e-02, Loss_u: 8.351e-02, Loss_r: 1.174e-02, Time: 0.05\n",
      "It: 10530, Loss: 9.074e-02, Loss_u: 7.839e-02, Loss_r: 1.235e-02, Time: 0.04\n",
      "It: 10540, Loss: 1.623e-01, Loss_u: 1.553e-01, Loss_r: 7.017e-03, Time: 0.06\n",
      "It: 10550, Loss: 7.204e-02, Loss_u: 4.890e-02, Loss_r: 2.313e-02, Time: 0.06\n",
      "It: 10560, Loss: 7.566e-02, Loss_u: 6.907e-02, Loss_r: 6.589e-03, Time: 0.40\n",
      "It: 10570, Loss: 5.090e-02, Loss_u: 4.486e-02, Loss_r: 6.031e-03, Time: 0.05\n",
      "It: 10580, Loss: 8.633e-02, Loss_u: 7.842e-02, Loss_r: 7.913e-03, Time: 0.18\n",
      "It: 10590, Loss: 7.444e-02, Loss_u: 6.697e-02, Loss_r: 7.462e-03, Time: 0.15\n",
      "It: 10600, Loss: 6.816e-02, Loss_u: 5.111e-02, Loss_r: 1.704e-02, Time: 0.05\n",
      "It: 10610, Loss: 7.628e-02, Loss_u: 6.531e-02, Loss_r: 1.097e-02, Time: 0.05\n",
      "It: 10620, Loss: 5.959e-02, Loss_u: 4.985e-02, Loss_r: 9.742e-03, Time: 0.05\n",
      "It: 10630, Loss: 4.728e-02, Loss_u: 3.825e-02, Loss_r: 9.030e-03, Time: 0.04\n",
      "It: 10640, Loss: 1.282e-01, Loss_u: 1.124e-01, Loss_r: 1.578e-02, Time: 0.05\n",
      "It: 10650, Loss: 6.534e-02, Loss_u: 5.775e-02, Loss_r: 7.588e-03, Time: 0.04\n",
      "It: 10660, Loss: 1.110e-01, Loss_u: 1.047e-01, Loss_r: 6.355e-03, Time: 0.06\n",
      "It: 10670, Loss: 6.521e-02, Loss_u: 4.552e-02, Loss_r: 1.969e-02, Time: 0.04\n",
      "It: 10680, Loss: 9.932e-02, Loss_u: 9.183e-02, Loss_r: 7.485e-03, Time: 0.05\n",
      "It: 10690, Loss: 7.982e-02, Loss_u: 7.408e-02, Loss_r: 5.732e-03, Time: 0.04\n",
      "It: 10700, Loss: 7.950e-02, Loss_u: 7.198e-02, Loss_r: 7.521e-03, Time: 0.06\n",
      "It: 10710, Loss: 6.548e-02, Loss_u: 5.823e-02, Loss_r: 7.248e-03, Time: 0.04\n",
      "It: 10720, Loss: 9.292e-02, Loss_u: 8.634e-02, Loss_r: 6.581e-03, Time: 0.05\n",
      "It: 10730, Loss: 1.006e-01, Loss_u: 8.659e-02, Loss_r: 1.403e-02, Time: 0.04\n",
      "It: 10740, Loss: 6.608e-02, Loss_u: 4.961e-02, Loss_r: 1.647e-02, Time: 0.05\n",
      "It: 10750, Loss: 7.191e-02, Loss_u: 6.350e-02, Loss_r: 8.408e-03, Time: 0.05\n",
      "It: 10760, Loss: 5.187e-02, Loss_u: 4.162e-02, Loss_r: 1.025e-02, Time: 0.05\n",
      "It: 10770, Loss: 5.920e-02, Loss_u: 5.367e-02, Loss_r: 5.533e-03, Time: 0.05\n",
      "It: 10780, Loss: 6.367e-02, Loss_u: 5.741e-02, Loss_r: 6.262e-03, Time: 0.04\n",
      "It: 10790, Loss: 1.141e-01, Loss_u: 6.945e-02, Loss_r: 4.470e-02, Time: 0.05\n",
      "It: 10800, Loss: 7.630e-02, Loss_u: 7.343e-02, Loss_r: 2.878e-03, Time: 0.09\n",
      "It: 10810, Loss: 7.382e-02, Loss_u: 6.514e-02, Loss_r: 8.689e-03, Time: 0.06\n",
      "It: 10820, Loss: 7.983e-02, Loss_u: 6.679e-02, Loss_r: 1.305e-02, Time: 0.04\n",
      "It: 10830, Loss: 9.091e-02, Loss_u: 8.226e-02, Loss_r: 8.643e-03, Time: 0.05\n",
      "It: 10840, Loss: 7.283e-02, Loss_u: 5.787e-02, Loss_r: 1.496e-02, Time: 0.05\n",
      "It: 10850, Loss: 7.262e-02, Loss_u: 6.920e-02, Loss_r: 3.427e-03, Time: 0.04\n",
      "It: 10860, Loss: 7.330e-02, Loss_u: 6.834e-02, Loss_r: 4.962e-03, Time: 0.05\n",
      "It: 10870, Loss: 5.341e-02, Loss_u: 4.743e-02, Loss_r: 5.979e-03, Time: 0.05\n",
      "It: 10880, Loss: 6.744e-02, Loss_u: 5.620e-02, Loss_r: 1.124e-02, Time: 0.05\n",
      "It: 10890, Loss: 9.011e-02, Loss_u: 7.333e-02, Loss_r: 1.678e-02, Time: 0.09\n",
      "It: 10900, Loss: 9.715e-02, Loss_u: 8.860e-02, Loss_r: 8.547e-03, Time: 0.04\n",
      "It: 10910, Loss: 9.741e-02, Loss_u: 9.045e-02, Loss_r: 6.955e-03, Time: 0.04\n",
      "It: 10920, Loss: 9.055e-02, Loss_u: 8.344e-02, Loss_r: 7.109e-03, Time: 0.06\n",
      "It: 10930, Loss: 1.221e-01, Loss_u: 1.130e-01, Loss_r: 9.161e-03, Time: 0.05\n",
      "It: 10940, Loss: 9.322e-02, Loss_u: 8.502e-02, Loss_r: 8.198e-03, Time: 0.07\n",
      "It: 10950, Loss: 8.113e-02, Loss_u: 7.309e-02, Loss_r: 8.040e-03, Time: 0.05\n",
      "It: 10960, Loss: 3.992e-02, Loss_u: 3.165e-02, Loss_r: 8.262e-03, Time: 0.05\n",
      "It: 10970, Loss: 8.076e-02, Loss_u: 6.454e-02, Loss_r: 1.621e-02, Time: 0.04\n",
      "It: 10980, Loss: 8.195e-02, Loss_u: 7.482e-02, Loss_r: 7.127e-03, Time: 0.05\n",
      "It: 10990, Loss: 6.989e-02, Loss_u: 6.184e-02, Loss_r: 8.053e-03, Time: 0.06\n",
      "It: 11000, Loss: 7.263e-02, Loss_u: 5.961e-02, Loss_r: 1.301e-02, Time: 0.06\n",
      "constant_bcs_val: 6.847\n",
      "It: 11010, Loss: 7.517e-02, Loss_u: 7.064e-02, Loss_r: 4.531e-03, Time: 0.05\n",
      "It: 11020, Loss: 1.072e-01, Loss_u: 1.001e-01, Loss_r: 7.099e-03, Time: 0.04\n",
      "It: 11030, Loss: 6.560e-02, Loss_u: 6.135e-02, Loss_r: 4.248e-03, Time: 0.04\n",
      "It: 11040, Loss: 8.912e-02, Loss_u: 8.353e-02, Loss_r: 5.592e-03, Time: 0.07\n",
      "It: 11050, Loss: 7.114e-02, Loss_u: 6.230e-02, Loss_r: 8.846e-03, Time: 0.04\n",
      "It: 11060, Loss: 1.148e-01, Loss_u: 8.801e-02, Loss_r: 2.680e-02, Time: 0.08\n",
      "It: 11070, Loss: 5.053e-02, Loss_u: 4.069e-02, Loss_r: 9.844e-03, Time: 0.04\n",
      "It: 11080, Loss: 7.941e-02, Loss_u: 6.737e-02, Loss_r: 1.204e-02, Time: 0.11\n",
      "It: 11090, Loss: 6.359e-02, Loss_u: 5.045e-02, Loss_r: 1.314e-02, Time: 0.08\n",
      "It: 11100, Loss: 7.023e-02, Loss_u: 6.378e-02, Loss_r: 6.455e-03, Time: 0.04\n",
      "It: 11110, Loss: 7.712e-02, Loss_u: 7.376e-02, Loss_r: 3.361e-03, Time: 0.04\n",
      "It: 11120, Loss: 7.624e-02, Loss_u: 6.789e-02, Loss_r: 8.347e-03, Time: 0.06\n",
      "It: 11130, Loss: 6.200e-02, Loss_u: 5.830e-02, Loss_r: 3.700e-03, Time: 0.07\n",
      "It: 11140, Loss: 1.141e-01, Loss_u: 1.019e-01, Loss_r: 1.220e-02, Time: 0.07\n",
      "It: 11150, Loss: 6.796e-02, Loss_u: 6.099e-02, Loss_r: 6.971e-03, Time: 0.04\n",
      "It: 11160, Loss: 5.374e-02, Loss_u: 5.120e-02, Loss_r: 2.542e-03, Time: 0.04\n",
      "It: 11170, Loss: 7.925e-02, Loss_u: 7.485e-02, Loss_r: 4.398e-03, Time: 0.04\n",
      "It: 11180, Loss: 7.626e-02, Loss_u: 6.762e-02, Loss_r: 8.644e-03, Time: 0.05\n",
      "It: 11190, Loss: 4.809e-02, Loss_u: 4.073e-02, Loss_r: 7.367e-03, Time: 0.04\n",
      "It: 11200, Loss: 6.103e-02, Loss_u: 5.022e-02, Loss_r: 1.082e-02, Time: 0.04\n",
      "It: 11210, Loss: 4.837e-02, Loss_u: 4.456e-02, Loss_r: 3.812e-03, Time: 0.05\n",
      "It: 11220, Loss: 7.957e-02, Loss_u: 7.337e-02, Loss_r: 6.193e-03, Time: 0.06\n",
      "It: 11230, Loss: 5.337e-02, Loss_u: 4.688e-02, Loss_r: 6.493e-03, Time: 0.04\n",
      "It: 11240, Loss: 6.863e-02, Loss_u: 6.010e-02, Loss_r: 8.538e-03, Time: 0.04\n",
      "It: 11250, Loss: 7.774e-02, Loss_u: 6.953e-02, Loss_r: 8.217e-03, Time: 0.06\n",
      "It: 11260, Loss: 8.352e-02, Loss_u: 6.277e-02, Loss_r: 2.075e-02, Time: 0.08\n",
      "It: 11270, Loss: 8.696e-02, Loss_u: 7.981e-02, Loss_r: 7.158e-03, Time: 0.04\n",
      "It: 11280, Loss: 5.183e-02, Loss_u: 3.883e-02, Loss_r: 1.299e-02, Time: 0.04\n",
      "It: 11290, Loss: 5.037e-02, Loss_u: 3.921e-02, Loss_r: 1.117e-02, Time: 0.07\n",
      "It: 11300, Loss: 7.401e-02, Loss_u: 6.247e-02, Loss_r: 1.154e-02, Time: 0.05\n",
      "It: 11310, Loss: 7.156e-02, Loss_u: 5.997e-02, Loss_r: 1.159e-02, Time: 0.04\n",
      "It: 11320, Loss: 6.712e-02, Loss_u: 6.149e-02, Loss_r: 5.631e-03, Time: 0.04\n",
      "It: 11330, Loss: 7.276e-02, Loss_u: 6.571e-02, Loss_r: 7.053e-03, Time: 0.04\n",
      "It: 11340, Loss: 6.265e-02, Loss_u: 5.435e-02, Loss_r: 8.299e-03, Time: 0.04\n",
      "It: 11350, Loss: 9.784e-02, Loss_u: 9.056e-02, Loss_r: 7.277e-03, Time: 0.04\n",
      "It: 11360, Loss: 6.970e-02, Loss_u: 6.327e-02, Loss_r: 6.425e-03, Time: 0.04\n",
      "It: 11370, Loss: 5.266e-02, Loss_u: 4.497e-02, Loss_r: 7.690e-03, Time: 0.04\n",
      "It: 11380, Loss: 8.434e-02, Loss_u: 6.660e-02, Loss_r: 1.774e-02, Time: 0.04\n",
      "It: 11390, Loss: 1.229e-01, Loss_u: 1.047e-01, Loss_r: 1.824e-02, Time: 0.05\n",
      "It: 11400, Loss: 8.551e-02, Loss_u: 7.441e-02, Loss_r: 1.110e-02, Time: 0.06\n",
      "It: 11410, Loss: 1.205e-01, Loss_u: 1.039e-01, Loss_r: 1.656e-02, Time: 0.05\n",
      "It: 11420, Loss: 5.143e-02, Loss_u: 4.469e-02, Loss_r: 6.741e-03, Time: 0.05\n",
      "It: 11430, Loss: 8.821e-02, Loss_u: 8.098e-02, Loss_r: 7.224e-03, Time: 0.04\n",
      "It: 11440, Loss: 7.352e-02, Loss_u: 6.836e-02, Loss_r: 5.164e-03, Time: 0.04\n",
      "It: 11450, Loss: 6.806e-02, Loss_u: 5.490e-02, Loss_r: 1.316e-02, Time: 0.05\n",
      "It: 11460, Loss: 7.383e-02, Loss_u: 7.102e-02, Loss_r: 2.803e-03, Time: 0.04\n",
      "It: 11470, Loss: 7.273e-02, Loss_u: 4.917e-02, Loss_r: 2.356e-02, Time: 0.07\n",
      "It: 11480, Loss: 8.462e-02, Loss_u: 7.857e-02, Loss_r: 6.054e-03, Time: 0.06\n",
      "It: 11490, Loss: 3.829e-02, Loss_u: 3.166e-02, Loss_r: 6.628e-03, Time: 0.04\n",
      "It: 11500, Loss: 2.989e-02, Loss_u: 2.568e-02, Loss_r: 4.214e-03, Time: 0.04\n",
      "It: 11510, Loss: 9.829e-02, Loss_u: 8.537e-02, Loss_r: 1.292e-02, Time: 0.05\n",
      "It: 11520, Loss: 9.122e-02, Loss_u: 8.141e-02, Loss_r: 9.808e-03, Time: 0.08\n",
      "It: 11530, Loss: 7.341e-02, Loss_u: 6.454e-02, Loss_r: 8.872e-03, Time: 0.05\n",
      "It: 11540, Loss: 7.927e-02, Loss_u: 6.650e-02, Loss_r: 1.276e-02, Time: 0.04\n",
      "It: 11550, Loss: 4.934e-02, Loss_u: 4.450e-02, Loss_r: 4.841e-03, Time: 0.05\n",
      "It: 11560, Loss: 7.774e-02, Loss_u: 7.328e-02, Loss_r: 4.459e-03, Time: 0.04\n",
      "It: 11570, Loss: 8.115e-02, Loss_u: 7.469e-02, Loss_r: 6.467e-03, Time: 0.04\n",
      "It: 11580, Loss: 5.843e-02, Loss_u: 4.468e-02, Loss_r: 1.375e-02, Time: 0.04\n",
      "It: 11590, Loss: 9.353e-02, Loss_u: 8.501e-02, Loss_r: 8.517e-03, Time: 0.05\n",
      "It: 11600, Loss: 5.574e-02, Loss_u: 4.668e-02, Loss_r: 9.062e-03, Time: 0.05\n",
      "It: 11610, Loss: 7.451e-02, Loss_u: 6.704e-02, Loss_r: 7.466e-03, Time: 0.04\n",
      "It: 11620, Loss: 3.877e-02, Loss_u: 2.931e-02, Loss_r: 9.466e-03, Time: 0.05\n",
      "It: 11630, Loss: 7.698e-02, Loss_u: 6.824e-02, Loss_r: 8.749e-03, Time: 0.04\n",
      "It: 11640, Loss: 8.686e-02, Loss_u: 8.060e-02, Loss_r: 6.260e-03, Time: 0.04\n",
      "It: 11650, Loss: 7.463e-02, Loss_u: 6.779e-02, Loss_r: 6.845e-03, Time: 0.05\n",
      "It: 11660, Loss: 7.597e-02, Loss_u: 6.736e-02, Loss_r: 8.609e-03, Time: 0.05\n",
      "It: 11670, Loss: 6.398e-02, Loss_u: 4.100e-02, Loss_r: 2.299e-02, Time: 0.06\n",
      "It: 11680, Loss: 4.545e-02, Loss_u: 4.070e-02, Loss_r: 4.751e-03, Time: 0.04\n",
      "It: 11690, Loss: 6.492e-02, Loss_u: 6.149e-02, Loss_r: 3.429e-03, Time: 0.08\n",
      "It: 11700, Loss: 7.502e-02, Loss_u: 5.672e-02, Loss_r: 1.830e-02, Time: 0.07\n",
      "It: 11710, Loss: 8.752e-02, Loss_u: 7.813e-02, Loss_r: 9.394e-03, Time: 0.05\n",
      "It: 11720, Loss: 3.612e-02, Loss_u: 2.692e-02, Loss_r: 9.195e-03, Time: 0.04\n",
      "It: 11730, Loss: 5.634e-02, Loss_u: 5.088e-02, Loss_r: 5.461e-03, Time: 0.05\n",
      "It: 11740, Loss: 7.899e-02, Loss_u: 6.631e-02, Loss_r: 1.268e-02, Time: 0.05\n",
      "It: 11750, Loss: 7.374e-02, Loss_u: 6.449e-02, Loss_r: 9.247e-03, Time: 0.05\n",
      "It: 11760, Loss: 7.796e-02, Loss_u: 6.634e-02, Loss_r: 1.162e-02, Time: 0.06\n",
      "It: 11770, Loss: 8.967e-02, Loss_u: 8.143e-02, Loss_r: 8.247e-03, Time: 0.05\n",
      "It: 11780, Loss: 6.006e-02, Loss_u: 5.153e-02, Loss_r: 8.537e-03, Time: 0.05\n",
      "It: 11790, Loss: 7.140e-02, Loss_u: 5.861e-02, Loss_r: 1.280e-02, Time: 0.05\n",
      "It: 11800, Loss: 6.954e-02, Loss_u: 5.728e-02, Loss_r: 1.226e-02, Time: 0.04\n",
      "It: 11810, Loss: 5.545e-02, Loss_u: 3.825e-02, Loss_r: 1.719e-02, Time: 0.05\n",
      "It: 11820, Loss: 7.016e-02, Loss_u: 5.837e-02, Loss_r: 1.179e-02, Time: 0.04\n",
      "It: 11830, Loss: 8.129e-02, Loss_u: 6.678e-02, Loss_r: 1.451e-02, Time: 0.05\n",
      "It: 11840, Loss: 6.790e-02, Loss_u: 6.211e-02, Loss_r: 5.792e-03, Time: 0.06\n",
      "It: 11850, Loss: 6.096e-02, Loss_u: 5.324e-02, Loss_r: 7.722e-03, Time: 0.04\n",
      "It: 11860, Loss: 6.724e-02, Loss_u: 6.390e-02, Loss_r: 3.346e-03, Time: 0.05\n",
      "It: 11870, Loss: 7.688e-02, Loss_u: 6.801e-02, Loss_r: 8.869e-03, Time: 0.05\n",
      "It: 11880, Loss: 7.271e-02, Loss_u: 6.880e-02, Loss_r: 3.908e-03, Time: 0.04\n",
      "It: 11890, Loss: 9.387e-02, Loss_u: 8.429e-02, Loss_r: 9.579e-03, Time: 0.05\n",
      "It: 11900, Loss: 7.193e-02, Loss_u: 6.461e-02, Loss_r: 7.316e-03, Time: 0.04\n",
      "It: 11910, Loss: 1.004e-01, Loss_u: 9.534e-02, Loss_r: 5.046e-03, Time: 0.05\n",
      "It: 11920, Loss: 8.911e-02, Loss_u: 8.066e-02, Loss_r: 8.449e-03, Time: 0.05\n",
      "It: 11930, Loss: 5.446e-02, Loss_u: 4.003e-02, Loss_r: 1.444e-02, Time: 0.05\n",
      "It: 11940, Loss: 7.090e-02, Loss_u: 6.704e-02, Loss_r: 3.857e-03, Time: 0.05\n",
      "It: 11950, Loss: 8.917e-02, Loss_u: 8.408e-02, Loss_r: 5.088e-03, Time: 0.04\n",
      "It: 11960, Loss: 8.047e-02, Loss_u: 7.421e-02, Loss_r: 6.255e-03, Time: 0.05\n",
      "It: 11970, Loss: 3.893e-02, Loss_u: 3.236e-02, Loss_r: 6.570e-03, Time: 0.05\n",
      "It: 11980, Loss: 5.277e-02, Loss_u: 4.885e-02, Loss_r: 3.923e-03, Time: 0.05\n",
      "It: 11990, Loss: 8.474e-02, Loss_u: 7.902e-02, Loss_r: 5.721e-03, Time: 0.05\n",
      "It: 12000, Loss: 4.241e-02, Loss_u: 3.801e-02, Loss_r: 4.405e-03, Time: 0.05\n",
      "constant_bcs_val: 7.261\n",
      "It: 12010, Loss: 5.836e-02, Loss_u: 4.740e-02, Loss_r: 1.096e-02, Time: 0.05\n",
      "It: 12020, Loss: 5.225e-02, Loss_u: 4.470e-02, Loss_r: 7.545e-03, Time: 0.05\n",
      "It: 12030, Loss: 7.585e-02, Loss_u: 6.736e-02, Loss_r: 8.492e-03, Time: 0.05\n",
      "It: 12040, Loss: 1.086e-01, Loss_u: 9.897e-02, Loss_r: 9.634e-03, Time: 0.05\n",
      "It: 12050, Loss: 9.359e-02, Loss_u: 8.426e-02, Loss_r: 9.334e-03, Time: 0.06\n",
      "It: 12060, Loss: 4.510e-02, Loss_u: 3.922e-02, Loss_r: 5.880e-03, Time: 0.06\n",
      "It: 12070, Loss: 7.870e-02, Loss_u: 6.540e-02, Loss_r: 1.330e-02, Time: 0.05\n",
      "It: 12080, Loss: 8.896e-02, Loss_u: 8.538e-02, Loss_r: 3.581e-03, Time: 0.05\n",
      "It: 12090, Loss: 6.711e-02, Loss_u: 6.288e-02, Loss_r: 4.231e-03, Time: 0.05\n",
      "It: 12100, Loss: 9.521e-02, Loss_u: 8.538e-02, Loss_r: 9.833e-03, Time: 0.07\n",
      "It: 12110, Loss: 8.303e-02, Loss_u: 7.646e-02, Loss_r: 6.572e-03, Time: 0.05\n",
      "It: 12120, Loss: 6.029e-02, Loss_u: 4.747e-02, Loss_r: 1.281e-02, Time: 0.12\n",
      "It: 12130, Loss: 6.870e-02, Loss_u: 4.763e-02, Loss_r: 2.107e-02, Time: 0.04\n",
      "It: 12140, Loss: 6.468e-02, Loss_u: 5.730e-02, Loss_r: 7.384e-03, Time: 0.05\n",
      "It: 12150, Loss: 8.137e-02, Loss_u: 6.794e-02, Loss_r: 1.343e-02, Time: 0.04\n",
      "It: 12160, Loss: 7.719e-02, Loss_u: 7.199e-02, Loss_r: 5.197e-03, Time: 0.05\n",
      "It: 12170, Loss: 3.160e-02, Loss_u: 2.759e-02, Loss_r: 4.014e-03, Time: 0.06\n",
      "It: 12180, Loss: 7.500e-02, Loss_u: 6.341e-02, Loss_r: 1.159e-02, Time: 0.05\n",
      "It: 12190, Loss: 5.772e-02, Loss_u: 5.477e-02, Loss_r: 2.952e-03, Time: 0.05\n",
      "It: 12200, Loss: 7.186e-02, Loss_u: 6.618e-02, Loss_r: 5.684e-03, Time: 0.05\n",
      "It: 12210, Loss: 8.374e-02, Loss_u: 7.504e-02, Loss_r: 8.699e-03, Time: 0.05\n",
      "It: 12220, Loss: 5.800e-02, Loss_u: 5.413e-02, Loss_r: 3.865e-03, Time: 0.05\n",
      "It: 12230, Loss: 6.448e-02, Loss_u: 5.981e-02, Loss_r: 4.665e-03, Time: 0.05\n",
      "It: 12240, Loss: 4.753e-02, Loss_u: 4.327e-02, Loss_r: 4.267e-03, Time: 0.07\n",
      "It: 12250, Loss: 7.835e-02, Loss_u: 6.725e-02, Loss_r: 1.109e-02, Time: 0.05\n",
      "It: 12260, Loss: 7.740e-02, Loss_u: 7.215e-02, Loss_r: 5.250e-03, Time: 0.06\n",
      "It: 12270, Loss: 7.458e-02, Loss_u: 5.959e-02, Loss_r: 1.499e-02, Time: 0.05\n",
      "It: 12280, Loss: 1.016e-01, Loss_u: 8.366e-02, Loss_r: 1.798e-02, Time: 0.05\n",
      "It: 12290, Loss: 7.083e-02, Loss_u: 5.581e-02, Loss_r: 1.502e-02, Time: 0.05\n",
      "It: 12300, Loss: 7.376e-02, Loss_u: 6.434e-02, Loss_r: 9.418e-03, Time: 0.09\n",
      "It: 12310, Loss: 8.317e-02, Loss_u: 7.658e-02, Loss_r: 6.581e-03, Time: 0.05\n",
      "It: 12320, Loss: 9.229e-02, Loss_u: 8.715e-02, Loss_r: 5.146e-03, Time: 0.07\n",
      "It: 12330, Loss: 9.493e-02, Loss_u: 7.846e-02, Loss_r: 1.647e-02, Time: 0.05\n",
      "It: 12340, Loss: 5.149e-02, Loss_u: 4.490e-02, Loss_r: 6.585e-03, Time: 0.06\n",
      "It: 12350, Loss: 6.497e-02, Loss_u: 6.074e-02, Loss_r: 4.227e-03, Time: 0.05\n",
      "It: 12360, Loss: 7.300e-02, Loss_u: 6.954e-02, Loss_r: 3.462e-03, Time: 0.06\n",
      "It: 12370, Loss: 9.880e-02, Loss_u: 9.344e-02, Loss_r: 5.369e-03, Time: 0.05\n",
      "It: 12380, Loss: 1.112e-01, Loss_u: 7.297e-02, Loss_r: 3.822e-02, Time: 0.05\n",
      "It: 12390, Loss: 7.187e-02, Loss_u: 6.451e-02, Loss_r: 7.361e-03, Time: 0.06\n",
      "It: 12400, Loss: 6.039e-02, Loss_u: 4.812e-02, Loss_r: 1.227e-02, Time: 0.05\n",
      "It: 12410, Loss: 1.344e-01, Loss_u: 9.380e-02, Loss_r: 4.060e-02, Time: 0.05\n",
      "It: 12420, Loss: 9.972e-02, Loss_u: 7.594e-02, Loss_r: 2.377e-02, Time: 0.07\n",
      "It: 12430, Loss: 6.777e-02, Loss_u: 5.769e-02, Loss_r: 1.008e-02, Time: 0.06\n",
      "It: 12440, Loss: 5.609e-02, Loss_u: 5.086e-02, Loss_r: 5.235e-03, Time: 0.05\n",
      "It: 12450, Loss: 8.084e-02, Loss_u: 4.964e-02, Loss_r: 3.120e-02, Time: 0.06\n",
      "It: 12460, Loss: 1.237e-01, Loss_u: 1.064e-01, Loss_r: 1.732e-02, Time: 0.05\n",
      "It: 12470, Loss: 1.214e-01, Loss_u: 1.032e-01, Loss_r: 1.815e-02, Time: 0.05\n",
      "It: 12480, Loss: 1.031e-01, Loss_u: 9.680e-02, Loss_r: 6.292e-03, Time: 0.05\n",
      "It: 12490, Loss: 8.066e-02, Loss_u: 7.243e-02, Loss_r: 8.232e-03, Time: 0.05\n",
      "It: 12500, Loss: 7.423e-02, Loss_u: 6.488e-02, Loss_r: 9.349e-03, Time: 0.06\n",
      "It: 12510, Loss: 5.942e-02, Loss_u: 5.259e-02, Loss_r: 6.835e-03, Time: 0.06\n",
      "It: 12520, Loss: 1.110e-01, Loss_u: 9.320e-02, Loss_r: 1.778e-02, Time: 0.05\n",
      "It: 12530, Loss: 7.354e-02, Loss_u: 6.094e-02, Loss_r: 1.260e-02, Time: 0.05\n",
      "It: 12540, Loss: 7.741e-02, Loss_u: 6.438e-02, Loss_r: 1.304e-02, Time: 0.04\n",
      "It: 12550, Loss: 6.827e-02, Loss_u: 5.214e-02, Loss_r: 1.613e-02, Time: 0.05\n",
      "It: 12560, Loss: 5.277e-02, Loss_u: 4.809e-02, Loss_r: 4.677e-03, Time: 0.04\n",
      "It: 12570, Loss: 7.617e-02, Loss_u: 6.569e-02, Loss_r: 1.049e-02, Time: 0.04\n",
      "It: 12580, Loss: 4.960e-02, Loss_u: 3.264e-02, Loss_r: 1.696e-02, Time: 0.04\n",
      "It: 12590, Loss: 7.161e-02, Loss_u: 6.452e-02, Loss_r: 7.090e-03, Time: 0.04\n",
      "It: 12600, Loss: 6.075e-02, Loss_u: 5.822e-02, Loss_r: 2.528e-03, Time: 0.05\n",
      "It: 12610, Loss: 8.070e-02, Loss_u: 6.249e-02, Loss_r: 1.822e-02, Time: 0.06\n",
      "It: 12620, Loss: 9.992e-02, Loss_u: 9.156e-02, Loss_r: 8.360e-03, Time: 0.05\n",
      "It: 12630, Loss: 6.931e-02, Loss_u: 6.123e-02, Loss_r: 8.085e-03, Time: 0.04\n",
      "It: 12640, Loss: 7.049e-02, Loss_u: 6.253e-02, Loss_r: 7.954e-03, Time: 0.04\n",
      "It: 12650, Loss: 6.795e-02, Loss_u: 6.188e-02, Loss_r: 6.076e-03, Time: 0.06\n",
      "It: 12660, Loss: 4.787e-02, Loss_u: 4.350e-02, Loss_r: 4.372e-03, Time: 0.07\n",
      "It: 12670, Loss: 9.227e-02, Loss_u: 8.523e-02, Loss_r: 7.032e-03, Time: 0.07\n",
      "It: 12680, Loss: 9.334e-02, Loss_u: 8.528e-02, Loss_r: 8.059e-03, Time: 0.04\n",
      "It: 12690, Loss: 4.839e-02, Loss_u: 4.177e-02, Loss_r: 6.621e-03, Time: 0.04\n",
      "It: 12700, Loss: 6.244e-02, Loss_u: 5.602e-02, Loss_r: 6.427e-03, Time: 0.04\n",
      "It: 12710, Loss: 8.720e-02, Loss_u: 7.984e-02, Loss_r: 7.356e-03, Time: 0.04\n",
      "It: 12720, Loss: 3.087e-02, Loss_u: 2.818e-02, Loss_r: 2.687e-03, Time: 0.05\n",
      "It: 12730, Loss: 5.575e-02, Loss_u: 5.253e-02, Loss_r: 3.223e-03, Time: 0.05\n",
      "It: 12740, Loss: 8.376e-02, Loss_u: 7.872e-02, Loss_r: 5.044e-03, Time: 0.04\n",
      "It: 12750, Loss: 5.471e-02, Loss_u: 5.143e-02, Loss_r: 3.281e-03, Time: 0.05\n",
      "It: 12760, Loss: 9.580e-02, Loss_u: 7.323e-02, Loss_r: 2.258e-02, Time: 0.04\n",
      "It: 12770, Loss: 4.807e-02, Loss_u: 4.203e-02, Loss_r: 6.045e-03, Time: 0.05\n",
      "It: 12780, Loss: 9.042e-02, Loss_u: 7.542e-02, Loss_r: 1.500e-02, Time: 0.04\n",
      "It: 12790, Loss: 4.266e-02, Loss_u: 3.022e-02, Loss_r: 1.243e-02, Time: 0.05\n",
      "It: 12800, Loss: 7.692e-02, Loss_u: 7.097e-02, Loss_r: 5.951e-03, Time: 0.04\n",
      "It: 12810, Loss: 7.131e-02, Loss_u: 6.716e-02, Loss_r: 4.154e-03, Time: 0.06\n",
      "It: 12820, Loss: 4.798e-02, Loss_u: 4.275e-02, Loss_r: 5.227e-03, Time: 0.04\n",
      "It: 12830, Loss: 3.417e-02, Loss_u: 2.752e-02, Loss_r: 6.649e-03, Time: 0.04\n",
      "It: 12840, Loss: 6.232e-02, Loss_u: 5.987e-02, Loss_r: 2.441e-03, Time: 0.05\n",
      "It: 12850, Loss: 6.653e-02, Loss_u: 6.136e-02, Loss_r: 5.170e-03, Time: 0.07\n",
      "It: 12860, Loss: 5.848e-02, Loss_u: 5.284e-02, Loss_r: 5.640e-03, Time: 0.05\n",
      "It: 12870, Loss: 7.566e-02, Loss_u: 6.559e-02, Loss_r: 1.007e-02, Time: 0.04\n",
      "It: 12880, Loss: 8.298e-02, Loss_u: 7.844e-02, Loss_r: 4.546e-03, Time: 0.04\n",
      "It: 12890, Loss: 9.113e-02, Loss_u: 7.791e-02, Loss_r: 1.323e-02, Time: 0.04\n",
      "It: 12900, Loss: 4.995e-02, Loss_u: 4.357e-02, Loss_r: 6.373e-03, Time: 0.05\n",
      "It: 12910, Loss: 7.540e-02, Loss_u: 6.386e-02, Loss_r: 1.154e-02, Time: 0.04\n",
      "It: 12920, Loss: 6.457e-02, Loss_u: 5.668e-02, Loss_r: 7.888e-03, Time: 0.04\n",
      "It: 12930, Loss: 7.122e-02, Loss_u: 6.624e-02, Loss_r: 4.984e-03, Time: 0.04\n",
      "It: 12940, Loss: 7.425e-02, Loss_u: 6.762e-02, Loss_r: 6.626e-03, Time: 0.05\n",
      "It: 12950, Loss: 9.663e-02, Loss_u: 8.503e-02, Loss_r: 1.160e-02, Time: 0.05\n",
      "It: 12960, Loss: 9.159e-02, Loss_u: 8.604e-02, Loss_r: 5.541e-03, Time: 0.05\n",
      "It: 12970, Loss: 5.856e-02, Loss_u: 4.911e-02, Loss_r: 9.455e-03, Time: 0.04\n",
      "It: 12980, Loss: 2.601e-02, Loss_u: 2.290e-02, Loss_r: 3.113e-03, Time: 0.05\n",
      "It: 12990, Loss: 7.506e-02, Loss_u: 6.477e-02, Loss_r: 1.029e-02, Time: 0.06\n",
      "It: 13000, Loss: 1.036e-01, Loss_u: 9.683e-02, Loss_r: 6.740e-03, Time: 0.05\n",
      "constant_bcs_val: 6.824\n",
      "It: 13010, Loss: 4.662e-02, Loss_u: 3.937e-02, Loss_r: 7.253e-03, Time: 0.05\n",
      "It: 13020, Loss: 7.825e-02, Loss_u: 6.882e-02, Loss_r: 9.434e-03, Time: 0.04\n",
      "It: 13030, Loss: 6.880e-02, Loss_u: 6.330e-02, Loss_r: 5.498e-03, Time: 0.05\n",
      "It: 13040, Loss: 7.979e-02, Loss_u: 6.808e-02, Loss_r: 1.171e-02, Time: 0.05\n",
      "It: 13050, Loss: 6.531e-02, Loss_u: 5.387e-02, Loss_r: 1.144e-02, Time: 0.25\n",
      "It: 13060, Loss: 5.031e-02, Loss_u: 4.408e-02, Loss_r: 6.227e-03, Time: 0.07\n",
      "It: 13070, Loss: 7.944e-02, Loss_u: 7.490e-02, Loss_r: 4.538e-03, Time: 0.04\n",
      "It: 13080, Loss: 9.212e-02, Loss_u: 8.598e-02, Loss_r: 6.140e-03, Time: 0.05\n",
      "It: 13090, Loss: 7.465e-02, Loss_u: 6.512e-02, Loss_r: 9.530e-03, Time: 0.08\n",
      "It: 13100, Loss: 6.348e-02, Loss_u: 5.580e-02, Loss_r: 7.682e-03, Time: 0.06\n",
      "It: 13110, Loss: 6.169e-02, Loss_u: 5.229e-02, Loss_r: 9.398e-03, Time: 0.04\n",
      "It: 13120, Loss: 1.115e-01, Loss_u: 1.008e-01, Loss_r: 1.069e-02, Time: 0.05\n",
      "It: 13130, Loss: 1.064e-01, Loss_u: 9.112e-02, Loss_r: 1.525e-02, Time: 0.04\n",
      "It: 13140, Loss: 5.136e-02, Loss_u: 4.681e-02, Loss_r: 4.546e-03, Time: 0.05\n",
      "It: 13150, Loss: 9.250e-02, Loss_u: 7.824e-02, Loss_r: 1.426e-02, Time: 0.07\n",
      "It: 13160, Loss: 7.562e-02, Loss_u: 5.255e-02, Loss_r: 2.308e-02, Time: 0.05\n",
      "It: 13170, Loss: 6.314e-02, Loss_u: 5.228e-02, Loss_r: 1.085e-02, Time: 0.06\n",
      "It: 13180, Loss: 9.982e-02, Loss_u: 8.497e-02, Loss_r: 1.485e-02, Time: 0.06\n",
      "It: 13190, Loss: 7.277e-02, Loss_u: 5.746e-02, Loss_r: 1.531e-02, Time: 0.05\n",
      "It: 13200, Loss: 4.183e-02, Loss_u: 3.753e-02, Loss_r: 4.300e-03, Time: 0.05\n",
      "It: 13210, Loss: 4.907e-02, Loss_u: 4.641e-02, Loss_r: 2.660e-03, Time: 0.08\n",
      "It: 13220, Loss: 8.848e-02, Loss_u: 7.763e-02, Loss_r: 1.085e-02, Time: 0.06\n",
      "It: 13230, Loss: 6.002e-02, Loss_u: 5.093e-02, Loss_r: 9.094e-03, Time: 0.05\n",
      "It: 13240, Loss: 9.273e-02, Loss_u: 8.169e-02, Loss_r: 1.105e-02, Time: 0.04\n",
      "It: 13250, Loss: 4.590e-02, Loss_u: 4.004e-02, Loss_r: 5.860e-03, Time: 0.05\n",
      "It: 13260, Loss: 8.631e-02, Loss_u: 7.530e-02, Loss_r: 1.101e-02, Time: 0.04\n",
      "It: 13270, Loss: 8.380e-02, Loss_u: 8.018e-02, Loss_r: 3.625e-03, Time: 0.05\n",
      "It: 13280, Loss: 7.791e-02, Loss_u: 7.102e-02, Loss_r: 6.896e-03, Time: 0.08\n",
      "It: 13290, Loss: 5.214e-02, Loss_u: 4.280e-02, Loss_r: 9.334e-03, Time: 0.04\n",
      "It: 13300, Loss: 4.817e-02, Loss_u: 4.062e-02, Loss_r: 7.555e-03, Time: 0.04\n",
      "It: 13310, Loss: 8.334e-02, Loss_u: 7.257e-02, Loss_r: 1.077e-02, Time: 0.04\n",
      "It: 13320, Loss: 7.659e-02, Loss_u: 6.865e-02, Loss_r: 7.941e-03, Time: 0.04\n",
      "It: 13330, Loss: 6.448e-02, Loss_u: 4.301e-02, Loss_r: 2.146e-02, Time: 0.04\n",
      "It: 13340, Loss: 5.006e-02, Loss_u: 4.011e-02, Loss_r: 9.947e-03, Time: 0.04\n",
      "It: 13350, Loss: 5.595e-02, Loss_u: 4.990e-02, Loss_r: 6.052e-03, Time: 0.07\n",
      "It: 13360, Loss: 6.772e-02, Loss_u: 5.172e-02, Loss_r: 1.600e-02, Time: 0.04\n",
      "It: 13370, Loss: 8.740e-02, Loss_u: 7.202e-02, Loss_r: 1.538e-02, Time: 0.04\n",
      "It: 13380, Loss: 8.589e-02, Loss_u: 6.566e-02, Loss_r: 2.023e-02, Time: 0.05\n",
      "It: 13390, Loss: 9.649e-02, Loss_u: 8.870e-02, Loss_r: 7.795e-03, Time: 0.04\n",
      "It: 13400, Loss: 1.269e-01, Loss_u: 9.423e-02, Loss_r: 3.268e-02, Time: 0.04\n",
      "It: 13410, Loss: 8.686e-02, Loss_u: 7.511e-02, Loss_r: 1.175e-02, Time: 0.05\n",
      "It: 13420, Loss: 7.737e-02, Loss_u: 6.862e-02, Loss_r: 8.745e-03, Time: 0.04\n",
      "It: 13430, Loss: 7.394e-02, Loss_u: 6.876e-02, Loss_r: 5.179e-03, Time: 0.04\n",
      "It: 13440, Loss: 3.438e-02, Loss_u: 2.749e-02, Loss_r: 6.887e-03, Time: 0.04\n",
      "It: 13450, Loss: 7.020e-02, Loss_u: 6.514e-02, Loss_r: 5.056e-03, Time: 0.04\n",
      "It: 13460, Loss: 6.803e-02, Loss_u: 4.115e-02, Loss_r: 2.689e-02, Time: 0.04\n",
      "It: 13470, Loss: 5.568e-02, Loss_u: 4.799e-02, Loss_r: 7.691e-03, Time: 0.05\n",
      "It: 13480, Loss: 6.376e-02, Loss_u: 5.927e-02, Loss_r: 4.496e-03, Time: 0.05\n",
      "It: 13490, Loss: 8.807e-02, Loss_u: 7.090e-02, Loss_r: 1.717e-02, Time: 0.05\n",
      "It: 13500, Loss: 9.191e-02, Loss_u: 8.616e-02, Loss_r: 5.750e-03, Time: 0.04\n",
      "It: 13510, Loss: 8.887e-02, Loss_u: 8.377e-02, Loss_r: 5.094e-03, Time: 0.05\n",
      "It: 13520, Loss: 7.402e-02, Loss_u: 6.898e-02, Loss_r: 5.039e-03, Time: 0.04\n",
      "It: 13530, Loss: 4.884e-02, Loss_u: 4.417e-02, Loss_r: 4.666e-03, Time: 0.05\n",
      "It: 13540, Loss: 8.003e-02, Loss_u: 6.241e-02, Loss_r: 1.762e-02, Time: 0.05\n",
      "It: 13550, Loss: 7.148e-02, Loss_u: 6.707e-02, Loss_r: 4.401e-03, Time: 0.05\n",
      "It: 13560, Loss: 3.924e-02, Loss_u: 3.396e-02, Loss_r: 5.287e-03, Time: 0.04\n",
      "It: 13570, Loss: 8.107e-02, Loss_u: 7.299e-02, Loss_r: 8.082e-03, Time: 0.04\n",
      "It: 13580, Loss: 5.390e-02, Loss_u: 4.583e-02, Loss_r: 8.071e-03, Time: 0.05\n",
      "It: 13590, Loss: 5.040e-02, Loss_u: 3.978e-02, Loss_r: 1.062e-02, Time: 0.04\n",
      "It: 13600, Loss: 6.073e-02, Loss_u: 5.297e-02, Loss_r: 7.758e-03, Time: 0.05\n",
      "It: 13610, Loss: 6.284e-02, Loss_u: 5.789e-02, Loss_r: 4.944e-03, Time: 0.06\n",
      "It: 13620, Loss: 6.138e-02, Loss_u: 5.524e-02, Loss_r: 6.143e-03, Time: 0.04\n",
      "It: 13630, Loss: 7.858e-02, Loss_u: 6.492e-02, Loss_r: 1.365e-02, Time: 0.05\n",
      "It: 13640, Loss: 1.017e-01, Loss_u: 5.160e-02, Loss_r: 5.007e-02, Time: 0.07\n",
      "It: 13650, Loss: 8.299e-02, Loss_u: 6.034e-02, Loss_r: 2.265e-02, Time: 0.07\n",
      "It: 13660, Loss: 6.454e-02, Loss_u: 4.763e-02, Loss_r: 1.691e-02, Time: 0.15\n",
      "It: 13670, Loss: 6.434e-02, Loss_u: 5.917e-02, Loss_r: 5.167e-03, Time: 0.05\n",
      "It: 13680, Loss: 6.795e-02, Loss_u: 6.112e-02, Loss_r: 6.831e-03, Time: 0.06\n",
      "It: 13690, Loss: 7.287e-02, Loss_u: 6.800e-02, Loss_r: 4.868e-03, Time: 0.06\n",
      "It: 13700, Loss: 8.663e-02, Loss_u: 8.278e-02, Loss_r: 3.854e-03, Time: 0.05\n",
      "It: 13710, Loss: 9.571e-02, Loss_u: 8.171e-02, Loss_r: 1.399e-02, Time: 0.05\n",
      "It: 13720, Loss: 8.300e-02, Loss_u: 7.262e-02, Loss_r: 1.039e-02, Time: 0.05\n",
      "It: 13730, Loss: 6.304e-02, Loss_u: 5.533e-02, Loss_r: 7.715e-03, Time: 0.06\n",
      "It: 13740, Loss: 4.824e-02, Loss_u: 2.135e-02, Loss_r: 2.689e-02, Time: 0.04\n",
      "It: 13750, Loss: 6.958e-02, Loss_u: 6.263e-02, Loss_r: 6.948e-03, Time: 0.05\n",
      "It: 13760, Loss: 7.867e-02, Loss_u: 7.191e-02, Loss_r: 6.759e-03, Time: 0.05\n",
      "It: 13770, Loss: 7.184e-02, Loss_u: 5.919e-02, Loss_r: 1.266e-02, Time: 0.05\n",
      "It: 13780, Loss: 4.449e-02, Loss_u: 3.822e-02, Loss_r: 6.265e-03, Time: 0.06\n",
      "It: 13790, Loss: 9.380e-02, Loss_u: 8.283e-02, Loss_r: 1.098e-02, Time: 0.05\n",
      "It: 13800, Loss: 7.335e-02, Loss_u: 6.529e-02, Loss_r: 8.058e-03, Time: 0.06\n",
      "It: 13810, Loss: 5.893e-02, Loss_u: 4.771e-02, Loss_r: 1.122e-02, Time: 0.05\n",
      "It: 13820, Loss: 6.048e-02, Loss_u: 5.634e-02, Loss_r: 4.142e-03, Time: 0.06\n",
      "It: 13830, Loss: 6.128e-02, Loss_u: 5.618e-02, Loss_r: 5.107e-03, Time: 0.05\n",
      "It: 13840, Loss: 6.042e-02, Loss_u: 5.393e-02, Loss_r: 6.486e-03, Time: 0.04\n",
      "It: 13850, Loss: 7.356e-02, Loss_u: 6.787e-02, Loss_r: 5.691e-03, Time: 0.10\n",
      "It: 13860, Loss: 4.724e-02, Loss_u: 3.533e-02, Loss_r: 1.191e-02, Time: 0.05\n",
      "It: 13870, Loss: 7.343e-02, Loss_u: 6.781e-02, Loss_r: 5.619e-03, Time: 0.05\n",
      "It: 13880, Loss: 7.707e-02, Loss_u: 7.239e-02, Loss_r: 4.675e-03, Time: 0.07\n",
      "It: 13890, Loss: 7.830e-02, Loss_u: 6.376e-02, Loss_r: 1.454e-02, Time: 0.05\n",
      "It: 13900, Loss: 6.066e-02, Loss_u: 5.772e-02, Loss_r: 2.932e-03, Time: 0.05\n",
      "It: 13910, Loss: 5.881e-02, Loss_u: 4.939e-02, Loss_r: 9.421e-03, Time: 0.05\n",
      "It: 13920, Loss: 9.416e-02, Loss_u: 8.704e-02, Loss_r: 7.119e-03, Time: 0.05\n",
      "It: 13930, Loss: 7.325e-02, Loss_u: 6.941e-02, Loss_r: 3.837e-03, Time: 0.05\n",
      "It: 13940, Loss: 3.447e-02, Loss_u: 3.028e-02, Loss_r: 4.191e-03, Time: 0.06\n",
      "It: 13950, Loss: 4.538e-02, Loss_u: 4.178e-02, Loss_r: 3.599e-03, Time: 0.06\n",
      "It: 13960, Loss: 1.061e-01, Loss_u: 9.963e-02, Loss_r: 6.516e-03, Time: 0.06\n",
      "It: 13970, Loss: 5.345e-02, Loss_u: 4.807e-02, Loss_r: 5.380e-03, Time: 0.05\n",
      "It: 13980, Loss: 9.196e-02, Loss_u: 7.811e-02, Loss_r: 1.385e-02, Time: 0.05\n",
      "It: 13990, Loss: 8.912e-02, Loss_u: 7.708e-02, Loss_r: 1.204e-02, Time: 0.06\n",
      "It: 14000, Loss: 5.569e-02, Loss_u: 4.835e-02, Loss_r: 7.335e-03, Time: 0.06\n",
      "constant_bcs_val: 6.704\n",
      "It: 14010, Loss: 4.471e-02, Loss_u: 3.479e-02, Loss_r: 9.916e-03, Time: 0.08\n",
      "It: 14020, Loss: 6.625e-02, Loss_u: 6.293e-02, Loss_r: 3.328e-03, Time: 0.08\n",
      "It: 14030, Loss: 7.938e-02, Loss_u: 7.193e-02, Loss_r: 7.444e-03, Time: 0.06\n",
      "It: 14040, Loss: 4.278e-02, Loss_u: 3.720e-02, Loss_r: 5.579e-03, Time: 0.06\n",
      "It: 14050, Loss: 6.786e-02, Loss_u: 6.379e-02, Loss_r: 4.069e-03, Time: 0.08\n",
      "It: 14060, Loss: 3.516e-02, Loss_u: 3.286e-02, Loss_r: 2.306e-03, Time: 0.05\n",
      "It: 14070, Loss: 9.210e-02, Loss_u: 7.631e-02, Loss_r: 1.579e-02, Time: 0.07\n",
      "It: 14080, Loss: 6.812e-02, Loss_u: 6.107e-02, Loss_r: 7.049e-03, Time: 0.06\n",
      "It: 14090, Loss: 4.794e-02, Loss_u: 4.222e-02, Loss_r: 5.716e-03, Time: 0.05\n",
      "It: 14100, Loss: 5.598e-02, Loss_u: 5.076e-02, Loss_r: 5.227e-03, Time: 0.07\n",
      "It: 14110, Loss: 5.362e-02, Loss_u: 4.838e-02, Loss_r: 5.241e-03, Time: 0.07\n",
      "It: 14120, Loss: 7.652e-02, Loss_u: 6.751e-02, Loss_r: 9.004e-03, Time: 0.05\n",
      "It: 14130, Loss: 7.762e-02, Loss_u: 7.288e-02, Loss_r: 4.741e-03, Time: 0.05\n",
      "It: 14140, Loss: 6.242e-02, Loss_u: 5.775e-02, Loss_r: 4.668e-03, Time: 0.05\n",
      "It: 14150, Loss: 6.388e-02, Loss_u: 4.804e-02, Loss_r: 1.584e-02, Time: 0.06\n",
      "It: 14160, Loss: 5.196e-02, Loss_u: 4.358e-02, Loss_r: 8.379e-03, Time: 0.06\n",
      "It: 14170, Loss: 5.606e-02, Loss_u: 5.374e-02, Loss_r: 2.315e-03, Time: 0.06\n",
      "It: 14180, Loss: 7.690e-02, Loss_u: 7.241e-02, Loss_r: 4.491e-03, Time: 0.07\n",
      "It: 14190, Loss: 6.915e-02, Loss_u: 6.521e-02, Loss_r: 3.944e-03, Time: 0.05\n",
      "It: 14200, Loss: 5.937e-02, Loss_u: 5.190e-02, Loss_r: 7.469e-03, Time: 0.05\n",
      "It: 14210, Loss: 8.923e-02, Loss_u: 7.347e-02, Loss_r: 1.576e-02, Time: 0.05\n",
      "It: 14220, Loss: 5.223e-02, Loss_u: 4.878e-02, Loss_r: 3.450e-03, Time: 0.05\n",
      "It: 14230, Loss: 4.740e-02, Loss_u: 4.123e-02, Loss_r: 6.173e-03, Time: 0.05\n",
      "It: 14240, Loss: 2.753e-02, Loss_u: 2.082e-02, Loss_r: 6.710e-03, Time: 0.05\n",
      "It: 14250, Loss: 4.593e-02, Loss_u: 4.272e-02, Loss_r: 3.217e-03, Time: 0.05\n",
      "It: 14260, Loss: 7.652e-02, Loss_u: 6.469e-02, Loss_r: 1.183e-02, Time: 0.05\n",
      "It: 14270, Loss: 7.188e-02, Loss_u: 5.790e-02, Loss_r: 1.398e-02, Time: 0.06\n",
      "It: 14280, Loss: 9.701e-02, Loss_u: 7.474e-02, Loss_r: 2.227e-02, Time: 0.07\n",
      "It: 14290, Loss: 7.168e-02, Loss_u: 6.454e-02, Loss_r: 7.135e-03, Time: 0.05\n",
      "It: 14300, Loss: 5.931e-02, Loss_u: 5.190e-02, Loss_r: 7.417e-03, Time: 0.06\n",
      "It: 14310, Loss: 5.508e-02, Loss_u: 4.298e-02, Loss_r: 1.210e-02, Time: 0.05\n",
      "It: 14320, Loss: 5.462e-02, Loss_u: 4.925e-02, Loss_r: 5.367e-03, Time: 0.05\n",
      "It: 14330, Loss: 6.248e-02, Loss_u: 5.713e-02, Loss_r: 5.349e-03, Time: 0.05\n",
      "It: 14340, Loss: 4.471e-02, Loss_u: 3.950e-02, Loss_r: 5.206e-03, Time: 0.05\n",
      "It: 14350, Loss: 7.289e-02, Loss_u: 6.828e-02, Loss_r: 4.613e-03, Time: 0.05\n",
      "It: 14360, Loss: 5.956e-02, Loss_u: 5.543e-02, Loss_r: 4.130e-03, Time: 0.06\n",
      "It: 14370, Loss: 7.316e-02, Loss_u: 6.204e-02, Loss_r: 1.112e-02, Time: 0.07\n",
      "It: 14380, Loss: 7.055e-02, Loss_u: 6.391e-02, Loss_r: 6.646e-03, Time: 0.07\n",
      "It: 14390, Loss: 6.690e-02, Loss_u: 4.940e-02, Loss_r: 1.750e-02, Time: 0.05\n",
      "It: 14400, Loss: 6.661e-02, Loss_u: 6.087e-02, Loss_r: 5.748e-03, Time: 0.05\n",
      "It: 14410, Loss: 7.643e-02, Loss_u: 6.970e-02, Loss_r: 6.729e-03, Time: 0.06\n",
      "It: 14420, Loss: 6.522e-02, Loss_u: 4.983e-02, Loss_r: 1.539e-02, Time: 0.05\n",
      "It: 14430, Loss: 6.961e-02, Loss_u: 6.644e-02, Loss_r: 3.166e-03, Time: 0.05\n",
      "It: 14440, Loss: 5.634e-02, Loss_u: 4.962e-02, Loss_r: 6.723e-03, Time: 0.10\n",
      "It: 14450, Loss: 4.709e-02, Loss_u: 4.393e-02, Loss_r: 3.162e-03, Time: 0.05\n",
      "It: 14460, Loss: 7.974e-02, Loss_u: 7.076e-02, Loss_r: 8.980e-03, Time: 0.05\n",
      "It: 14470, Loss: 7.906e-02, Loss_u: 6.663e-02, Loss_r: 1.243e-02, Time: 0.09\n",
      "It: 14480, Loss: 8.197e-02, Loss_u: 7.454e-02, Loss_r: 7.428e-03, Time: 0.05\n",
      "It: 14490, Loss: 7.314e-02, Loss_u: 6.311e-02, Loss_r: 1.003e-02, Time: 0.08\n",
      "It: 14500, Loss: 5.869e-02, Loss_u: 5.249e-02, Loss_r: 6.200e-03, Time: 0.05\n",
      "It: 14510, Loss: 3.464e-02, Loss_u: 2.880e-02, Loss_r: 5.833e-03, Time: 0.08\n",
      "It: 14520, Loss: 4.564e-02, Loss_u: 4.039e-02, Loss_r: 5.255e-03, Time: 0.06\n",
      "It: 14530, Loss: 5.830e-02, Loss_u: 5.401e-02, Loss_r: 4.293e-03, Time: 0.06\n",
      "It: 14540, Loss: 7.268e-02, Loss_u: 6.878e-02, Loss_r: 3.903e-03, Time: 0.05\n",
      "It: 14550, Loss: 4.238e-02, Loss_u: 3.748e-02, Loss_r: 4.894e-03, Time: 0.08\n",
      "It: 14560, Loss: 6.856e-02, Loss_u: 5.988e-02, Loss_r: 8.682e-03, Time: 0.05\n",
      "It: 14570, Loss: 5.356e-02, Loss_u: 4.593e-02, Loss_r: 7.629e-03, Time: 0.06\n",
      "It: 14580, Loss: 8.276e-02, Loss_u: 4.752e-02, Loss_r: 3.524e-02, Time: 0.06\n",
      "It: 14590, Loss: 6.827e-02, Loss_u: 5.885e-02, Loss_r: 9.422e-03, Time: 0.05\n",
      "It: 14600, Loss: 7.518e-02, Loss_u: 6.177e-02, Loss_r: 1.341e-02, Time: 0.05\n",
      "It: 14610, Loss: 6.479e-02, Loss_u: 3.298e-02, Loss_r: 3.181e-02, Time: 0.07\n",
      "It: 14620, Loss: 4.565e-02, Loss_u: 4.099e-02, Loss_r: 4.666e-03, Time: 0.07\n",
      "It: 14630, Loss: 5.798e-02, Loss_u: 5.226e-02, Loss_r: 5.725e-03, Time: 0.06\n",
      "It: 14640, Loss: 5.529e-02, Loss_u: 4.620e-02, Loss_r: 9.089e-03, Time: 0.05\n",
      "It: 14650, Loss: 7.117e-02, Loss_u: 6.518e-02, Loss_r: 5.982e-03, Time: 0.05\n",
      "It: 14660, Loss: 5.783e-02, Loss_u: 5.482e-02, Loss_r: 3.011e-03, Time: 0.06\n",
      "It: 14670, Loss: 8.101e-02, Loss_u: 7.199e-02, Loss_r: 9.020e-03, Time: 0.05\n",
      "It: 14680, Loss: 4.617e-02, Loss_u: 3.550e-02, Loss_r: 1.066e-02, Time: 0.08\n",
      "It: 14690, Loss: 6.387e-02, Loss_u: 5.747e-02, Loss_r: 6.403e-03, Time: 0.05\n",
      "It: 14700, Loss: 6.934e-02, Loss_u: 5.647e-02, Loss_r: 1.288e-02, Time: 0.10\n",
      "It: 14710, Loss: 3.698e-02, Loss_u: 3.392e-02, Loss_r: 3.059e-03, Time: 0.05\n",
      "It: 14720, Loss: 5.216e-02, Loss_u: 4.640e-02, Loss_r: 5.759e-03, Time: 0.09\n",
      "It: 14730, Loss: 5.362e-02, Loss_u: 4.434e-02, Loss_r: 9.276e-03, Time: 0.06\n",
      "It: 14740, Loss: 7.164e-02, Loss_u: 6.716e-02, Loss_r: 4.487e-03, Time: 0.12\n",
      "It: 14750, Loss: 5.645e-02, Loss_u: 5.252e-02, Loss_r: 3.937e-03, Time: 0.05\n",
      "It: 14760, Loss: 4.403e-02, Loss_u: 3.043e-02, Loss_r: 1.359e-02, Time: 0.09\n",
      "It: 14770, Loss: 8.142e-02, Loss_u: 7.841e-02, Loss_r: 3.015e-03, Time: 0.05\n",
      "It: 14780, Loss: 5.716e-02, Loss_u: 5.223e-02, Loss_r: 4.931e-03, Time: 0.06\n",
      "It: 14790, Loss: 6.067e-02, Loss_u: 5.526e-02, Loss_r: 5.416e-03, Time: 0.08\n",
      "It: 14800, Loss: 4.164e-02, Loss_u: 3.546e-02, Loss_r: 6.183e-03, Time: 0.08\n",
      "It: 14810, Loss: 6.041e-02, Loss_u: 5.162e-02, Loss_r: 8.794e-03, Time: 0.05\n",
      "It: 14820, Loss: 3.270e-02, Loss_u: 2.922e-02, Loss_r: 3.482e-03, Time: 0.05\n",
      "It: 14830, Loss: 1.088e-01, Loss_u: 1.038e-01, Loss_r: 4.979e-03, Time: 0.06\n",
      "It: 14840, Loss: 6.326e-02, Loss_u: 5.489e-02, Loss_r: 8.376e-03, Time: 0.05\n",
      "It: 14850, Loss: 7.420e-02, Loss_u: 5.994e-02, Loss_r: 1.426e-02, Time: 0.05\n",
      "It: 14860, Loss: 3.135e-02, Loss_u: 2.762e-02, Loss_r: 3.732e-03, Time: 0.05\n",
      "It: 14870, Loss: 3.279e-02, Loss_u: 2.680e-02, Loss_r: 5.985e-03, Time: 0.06\n",
      "It: 14880, Loss: 7.795e-02, Loss_u: 7.490e-02, Loss_r: 3.048e-03, Time: 0.06\n",
      "It: 14890, Loss: 3.991e-02, Loss_u: 3.444e-02, Loss_r: 5.476e-03, Time: 0.06\n",
      "It: 14900, Loss: 6.447e-02, Loss_u: 5.660e-02, Loss_r: 7.874e-03, Time: 0.06\n",
      "It: 14910, Loss: 6.239e-02, Loss_u: 5.864e-02, Loss_r: 3.750e-03, Time: 0.05\n",
      "It: 14920, Loss: 6.101e-02, Loss_u: 4.750e-02, Loss_r: 1.350e-02, Time: 0.05\n",
      "It: 14930, Loss: 4.735e-02, Loss_u: 4.255e-02, Loss_r: 4.800e-03, Time: 0.05\n",
      "It: 14940, Loss: 9.964e-02, Loss_u: 8.539e-02, Loss_r: 1.425e-02, Time: 0.05\n",
      "It: 14950, Loss: 6.817e-02, Loss_u: 4.140e-02, Loss_r: 2.678e-02, Time: 0.05\n",
      "It: 14960, Loss: 5.244e-02, Loss_u: 4.452e-02, Loss_r: 7.920e-03, Time: 0.06\n",
      "It: 14970, Loss: 5.902e-02, Loss_u: 5.449e-02, Loss_r: 4.531e-03, Time: 0.07\n",
      "It: 14980, Loss: 6.677e-02, Loss_u: 6.243e-02, Loss_r: 4.334e-03, Time: 0.05\n",
      "It: 14990, Loss: 9.018e-02, Loss_u: 7.922e-02, Loss_r: 1.096e-02, Time: 0.05\n",
      "It: 15000, Loss: 8.605e-02, Loss_u: 8.124e-02, Loss_r: 4.816e-03, Time: 0.06\n",
      "constant_bcs_val: 7.460\n",
      "It: 15010, Loss: 5.786e-02, Loss_u: 4.227e-02, Loss_r: 1.558e-02, Time: 0.06\n",
      "It: 15020, Loss: 8.413e-02, Loss_u: 8.022e-02, Loss_r: 3.909e-03, Time: 0.08\n",
      "It: 15030, Loss: 7.339e-02, Loss_u: 6.890e-02, Loss_r: 4.487e-03, Time: 0.05\n",
      "It: 15040, Loss: 4.921e-02, Loss_u: 4.517e-02, Loss_r: 4.036e-03, Time: 0.09\n",
      "It: 15050, Loss: 5.424e-02, Loss_u: 4.282e-02, Loss_r: 1.142e-02, Time: 0.10\n",
      "It: 15060, Loss: 5.114e-02, Loss_u: 4.412e-02, Loss_r: 7.023e-03, Time: 0.05\n",
      "It: 15070, Loss: 7.299e-02, Loss_u: 6.212e-02, Loss_r: 1.087e-02, Time: 0.05\n",
      "It: 15080, Loss: 4.423e-02, Loss_u: 3.417e-02, Loss_r: 1.006e-02, Time: 0.05\n",
      "It: 15090, Loss: 4.887e-02, Loss_u: 4.129e-02, Loss_r: 7.577e-03, Time: 0.05\n",
      "It: 15100, Loss: 5.295e-02, Loss_u: 3.714e-02, Loss_r: 1.581e-02, Time: 0.06\n",
      "It: 15110, Loss: 5.375e-02, Loss_u: 4.450e-02, Loss_r: 9.249e-03, Time: 0.11\n",
      "It: 15120, Loss: 3.488e-02, Loss_u: 2.697e-02, Loss_r: 7.907e-03, Time: 0.05\n",
      "It: 15130, Loss: 8.728e-02, Loss_u: 8.138e-02, Loss_r: 5.897e-03, Time: 0.05\n",
      "It: 15140, Loss: 9.598e-02, Loss_u: 9.198e-02, Loss_r: 4.003e-03, Time: 0.06\n",
      "It: 15150, Loss: 7.105e-02, Loss_u: 6.810e-02, Loss_r: 2.946e-03, Time: 0.07\n",
      "It: 15160, Loss: 6.364e-02, Loss_u: 6.059e-02, Loss_r: 3.053e-03, Time: 0.08\n",
      "It: 15170, Loss: 7.578e-02, Loss_u: 5.455e-02, Loss_r: 2.123e-02, Time: 0.06\n",
      "It: 15180, Loss: 5.462e-02, Loss_u: 3.835e-02, Loss_r: 1.628e-02, Time: 0.05\n",
      "It: 15190, Loss: 4.584e-02, Loss_u: 3.365e-02, Loss_r: 1.219e-02, Time: 0.05\n",
      "It: 15200, Loss: 5.954e-02, Loss_u: 5.030e-02, Loss_r: 9.239e-03, Time: 0.06\n",
      "It: 15210, Loss: 6.036e-02, Loss_u: 4.987e-02, Loss_r: 1.049e-02, Time: 0.06\n",
      "It: 15220, Loss: 5.682e-02, Loss_u: 5.246e-02, Loss_r: 4.355e-03, Time: 0.05\n",
      "It: 15230, Loss: 8.486e-02, Loss_u: 7.988e-02, Loss_r: 4.986e-03, Time: 0.06\n",
      "It: 15240, Loss: 8.780e-02, Loss_u: 8.222e-02, Loss_r: 5.580e-03, Time: 0.06\n",
      "It: 15250, Loss: 6.855e-02, Loss_u: 5.408e-02, Loss_r: 1.447e-02, Time: 0.05\n",
      "It: 15260, Loss: 6.507e-02, Loss_u: 5.923e-02, Loss_r: 5.848e-03, Time: 0.05\n",
      "It: 15270, Loss: 6.870e-02, Loss_u: 6.178e-02, Loss_r: 6.918e-03, Time: 0.07\n",
      "It: 15280, Loss: 8.282e-02, Loss_u: 7.636e-02, Loss_r: 6.457e-03, Time: 0.06\n",
      "It: 15290, Loss: 9.438e-02, Loss_u: 9.056e-02, Loss_r: 3.821e-03, Time: 0.05\n",
      "It: 15300, Loss: 6.757e-02, Loss_u: 4.839e-02, Loss_r: 1.918e-02, Time: 0.04\n",
      "It: 15310, Loss: 7.163e-02, Loss_u: 5.837e-02, Loss_r: 1.326e-02, Time: 0.05\n",
      "It: 15320, Loss: 6.937e-02, Loss_u: 6.462e-02, Loss_r: 4.753e-03, Time: 0.05\n",
      "It: 15330, Loss: 7.521e-02, Loss_u: 6.890e-02, Loss_r: 6.307e-03, Time: 0.05\n",
      "It: 15340, Loss: 5.725e-02, Loss_u: 5.490e-02, Loss_r: 2.356e-03, Time: 0.13\n",
      "It: 15350, Loss: 5.706e-02, Loss_u: 5.112e-02, Loss_r: 5.946e-03, Time: 0.07\n",
      "It: 15360, Loss: 5.217e-02, Loss_u: 4.769e-02, Loss_r: 4.477e-03, Time: 0.05\n",
      "It: 15370, Loss: 1.308e-01, Loss_u: 1.119e-01, Loss_r: 1.882e-02, Time: 0.05\n",
      "It: 15380, Loss: 9.783e-02, Loss_u: 7.081e-02, Loss_r: 2.702e-02, Time: 0.08\n",
      "It: 15390, Loss: 1.145e-01, Loss_u: 9.718e-02, Loss_r: 1.728e-02, Time: 0.05\n",
      "It: 15400, Loss: 1.016e-01, Loss_u: 9.392e-02, Loss_r: 7.639e-03, Time: 0.05\n",
      "It: 15410, Loss: 2.158e-02, Loss_u: 1.631e-02, Loss_r: 5.278e-03, Time: 0.06\n",
      "It: 15420, Loss: 5.374e-02, Loss_u: 4.335e-02, Loss_r: 1.040e-02, Time: 0.05\n",
      "It: 15430, Loss: 8.612e-02, Loss_u: 7.635e-02, Loss_r: 9.768e-03, Time: 0.05\n",
      "It: 15440, Loss: 6.592e-02, Loss_u: 5.090e-02, Loss_r: 1.502e-02, Time: 0.05\n",
      "It: 15450, Loss: 6.790e-02, Loss_u: 6.440e-02, Loss_r: 3.507e-03, Time: 0.06\n",
      "It: 15460, Loss: 7.907e-02, Loss_u: 6.650e-02, Loss_r: 1.256e-02, Time: 0.05\n",
      "It: 15470, Loss: 7.301e-02, Loss_u: 6.692e-02, Loss_r: 6.087e-03, Time: 0.06\n",
      "It: 15480, Loss: 6.022e-02, Loss_u: 5.293e-02, Loss_r: 7.296e-03, Time: 0.07\n",
      "It: 15490, Loss: 8.079e-02, Loss_u: 6.641e-02, Loss_r: 1.438e-02, Time: 0.07\n",
      "It: 15500, Loss: 7.434e-02, Loss_u: 7.156e-02, Loss_r: 2.774e-03, Time: 0.05\n",
      "It: 15510, Loss: 6.755e-02, Loss_u: 6.493e-02, Loss_r: 2.617e-03, Time: 0.05\n",
      "It: 15520, Loss: 6.330e-02, Loss_u: 5.323e-02, Loss_r: 1.007e-02, Time: 0.06\n",
      "It: 15530, Loss: 6.605e-02, Loss_u: 5.565e-02, Loss_r: 1.041e-02, Time: 0.05\n",
      "It: 15540, Loss: 1.177e-01, Loss_u: 9.727e-02, Loss_r: 2.045e-02, Time: 0.05\n",
      "It: 15550, Loss: 5.738e-02, Loss_u: 3.908e-02, Loss_r: 1.830e-02, Time: 0.05\n",
      "It: 15560, Loss: 8.704e-02, Loss_u: 7.854e-02, Loss_r: 8.505e-03, Time: 0.05\n",
      "It: 15570, Loss: 7.924e-02, Loss_u: 6.681e-02, Loss_r: 1.243e-02, Time: 0.05\n",
      "It: 15580, Loss: 5.296e-02, Loss_u: 4.775e-02, Loss_r: 5.218e-03, Time: 0.05\n",
      "It: 15590, Loss: 7.530e-02, Loss_u: 4.247e-02, Loss_r: 3.284e-02, Time: 0.07\n",
      "It: 15600, Loss: 7.938e-02, Loss_u: 6.599e-02, Loss_r: 1.339e-02, Time: 0.06\n",
      "It: 15610, Loss: 6.376e-02, Loss_u: 5.939e-02, Loss_r: 4.374e-03, Time: 0.08\n",
      "It: 15620, Loss: 8.895e-02, Loss_u: 8.022e-02, Loss_r: 8.734e-03, Time: 0.05\n",
      "It: 15630, Loss: 1.041e-01, Loss_u: 9.654e-02, Loss_r: 7.519e-03, Time: 0.05\n",
      "It: 15640, Loss: 9.002e-02, Loss_u: 8.454e-02, Loss_r: 5.478e-03, Time: 0.05\n",
      "It: 15650, Loss: 9.318e-02, Loss_u: 8.534e-02, Loss_r: 7.839e-03, Time: 0.09\n",
      "It: 15660, Loss: 1.195e-01, Loss_u: 1.147e-01, Loss_r: 4.871e-03, Time: 0.06\n",
      "It: 15670, Loss: 9.835e-02, Loss_u: 9.426e-02, Loss_r: 4.087e-03, Time: 0.05\n",
      "It: 15680, Loss: 6.902e-02, Loss_u: 5.609e-02, Loss_r: 1.292e-02, Time: 0.07\n",
      "It: 15690, Loss: 6.481e-02, Loss_u: 6.107e-02, Loss_r: 3.735e-03, Time: 0.05\n",
      "It: 15700, Loss: 5.041e-02, Loss_u: 4.439e-02, Loss_r: 6.020e-03, Time: 0.06\n",
      "It: 15710, Loss: 3.706e-02, Loss_u: 2.245e-02, Loss_r: 1.461e-02, Time: 0.06\n",
      "It: 15720, Loss: 4.747e-02, Loss_u: 4.360e-02, Loss_r: 3.876e-03, Time: 0.10\n",
      "It: 15730, Loss: 1.043e-01, Loss_u: 6.159e-02, Loss_r: 4.276e-02, Time: 0.08\n",
      "It: 15740, Loss: 5.970e-02, Loss_u: 4.620e-02, Loss_r: 1.350e-02, Time: 0.05\n",
      "It: 15750, Loss: 6.146e-02, Loss_u: 5.753e-02, Loss_r: 3.925e-03, Time: 0.09\n",
      "It: 15760, Loss: 6.865e-02, Loss_u: 4.390e-02, Loss_r: 2.475e-02, Time: 0.05\n",
      "It: 15770, Loss: 4.928e-02, Loss_u: 4.077e-02, Loss_r: 8.509e-03, Time: 0.05\n",
      "It: 15780, Loss: 6.747e-02, Loss_u: 6.068e-02, Loss_r: 6.791e-03, Time: 0.05\n",
      "It: 15790, Loss: 5.241e-02, Loss_u: 4.130e-02, Loss_r: 1.111e-02, Time: 0.05\n",
      "It: 15800, Loss: 5.801e-02, Loss_u: 5.141e-02, Loss_r: 6.597e-03, Time: 0.05\n",
      "It: 15810, Loss: 6.037e-02, Loss_u: 4.750e-02, Loss_r: 1.286e-02, Time: 0.05\n",
      "It: 15820, Loss: 7.303e-02, Loss_u: 6.810e-02, Loss_r: 4.936e-03, Time: 0.05\n",
      "It: 15830, Loss: 5.883e-02, Loss_u: 5.206e-02, Loss_r: 6.766e-03, Time: 0.05\n",
      "It: 15840, Loss: 4.895e-02, Loss_u: 4.338e-02, Loss_r: 5.568e-03, Time: 0.05\n",
      "It: 15850, Loss: 3.984e-02, Loss_u: 3.579e-02, Loss_r: 4.053e-03, Time: 0.08\n",
      "It: 15860, Loss: 7.273e-02, Loss_u: 7.070e-02, Loss_r: 2.030e-03, Time: 0.05\n",
      "It: 15870, Loss: 7.156e-02, Loss_u: 6.812e-02, Loss_r: 3.438e-03, Time: 0.06\n",
      "It: 15880, Loss: 5.612e-02, Loss_u: 5.160e-02, Loss_r: 4.529e-03, Time: 0.05\n",
      "It: 15890, Loss: 2.982e-02, Loss_u: 2.463e-02, Loss_r: 5.188e-03, Time: 0.06\n",
      "It: 15900, Loss: 9.442e-02, Loss_u: 8.635e-02, Loss_r: 8.073e-03, Time: 0.05\n",
      "It: 15910, Loss: 8.845e-02, Loss_u: 7.554e-02, Loss_r: 1.291e-02, Time: 0.05\n",
      "It: 15920, Loss: 6.094e-02, Loss_u: 5.674e-02, Loss_r: 4.203e-03, Time: 0.06\n",
      "It: 15930, Loss: 4.734e-02, Loss_u: 3.753e-02, Loss_r: 9.810e-03, Time: 0.07\n",
      "It: 15940, Loss: 9.193e-02, Loss_u: 7.431e-02, Loss_r: 1.762e-02, Time: 0.09\n",
      "It: 15950, Loss: 8.061e-02, Loss_u: 7.076e-02, Loss_r: 9.860e-03, Time: 0.07\n",
      "It: 15960, Loss: 6.321e-02, Loss_u: 5.510e-02, Loss_r: 8.106e-03, Time: 0.05\n",
      "It: 15970, Loss: 3.377e-02, Loss_u: 2.563e-02, Loss_r: 8.139e-03, Time: 0.05\n",
      "It: 15980, Loss: 6.129e-02, Loss_u: 5.665e-02, Loss_r: 4.639e-03, Time: 0.08\n",
      "It: 15990, Loss: 4.782e-02, Loss_u: 4.561e-02, Loss_r: 2.207e-03, Time: 0.06\n",
      "It: 16000, Loss: 7.475e-02, Loss_u: 6.471e-02, Loss_r: 1.004e-02, Time: 0.06\n",
      "constant_bcs_val: 7.834\n",
      "It: 16010, Loss: 6.354e-02, Loss_u: 5.242e-02, Loss_r: 1.111e-02, Time: 0.06\n",
      "It: 16020, Loss: 6.366e-02, Loss_u: 5.894e-02, Loss_r: 4.722e-03, Time: 0.06\n",
      "It: 16030, Loss: 7.605e-02, Loss_u: 6.813e-02, Loss_r: 7.922e-03, Time: 0.09\n",
      "It: 16040, Loss: 6.829e-02, Loss_u: 6.234e-02, Loss_r: 5.948e-03, Time: 0.05\n",
      "It: 16050, Loss: 5.755e-02, Loss_u: 4.575e-02, Loss_r: 1.180e-02, Time: 0.06\n",
      "It: 16060, Loss: 8.753e-02, Loss_u: 7.901e-02, Loss_r: 8.522e-03, Time: 0.05\n",
      "It: 16070, Loss: 1.034e-01, Loss_u: 8.539e-02, Loss_r: 1.799e-02, Time: 0.07\n",
      "It: 16080, Loss: 8.602e-02, Loss_u: 7.654e-02, Loss_r: 9.479e-03, Time: 0.08\n",
      "It: 16090, Loss: 8.833e-02, Loss_u: 8.350e-02, Loss_r: 4.827e-03, Time: 0.06\n",
      "It: 16100, Loss: 6.103e-02, Loss_u: 4.691e-02, Loss_r: 1.413e-02, Time: 0.05\n",
      "It: 16110, Loss: 9.670e-02, Loss_u: 9.059e-02, Loss_r: 6.117e-03, Time: 0.05\n",
      "It: 16120, Loss: 7.991e-02, Loss_u: 7.412e-02, Loss_r: 5.788e-03, Time: 0.08\n",
      "It: 16130, Loss: 6.947e-02, Loss_u: 6.034e-02, Loss_r: 9.134e-03, Time: 0.18\n",
      "It: 16140, Loss: 7.266e-02, Loss_u: 5.876e-02, Loss_r: 1.390e-02, Time: 0.06\n",
      "It: 16150, Loss: 3.586e-02, Loss_u: 3.033e-02, Loss_r: 5.534e-03, Time: 0.06\n",
      "It: 16160, Loss: 7.387e-02, Loss_u: 6.742e-02, Loss_r: 6.457e-03, Time: 0.07\n",
      "It: 16170, Loss: 5.421e-02, Loss_u: 4.911e-02, Loss_r: 5.103e-03, Time: 0.06\n",
      "It: 16180, Loss: 4.064e-02, Loss_u: 3.809e-02, Loss_r: 2.552e-03, Time: 0.06\n",
      "It: 16190, Loss: 5.097e-02, Loss_u: 4.188e-02, Loss_r: 9.094e-03, Time: 0.05\n",
      "It: 16200, Loss: 7.971e-02, Loss_u: 6.883e-02, Loss_r: 1.088e-02, Time: 0.05\n",
      "It: 16210, Loss: 7.717e-02, Loss_u: 6.896e-02, Loss_r: 8.210e-03, Time: 0.05\n",
      "It: 16220, Loss: 4.437e-02, Loss_u: 3.962e-02, Loss_r: 4.748e-03, Time: 0.06\n",
      "It: 16230, Loss: 4.655e-02, Loss_u: 3.775e-02, Loss_r: 8.806e-03, Time: 0.06\n",
      "It: 16240, Loss: 5.089e-02, Loss_u: 4.378e-02, Loss_r: 7.114e-03, Time: 0.08\n",
      "It: 16250, Loss: 6.289e-02, Loss_u: 5.863e-02, Loss_r: 4.265e-03, Time: 0.11\n",
      "It: 16260, Loss: 5.327e-02, Loss_u: 4.936e-02, Loss_r: 3.913e-03, Time: 0.06\n",
      "It: 16270, Loss: 8.544e-02, Loss_u: 7.611e-02, Loss_r: 9.331e-03, Time: 0.10\n",
      "It: 16280, Loss: 8.239e-02, Loss_u: 7.537e-02, Loss_r: 7.017e-03, Time: 0.06\n",
      "It: 16290, Loss: 5.576e-02, Loss_u: 4.537e-02, Loss_r: 1.039e-02, Time: 0.06\n",
      "It: 16300, Loss: 8.781e-02, Loss_u: 7.023e-02, Loss_r: 1.758e-02, Time: 0.06\n",
      "It: 16310, Loss: 7.531e-02, Loss_u: 5.263e-02, Loss_r: 2.268e-02, Time: 0.05\n",
      "It: 16320, Loss: 5.022e-02, Loss_u: 4.552e-02, Loss_r: 4.701e-03, Time: 0.07\n",
      "It: 16330, Loss: 5.227e-02, Loss_u: 4.088e-02, Loss_r: 1.139e-02, Time: 0.08\n",
      "It: 16340, Loss: 7.789e-02, Loss_u: 7.171e-02, Loss_r: 6.179e-03, Time: 0.11\n",
      "It: 16350, Loss: 6.147e-02, Loss_u: 5.328e-02, Loss_r: 8.190e-03, Time: 0.11\n",
      "It: 16360, Loss: 9.263e-02, Loss_u: 8.606e-02, Loss_r: 6.566e-03, Time: 0.11\n",
      "It: 16370, Loss: 8.639e-02, Loss_u: 7.959e-02, Loss_r: 6.803e-03, Time: 0.07\n",
      "It: 16380, Loss: 6.372e-02, Loss_u: 4.782e-02, Loss_r: 1.590e-02, Time: 0.05\n",
      "It: 16390, Loss: 6.116e-02, Loss_u: 5.840e-02, Loss_r: 2.760e-03, Time: 0.06\n",
      "It: 16400, Loss: 8.058e-02, Loss_u: 7.120e-02, Loss_r: 9.383e-03, Time: 0.05\n",
      "It: 16410, Loss: 5.837e-02, Loss_u: 5.280e-02, Loss_r: 5.567e-03, Time: 0.05\n",
      "It: 16420, Loss: 9.000e-02, Loss_u: 8.426e-02, Loss_r: 5.737e-03, Time: 0.06\n",
      "It: 16430, Loss: 4.280e-02, Loss_u: 4.028e-02, Loss_r: 2.522e-03, Time: 0.06\n",
      "It: 16440, Loss: 4.335e-02, Loss_u: 3.910e-02, Loss_r: 4.254e-03, Time: 0.05\n",
      "It: 16450, Loss: 4.844e-02, Loss_u: 4.607e-02, Loss_r: 2.376e-03, Time: 0.09\n",
      "It: 16460, Loss: 9.087e-02, Loss_u: 8.573e-02, Loss_r: 5.142e-03, Time: 0.06\n",
      "It: 16470, Loss: 7.162e-02, Loss_u: 6.441e-02, Loss_r: 7.210e-03, Time: 0.32\n",
      "It: 16480, Loss: 7.694e-02, Loss_u: 6.019e-02, Loss_r: 1.675e-02, Time: 0.09\n",
      "It: 16490, Loss: 6.691e-02, Loss_u: 6.083e-02, Loss_r: 6.081e-03, Time: 0.07\n",
      "It: 16500, Loss: 7.016e-02, Loss_u: 5.550e-02, Loss_r: 1.465e-02, Time: 0.10\n",
      "It: 16510, Loss: 4.041e-02, Loss_u: 3.604e-02, Loss_r: 4.368e-03, Time: 0.06\n",
      "It: 16520, Loss: 7.457e-02, Loss_u: 7.080e-02, Loss_r: 3.779e-03, Time: 0.06\n",
      "It: 16530, Loss: 8.241e-02, Loss_u: 7.681e-02, Loss_r: 5.603e-03, Time: 0.07\n",
      "It: 16540, Loss: 6.033e-02, Loss_u: 5.341e-02, Loss_r: 6.916e-03, Time: 0.14\n",
      "It: 16550, Loss: 6.226e-02, Loss_u: 5.678e-02, Loss_r: 5.485e-03, Time: 0.07\n",
      "It: 16560, Loss: 1.269e-01, Loss_u: 1.164e-01, Loss_r: 1.047e-02, Time: 0.07\n",
      "It: 16570, Loss: 1.122e-01, Loss_u: 1.062e-01, Loss_r: 5.994e-03, Time: 0.14\n",
      "It: 16580, Loss: 9.308e-02, Loss_u: 6.830e-02, Loss_r: 2.477e-02, Time: 0.07\n",
      "It: 16590, Loss: 4.236e-02, Loss_u: 3.527e-02, Loss_r: 7.092e-03, Time: 0.07\n",
      "It: 16600, Loss: 8.114e-02, Loss_u: 7.236e-02, Loss_r: 8.781e-03, Time: 0.07\n",
      "It: 16610, Loss: 9.045e-02, Loss_u: 6.861e-02, Loss_r: 2.184e-02, Time: 0.06\n",
      "It: 16620, Loss: 7.054e-02, Loss_u: 6.730e-02, Loss_r: 3.236e-03, Time: 0.10\n",
      "It: 16630, Loss: 5.443e-02, Loss_u: 4.759e-02, Loss_r: 6.839e-03, Time: 0.09\n",
      "It: 16640, Loss: 7.668e-02, Loss_u: 6.302e-02, Loss_r: 1.366e-02, Time: 0.05\n",
      "It: 16650, Loss: 7.905e-02, Loss_u: 7.307e-02, Loss_r: 5.977e-03, Time: 0.06\n",
      "It: 16660, Loss: 6.236e-02, Loss_u: 5.909e-02, Loss_r: 3.269e-03, Time: 0.08\n",
      "It: 16670, Loss: 5.050e-02, Loss_u: 4.658e-02, Loss_r: 3.919e-03, Time: 0.07\n",
      "It: 16680, Loss: 7.965e-02, Loss_u: 7.591e-02, Loss_r: 3.742e-03, Time: 0.08\n",
      "It: 16690, Loss: 6.600e-02, Loss_u: 6.108e-02, Loss_r: 4.915e-03, Time: 0.06\n",
      "It: 16700, Loss: 7.927e-02, Loss_u: 7.187e-02, Loss_r: 7.396e-03, Time: 0.09\n",
      "It: 16710, Loss: 7.711e-02, Loss_u: 7.295e-02, Loss_r: 4.158e-03, Time: 0.08\n",
      "It: 16720, Loss: 6.530e-02, Loss_u: 5.745e-02, Loss_r: 7.850e-03, Time: 0.10\n",
      "It: 16730, Loss: 7.001e-02, Loss_u: 6.366e-02, Loss_r: 6.351e-03, Time: 0.06\n",
      "It: 16740, Loss: 7.159e-02, Loss_u: 5.993e-02, Loss_r: 1.167e-02, Time: 0.06\n",
      "It: 16750, Loss: 1.082e-01, Loss_u: 1.036e-01, Loss_r: 4.620e-03, Time: 0.09\n",
      "It: 16760, Loss: 5.396e-02, Loss_u: 4.999e-02, Loss_r: 3.963e-03, Time: 0.06\n",
      "It: 16770, Loss: 8.722e-02, Loss_u: 6.318e-02, Loss_r: 2.405e-02, Time: 0.07\n",
      "It: 16780, Loss: 6.476e-02, Loss_u: 5.794e-02, Loss_r: 6.813e-03, Time: 0.09\n",
      "It: 16790, Loss: 4.817e-02, Loss_u: 4.094e-02, Loss_r: 7.229e-03, Time: 0.10\n",
      "It: 16800, Loss: 5.599e-02, Loss_u: 4.746e-02, Loss_r: 8.532e-03, Time: 0.08\n",
      "It: 16810, Loss: 7.830e-02, Loss_u: 7.339e-02, Loss_r: 4.914e-03, Time: 0.08\n",
      "It: 16820, Loss: 5.211e-02, Loss_u: 4.600e-02, Loss_r: 6.115e-03, Time: 0.06\n",
      "It: 16830, Loss: 4.070e-02, Loss_u: 3.629e-02, Loss_r: 4.418e-03, Time: 0.06\n",
      "It: 16840, Loss: 9.328e-02, Loss_u: 8.325e-02, Loss_r: 1.003e-02, Time: 0.09\n",
      "It: 16850, Loss: 4.180e-02, Loss_u: 3.433e-02, Loss_r: 7.462e-03, Time: 0.08\n",
      "It: 16860, Loss: 8.607e-02, Loss_u: 7.658e-02, Loss_r: 9.495e-03, Time: 0.11\n",
      "It: 16870, Loss: 8.305e-02, Loss_u: 7.722e-02, Loss_r: 5.825e-03, Time: 0.07\n",
      "It: 16880, Loss: 6.025e-02, Loss_u: 5.579e-02, Loss_r: 4.468e-03, Time: 0.08\n",
      "It: 16890, Loss: 7.593e-02, Loss_u: 7.000e-02, Loss_r: 5.931e-03, Time: 0.09\n",
      "It: 16900, Loss: 4.707e-02, Loss_u: 3.861e-02, Loss_r: 8.462e-03, Time: 0.18\n",
      "It: 16910, Loss: 6.381e-02, Loss_u: 5.503e-02, Loss_r: 8.778e-03, Time: 0.06\n",
      "It: 16920, Loss: 7.951e-02, Loss_u: 6.780e-02, Loss_r: 1.171e-02, Time: 0.08\n",
      "It: 16930, Loss: 4.990e-02, Loss_u: 4.739e-02, Loss_r: 2.506e-03, Time: 0.07\n",
      "It: 16940, Loss: 7.980e-02, Loss_u: 7.349e-02, Loss_r: 6.313e-03, Time: 0.06\n",
      "It: 16950, Loss: 6.963e-02, Loss_u: 5.932e-02, Loss_r: 1.031e-02, Time: 0.08\n",
      "It: 16960, Loss: 3.864e-02, Loss_u: 3.489e-02, Loss_r: 3.751e-03, Time: 0.08\n",
      "It: 16970, Loss: 5.611e-02, Loss_u: 5.279e-02, Loss_r: 3.320e-03, Time: 0.08\n",
      "It: 16980, Loss: 7.898e-02, Loss_u: 7.546e-02, Loss_r: 3.514e-03, Time: 0.09\n",
      "It: 16990, Loss: 6.323e-02, Loss_u: 4.595e-02, Loss_r: 1.727e-02, Time: 0.06\n",
      "It: 17000, Loss: 5.891e-02, Loss_u: 5.492e-02, Loss_r: 3.997e-03, Time: 0.10\n",
      "constant_bcs_val: 8.424\n",
      "It: 17010, Loss: 6.433e-02, Loss_u: 5.588e-02, Loss_r: 8.446e-03, Time: 0.10\n",
      "It: 17020, Loss: 5.834e-02, Loss_u: 5.336e-02, Loss_r: 4.977e-03, Time: 0.12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "nIter =40001\n",
    "bcbatch_size = 350\n",
    "ubatch_size = 5000\n",
    "mbbatch_size = 128\n",
    "\n",
    "# Parameters of equations\n",
    "Re = 100.0\n",
    "\n",
    "# Domain boundaries\n",
    "bc1_coords = np.array([[0.0, 1.0], [1.0, 1.0]])\n",
    "bc2_coords = np.array([[0.0, 0.0], [0.0, 1.0]])\n",
    "bc3_coords = np.array([[1.0, 0.0], [1.0, 1.0]])\n",
    "bc4_coords = np.array([[0.0, 0.0], [1.0, 0.0]])\n",
    "dom_coords = np.array([[0.0, 0.0], [1.0, 1.0]])\n",
    "\n",
    "# Define model\n",
    "model = 'M2'\n",
    "layers = [2, 50, 50, 50, 50 , 50 , 2]\n",
    "\n",
    "stiff_ratio = False  # Log the eigenvalues of Hessian of losses\n",
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "\n",
    "iterations = 1\n",
    "methods = [\"mini_batch\" ]\n",
    "\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "for mtd in methods:\n",
    "    print(\"Method: \", mtd)\n",
    "    time_list = []\n",
    "    error_u_list = []\n",
    "    error_v_list = []\n",
    "    \n",
    "    for index in range(iterations):\n",
    "\n",
    "        print(\"Epoch: \", str(index+1))\n",
    "\n",
    "        # Create boundary conditions samplers\n",
    "        bc1 = Sampler(2, bc1_coords, lambda x: U_gamma_1(x), name='Dirichlet BC1')\n",
    "        bc2 = Sampler(2, bc2_coords, lambda x: U_gamma_2(x), name='Dirichlet BC2')\n",
    "        bc3 = Sampler(2, bc3_coords, lambda x: U_gamma_2(x), name='Dirichlet BC3')\n",
    "        bc4 = Sampler(2, bc4_coords, lambda x: U_gamma_2(x), name='Dirichlet BC4')\n",
    "        bcs_sampler = [bc1, bc2, bc3, bc4]\n",
    "\n",
    "        # Create residual sampler\n",
    "        res_sampler = Sampler(2, dom_coords, lambda x: f(x), name='Forcing')\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        gpu_options = tf.GPUOptions(visible_device_list=\"0\")\n",
    "        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=False, log_device_placement=False)) as sess:\n",
    "\n",
    "            mode = Navier_Stokes2D(layers, operator, bcs_sampler, res_sampler, Re, model , sess)\n",
    "\n",
    "            # Train model\n",
    "            start_time = time.time()\n",
    "\n",
    "            if mtd ==\"full_batch\":\n",
    "                mode.train(nIter  , bcbatch_size , ubatch_size  )\n",
    "            elif mtd ==\"mini_batch\":\n",
    "                mode.trainmb(nIter, mbbatch_size)\n",
    "            else:\n",
    "                mode.print(\"unknown method!\")\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            # Predictions\n",
    "            _, p_pred = mode.predict_psi_p(X_star)\n",
    "            u_pred, v_pred = mode.predict_uv(X_star)\n",
    "\n",
    "            # Reference\n",
    "            u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "            v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "            # velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "            u_pred = u_pred.reshape(100,100)\n",
    "            v_pred = v_pred.reshape(100,100)\n",
    "\n",
    "            # Relative error\n",
    "            error_u = np.linalg.norm(u_ref - u_pred.T, 2) / np.linalg.norm(u_ref, 2)\n",
    "            error_v = np.linalg.norm(v_ref - v_pred.T, 2) / np.linalg.norm(v_ref, 2)\n",
    "\n",
    "            mode.print('l2 error: {:.2e}'.format(error_u))\n",
    "            mode.print('l2 error: {:.2e}'.format(error_v))\n",
    "\n",
    "       \n",
    "            mode.print(\"average lambda_bc\" , np.average(mode.adpative_constant_bcs_log))\n",
    "            mode.print(\"average lambda_res\" , str(1.0))\n",
    "            # sess.close\n",
    "            mode.plot_grad()\n",
    "            mode.save_NN()\n",
    "            mode.plt_prediction( X , Y , X_star , u_ref , u_pred , v_ref , v_pred)\n",
    "\n",
    "\n",
    "        mode.print('elapsed: {:.2e}'.format(elapsed))\n",
    "        mode.print('Relative L2 error_u: {:.2e}'.format(error_u))\n",
    "        mode.print('Relative L2 error_v: {:.2e}'.format(error_v))\n",
    "\n",
    "        time_list.append(elapsed)\n",
    "        error_u_list.append(error_u)\n",
    "        error_v_list.append(error_v)\n",
    "\n",
    "    mode.print(\"\\n\\nMethod: \", mtd)\n",
    "    mode.print(\"\\naverage of time_list:\" , sum(time_list) / len(time_list) )\n",
    "    mode.print(\"average of error_u_list:\" , sum(error_u_list) / len(error_u_list) )\n",
    "    mode.print(\"average of error_v_list:\" , sum(error_v_list) / len(error_v_list) )\n",
    "\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ]\n",
    "    # scipy.io.savemat(\"M2_result_\"+str(iterations)+\"_\"+mtd+\".mat\" , {'time_list':np.array(time_list),'error_u_list':np.array(error_u_list),'error_f_list':np.array(error_f_list)})\n",
    "\n",
    "    scipy.io.savemat(os.path.join(mode.dirname,\"\"+mtd+\"_Lid-driven-Cavity_\"+model+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_bc\"+str(bcbatch_size)+\"_exp\"+str(iterations)+\"nIter\"+str(nIter)+\".mat\") , result_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode.plot_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_prediction(self , x1 , x2 , X_star , u_star , u_pred , f_star , f_pred):\n",
    "    from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "    ### Plot ###\n",
    "\n",
    "    # Exact solution & Predicted solution\n",
    "    # Exact soluton\n",
    "    U_star = griddata(X_star, u_star.flatten(), (x1, x2), method='cubic')\n",
    "    F_star = griddata(X_star, f_star.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "    # Predicted solution\n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (x1, x2), method='cubic')\n",
    "    F_pred = griddata(X_star, f_pred.flatten(), (x1, x2), method='cubic')\n",
    "\n",
    "    titles = ['Exact $u(x)$' , 'Predicted $u(x)$' , 'Absolute error' , 'Exact $v(x)$' , 'Predicted $v(x)$' , 'Absolute error']\n",
    "    data = [U_star.T , U_pred ,  np.abs(U_star.T - U_pred) , F_star.T , F_pred ,  np.abs(F_star.T - F_pred) ]\n",
    "\n",
    "    fig_1 = plt.figure(1, figsize=(13, 5))\n",
    "    grid = ImageGrid(fig_1, 111, direction=\"row\", nrows_ncols=(2,3), \n",
    "                    label_mode=\"1\", axes_pad=1.7, share_all=False, \n",
    "                    cbar_mode=\"each\", cbar_location=\"right\", \n",
    "                    cbar_size=\"5%\", cbar_pad=0.0)\n",
    "# CREATE ARGUMENTS DICT FOR CONTOURPLOTS\n",
    "    minmax_list = []\n",
    "    for d in data:\n",
    "        # if(local):\n",
    "        #     minmax_list.append([np.min(d), np.max(d)])\n",
    "        # else:\n",
    "        minmax_list.append([np.min(d), np.max(d)])\n",
    "\n",
    "        # kwargs_list.append(dict(levels=np.linspace(minmax_list[-1][0],minmax_list[-1][1], 60), cmap=\"coolwarm\", vmin=minmax_list[-1][0], vmax=minmax_list[-1][1]))\n",
    "\n",
    "    for ax, z, minmax, title in zip(grid, data, minmax_list, titles):\n",
    "    #pcf = [ax.tricontourf(x, y, z[0,:], **kwargs)]\n",
    "        #pcfsets.append(pcf)\n",
    "        # if (timeStp == 0):\n",
    "        pcf = [ax.pcolor(x1, x2, z , cmap='jet')]\n",
    "        cb = ax.cax.colorbar(pcf[0], ticks=np.linspace(minmax[0],minmax[1],7),  format='%.3e')\n",
    "        ax.cax.tick_params(labelsize=14.5)\n",
    "        ax.set_title(title, fontsize=14.5, pad=7)\n",
    "        ax.set_ylabel(\"y\", labelpad=14.5, fontsize=14.5, rotation=\"horizontal\")\n",
    "        ax.set_xlabel(\"x\", fontsize=14.5)\n",
    "        ax.tick_params(labelsize=14.5)\n",
    "        ax.set_xlim(x1.min(), x1.max())\n",
    "        ax.set_ylim(x2.min(), x2.max())\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "    fig_1.set_size_inches(15, 10, True)\n",
    "    fig_1.subplots_adjust(left=0.7, bottom=0, right=2.2, top=0.5, wspace=None, hspace=None)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(self.dirname,\"prediction.png\"), dpi=300 , bbox_inches='tight')\n",
    "    plt.close(\"all\" , )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_ref.flatten()[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.io\n",
    "\n",
    "mode = 'M2'\n",
    "mbbatch_size = 128\n",
    "ubatch_size = 5000\n",
    "bcbatch_size = 500\n",
    "iterations = 40000\n",
    "\n",
    "time_list = []\n",
    "error_u_list = []\n",
    "error_v_list = []\n",
    "error_p_list = []\n",
    "    \n",
    "methods = [\"mini_batch\" , \"full_batch\"]\n",
    "result_dict =  dict((mtd, []) for mtd in methods)\n",
    "\n",
    "##Mini Batch\n",
    "time_list = [1831.07 , ]\n",
    "error_u_list = [0.2112 , 0.]\n",
    "error_v_list = [0.3271 , 0.]\n",
    "error_p_list = [0.094 , 0.]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "\n",
    "##Full Batch\n",
    "time_list = [8048.86 , ]\n",
    "error_u_list = [ 0.0367 ,0. ]\n",
    "error_v_list = [0.0706 , 0.]\n",
    "error_p_list = [0.465 , 0.]\n",
    "\n",
    "for mtd in methods:\n",
    "    result_dict[mtd] = [time_list ,error_u_list ,error_v_list ,  error_p_list]\n",
    "\n",
    "\n",
    "scipy.io.savemat(\"./dataset/NS_model_\"+mode+\"_result_mb\"+str(mbbatch_size)+\"_fb\"+str(ubatch_size)+\"_\"+str(bcbatch_size)+\"_\"+str(iterations)+\".mat\" , result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Plot ###\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test Data\n",
    "nx = 100\n",
    "ny = 100  # change to 100\n",
    "x = np.linspace(0.0, 1.0, nx)\n",
    "y = np.linspace(0.0, 1.0, ny)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], Y.flatten()[:, None]))\n",
    "\n",
    "# Predictions\n",
    "psi_pred, p_pred = mode.predict_psi_p(X_star)\n",
    "u_pred, v_pred = mode.predict_uv(X_star)\n",
    "\n",
    "psi_star = griddata(X_star, psi_pred.flatten(), (X, Y), method='cubic')\n",
    "p_star = griddata(X_star, p_pred.flatten(), (X, Y), method='cubic')\n",
    "u_star = griddata(X_star, u_pred.flatten(), (X, Y), method='cubic')\n",
    "v_star = griddata(X_star, v_pred.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "velocity = np.sqrt(u_pred**2 + v_pred**2)\n",
    "velocity_star = griddata(X_star, velocity.flatten(), (X, Y), method='cubic')\n",
    "\n",
    "# Reference\n",
    "u_ref= np.genfromtxt(\"reference_u.csv\", delimiter=',')\n",
    "v_ref= np.genfromtxt(\"reference_v.csv\", delimiter=',')\n",
    "velocity_ref = np.sqrt(u_ref**2 + v_ref**2)\n",
    "\n",
    "# Relative error\n",
    "error = np.linalg.norm(u_star - u_pred.T, 2) / np.linalg.norm(u_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "error = np.linalg.norm(v_star - v_pred.T, 2) / np.linalg.norm(v_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "error = np.linalg.norm(p_pred - p_star.T, 2) / np.linalg.norm(p_star, 2)\n",
    "print('l2 error: {:.2e}'.format(error))\n",
    "\n",
    "### Plot ###\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference solution & Prediceted solution\n",
    "fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "fig_1.add_subplot(1, 3, 1)\n",
    "plt.pcolor(X.T, Y.T, velocity_ref, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Reference Velocity')\n",
    "\n",
    "fig_1.add_subplot(1, 3, 2)\n",
    "plt.pcolor(x, Y, velocity_star, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predicted Velocity')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_1.add_subplot(1, 3, 3)\n",
    "plt.pcolor(X, Y, np.abs(velocity_star - velocity_ref.T), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Absolute Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ## Loss ##\n",
    "loss_res = mode.loss_res_log\n",
    "loss_bcs = mode.loss_bcs_log\n",
    "\n",
    "fig_2 = plt.figure(2)\n",
    "ax = fig_2.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_res, label='$\\mathcal{L}_{r}$')\n",
    "ax.plot(loss_bcs, label='$\\mathcal{L}_{u_b}$')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Adaptive Constant\n",
    "adaptive_constant = mode.adpative_constant_bcs_log\n",
    "    \n",
    "fig_3 = plt.figure(3)\n",
    "ax = fig_3.add_subplot(1, 1, 1)\n",
    "ax.plot(adaptive_constant, label='$\\lambda_{u_b}$')\n",
    "ax.set_xlabel('iterations')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradients #\n",
    "data_gradients_res = mode.dict_gradients_res_layers\n",
    "data_gradients_bcs = mode.dict_gradients_bcs_layers\n",
    "\n",
    "num_hidden_layers = len(layers) -1\n",
    "cnt = 1\n",
    "fig_4 = plt.figure(4, figsize=(13, 4))\n",
    "for j in range(num_hidden_layers):\n",
    "    ax = plt.subplot(1, 4, cnt)\n",
    "    ax.set_title('Layer {}'.format(j + 1))\n",
    "    ax.set_yscale('symlog')\n",
    "    gradients_res = data_gradients_res['layer_' + str(j + 1)][-1]\n",
    "    gradients_bcs = data_gradients_bcs['layer_' + str(j + 1)][-1]\n",
    "    \n",
    "    sns.distplot(gradients_res, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_r$')\n",
    "    sns.distplot(gradients_bcs, hist=False,\n",
    "                    kde_kws={\"shade\": False},\n",
    "                    norm_hist=True, label=r'$\\nabla_\\theta \\mathcal{L}_{u_b}$')\n",
    "\n",
    "    # ax.get_legend().remove()\n",
    "    ax.set_xlim([-1.0, 1.0])\n",
    "    ax.set_ylim([0, 100])\n",
    "    cnt += 1\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "fig_4.legend(handles, labels, loc=\"upper left\", bbox_to_anchor=(0.35, -0.01),\n",
    "            borderaxespad=0, bbox_transform=fig_4.transFigure, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twoPhase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
